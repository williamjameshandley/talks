\section{Introduction}

Type Ia supernovae (SNe Ia) are powerful cosmological probes. Their utility stems from their nature as standardisable candles, allowing for precise distance measurements across vast cosmic scales \citep{Huang2019, Jha2019}. Seminal observations of high redshift SNe Ia provided the first direct evidence for an accelerating expansion of the Universe, a discovery that reshaped modern cosmology and implied the existence of dark energy \citep{Riess1998, Perlmutter1999}.

The contemporary era of supernova cosmology has moved beyond discovery to focus on precision measurements of cosmological parameters \citep{Gall2024, Risaliti2018, Liang2008}. This pursuit requires the compilation and analysis of large, heterogeneous datasets, combining observations from numerous ground based and space based telescopes \citep[e.g.,][]{Scolnic2022}. The inherent diversity in instrumentation, observational conditions, and data reduction pipelines introduces complex systematic uncertainties \citep{Goobar2008, Menard2009, Mandel2016}. The management of these systematic effects has become a central challenge, addressed by a growing suite of sophisticated hierarchical Bayesian models. These frameworks are designed to model the population distributions of supernova and host galaxy properties, allowing for the principled propagation of uncertainties. Applications include disentangling intrinsic colour magnitude relations from dust reddening \citep{Mandel2011, Mandel2014, Mandel2016_dust, Mandel2017}, handling photometric classification and selection biases through frameworks such as BEAMS and BAHAMAS \citep{Hlozek2012, Shariff2015}, and incorporating photometric redshift uncertainties \citep{Roberts2017}. Indeed, ongoing investigations are exploring the impact of cross calibration between different surveys and the choice of light curve modelling frameworks on these results \citep{arXiv:2506.05471, arXiv:2410.13747}. Consequently, the processing of these combined datasets is an intricate and often time consuming task, frequently reliant on significant manual inspection and quality control to ensure the integrity of the final sample \citep{Chotard2011, Hauret2018, Gonzalez2020}.

The significance of addressing these systematic effects has been brought into sharp focus by the recent five year results from the Dark Energy Survey (DESY5). This dataset, one of the largest and most uniform high redshift SNe Ia samples to date, provides powerful new constraints on cosmological parameters. Analysis of the DESY5 sample has revealed intriguing tensions with the standard Lambda Cold Dark Matter ($\Lambda$CDM) model, with some studies interpreting these findings as evidence favouring a dynamic form of dark energy \citep{arXiv:2406.06389, arXiv:2405.03983}. The interpretation of these cosmological implications continues to be debated, emphasising the importance of meticulous scrutiny of the underlying data analysis \citep{arXiv:2503.17342}. The DESY5 findings therefore serve as a powerful contemporary example, underscoring that as statistical power increases, the reliability of cosmological inference becomes increasingly dependent on robust, objective, and reproducible data processing methods.

A particularly challenging step in this process is the selection of photometric data for inclusion in the light curve fit. For a given supernova, analysts often make decisions to accept or reject entire photometric bandpasses based on visual inspection or goodness of fit metrics. This procedure necessarily involves subjective judgements and may contribute to differences between analyses. One consideration with this approach is that an entire filter containing many valid data points may be rejected due to a few outliers or a short period of poor observing conditions, potentially discarding useful information. This problem of data curation is set to become intractable with the advent of next generation surveys. The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) is expected to discover millions of SNe Ia, rendering any form of manual vetting or subjective filter selection entirely infeasible \citep{Ivezic2019, LSST_DESC2012, Mandelbaum2019}. The anticipated scale of LSST's supernova sample will require automated methods for identifying and mitigating systematic effects \citep{Arendse2023, Wojtak2019, Hlozek2019, Foley2018}.

These escalating challenges in data volume and complexity necessitate a fundamental shift away from manual interventions towards fully automated, statistically robust, and reproducible methods for data processing. Hierarchical Bayesian models, such as BayeSN \citep{Mandel2020, Thorp2021, Dhawan2022}, have demonstrated the potential for improved precision in SNe Ia distance measurements by simultaneously modelling spectral energy distributions and dust extinction. Beyond simply identifying contaminated data, understanding how anomalous measurements systematically bias physical parameters is crucial for precision cosmology. The colour parameter in particular, which captures both intrinsic supernova variation and dust extinction, directly impacts distance moduli through the Tripp relation and can introduce systematic errors if contamination effects are not properly quantified. It would be beneficial to have a unified framework that can simultaneously fit the physical supernova model, account for potential data contamination in a principled manner, and quantify the systematic biases introduced by anomalous data. In this paper, we present such an approach: a Bayesian anomaly detection framework designed for the fully automated processing of SNe Ia light curves with integrated contamination quantification.

This Bayesian anomaly detection framework, adapted from techniques developed for 21cm cosmology~\citep{Leeney2022, Anstey2023, Roque2023, de2022reach}, directly integrates anomaly detection into the likelihood function of the light curve fitting process. Our implementation leverages GPU acceleration to achieve the computational efficiency necessary for processing the vast data volumes anticipated from next generation surveys. With LSST expected to discover 3 to 4 million supernovae over its 10 year survey and generate over 10 million transient alerts nightly \citep{ivezic2019lsst}, traditional CPU based processing pipelines will struggle to keep pace with this data avalanche. Our GPU accelerated framework demonstrates that modern parallel computing architectures can enable the real time, automated analysis required for the LSST era.

The anomaly detection component is agnostic to the underlying physical model, allowing it to be coupled with any light curve fitter such as BayeSN or SNooPy; for this work, we integrate it with the widely used SALT3 model \citep{Guy2005orig, Guy2007, Kenworthy2021}. This approach extends traditional data curation methods in three important ways. Firstly, it robustly mitigates the impact of isolated, anomalous data points that can otherwise bias parameter estimates. Secondly, it automates the process of filter selection, providing a probabilistic assessment of whether a given bandpass is reliable without human intervention. Thirdly, and most significantly, our framework facilitates enhanced data preservation. By identifying and modelling only the specific anomalous epochs within a given filter, it retains the surrounding valid data points that would otherwise be lost when comprehensive filter level rejection is necessary, as demonstrated in our application to the carefully processed Hawaii Supernova Flows (HSF) \citep{do2025hawaii} dataset. This ensures that the maximum amount of information is extracted from the data, leading to more precise and robust cosmological constraints than is possible with all-or-nothing filter rejection schemes.

The remainder of this paper is structured as follows. In Section~\ref{sec:theory}, we present the theoretical foundation of our Bayesian anomaly detection framework, including the mathematical formulation of the model and its integration with the SALT light curve template. Section~\ref{sec:methods} describes the practical implementation details, including the computational approach using GPU accelerated model evaluation and the application to the Hawaii Supernova Flows dataset. In Section~\ref{sec:results}, we demonstrate the framework's performance through both a comprehensive statistical comparison across the full dataset and detailed case studies of individual supernovae that illustrate the three key benefits of the method. Finally, Section~\ref{sec:conclusion} summarises our findings and discusses the broader implications for precision cosmology with future large scale surveys.