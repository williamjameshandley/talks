## Raw transcript

07:06:32 RRI: Okay. Overdue. Yeah. Okay, many. Thanks. 15 min is not quite enough time to give you the detail on the the cool idea of the subject. So I'm going to spend 5 min actually really working on. So that a few people are on the same page, so that when you start to hear material about simulation based inference, you have some framework in which to think about that, and only the final sort of 5 min to talk. We'll be talking about how you do, Sbi, with nested sampling, which is work I've been doing with Killian Shirt. Winkle, who's sadly now left us to go work for some nonsense and blockchain and Crystal fed, who works very well in.
07:07:15 RRI: I mean. Can you hear me right. Yes, I would talk about. New erasure estimation and nested sampling, and how this fits in with some of the modern. 20 or centre ideas. Okay. So 1st of all, I will talk about what I now call Lbi, which is likely a based. Nope.
07:08:16 RRI: Can people hear me. Now, okay. Alright. So what most people do is terms. Likelihood based inference, which is, you start by writing down a likelihood function, which is the probability of the data given someone's model. This is what you're taught to do at undergraduate levels. Example, given a biased coin. What's the chance that you observe? 30 out of 100 heads, or given a flat lambda cosmology? What's the chance that you'd see this pattern of galaxies in the night sky. Then, if you're Bayesian, you define a prior, and if you have enough time, you spend some time being philosophical. You then sample your posterior, using out of the box tools, such as Mc or Mult. And then you, optionally, might confuse some evidences and do some tension.
07:09:05 RRI: Who here has made a triangle plot in their career. It's quite a generational divide there, I think. So this thing here, which is a triangle on the left, shows you on the right here. Shows you. The posterior probability regions the Dark Energy Survey which came out in January of this year, showing, if you want to know how much dark energy or dark matter in the universe, where we think that poster is, and as you combine multiple probes, you grudge to get better and better in. For instance, we heard from Andre earlier, as you combine Lyman Alpha with global twentil, said to me that you can get a lot more.
07:09:41 RRI: Machinery, however. Most people cannot do this. And, in fact, if I'm being really hardcore only really, Cmb people understand their likelihood with all the systematics and bells and whistles. Everybody else has. Cmb. Nv. And wants to kind of do what Cmb. Cosmologists do. But can't quite get there. Of course a radiometric is a likelihood that's very well defined, but there's a lot of other stuff that happens under the hood that slips in there with analytic marginalizations or or systematics, where you could have to assume a certain level of chromatin. That is just not true. So what I like to work on is, what can you do if you don't have a likelihood.
07:10:17 RRI: So this used to be called likelihood free inference. So if you ever heard of Lfi you, you should now transplant to Sbi we change the name because likelihood free sounds like stake oil that you know it's not that there's no likelihood. It's just that you don't know what it is. Okay? So. The idea here is that let's imagine you are, say, a 21 cosmologist with a very sophisticated forward, semi numerical pipeline. Or you are a f 1 car designer in Mclaren or a nuclear fusion reactor where you have. All these very clever, sophisticated simulations, where you can start from initial conditions and parametric models and move to. Example, datasets that might be seen if that model were.
07:11:02 RRI: This, of course, is the usual case, because this is how scientists really think about problems. They think forwards, not backwards. And likelihoods are a big ask. So the key core idea in simulation based inference is that if you have a forward model. That generative process by which you generate data is a likelihood. Okay, it's just not one for which you know the density. But you do know that there is a likelihood that process. And nowadays what you try to do is to take. Simulations, by pairs. Here's my parameters. And here's my generated data that those parameters could have created in different universes. And then Machine learned the answer from those so.
07:11:42 RRI: The state of the art is currently achieved using machine learning. But I do have a rather rich program at the moment going on where we try to remove machine learning from that. So do ask me. About that afterwards. Here's a nice diagram that took me far too long to produce. On the x-axis, we have theta. So this is typically 6 dimensional. And on the Y-axis we have data which is hundreds to thousands of dimensional data might be a power spectrum. It might be this, it might be a compressed set of summary statistics, but fundamentally you have a Theta plane.
07:12:12 RRI: Which in reality is quite a high dimensional thing, and for each theta value you can generate a data point randomly with stock. Over time you would build up a set of data points from simulations that create this. Cloud of theta pairs. And then the challenge of simulation based is to take those. Theta that Theta data cloud and learn. The structure that underlies it. Not writing down a likelihood, but using some. Adaptive tool that could find that out, and that at the moment is achieving. From that. Once you've got that, if you then look at what is the data that I actually received? Because, of course, this data on the y axis is.
07:12:51 RRI: A whole lot of things that you could have observed. But if we did observe that then the prior and the posterior come out here, so prior in blue. And then, if the data came out slightly lower down, you might get a broader. And if you're really lucky and the data came out here, you might even get a multimodal posterior. But this feature viewpoint of of inference is, if you like, an extension to Bayesian inference, where you you extend your view, not just from thinking about things in terms of theta, but also thinking in terms of data, which means you can start to use some more frequentist tools.
07:13:23 RRI: Although no frequentists would believe any of this because of that X-axis, because we put probably distributions on Theta. Alright. So here's my summary as to why we should all be interested. So 1st of all, this is the usual case. Very few people actually have likelihoods. They have a very rough approximation. They slapped down and feed into polycord or into Mc, and hope that it will do right.
07:13:49 RRI: So I would say at the very least, that's why you should be. Aware of this, a lot of you will claim that it's faster than Lbi. I think this is slightly dubious, because in practice it's only faster because you're using machine learning tools under the hood. That means that the emulators that we see get these enormous speed ups in our Lbi are there by default in Sbi. So Lbi can be as fast. Sbi. If you use Mba emulators.
07:14:14 RRI: And this is the main thing here. You don't need to pragmatically cosmologies or fiducial assumptions about the beam or fiducial assumptions about about any other kind of bit of your experiment in principle. It's all carried through when you write it down as a forwards, stochastic, generative model. And this is therefore very useful. If you're writing grants, it's very good because it equates AI and machine learning with Bayesian interpretability. I what other people call uncertainty quantification. If you want to apply the usual Bayesian stuff, 2 things but doing AI. This is the way to do it, because how you inject. Phasing posteriors into that. But the main reason why everybody should be aware of this is that this is a much lower barrier to entry than likely based approaches. Okay.
07:14:57 RRI: If you are a new student and you have. The nightmare that is installing polycord, or the thing where you take a pip and package that just takes a data, pairs, trains it, and gives you posterior. That is easier. Okay? And for that reason alone, whether or not it's the right thing to do. Everybody is going to be doing this in 10 years time. Okay, so my job is to work out how we make sure that we are doing the right thing. Given that everybody is, gonna be completely entranced by that lower barrier entry.
07:15:27 RRI: Okay, we are already using this in cosmology. 24 is really the year this came of age. You see it in a lot of dark energy survey in a lot of weak, lensing things, such as the dark energy survey. You see it in kids. And now Hsc. Also had that. So it's mostly in those. The only reason you're not seeing it in old historical things, like the Cmb is because, it's a lot of work to upgrade your existing pipelines.
07:15:48 RRI: To be generative, I to be able to generate data. But it's not impossible. And I am working on this with students. In the context of 21 cosmology. I went to this very, very good list of Uhbi references and search for 21. These are the 6 papers that came out of this. So if you're not on this list, go put yourself on this very good community repository. That's just a full request away but you can see here Ben Bundo is obviously a big name in in Cmb. Anshel appears twice on this list and I think Andre is also one of the cool kids on this list. So there's lots of there's lots of people that have been doing Sbi, and more and more people will be doing it.
07:16:29 RRI: Alright, cool. So this won't be as detailed as it could be. So do ask me if you are more interested in this the broad strokes. Idea that you might want. Is that there are various of Sbi at the moment. They all begin with N, where N stands for neuron. In principle, you could take that neural away and replace it with whatever neural networks become in 10 years time, 4 years ago everyone was talking about Gans. I don't know. If you remember adversar on your networks. No one talks about gans now. Everyone talks about diffusion models and transformers and things like that, and in 4 years time diffusion models will be old. So this N. Whatever we do, won't necessarily remain the case. But the underlying core ideas do match.
07:17:08 RRI: For example, you can basically either try to estimate the posterior, estimate the likelihood, estimate the joint, or estimate the ratio. Here. Is, if I have an extra 10 min I would go through quite carefully through these ideas. But for now there are basically 2 camps. There are these 3, which are all forms of dense. And there's this one, which is a form of ratio estimation. So I like nre. Because I think the fundamental thing that underlies Nra is that your ability to inference is linked to your ability to classify things. So if you want to know. If things have a connection or not. If there's a connection between parameters and data. That problem is just a classification problem in the language of neuro ratio estimation.
07:17:55 RRI: And I also like it, because, fundamentally, for the past 50 years and statistics, we've avoided desperately doing dense estimation because it's hard. It doesn't scale, and you can't do dense estimation, high dimensions. The fact that modern neural networks can kind of do it in 14 does not get round this fundamental fact. And if I want to be able to do. And if I want to be able to do Sbi in hundreds or thousands or millions of dimensions, for example, on images, I don't think we're ever going to get there with dense. So I'm working on. Brace your estimation, which divides out the hard, calculate bits. Now we'll talk here in the final 5 ish minutes of how we try to solve this using modern sampling. So modern neuro ratio estimation tools, for example, swift.
07:18:37 RRI: Which you may all use, and when you, swift here. Think somebody at least is used. Yeah, cool. these require 2 tricks, namely, marginalization. And truncation, or autogression. And I also don't like that. It separates out model comparison and parameters to mission. I still haven't really cracked this yet. Okay. This is a picture that comes from the 1st swift paper. For those that don't know. Swift is an acronym for stop wasting your precious time.
07:19:05 RRI: And This is Christoph. I don't think you get away with this nowadays. But basically, in order to get this right you need to do 2 tricks, you 1st of all. Marginalized by only considering one or 2 parameters a time. This is a big shortcoming. Ideally, we would do inference in the full parameter space that's relevant, which is typically 5 or 6 dimensional, but many cosmological problems. You've got a whole suite. Of nuisance parameters associated with beam. And with sky, and with anything on the radiometer. And all this other kind of stuff there. By the cool cosmology parameters that go into a 21 cent or 21 centimetre space code are usually of order, 5 or 6 dimensional, or maybe 7 or 8, depending on how complicated it is.
07:19:49 RRI: So this is a problem, and the other is truncation that in order to work at all, you, although you ideally, would sample from the whole prior you practice need to focus your simulations around some important bit of pram space where the importance of the param space is driven by the data that you've observed on the sky.
07:20:07 RRI: So those of you that been to a sampling talk before may have also seen this kind of a plot. Which does that. This is the idea of how the polycord and other things work under the hood. You have a set of samples drawn from the prior that steadily contract around the peaks of a distribution. These are called live points, and at the end you're left with a set of dead points that look something like That. Okay, where you could have had these points that zoom in around the peak.
07:20:33 RRI: And for those of you that are aware at this point, although we're only 12 min in these 2 things look a bit similar. Right. You've got this truncation picture of how swift works with these box like region zooming in on bits of parameter space. And you've got this set of more adaptive contours that comes from nest sampling. So the idea is, can we combine those 2 and get sampling to do this kind of thing? So just to outline why, it's hard to do. Why, you can't just drop Sbi into Polycord.
07:21:06 RRI: At the core of nest sampling is this, draw me a new sample from the prior subject to a hard likelihood. This is hard if you don't have a likelihood. Okay, if you don't know what this L is, you don't know what this constraint number. Is but worse than that, usually most methods will give you an approximations that likelihoods, but that approximation will be. Stochastic because of the generative process that underpins it. And again, this breaks. This is something in a nasty way.
07:21:30 RRI: Previous tends to do this have been in the Denis paper, which is an absolute cracker from 2,016, where there's loads of stuff in there that's just hidden in like section 10 and in a a and nre which is Crystal's latest unproductable set of 4 letters to to do this. Which is another way of attempting to solve these kind of problems. The cool idea behind sequential. The sequential Nra with Nest. You start as you do with this sampling sampling from the prior. You train swift. You then, what you train Swift, which is these 2 steps of simulating and then training a new erasure estimator. And then the core bit here is that instead of imposing some weird box, you say, let's sampling. Tell you where to truncate. Okay.
07:22:14 RRI: And then you terminate. If there's some criterion, and if not, you then go around that loop where, instead of zooming in boxes. You zoom in in contours that are driven by nested sampling. This is 18 months worth of work from Killian, very solid. Hard to do problem. So solving lots of very difficult problems. So this is not something you should attempt. if you're fainted. but what we do here is we combine polycord. But again, this general principle, you could put any sample in here and any ratio estimator in here, and it would work. So we have poly swift that you could replace polycord in principle with multi nest or with dynasty or Sai, and you could replace Swift with any of the Spi other packages that might exist.
07:23:00 RRI: and in terms of results, we, because obviously, there's a little bit of bias. From my standpoint, we do this in a cosmological problem where we have 5 dimensional parameter space. It, could it? In principle it might be 6. I think we have updated this plot since then, and ideally, this plot would have labels that aren't just Theta one, these 2, Theta 4. But these, if you look carefully. Do correspond to a cosmological like problems. And we've also upgraded this analysis here to be for a full. Yeah, this is an old slides annoying for for a Cmb like problem. So we are, we are now able to solve Cmb, like problems that have.
07:23:39 RRI: 6 parameters and hundreds of data points, which is therefore in the cosmological. And this is not something that Swift can do natively. And you don't need to put in any extra marginalization. Okay, I'm pretty much out of time. So this is the main punchline. Poly swift is a way of doing swift on 6 dimensional spaces without. Trying without marginalization, and this is therefore suitable for many cosmodians. If you want to hear more about this, ask Killian for either his thesis or the latest draft, and this is done. Using my lsb, I package, which is quite a. A psychic learn s way of producing plots that look pretty. Alright, many thanks. Thanks for the doc. It was. But it was delightful and.
07:24:30 RRI: Yeah, are there any questions in the audience? Online and offline. Like, was. Very cool stuff. I I just had a quick question. Maybe you rushed over this. But it wasn't clear to me the benefits of. Combining the multiness like non-box car thing. Do you get like a better posterior, or is it. So, or do you actually get? Are you saying you actually get the joint distribution? You actually get a whole joint. Distribution. Because, of course, anybody that spent a decade in this field views the swift way of projecting into 2D marshes as absolute nonsense. So, okay, so and that comes with some computational cost I present, it's impossible to do with Swift, because these boxes aren't good enough in 5. But in yours, but it is up to 60 or so.
07:25:26 RRI: We then hit another scaling problem. That is a bit more subtle. Which is which I'm now currently trying to solve. The fact that. In 6 dimensions you have to cross. The typical set, and that typical sets to get larger than what a normal ratio estimator can train. So there is a limit on how you scale because you had 5, I guess parameters and your fiducial ones. If I scale that to 20, do you, at the moment. This wouldn't work on that, but I am working on it. Okay, thank you.
07:25:58 RRI: Hello! Can you comment about the computational time or cost like How much like you are getting benefit. In terms of the like. This the I mean. This the original nested sampling. If you're. Like replacing this, your Svi, or Lbi. So there's a fair number of different parts. That question. There's there's a question that might be specific to Swift, and there might be a question specific to Sbi in general. Okay, Sbi. In general. You. The game we play is, you assume the simulations of the expense a bit. Okay? So we assume that gathering simulations is the bit that takes a lot of time and effort, and everything else is effectively free, so that the name of the game is not getting it faster is getting it possible. For a given number of simulations.
07:26:49 RRI: In terms of swift versus versus poly swift. It's not faster. But the number of rounds and truncations ends up being similar. It's just that our truncations are better, and therefore able to do 6 dimensions jointly. But not Foster just at all. Okay? And did you try, like other dimensions like more than 6 or like. Lower. How the performance change, if you're like doing with the 5 dimension or 8 dimension. So. So again there are a whole host of things. Why. Things.
07:27:27 RRI: What kind of an answer are you looking for here? So I mean, I mean, how I mean, how was difficulty? I mean, like, if you're adding the dimension. So I mean, how difficult the problem. 2 8. So the scaling, the dimensionality at the moment. It scales very nicely up until about 6, and then can't go any higher, and that's me being honest about the current state of Polysw, and if we had more than 5 min I would give you a diagram show you why, I think that is there at the moment, and how we're trying to address that so the scaling.
07:28:01 RRI: It's only got 6 points on the X-axis so far because it doesn't go higher than that. But the point, the kind of Takea message, though, really is that Swift on its own, only goes to. So swift on its own, can only do 2D. And therefore, if it wants a 60 problem, it does it by breaking it into 15 2 dimensional problems. This here can do 60 problems. So if you want to do a 20 problem, you might be able to break that into some smaller number of 6 dimensional problems.
07:28:36 RRI: Hello! Nice talk so I might have missed it So have you explored the evidence estimation Meaning, the selecting different models. It's a it's a great question so the so if I if I give you the pithy answer to this ratio estimation. Divides out the evidence. So there is no evidence here. Obviously as somebody that's relatively, ideologically tied to the evidence. I am trying to find ways of getting Sbi to do it that do scale. The current state of the art is achieved using Ben out. And now, Jeffrey, Evidence networks. So if you Google evidence, you'll get that. And indeed, Thomas Guessy Jones, who's gonna feature in a few different floor talks here, I think, did this for what we call fully Bayesian forecasting, which is a way of getting evidences out of these things. All rather getting base ratios out of these things. You can't get evidences out of this because you've divided out it out fundamentally.
07:29:38 RRI: Are you like planning to do? Yes, kind of. Okay. I don't know more questions. Okay, then let's thank the speaker once again. And now we'll move forward to the lightning talks for the. Poster sessions, so.
