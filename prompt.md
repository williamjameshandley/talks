I'm writing a talk for a workshop. 


Here is the email with the details sent to the speakers:

```mail
Dear all,

We are ~1month away from the workshop in Valencia, and we are trying to design the schedule
according to the constraints you have provided.
Each of you will have 1 hour slot, split in 45’ talk + 15’ questions, and there will be 3 of
these slots every morning (the afternoons, instead, will be fully dedicated to organised
discussions)
Here’s a tentative program of your talks, with suggested broad topic.

Monday: Pieroni (reconstruction techniques), Zaldivar (reconstruction techniques), Kernesis
(reconstruction techniques or astro signal)
Tuesday: Handley (statistical techniques), Korol (astro signal), Baghi (TBC, reconstruction
techniques)
Wednesday: Babak (LISA noise/long-vsbackground signals), TBA (inflation analytics), Lizarraga
(inflation lattice)
Thursday: Contaldi (noise/techniques/other), Simakarchorn (Cosmic Strings), Hindmarsh (PhT’s)
Friday: Caprini (PhT’s), Mitridate (PTA’s), Domcke (PTA anistropies/other?)

Please write to us in private and let us know whether the talk schedule and proposed topic
suites you. If possible send us a tentative title.
In case you prefer another topic, kindly let us know your suggestion and hopefully we will be
able to accommodate it in the program.
Please, keep an eye in our website https://indico.ific.uv.es/event/7656/
<https://indico.ific.uv.es/event/7656/> for updated info.

Looking forward to seeing you all in Valencia!

Best wishes,
Dani & Bryan

Date: Fri, 8 Nov 2024 16:02:43 +0100
From: "Daniel G. Figueroa" <daniel.figueroa@ific.uv.es>
To: Bryan Zaldivar <bryan.zaldivar@ific.uv.es>
Cc: "Daniel G. Figueroa" <daniel.figueroa@ific.uv.es>
Subject: Tentative schedule - workshop "Towards realistic detection forecasts of primordial
	GWBs”, Dec 9-13, 2024, IFIC, Valencia
X-Mailer: Apple Mail (2.3608.120.23.2.7)

Dear all,

We are ~1month away from the workshop in Valencia, and we are trying to design the schedule according to the constraints you have provided.
Each of you will have 1 hour slot, split in 45’ talk + 15’ questions, and there will be 3 of these slots every morning (the afternoons, instead, will be fully dedicated to organised discussions)
Here’s a tentative program of your talks, with suggested broad topic.

Monday: Pieroni (reconstruction techniques), Zaldivar (reconstruction techniques), Kernesis (reconstruction techniques or astro signal)
Tuesday: Handley (statistical techniques), Korol (astro signal), Baghi (TBC, reconstruction techniques)
Wednesday: Babak (LISA noise/long-vsbackground signals), TBA (inflation analytics), Lizarraga (inflation lattice)
Thursday: Contaldi (noise/techniques/other), Simakarchorn (Cosmic Strings), Hindmarsh (PhT’s)
Friday: Caprini (PhT’s), Mitridate (PTA’s), Domcke (PTA anistropies/other?)

Please write to us in private and let us know whether the talk schedule and proposed topic suites you. If possible send us a tentative title.
In case you prefer another topic, kindly let us know your suggestion and hopefully we will be able to accommodate it in the program.
Please, keep an eye in our website https://indico.ific.uv.es/event/7656/ <https://indico.ific.uv.es/event/7656/> for updated info.

Looking forward to seeing you all in Valencia!

Best wishes,
Dani & Bryan


*****************************
Daniel G. Figueroa
IFIC (CSIC/UV), Valencia
*****************************
```

Here is material from the indico website https://indico.ific.uv.es/event/7656/
```markdown
# Overview
Summary: The possibility to detect a gravitational wave background (GWB) of primordial origin is a holy grail of gravitational wave (GW) astronomy. Any such fundamental background will be very likely buried under a plethora of astrophysical signals, and hence its detection represents a major challenge for upcoming PTA observations and next generation of GW experiments, expected to become operative by the mid 2030’s. In light of the growing interest in GWB source modelling and signal reconstruction, the present workshop has a twofold focus:

i) discussing the latest advances in the theoretical modelling of GWBs

ii) discussing statistical techniques for realistic background reconstruction on top of astrophysical signals, within the spirit of a global fit. 

This meeting will bring together astrophysicists, cosmologists, particle physicists and statisticians, with a varied expertise from theoretical physics to data analysis, to discuss open questions on the modelling and detection of GWBs.


```

I've been asked to give the 'statistical inference methods' talk.

Here is the latex of the talks I have given in the past:
# branch: remotes/origin/garching_2015
```tex
\documentclass[]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}



\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\mathrm{knots}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}





\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{PolyChord: Next Generation Nested Sampling}
\subtitle{Sampling, Parameter Estimation and Bayesian Model Comparison}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Supervisors: Anthony Lasenby \& Mike Hobson \\
Astrophysics Department \\
Cavendish Laboratory\\
University of Cambridge
}
\date{December 11, 2015}



\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \tableofcontents
\end{frame}



\section{Parameter estimation \& model comparison}
\begin{frame}
  \frametitle{Notation} 

  \begin{itemize}
      \pause
    \item Data: $D$
      \pause
    \item Model: $M$
      \pause
    \item Parameters: $\Theta$
      \pause
    \item Likelihood: $\prob(D|\Theta,M)=\lik(\Theta)$
      \pause
    \item Posterior: $\prob(\Theta|D,M)=\posterior(\Theta)$
      \pause
    \item Prior: $\prob(\Theta|M)=\prior(\Theta)$
      \pause
    \item Evidence: $\prob(D|M)=\ev$
  \end{itemize}


\end{frame}

\begin{frame}
  \frametitle{Bayes' theorem}
  \framesubtitle{Parameter estimation}

  \pause
  What does the data tell us about the params $\Theta$ of our model $M$?

  \pause
  \begin{description}
  \item[Objective:] Update our prior information $\prior(\Theta)$ in light of data $D$.
  \end{description}

  \pause
    \[\prior(\Theta) = \prob(\Theta|M) \xrightarrow{D} \prob(\Theta|D,M) = \posterior(\Theta)\] 

    \pause
  \begin{description}
    \item[Solution:] Use the likelihood $\lik$ via Bayes' theorem:
  \end{description}

  \pause
    \[\prob(\Theta|D,M) = \frac{\prob(D|\Theta,M) \prob(\Theta|M) }{ \prob(D|M) }\] 
    \pause
    \[\text{Posterior } = \frac{\text{Likelihood} \times \text{Prior} } {\text{Evidence} }\] 

\end{frame}

\begin{frame}
  \frametitle{Bayes' theorem}
  \framesubtitle{Model comparison}
  \pause
  What does the data tell us about our model $M_i$ in relation to other models $\{M_1,M_2,\cdots\}$?
  \pause
    \[\prob(M_i) \xrightarrow{D} \prob(M_i|D)\] 

    \pause
    \[\prob(M_i|D) = \frac{\prob(D|M_i) \prob(M_i) }{ \prob(D) }\] 

    \pause
    \[\prob(D|M_i) = \ev_i = \text{Evidence of $M_i$} \]
\end{frame}

\begin{frame}
  \frametitle{Parameter estimation \& model comparison} 
  \framesubtitle{The challenge}

  \begin{description}
      \pause
    \item[Parameter estimation:] what does the data tell us about a model? (Computing posteriors)
      \pause
    \item[Model comparison:] what does the data tell us about all models? (Computing evidences)
  \end{description}

  \pause
  Both of these are challenging things to compute.
  \begin{itemize}
      \pause
    \item Markov-Chain Monte-Carlo (MCMC) can solve the first of these (kind of)
      \pause
    \item Nested sampling (NS) promises to solve both simultaneously.
  \end{itemize}
 
\end{frame}

\begin{frame}
  \frametitle{Parameter estimation \& model comparison} 
  \framesubtitle{Why is it difficult?} 

	\begin{columns}
	\begin{column}{0.5\textwidth}
		\begin{enumerate}
          \item<2-> In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item<3-> Worse, you don't know where this region is.
		\end{enumerate}
	\end{column}
	\begin{column}{0.5\textwidth}
		\includegraphics<1->[width=\textwidth]{figures/posteriors}	
	\end{column}
	\end{columns}
    \begin{itemize}
      \item<4-> Describing an $N$-dimensional posterior fully is impossible.
      \item<5-> Project/marginalise into $2$- or $3$-dimensions at best
      \item<6-> {\em Sampling} the posterior is an excellent compression scheme.
    \end{itemize}
 
\end{frame}

\section{Metropolis Hastings}

\begin{frame}
  \frametitle{Markov-Chain Monte-Carlo (MCMC)} 
  \framesubtitle{Metropolis-Hastings, Gibbs, Hamiltonian\ldots} 
  \begin{itemize}
      \pause
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \pause
    \item Explore the space via a biased random walk.
      \begin{enumerate}
          \pause
        \item Pick random direction
          \pause
        \item Choose step length
          \pause
        \item If uphill, make step\ldots
          \pause
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
  \end{itemize}
 
\end{frame}

\begin{frame}
  \frametitle{MCMC in action} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_0.pdf}}{movies/MCMC.mp4}
\end{frame}
\begin{frame}
  \frametitle{MCMC in action} 
  \includegraphics[width=\textwidth]{movies/MCMC_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Burn in} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_burn_in_0.pdf}}{movies/MCMC_burn_in.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Burn in} 
  \includegraphics[width=\textwidth]{movies/MCMC_burn_in_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Tuning the proposal distribution} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_proposal_0.pdf}}{movies/MCMC_proposal.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Tuning the proposal distribution} 
  \includegraphics[width=\textwidth]{movies/MCMC_proposal_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Multimodality} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_multimodal_0.pdf}}{movies/MCMC_multimodal.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Multimodality} 
  \includegraphics[width=\textwidth]{movies/MCMC_multimodal_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Phase transitions} 

  \includegraphics[width=\textwidth]{figures/phase_transition}
 
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{The real reason\ldots} 

  \begin{itemize}
    \item<2-> MCMC does not give you evidences!
  \end{itemize}

  \begin{align}
    \onslide<3->{%
    \ev 
    &= \prob(D|M) 
    \nonumber\\
  }
    \onslide<4->{%
    &= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
  }
    \onslide<5->{%
    &= \int\lik(\Theta)\prior(\Theta) d\Theta 
    \nonumber\\
  }
    \onslide<6->{%
    &= \left\langle \lik \right\rangle_\prior
    \nonumber
  }
  \end{align}
  
  \begin{itemize}
    \item<7-> MCMC fundamentally explores the posterior, and cannot average over the prior.
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}
\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to MCMC!} 

  \pause
  New procedure: 

  \pause
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      \pause
    \item[$S_0$:] Generate $n$ samples from the prior $\prior$. 
      \pause
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new sample with higher likelihood
  \end{description}

  \pause
  Requires one to be able to sample from the prior, subject to a {\em hard likelihood constraint}.

\end{frame}




\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}


\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{Why bother?} 
  
  \begin{itemize}
      \pause
    \item At each iteration, the likelihood contour will shrink in volume by a factor of $\approx 1/n$.
      \pause
    \item Nested sampling zooms in to the peak of the posterior {\em exponentially}.
      \pause
    \item Nested sampling can be used to get evidences!
  \end{itemize}

  
\end{frame}

\begin{frame}
  \frametitle{Calculating evidences}

  \begin{itemize}
    \item<2-> \onslide<3->{Transform to 1 dimensional integral} \onslide<4->{$\pi(\theta)d\theta = dX$}
      \onslide<2->{\[ 
        \ev = \int \lik(\theta)\pi(\theta)d\theta \onslide<5->{= \int \lik(X)dX}
      \] }
    \item<6-> $X$ is the {\em prior volume}
      \onslide<7->{%
        \[ X(\lik) = \int_{\lik(\theta)>\lik} \pi(\theta) d\theta \]
      }
    \item<8-> i.e.\ the fraction of the prior which the iso-likelihood contour $\lik$ encloses.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}
  \includegraphics[width=\textwidth,page=38]{figures/nested_sampling}
\end{frame}

\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}

\foreach \pagenum in {1,...,5} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling_sum}
}
\end{frame}

\begin{frame}
  \frametitle{Estimating evidences} 
  \framesubtitle{Evidence error} 

\foreach \pagenum in {1,...,9} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/areas}
}
 
\end{frame}

\begin{frame}
  \frametitle{Estimating evidences} 
  \framesubtitle{Evidence error} 


  \begin{itemize}
    \item<2-> approximate compression:
  \end{itemize}
  \[ 
    \onslide<2->{\Delta \log X \sim -\frac{1}{n}} 
    \onslide<3->{\pm \frac{1}{n}} 
  \]
  \[ 
    \onslide<4->{\log X_i \sim -\frac{i}{n}}
    \onslide<5->{\pm \frac{\sqrt{i}}{n}} 
  \]
  \onslide<6->{%
  \begin{itemize}
    \item<6-> \# of steps to get to $H$:
  \end{itemize}
  \[ i_H \sim n H \]
  }
  \onslide<7->{%
  \begin{itemize}
    \item estimate of volume at $H$:
  \end{itemize}
  \[ \log X_H \approx -H \pm \sqrt{\frac{H}{n}} \]
}

  \onslide<8->{%
  \begin{itemize}
    \item estimate of evidence error:
  \end{itemize}
  \[ \log\ev \approx \sum w_i \lik_i  \pm \sqrt{\frac{H}{n}} \]
}

\end{frame}


\begin{frame}
  \frametitle{Nested sampling} 
  \framesubtitle{Parameter estimation} 

  \begin{itemize}
      \pause
    \item NS can also be used to sample the posterior
      \pause
    \item The set of dead points are posterior samples with an appropriate weighting factor
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{When NS succeeds} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/NS_0.pdf}}{movies/NS.mp4}
\end{frame}
\begin{frame}
  \frametitle{When NS suceeds} 
  \includegraphics[width=\textwidth]{movies/NS_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When NS succeeds} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/NS_multimodal_0.pdf}}{movies/NS_multimodal.mp4}
\end{frame}
\begin{frame}
  \frametitle{When NS suceeds} 
  \includegraphics[width=\textwidth]{movies/NS_multimodal_1.pdf}
\end{frame}


\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  \pause
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling within an iso-likelihood contour}
  \framesubtitle{Previous attempts}


  \begin{description}
    \item<2->[Rejection Sampling] MultiNest; F.\ Feroz \& M.\ Hobson (2009).
      \begin{itemize}
        \item<3-> Suffers in high dimensions
      \end{itemize}
    \item<4->[Hamiltonian sampling] F.\ Feroz \& J.\ Skilling (2013).
      \begin{itemize}
        \item<5-> Requires gradients and tuning
      \end{itemize}
    \item<6->[Diffusion Nested Sampling] B.\ Brewer et al.\ (2009).
      \begin{itemize}
        \item<7-> Very promising
        \item<8-> Too many tuning parameters
      \end{itemize}
    %\item<8->[Slice Sampling] R.\ Neal (2000), Aitken \& Akman (2013).
  \end{description}

\end{frame}

\section{PolyChord}
\begin{frame}
  \frametitle{PolyChord}
  \framesubtitle{``Hit and run'' slice sampling}
\foreach \pagenum in {1,...,32} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/contour_step}
}
\end{frame}


\begin{frame}
  \frametitle{PolyChord}
  \framesubtitle{Key points}

  \begin{itemize}
    \item<2-> This procedure satisfies detailed balance.
    \item<3-> Works even if $\mathcal{L}_0$ contour is disjoint.
    \item<5-> Need $N$ reasonably large $\sim\mathcal{O}(n_\mathrm{dims})$ so that $x_N$ is de-correlated from $x_1$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Issues with Slice Sampling}

  \begin{enumerate}
    \item<2-> Does not deal well with correlated distributions.
    \item<3-> Need to ``tune'' $w$ parameter.
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{PolyChord's solutions}
  \framesubtitle{Correlated distributions}

  \includegraphics[width=\textwidth]{figures/contour_transform}

\end{frame}


\begin{frame}
  \frametitle{PolyChord's solutions}
  \framesubtitle{Correlated distributions}

  \begin{itemize}
    \item<2-> We make an affine transformation to remove degeneracies, and ``whiten'' the space.
    \item<3-> Samples remain uniformly sampled
    \item<4-> We use the covariance matrix of the live points and all inter-chain points 
    \item<5-> Cholesky decomposition is the required skew transformation
    \item<6-> $w=1$ in this transformed space
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{PolyChord's Additions}

  \begin{itemize}
    \item<2-> Parallelised up to number of live points with openMPI.
    \item<3-> Novel method for identifying and evolving modes separately.
    \item<4-> Implemented in CosmoMC, as ``CosmoChord'', with fast-slow parameters.
  \end{itemize}
\end{frame}






\section{Applications}



\begin{frame}
  \frametitle{PolyChord in action}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}


\begin{frame}
  \frametitle{Planck data}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
  \begin{itemize}
    \item<2-> Temperature data TT+lowP
    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
    \item<4-> Marginalised plots of $\PR(k)$
    \item<5->
      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
  \end{itemize}
\end{frame}



\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knots}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{8 internal knots}
  \frametitle<10>{Bayes Factors}
  \frametitle<11>{Marginalised plot}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.9\textwidth]{figures/0TT_fgivenx}
    \includegraphics<2>[width=0.9\textwidth]{figures/1TT_fgivenx}
    \includegraphics<3>[width=0.9\textwidth]{figures/2TT_fgivenx}
    \includegraphics<4>[width=0.9\textwidth]{figures/3TT_fgivenx}
    \includegraphics<5>[width=0.9\textwidth]{figures/4TT_fgivenx}
    \includegraphics<6>[width=0.9\textwidth]{figures/5TT_fgivenx}
    \includegraphics<7>[width=0.9\textwidth]{figures/6TT_fgivenx}
    \includegraphics<8>[width=0.9\textwidth]{figures/7TT_fgivenx}
    \includegraphics<9>[width=0.9\textwidth]{figures/8TT_fgivenx}
    \includegraphics<10>[width=0.9\textwidth]{figures/Bayes_TT.pdf}
    \includegraphics<11>[width=0.9\textwidth]{figures/combined_fgivenx.pdf}

  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Object detection}
  \framesubtitle{Toy problem}

  \centerline{%
  \includegraphics[width=0.7\textwidth]{figures/object_detection_image}
}

\end{frame}

\begin{frame}
  \frametitle{Object detection}
  \framesubtitle{Evidences}

  \begin{itemize}
      \pause
    \item $\log\ev$ ratio: $-251:-156:-114:-117:-136$
      \pause
    \item odds ratio: $10^{-60}:10^{-19}:1:0.04:10^{-10}$
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{PolyChord vs.\ MultiNest}
  \framesubtitle{Gaussian likelihood}

  \includegraphics<1>[width=\textwidth]{figures/polychord_vs_multinest.pdf}
  \includegraphics<2>[width=\textwidth]{figures/polychord_vs_multinest_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{Conclusions}
  \framesubtitle{The future of nested sampling}

  \begin{itemize}
      \pause
    \item We are at the beginning of a new era of sampling algorithms
      \pause
    \item Plenty of more work in to be done in exploring new versions of nested sampling
      \pause
    \item Nested sampling is just the beginning
      \pause
    \item arXiv:1506.00171
      \pause
    \item \href{http://ccpforge.cse.rl.ac.uk/gf/project/polychord/}{http://ccpforge.cse.rl.ac.uk/gf/project/polychord/}
  \end{itemize}

\end{frame}


\end{document}
```
# branch: remotes/origin/edinburgh_2016
```tex
\documentclass[]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}



\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\mathrm{knots}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}





\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{PolyChord \& the future of nested sampling}
\subtitle{Sampling, Parameter Estimation and Bayesian Model Comparison}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Supervisors: Anthony Lasenby \& Mike Hobson \\
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{March 8, 2016}



\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \tableofcontents
\end{frame}


\section{Parameter estimation \& model comparison}
%\begin{frame}
%  \frametitle{Notation} 
%
%  \begin{itemize}
%      \pause
%    \item Data: $D$
%      \pause
%    \item Model: $M$
%      \pause
%    \item Parameters: $\Theta$
%      \pause
%    \item Likelihood: $\prob(D|\Theta,M)=\lik(\Theta)$
%      \pause
%    \item Posterior: $\prob(\Theta|D,M)=\posterior(\Theta)$
%      \pause
%    \item Prior: $\prob(\Theta|M)=\prior(\Theta)$
%      \pause
%    \item Evidence: $\prob(D|M)=\ev$
%  \end{itemize}
%
%
%\end{frame}

\begin{frame}
  \frametitle{What is nested sampling?}
  \begin{itemize}
    \item<2-> Nested sampling is an alternative sampling approach. 
    \item<3-> Unlike traditional methods, it does not sample from the posterior directly.
    \item<4-> Instead it gradually compresses the prior onto the posterior.
    \item<5-> In doing so, it circumvents many issues (dimensionality, topology, geometry) that beset normal approaches.
    \item<5-> Similar to simulated annealing, but automatically picks the ``correct'' annealing schedule.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bayes' theorem}
  \framesubtitle{Parameter estimation}

  \pause
  What does data tell us about the params $\Theta$ of our model $M$?
    \[\prob(\Theta|D,M) = \frac{\prob(D|\Theta,M) \prob(\Theta|M) }{ \prob(D|M) }\] 
    \pause
    \[\text{Posterior } = \frac{\text{Likelihood} \times \text{Prior} } {\text{Evidence} }\] 
    \pause
    \[ \posterior(\Theta)= \frac{\lik(\Theta) \prior(\Theta)} {\ev}\] 

\end{frame}

\begin{frame}
  \frametitle{Bayes' theorem}
  \framesubtitle{Model comparison}
  \pause
  What does data tell us about our model $M_i$ in relation to other models $\{M_1,M_2,\cdots\}$?
  \pause
    \[\prob(M_i|D) = \frac{\prob(D|M_i) \prob(M_i) }{ \prob(D) }\] 

    \pause
    \[\prob(M_i|D) = \frac{\ev_i \: \mu_i}{\sum_k \ev_k \: \mu_k}\] 
\end{frame}

\begin{frame}
  \frametitle{Parameter estimation \& model comparison} 
  \framesubtitle{The challenge}

  \pause
  Both of these are challenging things to compute.
  \begin{itemize}
      \pause
    \item Markov-Chain Monte-Carlo (MCMC) can solve the first of these (kind of)
      \pause
    \item Nested sampling (NS) promises to solve both simultaneously.
  \end{itemize}
 
\end{frame}

\begin{frame}
  \frametitle{Parameter estimation \& model comparison} 
  \framesubtitle{Why is it difficult?} 

	\begin{columns}
	\begin{column}{0.5\textwidth}
		\begin{enumerate}
          \item<2-> In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item<3-> Worse, you don't know where this region is.
		\end{enumerate}
	\end{column}
	\begin{column}{0.5\textwidth}
		\includegraphics<1->[width=\textwidth]{figures/posteriors}	
	\end{column}
	\end{columns}
    \begin{itemize}
      \item<4-> Describing an $N$-dimensional posterior fully is impossible.
      \item<5-> {\em Sampling} the posterior is an excellent compression scheme.
    \end{itemize}
 
\end{frame}


\section{Metropolis Hastings}

\begin{frame}
  \frametitle{Markov-Chain Monte-Carlo (MCMC)} 
  \framesubtitle{Metropolis-Hastings, Gibbs, Hamiltonian\ldots} 
  \begin{itemize}
      \pause
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \pause
    \item Explore the space via a biased random walk.
      \begin{enumerate}
          \pause
        \item Pick random direction
          \pause
        \item Choose step length
          \pause
        \item If uphill, make step\ldots
          \pause
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
  \end{itemize}
 
\end{frame}

\begin{frame}
  \frametitle{MCMC in action} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_0.pdf}}{movies/MCMC.mp4}
\end{frame}
\begin{frame}
  \frametitle{MCMC in action} 
  \includegraphics[width=\textwidth]{movies/MCMC_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Burn in} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_burn_in_0.pdf}}{movies/MCMC_burn_in.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Burn in} 
  \includegraphics[width=\textwidth]{movies/MCMC_burn_in_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Tuning the proposal distribution} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_proposal_0.pdf}}{movies/MCMC_proposal.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Tuning the proposal distribution} 
  \includegraphics[width=\textwidth]{movies/MCMC_proposal_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Multimodality} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_multimodal_0.pdf}}{movies/MCMC_multimodal.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Multimodality} 
  \includegraphics[width=\textwidth]{movies/MCMC_multimodal_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Phase transitions} 

  \includegraphics[width=\textwidth]{figures/phase_transition}
 
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{The real reason\ldots} 

  \begin{itemize}
    \item<2-> MCMC does not give you evidences!
  \end{itemize}

  \begin{align}
    \onslide<3->{%
    \ev 
    &= \prob(D|M) 
    \nonumber\\
  }
    \onslide<4->{%
    &= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
  }
    \onslide<5->{%
    &= \int\lik(\Theta)\prior(\Theta) d\Theta 
    \nonumber\\
  }
    \onslide<6->{%
    &= \left\langle \lik \right\rangle_\prior
    \nonumber
  }
  \end{align}
  
  \begin{itemize}
    \item<7-> MCMC fundamentally explores the posterior, and cannot average over the prior.
    \item<8-> Simulated annealing gives one possibility for computing evidences.
      \begin{itemize}
        \item<9-> Suffers from similar issues to MCMC, especially phase transitions 
      \end{itemize}
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}
\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to MCMC!} 

  \pause
  New procedure: 

  \pause
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      \pause
    \item[$S_0$:] Generate $n$ samples from the prior $\prior$. 
      \pause
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new sample with higher likelihood
  \end{description}

  \pause
  Requires one to be able to sample from the prior, subject to a {\em hard likelihood constraint}.

\end{frame}




\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}



\begin{frame}
  \frametitle{Calculating evidences}

  \begin{itemize}
    \item<2-> \onslide<3->{Transform to 1 dimensional integral} \onslide<4->{$\pi(\theta)d\theta = dX$}
      \onslide<2->{\[ 
        \ev = \int \lik(\theta)\pi(\theta)d\theta \onslide<5->{= \int \lik(X)dX}
      \] }
    \item<6-> $X$ is the {\em prior volume}
      \onslide<7->{%
        \[ X(\lik) = \int_{\lik(\theta)>\lik} \pi(\theta) d\theta \]
      }
    \item<8-> i.e.\ the fraction of the prior which the iso-likelihood contour $\lik$ encloses.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}
  \includegraphics[width=\textwidth,page=38]{figures/nested_sampling}
\end{frame}

\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}

\foreach \pagenum in {1,...,5} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling_sum}
}
\end{frame}


\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{Exponential volume contraction} 
  
  \begin{itemize}
      \pause
    \item At each iteration, the likelihood contour will shrink in volume by a factor of $\approx 1/n$.
      \pause
    \item Nested sampling zooms in to the peak of the posterior {\em exponentially}.
      \pause
    \item 
      \begin{equation}
        \ev \approx \sum_i \lik_i (X_{i-1} - X_{i})
      \end{equation}
      \pause
    \item 
      \begin{equation}
        X_{i+1} \approx \frac{n}{n+1}X_i, \qquad X_{0} = 1
      \end{equation}
  \end{itemize}
  
\end{frame}

%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%\foreach \pagenum in {1,...,9} {%
%  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/areas}
%}
% 
%\end{frame}
%
%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%
%  \begin{itemize}
%    \item<2-> approximate compression:
%  \end{itemize}
%  \[ 
%    \onslide<2->{\Delta \log X \sim -\frac{1}{n}} 
%    \onslide<3->{\pm \frac{1}{n}} 
%  \]
%  \[ 
%    \onslide<4->{\log X_i \sim -\frac{i}{n}}
%    \onslide<5->{\pm \frac{\sqrt{i}}{n}} 
%  \]
%  \onslide<6->{%
%  \begin{itemize}
%    \item<6-> \# of steps to get to $H$:
%  \end{itemize}
%  \[ i_H \sim n H \]
%  }
%  \onslide<7->{%
%  \begin{itemize}
%    \item estimate of volume at $H$:
%  \end{itemize}
%  \[ \log X_H \approx -H \pm \sqrt{\frac{H}{n}} \]
%}
%
%  \onslide<8->{%
%  \begin{itemize}
%    \item estimate of evidence error:
%  \end{itemize}
%  \[ \log\ev \approx \sum w_i \lik_i  \pm \sqrt{\frac{H}{n}} \]
%}
%
%\end{frame}


\begin{frame}
  \frametitle{Nested sampling} 
  \framesubtitle{Parameter estimation} 

  \begin{itemize}
      \pause
    \item NS can also be used to sample the posterior
      \pause
    \item The set of dead points are posterior samples with an appropriate weighting factor
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{When NS succeeds} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/NS_0.pdf}}{movies/NS.mp4}
\end{frame}
\begin{frame}
  \frametitle{When NS suceeds} 
  \includegraphics[width=\textwidth]{movies/NS_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When NS succeeds} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/NS_multimodal_0.pdf}}{movies/NS_multimodal.mp4}
\end{frame}
\begin{frame}
  \frametitle{When NS suceeds} 
  \includegraphics[width=\textwidth]{movies/NS_multimodal_1.pdf}
\end{frame}


\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  \pause
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      \pause
    \item Most of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm.
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling within an iso-likelihood contour}
  \framesubtitle{Previous attempts}


  \begin{description}
    \item<2->[Rejection Sampling] MultiNest; F.\ Feroz \& M.\ Hobson (2009).
      \begin{itemize}
        \item<3-> Suffers in high dimensions
      \end{itemize}
    \item<4->[Hamiltonian sampling] F.\ Feroz \& J.\ Skilling (2013).
      \begin{itemize}
        \item<5-> Requires gradients and tuning
      \end{itemize}
    \item<6->[Diffusion Nested Sampling] B.\ Brewer et al.\ (2009).
      \begin{itemize}
        \item<7-> Very promising
        \item<8-> Too many tuning parameters
      \end{itemize}
    %\item<8->[Slice Sampling] R.\ Neal (2000), Aitken \& Akman (2013).
  \end{description}

\end{frame}

\section{PolyChord}
\begin{frame}
  \frametitle{PolyChord}
  \framesubtitle{``Hit and run'' slice sampling}
\foreach \pagenum in {1,...,32} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/contour_step}
}
\end{frame}


\begin{frame}
  \frametitle{PolyChord}
  \framesubtitle{Key points}

  \begin{itemize}
    \item<2-> This procedure satisfies detailed balance.
      \begin{itemize}
        \item<3-> Works even if $\mathcal{L}_0$ contour is disjoint.
      \end{itemize}
    \item<4-> Need $N$ reasonably large $\sim\mathcal{O}(n_\mathrm{dims})$ so that $x_N$ is de-correlated from $x_1$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Issues with Slice Sampling}

  \begin{enumerate}
    \item<2-> Does not deal well with correlated distributions.
    \item<3-> Need to ``tune'' $w$ parameter.
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{PolyChord's solutions}
  \framesubtitle{Correlated distributions}

  \includegraphics[width=\textwidth]{figures/contour_transform}

\end{frame}


\begin{frame}
  \frametitle{PolyChord's solutions}
  \framesubtitle{Correlated distributions}

  \begin{itemize}
    \item<2-> We make an affine transformation to remove degeneracies, and ``whiten'' the space.
    \item<3-> Samples remain uniformly sampled
    \item<4-> We use the covariance matrix of the live points and all inter-chain points 
    \item<5-> Cholesky decomposition is the required skew transformation
    \item<6-> $w=1$ in this transformed space
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{PolyChord's Additions}

  \begin{itemize}
    \item<2-> Parallelised up to number of live points with openMPI.
    \item<3-> Novel method for identifying and evolving modes separately.
    \item<4-> Implemented in CosmoMC, as ``CosmoChord'', with fast-slow parameters.
  \end{itemize}
\end{frame}






\section{Applications}

\begin{frame}
  \frametitle{PolyChord in action}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}


%\begin{frame}
%  \frametitle{Planck data}
%  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
%  \begin{itemize}
%    \item<2-> Temperature data TT+lowP
%    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
%    \item<4-> Marginalised plots of $\PR(k)$
%    \item<5->
%      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
%  \end{itemize}
%\end{frame}



\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knots}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{8 internal knots}
  \frametitle<10>{Bayes Factors}
  \frametitle<11>{Marginalised plot}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.9\textwidth]{figures/0TT_fgivenx}
    \includegraphics<2>[width=0.9\textwidth]{figures/1TT_fgivenx}
    \includegraphics<3>[width=0.9\textwidth]{figures/2TT_fgivenx}
    \includegraphics<4>[width=0.9\textwidth]{figures/3TT_fgivenx}
    \includegraphics<5>[width=0.9\textwidth]{figures/4TT_fgivenx}
    \includegraphics<6>[width=0.9\textwidth]{figures/5TT_fgivenx}
    \includegraphics<7>[width=0.9\textwidth]{figures/6TT_fgivenx}
    \includegraphics<8>[width=0.9\textwidth]{figures/7TT_fgivenx}
    \includegraphics<9>[width=0.9\textwidth]{figures/8TT_fgivenx}
    \includegraphics<10>[width=0.9\textwidth]{figures/Bayes_TT.pdf}
    \includegraphics<11>[width=0.9\textwidth]{figures/combined_fgivenx.pdf}

  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Object detection}
  \framesubtitle{Toy problem}

  \centerline{%
  \includegraphics[width=0.7\textwidth]{figures/object_detection_image}
}

\end{frame}

\begin{frame}
  \frametitle{Object detection}
  \framesubtitle{Evidences}

  \begin{itemize}
      \pause
    \item $\log\ev$ ratio: $-251:-156:-114:-117:-136$
      \pause
    \item odds ratio: $10^{-60}:10^{-19}:1:0.04:10^{-10}$
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{PolyChord vs.\ MultiNest}
  \framesubtitle{Gaussian likelihood}

  \includegraphics<1>[width=\textwidth]{figures/polychord_vs_multinest.pdf}
  \includegraphics<2>[width=\textwidth]{figures/polychord_vs_multinest_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{The future of nested sampling}
  \begin{itemize}
      \pause
    \item All previous approaches have focussed on trying to implement John's theoretical algorithm (still some work to be done here).
      \pause
    \item Nested sampling is really the first in a new class of ``probabilistic integration'' algorithms.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Adjusting the meta-algorithm}
  \begin{itemize}
      \pause
    \item Need one sample from prior each time?
      \pause
    \item Adaptive numbers of live points?
      \pause
    \item Diffusive nested sampling?
      \pause
    \item Parallelisation?
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Conclusions}
  \framesubtitle{The future of nested sampling}

  \begin{itemize}
      \pause
    \item We are at the beginning of a new era of sampling algorithms
      \pause
    \item Plenty of more work in to be done in exploring new versions of nested sampling
      \pause
    \item Nested sampling is just the beginning
      \pause
    \item arXiv:1506.00171
      \pause
    \item \href{http://ccpforge.cse.rl.ac.uk/gf/project/polychord/}{http://ccpforge.cse.rl.ac.uk/gf/project/polychord/}
  \end{itemize}

\end{frame}


\end{document}
```
# branch: remotes/origin/sussex_2016
```tex
\documentclass[]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}



\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\mathrm{knots}}
\newcommand{\nlive}{n_\mathrm{live}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}





\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{PolyChord \& the future of nested sampling}
\subtitle{Tools for sampling, Parameter Estimation and Bayesian Model Comparison}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Supervisors: Anthony Lasenby \& Mike Hobson \\
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{Monday 9\textsuperscript{th} May, 2016}



\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \tableofcontents
\end{frame}


\section{Parameter estimation \& model comparison}
%\begin{frame}
%  \frametitle{Notation} 
%
%  \begin{itemize}
%      \pause
%    \item Data: $D$
%      \pause
%    \item Model: $M$
%      \pause
%    \item Parameters: $\Theta$
%      \pause
%    \item Likelihood: $\prob(D|\Theta,M)=\lik(\Theta)$
%      \pause
%    \item Posterior: $\prob(\Theta|D,M)=\posterior(\Theta)$
%      \pause
%    \item Prior: $\prob(\Theta|M)=\prior(\Theta)$
%      \pause
%    \item Evidence: $\prob(D|M)=\ev$
%  \end{itemize}
%
%
%\end{frame}

\begin{frame}
  \frametitle{What is nested sampling?}
  \begin{itemize}
    \pause\item Nested sampling is an alternative sampling approach. 
    \pause\item Unlike traditional methods, it does not sample from the posterior directly.
    \pause\item Instead it gradually compresses the prior onto the posterior.
    \pause\item In doing so, it circumvents many issues (dimensionality, topology, geometry) that beset normal approaches.
    \pause\item Similar to simulated annealing, but automatically picks the ``correct'' annealing schedule.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bayes' theorem}
  \framesubtitle{Parameter estimation}

  \pause
  What does data tell us about the params $\Theta$ of our model $M$?
    \[\prob(\Theta|D,M) = \frac{\prob(D|\Theta,M) \prob(\Theta|M) }{ \prob(D|M) }\] 
    \pause
    \[\text{Posterior } = \frac{\text{Likelihood} \times \text{Prior} } {\text{Evidence} }\] 
    \pause
    \[ \posterior(\Theta)= \frac{\lik(\Theta) \prior(\Theta)} {\ev}\] 

\end{frame}

\begin{frame}
  \frametitle{Bayes' theorem}
  \framesubtitle{Model comparison}
  \pause
  What does data tell us about our model $M_i$?
  \pause
    \[\prob(M_i|D) = \frac{\prob(D|M_i) \prob(M_i) }{ \prob(D) }\] 

  \pause
    \[\prob(M_i|D) = \frac{\ev_i \: \mu_i}{\sum_k \ev_k \: \mu_k}\] 

  \pause
  Model averaging:
  \begin{itemize}
        \item Multiple models with posterior on the same parameter: ${\prob(y|M_i,D)}$
  \end{itemize}
  \[\prob(y|D) = \sum_i \prob(y|M_i,D) \prob(M_i|D)\]


\end{frame}

%\begin{frame}
%  \frametitle{Parameter estimation \& model comparison} 
%  \framesubtitle{The challenge}
%
%  \pause
%  Both of these are challenging things to compute.
%  \begin{itemize}
%      \pause
%    \item Markov-Chain Monte-Carlo (MCMC) can solve the first of these (kind of)
%      \pause
%    \item Nested sampling (NS) promises to solve both simultaneously.
%  \end{itemize}
% 
%\end{frame}
%
%\begin{frame}
%  \frametitle{Parameter estimation \& model comparison} 
%  \framesubtitle{Why is it difficult?} 
%
%	\begin{columns}
%	\begin{column}{0.5\textwidth}
%		\begin{enumerate}
%          \pause\item In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
%          \pause\item Worse, you don't know where this region is.
%		\end{enumerate}
%	\end{column}
%	\begin{column}{0.5\textwidth}
%		\includegraphics<1->[width=\textwidth]{figures/posteriors}	
%	\end{column}
%	\end{columns}
%    \begin{itemize}
%      \pause\item Describing an $N$-dimensional posterior fully is impossible.
%      \pause\item {\em Sampling\/} the posterior is an excellent compression scheme.
%    \end{itemize}
% 
%\end{frame}


\section{Metropolis Hastings}

\begin{frame}
  \frametitle{Markov-Chain Monte-Carlo (MCMC)} 
  \framesubtitle{Metropolis-Hastings, Gibbs, Hamiltonian\ldots} 
  \begin{itemize}
      \pause
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \pause
    \item Explore the space via a biased random walk.
      \begin{enumerate}
          \pause
        \item Pick random direction
          \pause
        \item Choose step length
          \pause
        \item If uphill, make step\ldots
          \pause
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
  \end{itemize}
 
\end{frame}

\begin{frame}
  \frametitle{MCMC in action} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_0.pdf}}{movies/MCMC.mp4}
\end{frame}
\begin{frame}
  \frametitle{MCMC in action} 
  \includegraphics[width=\textwidth]{movies/MCMC_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Burn in} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_burn_in_0.pdf}}{movies/MCMC_burn_in.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Burn in} 
  \includegraphics[width=\textwidth]{movies/MCMC_burn_in_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Tuning the proposal distribution} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_proposal_0.pdf}}{movies/MCMC_proposal.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Tuning the proposal distribution} 
  \includegraphics[width=\textwidth]{movies/MCMC_proposal_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Multimodality} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_multimodal_0.pdf}}{movies/MCMC_multimodal.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Multimodality} 
  \includegraphics[width=\textwidth]{movies/MCMC_multimodal_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Phase transitions} 

  \includegraphics[width=\textwidth]{figures/phase_transition}
 
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{The real reason\ldots} 

  \begin{itemize}
    \pause\item MCMC does not give you evidences!
  \end{itemize}

  \begin{align}
    \onslide<3->{%
    \ev 
    &= \prob(D|M) 
    \nonumber\\
  }
    \onslide<4->{%
    &= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
  }
    \onslide<5->{%
    &= \int\lik(\Theta)\prior(\Theta) d\Theta 
    \nonumber\\
  }
    \onslide<6->{%
    &= \left\langle \lik \right\rangle_\prior
    \nonumber
  }
  \end{align}
  
  \begin{itemize}
    \pause\item MCMC fundamentally explores the posterior, and cannot average over the prior.
    \pause\item Simulated annealing gives one possibility for computing evidences.
      \begin{itemize}
        \pause\item Suffers from similar issues to MCMC, especially phase transitions 
      \end{itemize}
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}
\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to MCMC!} 

  \pause
  New procedure: 

  \pause
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      \pause
    \item[$S_0$:] Generate $n$ samples from the prior $\prior$. 
      \pause
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new sample with higher likelihood
  \end{description}

  \pause
  Requires one to be able to sample from the prior, subject to a {\em hard likelihood constraint}.

\end{frame}



\begin{frame}
  \frametitle{Nested sampling}
  \framesubtitle{Unimodal} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/NS_0.pdf}}{movies/NS.mp4}
\end{frame}
\begin{frame}
  \frametitle{Nested sampling}
  \framesubtitle{Unimodal} 
  \includegraphics[width=\textwidth]{movies/NS_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{Nested sampling}
  \framesubtitle{Multimodal} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/NS_multimodal_0.pdf}}{movies/NS_multimodal.mp4}
\end{frame}
\begin{frame}
  \frametitle{Nested sampling}
  \framesubtitle{Multimodal} 
  \frametitle{When NS suceeds} 
  \includegraphics[width=\textwidth]{movies/NS_multimodal_1.pdf}
\end{frame}



\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}



\begin{frame}
  \frametitle{Calculating evidences}

  \begin{itemize}
    \item<2-> \onslide<3->{Transform to 1 dimensional integral} \onslide<4->{$\pi(\theta)d\theta = dX$}
      \onslide<2->{\[ 
        \ev = \int \lik(\theta)\pi(\theta)d\theta \onslide<5->{= \int \lik(X)dX}
      \] }
    \item<6-> $X$ is the {\em prior volume}
      \onslide<7->{%
        \[ X(\lik) = \int_{\lik(\theta)>\lik} \pi(\theta) d\theta \]
      }
    \item<8-> i.e.\ the fraction of the prior which the iso-likelihood contour $\lik$ encloses.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}
  \includegraphics[width=\textwidth,page=38]{figures/nested_sampling}
\end{frame}

\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}

\foreach \pagenum in {1,...,5} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling_sum}
}
\end{frame}


\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{Exponential volume contraction} 
  
  \begin{itemize}
      \pause
    \item At each iteration, the likelihood contour will shrink in volume by a factor of $\approx 1/n$.
      \pause
    \item Nested sampling zooms in to the peak of the posterior {\em exponentially}.
      \pause
    \item 
      \begin{equation}
        \ev \approx \sum_i \lik_i (X_{i-1} - X_{i})
      \end{equation}
      \pause
    \item 
      \begin{equation}
        X_{i+1} \approx \frac{n}{n+1}X_i, \qquad X_{0} = 1
      \end{equation}
  \end{itemize}
  
\end{frame}

%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%\foreach \pagenum in {1,...,9} {%
%  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/areas}
%}
% 
%\end{frame}
%
%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%
%  \begin{itemize}
%    \item<2-> approximate compression:
%  \end{itemize}
%  \[ 
%    \onslide<2->{\Delta \log X \sim -\frac{1}{n}} 
%    \onslide<3->{\pm \frac{1}{n}} 
%  \]
%  \[ 
%    \onslide<4->{\log X_i \sim -\frac{i}{n}}
%    \onslide<5->{\pm \frac{\sqrt{i}}{n}} 
%  \]
%  \onslide<6->{%
%  \begin{itemize}
%    \item<6-> \# of steps to get to $H$:
%  \end{itemize}
%  \[ i_H \sim n H \]
%  }
%  \onslide<7->{%
%  \begin{itemize}
%    \item estimate of volume at $H$:
%  \end{itemize}
%  \[ \log X_H \approx -H \pm \sqrt{\frac{H}{n}} \]
%}
%
%  \onslide<8->{%
%  \begin{itemize}
%    \item estimate of evidence error:
%  \end{itemize}
%  \[ \log\ev \approx \sum w_i \lik_i  \pm \sqrt{\frac{H}{n}} \]
%}
%
%\end{frame}


\begin{frame}
  \frametitle{Nested sampling} 
  \framesubtitle{Parameter estimation} 

  \begin{itemize}
      \pause
    \item NS can also be used to sample the posterior
      \pause
    \item The set of dead points are posterior samples with an appropriate weighting factor
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  \pause
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      \pause
    \item Most of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm.
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling within an iso-likelihood contour}
  \framesubtitle{Previous attempts}


  \begin{description}
    \pause\item[Rejection Sampling] MultiNest; F.\ Feroz \& M.\ Hobson (2009).
      \begin{itemize}
        \pause\item Suffers in high dimensions
      \end{itemize}
    \pause\item[Hamiltonian sampling] F.\ Feroz \& J.\ Skilling (2013).
      \begin{itemize}
        \pause\item Requires gradients and tuning
      \end{itemize}
    \pause\item[Diffusion Nested Sampling] B.\ Brewer et al.\ (2009).
      \begin{itemize}
        \pause\item Very promising
        \pause\item Too many tuning parameters
      \end{itemize}
    %\pause\item[Slice Sampling] R.\ Neal (2000), Aitken \& Akman (2013).
  \end{description}

\end{frame}

\section{PolyChord}
\begin{frame}
  \frametitle{``Hit and run'' slice sampling}
\foreach \pagenum in {1,...,32} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/contour_step}
}
\end{frame}


\begin{frame}
  \frametitle{``Hit and run'' slice sampling}
  \framesubtitle{Key points}

  \begin{itemize}
    \pause\item This procedure satisfies detailed balance.
    \pause\item Need $N$ reasonably large $\sim\mathcal{O}(n_\mathrm{dims})$ so that $x_N$ is de-correlated from $x_1$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Issues with Slice Sampling}
  \framesubtitle{Correlated distributions}

  \begin{enumerate}
    \pause\item Does not deal well with correlated distributions.
    \pause\item Need to ``tune'' $w$ parameter.
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{PolyChord's solutions}
  \framesubtitle{Correlated distributions}

  \includegraphics[width=\textwidth]{figures/contour_transform}

\end{frame}

\begin{frame}
  \frametitle{PolyChord's solutions}
  \framesubtitle{Correlated distributions}

  \begin{itemize}
    \pause\item We make an affine transformation to remove degeneracies, and ``whiten'' the space.
    \pause\item Samples remain uniformly sampled
    \pause\item We use the covariance matrix of the live points and all inter-chain points 
    \pause\item Cholesky decomposition is the required skew transformation
    \pause\item $w=1$ in this transformed space
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Issues with Slice Sampling}
  \framesubtitle{Multimodality}

  \begin{enumerate}
    \pause\item Although it satisfies detailed balance practically this isn't good enough.
    \pause\item Affine transformation is useless.
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{PolyChord's solutions}
  \framesubtitle{Multimodality}

  \begin{enumerate}
    \pause\item Identifies separate modes via clustering algorithm on live points.
    \pause\item Evolves these modes ``semi-independently''
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{PolyChord's Additions}

  \begin{itemize}
    \pause\item Parallelised up to number of live points with openMPI.
    \pause\item Implemented in CosmoMC, as ``CosmoChord'', with fast-slow parameters.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{PolyChord vs.\ MultiNest}
  \framesubtitle{Gaussian likelihood}
  \includegraphics[width=\textwidth]{figures/polychord_vs_multinest.pdf}
\end{frame}

\begin{frame}
  \frametitle{PolyChord 1.0}

  \begin{itemize}
    \pause\item Well tested.
    \pause\item arXiv:1502.01856
    \pause\item arXiv:1506.00171
    \pause\item \url{ccpforge.cse.rl.ac.uk/gf/project/polychord/}
  \end{itemize}
\end{frame}

\section{PolyChord 2.0}

\begin{frame}
  \frametitle{PolyChord}
  \framesubtitle{Scaling with dimensionality}
  \begin{itemize}
    \pause\item Perfect nested sampling has $N_\lik \sim \mathcal{O}(D^2)$
    \pause\item PolyChord 1.0 has $N_\lik \sim \mathcal{O}(D^3)$
      \begin{itemize}
        \pause\item Need $\sim \mathcal{O}(D)$ to de-correlate at each step
        \pause\item Forced to throw $\sim \mathcal{O}(D)$ inter-chain points away.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{PolyChord 2.0}
  \framesubtitle{Inter-chain evaluations}
  \begin{itemize}
    \pause\item Traditional nested sampling requires the ability to easily generate a single sample at each iteration.
    \pause\item MCMC-like approaches generate many (correlated) samples
    \pause\item At each iteration:
      \begin{itemize}
        \pause\item Throw away $n_\mathrm{chain}$ samples.
        \pause\item Generate $n_\mathrm{chain}$ new (correlated) samples to replace them.
      \end{itemize}
    \pause\item If $n_\mathrm{chain}\sim\mathcal{O}(D)$ (as required), then overall $\sim\mathcal{O}(D^3)\to\sim\mathcal{O}(D^2)$.
    \pause\item Need to be able to quantify degree of correlation for correct inference.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Aside: Merging nested sampling runs}

  \begin{itemize}
    \pause\item In his original paper, John Skilling noted that nested sampling runs can be merged.
    \pause\item Take two complete nested sampling runs generated by $\nlive^{(1)}$ and $\nlive^{(2)}$ live points.
    \pause\item Combining the two runs in likelihood order gives a new run generated by $\nlive^{(1)}+\nlive^{(2)}$ live points. 
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Aside: Unweaving nested sampling runs}

  \begin{itemize}
    \pause\item The reverse is also true.
    \pause\item Given a nested sampling run with $\nlive$ points, there is a unique way of separating it into $\nlive$ single-point runs (threads).
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{PolyChord 2.0}
  \framesubtitle{Handling correlations}

  \begin{itemize}
    \pause\item Unweave the run into $\nlive$ threads.
    \pause\item Each thread is a ``true'' nested sampling run, although threads are correlated.
    \pause\item Can use traditional techniques on threads to quantify correlation 
      \begin{itemize}
        \pause\item Batch means
        \item Jacknifing
        \item Bootstrapping
      \end{itemize}
    \pause\item With this in hand, can produce correct inferences from correlated runs.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{PolyChord 2.0 vs.\ MultiNest}
  \framesubtitle{Gaussian likelihood}
  \includegraphics[width=\textwidth]{figures/polychord_vs_multinest_1.pdf}
\end{frame}


\begin{frame}
  \frametitle{Correlated distributions}
  \begin{itemize}
    \pause\item Correlated distributions are hard
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Correlated distributions}

  \includegraphics[width=\textwidth]{figures/contour_transform}

\end{frame}

\begin{frame}
  \frametitle{Affine invariance}
  \begin{itemize}
    \pause\item The optimal exploration technique is be affine invariant.
    \pause\item Treat distribution $\prob(\mathbf{x})$ and $\prob(R \mathbf{x})$ the same.
    \pause\item No need to worry about correlations.
    \pause\item Good example: Now highly successful emcee (MCMC hammer).
      \begin{itemize}
        \item Important: emcee is not unique (or necessarily best)
      \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Skillings affine invariant ideas}
  \framesubtitle{Leapfrog}
\foreach \pagenum in {1,...,6} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/leapfrog}
}
\end{frame}

\begin{frame}
  \frametitle{Skillings affine invariant ideas}
  \framesubtitle{Parallel walk}
\foreach \pagenum in {1,...,6} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/parallel_walk}
}
\end{frame}

\begin{frame}
  \frametitle{Skillings affine invariant ideas}
  \framesubtitle{Guided walk}
\foreach \pagenum in {1,...,7} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/guided_walk}
}
\end{frame}

\begin{frame}
  \frametitle{Affine invariance}
  \framesubtitle{Subspace collapse}
  \begin{itemize}
    \pause\item The main problem that besets these techniques is ``subspace collapse''.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Subspace collapse}
  \framesubtitle{Leapfrog}
\foreach \pagenum in {4,...,6} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/leapfrog}
}
\end{frame}

\begin{frame}
  \frametitle{Subspace collapse}
  \framesubtitle{Parallel walk}
\foreach \pagenum in {4,...,7} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/leapfrog}
}
\end{frame}

\begin{frame}
  \frametitle{Subspace collapse}
  \framesubtitle{Solution}
  \begin{itemize}
    \pause\item Need to use $\sim\mathcal{O}(D)$ points to avoid this.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Skillings affine invariant ideas}
  \framesubtitle{Guided walk}
\foreach \pagenum in {4,...,8} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/guided_walk}
}
\end{frame}

\begin{frame}
  \frametitle{Affine invariant}
  \framesubtitle{Other variations}
  \begin{itemize}
    \pause\item Generalise guided walk to $D$ dimensions (slice through the mean of $D$ other points).
    \pause\item Slice through a ``random'' linear combination of $D$ points.
    \pause\item Slice through a ``random'' linear combination of all points
    \pause\item There are lots of variations: This is an underused area of the field.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{PolyChord 2.0}
  \begin{itemize}
    \pause\item Using intermediate points so $\sim\mathcal{O}(D^3)\to\sim\mathcal{O}(D^2)$.
    \pause\item Unweaving runs to quantify correlations
    \pause\item Affine invariant sampling
  \end{itemize}
\end{frame}


\section{Examples}

\begin{frame}
  \frametitle{Object detection}
  \framesubtitle{Toy problem}

  \centerline{%
  \includegraphics[width=0.7\textwidth]{figures/object_detection_image}
}

\end{frame}

\begin{frame}
  \frametitle{Object detection}
  \framesubtitle{Evidences}

  \begin{itemize}
      \pause
    \item $\log\ev$ ratio: $-251:-156:-114:-117:-136$
      \pause
    \item odds ratio: $10^{-60}:10^{-19}:1:0.04:10^{-10}$
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{PolyChord in action}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}


%\begin{frame}
%  \frametitle{Planck data}
%  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
%  \begin{itemize}
%    \item<2-> Temperature data TT+lowP
%    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
%    \item<4-> Marginalised plots of $\PR(k)$
%    \item<5->
%      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
%  \end{itemize}
%\end{frame}



\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knots}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{8 internal knots}
  \frametitle<10>{Bayes Factors}
  \frametitle<11>{Marginalised plot}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.9\textwidth]{figures/0TT_fgivenx}
    \includegraphics<2>[width=0.9\textwidth]{figures/1TT_fgivenx}
    \includegraphics<3>[width=0.9\textwidth]{figures/2TT_fgivenx}
    \includegraphics<4>[width=0.9\textwidth]{figures/3TT_fgivenx}
    \includegraphics<5>[width=0.9\textwidth]{figures/4TT_fgivenx}
    \includegraphics<6>[width=0.9\textwidth]{figures/5TT_fgivenx}
    \includegraphics<7>[width=0.9\textwidth]{figures/6TT_fgivenx}
    \includegraphics<8>[width=0.9\textwidth]{figures/7TT_fgivenx}
    \includegraphics<9>[width=0.9\textwidth]{figures/8TT_fgivenx}
    \includegraphics<10>[width=0.9\textwidth]{figures/Bayes_TT.pdf}
    \includegraphics<11>[width=0.9\textwidth]{figures/combined_fgivenx.pdf}

  \end{center}
\end{frame}

\end{document}
```
# branch: remotes/origin/ucl_2016
```tex
\documentclass[]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}



\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\mathrm{knots}}
\newcommand{\nlive}{n_\mathrm{live}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}





\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{PolyChord \& the future of nested sampling}
\subtitle{Tools for sampling, Parameter Estimation and Bayesian Model Comparison}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Supervisors: Anthony Lasenby \& Mike Hobson \\
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{Thursday 15\textsuperscript{th} September, 2016}



\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \tableofcontents
\end{frame}


\section{Parameter estimation \& model comparison}
%\begin{frame}
%  \frametitle{Notation} 
%
%  \begin{itemize}
%      \pause
%    \item Data: $D$
%      \pause
%    \item Model: $M$
%      \pause
%    \item Parameters: $\Theta$
%      \pause
%    \item Likelihood: $\prob(D|\Theta,M)=\lik(\Theta)$
%      \pause
%    \item Posterior: $\prob(\Theta|D,M)=\posterior(\Theta)$
%      \pause
%    \item Prior: $\prob(\Theta|M)=\prior(\Theta)$
%      \pause
%    \item Evidence: $\prob(D|M)=\ev$
%  \end{itemize}
%
%
%\end{frame}

\begin{frame}
  \frametitle{What is nested sampling?}
  \begin{itemize}
    \pause\item Nested sampling is an alternative sampling approach. 
    \pause\item Unlike traditional methods, it does not sample from the posterior directly.
    \pause\item Instead it gradually compresses the prior onto the posterior.
    \pause\item In doing so, it circumvents many issues (dimensionality, topology, geometry) that beset normal approaches.
    \pause\item Similar to simulated annealing, but automatically picks the ``correct'' annealing schedule.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bayes' theorem}
  \framesubtitle{Parameter estimation}

  \pause
  What does data tell us about the params $\Theta$ of our model $M$?
    \[\prob(\Theta|D,M) = \frac{\prob(D|\Theta,M) \prob(\Theta|M) }{ \prob(D|M) }\] 
    \pause
    \[\text{Posterior } = \frac{\text{Likelihood} \times \text{Prior} } {\text{Evidence} }\] 
    \pause
    \[ \posterior(\Theta)= \frac{\lik(\Theta) \prior(\Theta)} {\ev}\] 

\end{frame}

\begin{frame}
  \frametitle{Bayes' theorem}
  \framesubtitle{Model comparison}
  \pause
  What does data tell us about our model $M_i$?
  \pause
    \[\prob(M_i|D) = \frac{\prob(D|M_i) \prob(M_i) }{ \prob(D) }\] 

  \pause
    \[\prob(M_i|D) = \frac{\ev_i \: \mu_i}{\sum_k \ev_k \: \mu_k}\] 

  \pause
  Model averaging:
  \begin{itemize}
        \item Multiple models with posterior on the same parameter: ${\prob(y|M_i,D)}$
  \end{itemize}
  \[\prob(y|D) = \sum_i \prob(y|M_i,D) \prob(M_i|D)\]


\end{frame}

%\begin{frame}
%  \frametitle{Parameter estimation \& model comparison} 
%  \framesubtitle{The challenge}
%
%  \pause
%  Both of these are challenging things to compute.
%  \begin{itemize}
%      \pause
%    \item Markov-Chain Monte-Carlo (MCMC) can solve the first of these (kind of)
%      \pause
%    \item Nested sampling (NS) promises to solve both simultaneously.
%  \end{itemize}
% 
%\end{frame}
%
%\begin{frame}
%  \frametitle{Parameter estimation \& model comparison} 
%  \framesubtitle{Why is it difficult?} 
%
%	\begin{columns}
%	\begin{column}{0.5\textwidth}
%		\begin{enumerate}
%          \pause\item In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
%          \pause\item Worse, you don't know where this region is.
%		\end{enumerate}
%	\end{column}
%	\begin{column}{0.5\textwidth}
%		\includegraphics<1->[width=\textwidth]{figures/posteriors}	
%	\end{column}
%	\end{columns}
%    \begin{itemize}
%      \pause\item Describing an $N$-dimensional posterior fully is impossible.
%      \pause\item {\em Sampling\/} the posterior is an excellent compression scheme.
%    \end{itemize}
% 
%\end{frame}


\section{Metropolis Hastings}

\begin{frame}
  \frametitle{Markov-Chain Monte-Carlo (MCMC)} 
  \framesubtitle{Metropolis-Hastings, Gibbs, Hamiltonian\ldots} 
  \begin{itemize}
      \pause
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \pause
    \item Explore the space via a biased random walk.
      \begin{enumerate}
          \pause
        \item Pick random direction
          \pause
        \item Choose step length
          \pause
        \item If uphill, make step\ldots
          \pause
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
  \end{itemize}
 
\end{frame}

\begin{frame}
  \frametitle{MCMC in action} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_0.pdf}}{movies/MCMC.mp4}
\end{frame}
\begin{frame}
  \frametitle{MCMC in action} 
  \includegraphics[width=\textwidth]{movies/MCMC_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Burn in} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_burn_in_0.pdf}}{movies/MCMC_burn_in.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Burn in} 
  \includegraphics[width=\textwidth]{movies/MCMC_burn_in_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Tuning the proposal distribution} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_proposal_0.pdf}}{movies/MCMC_proposal.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Tuning the proposal distribution} 
  \includegraphics[width=\textwidth]{movies/MCMC_proposal_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Multimodality} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/MCMC_multimodal_0.pdf}}{movies/MCMC_multimodal.mp4}
\end{frame}
\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Multimodality} 
  \includegraphics[width=\textwidth]{movies/MCMC_multimodal_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{Phase transitions} 

  \includegraphics[width=\textwidth]{figures/phase_transition}
 
\end{frame}

\begin{frame}
  \frametitle{When MCMC fails} 
  \framesubtitle{The real reason\ldots} 

  \begin{itemize}
    \pause\item MCMC does not give you evidences!
  \end{itemize}

  \begin{align}
    \onslide<3->{%
    \ev 
    &= \prob(D|M) 
    \nonumber\\
  }
    \onslide<4->{%
    &= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
  }
    \onslide<5->{%
    &= \int\lik(\Theta)\prior(\Theta) d\Theta 
    \nonumber\\
  }
    \onslide<6->{%
    &= \left\langle \lik \right\rangle_\prior
    \nonumber
  }
  \end{align}
  
  \begin{itemize}
    \pause\item MCMC fundamentally explores the posterior, and cannot average over the prior.
    \pause\item Simulated annealing gives one possibility for computing evidences.
      \begin{itemize}
        \pause\item Suffers from similar issues to MCMC, especially phase transitions 
      \end{itemize}
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}
\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to MCMC!} 

  \pause
  New procedure: 

  \pause
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      \pause
    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\prior$). 
      \pause
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new sample with higher likelihood
  \end{description}

  \pause
  Requires one to be able to uniformly within a region, subject to a {\em hard likelihood constraint}.

\end{frame}



\begin{frame}
  \frametitle{Nested sampling}
  \framesubtitle{Unimodal} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/NS_0.pdf}}{movies/NS.mp4}
\end{frame}
\begin{frame}
  \frametitle{Nested sampling}
  \framesubtitle{Unimodal} 
  \includegraphics[width=\textwidth]{movies/NS_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{Nested sampling}
  \framesubtitle{Multimodal} 
  \movie[width=\textwidth,height=0.52\textwidth]{%
  \includegraphics[width=\textwidth]{movies/NS_multimodal_0.pdf}}{movies/NS_multimodal.mp4}
\end{frame}
\begin{frame}
  \frametitle{Nested sampling}
  \framesubtitle{Multimodal} 
  \frametitle{When NS suceeds} 
  \includegraphics[width=\textwidth]{movies/NS_multimodal_1.pdf}
\end{frame}



\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}


\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}
  \foreach \pagenum in {1,...,16} {%
      \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/lesbesgue}
  }
\end{frame}



\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{Exponential volume contraction} 
  
  \begin{itemize}
      \pause
    \item At each iteration, the likelihood contour will shrink in volume by  $\approx 1/n$.
      \pause
    \item Nested sampling zooms in to the peak of the posterior {\em exponentially}.
      \pause
    \item 
      \begin{equation}
          \ev \approx \sum_i \Delta\lik_i X_{i}
      \end{equation}
      \pause
    \item 
      \begin{equation}
        X_{i+1} \approx \frac{n}{n+1}X_i, \qquad X_{0} = 1
      \end{equation}
  \end{itemize}
  
\end{frame}

%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%\foreach \pagenum in {1,...,9} {%
%  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/areas}
%}
% 
%\end{frame}
%
%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%
%  \begin{itemize}
%    \item<2-> approximate compression:
%  \end{itemize}
%  \[ 
%    \onslide<2->{\Delta \log X \sim -\frac{1}{n}} 
%    \onslide<3->{\pm \frac{1}{n}} 
%  \]
%  \[ 
%    \onslide<4->{\log X_i \sim -\frac{i}{n}}
%    \onslide<5->{\pm \frac{\sqrt{i}}{n}} 
%  \]
%  \onslide<6->{%
%  \begin{itemize}
%    \item<6-> \# of steps to get to $H$:
%  \end{itemize}
%  \[ i_H \sim n H \]
%  }
%  \onslide<7->{%
%  \begin{itemize}
%    \item estimate of volume at $H$:
%  \end{itemize}
%  \[ \log X_H \approx -H \pm \sqrt{\frac{H}{n}} \]
%}
%
%  \onslide<8->{%
%  \begin{itemize}
%    \item estimate of evidence error:
%  \end{itemize}
%  \[ \log\ev \approx \sum w_i \lik_i  \pm \sqrt{\frac{H}{n}} \]
%}
%
%\end{frame}


\begin{frame}
  \frametitle{Nested sampling} 
  \framesubtitle{Parameter estimation} 

  \begin{itemize}
      \pause
    \item NS can also be used to sample the posterior
      \pause
    \item The set of dead points are posterior samples with an appropriate weighting factor
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  \pause
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      \pause
    \item Most of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm.
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling within an iso-likelihood contour}
  \framesubtitle{Previous attempts}


  \begin{description}
    \pause\item[Rejection Sampling] MultiNest; F.\ Feroz \& M.\ Hobson (2009).
      \begin{itemize}
        \pause\item Suffers in high dimensions
      \end{itemize}
    \pause\item[Hamiltonian sampling] F.\ Feroz \& J.\ Skilling (2013).
      \begin{itemize}
        \pause\item Requires gradients and tuning
      \end{itemize}
    \pause\item[Diffusion Nested Sampling] B.\ Brewer et al.\ (2009).
      \begin{itemize}
        \pause\item Very promising
        \pause\item Too many tuning parameters
      \end{itemize}
    %\pause\item[Slice Sampling] R.\ Neal (2000), Aitken \& Akman (2013).
  \end{description}

\end{frame}

\section{PolyChord}
\begin{frame}
  \frametitle{``Hit and run'' slice sampling}
\foreach \pagenum in {1,...,32} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/contour_step}
}
\end{frame}


\begin{frame}
  \frametitle{``Hit and run'' slice sampling}
  \framesubtitle{Key points}

  \begin{itemize}
    \pause\item This procedure satisfies detailed balance.
    \pause\item Need $N$ reasonably large $\sim\mathcal{O}(n_\mathrm{dims})$ so that $x_N$ is de-correlated from $x_1$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Issues with Slice Sampling}
  \framesubtitle{Correlated distributions}

  \begin{enumerate}
    \pause\item Does not deal well with correlated distributions.
    \pause\item Need to ``tune'' $w$ parameter.
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{PolyChord's solutions}
  \framesubtitle{Correlated distributions}

  \includegraphics[width=\textwidth]{figures/contour_transform}

\end{frame}

\begin{frame}
  \frametitle{PolyChord's solutions}
  \framesubtitle{Correlated distributions}

  \begin{itemize}
    \pause\item We make an affine transformation to remove degeneracies, and ``whiten'' the space.
    \pause\item Samples remain uniformly sampled
    \pause\item We use the covariance matrix of the live points and all inter-chain points 
    \pause\item Cholesky decomposition is the required skew transformation
    \pause\item $w=1$ in this transformed space
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Issues with Slice Sampling}
  \framesubtitle{Multimodality}

  \begin{enumerate}
    \pause\item Although it satisfies detailed balance practically this isn't good enough.
    \pause\item Affine transformation is useless.
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{PolyChord's solutions}
  \framesubtitle{Multimodality}

  \begin{enumerate}
    \pause\item Identifies separate modes via clustering algorithm on live points.
    \pause\item Evolves these modes ``semi-independently''
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{PolyChord's Additions}

  \begin{itemize}
    \pause\item Parallelised up to number of live points with openMPI.
    \pause\item Implemented in CosmoMC, as ``CosmoChord'', with fast-slow parameters.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{PolyChord vs.\ MultiNest}
  \framesubtitle{Gaussian likelihood}
  \includegraphics[width=\textwidth]{figures/polychord_vs_multinest.pdf}
\end{frame}


\begin{frame}
  \frametitle{PolyChord 1.0}

  \begin{itemize}
    \pause\item Well tested.
    \pause\item arXiv:1502.01856
    \pause\item arXiv:1506.00171
    \pause\item \url{ccpforge.cse.rl.ac.uk/gf/project/polychord/}
  \end{itemize}
\end{frame}

\section{PolyChord 2.0}

\begin{frame}
  \frametitle{PolyChord}
  \framesubtitle{Scaling with dimensionality}
  \begin{itemize}
    \pause\item Perfect nested sampling has $N_\lik \sim \mathcal{O}(D^2)$
    \pause\item PolyChord 1.0 has $N_\lik \sim \mathcal{O}(D^3)$
      \begin{itemize}
        \pause\item Need $\sim \mathcal{O}(D)$ to de-correlate at each step
        \pause\item Forced to throw $\sim \mathcal{O}(D)$ inter-chain points away.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{PolyChord 2.0}
  \framesubtitle{Inter-chain evaluations}
  \begin{itemize}
    \pause\item Traditional nested sampling requires the ability to easily generate a single sample at each iteration.
    \pause\item MCMC-like approaches generate many (correlated) samples
    \pause\item At each iteration:
      \begin{itemize}
        \pause\item Throw away $n_\mathrm{chain}$ samples.
        \pause\item Generate $n_\mathrm{chain}$ new (correlated) samples to replace them.
      \end{itemize}
    \pause\item If $n_\mathrm{chain}\sim\mathcal{O}(D)$ (as required), then overall $\sim\mathcal{O}(D^3)\to\sim\mathcal{O}(D^2)$.
    \pause\item Need to be able to quantify degree of correlation for correct inference.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Aside: Merging nested sampling runs}

  \begin{itemize}
    \pause\item In his original paper, John Skilling noted that nested sampling runs can be merged.
    \pause\item Take two complete nested sampling runs generated by $\nlive^{(1)}$ and $\nlive^{(2)}$ live points.
    \pause\item Combining the two runs in likelihood order gives a new run generated by $\nlive^{(1)}+\nlive^{(2)}$ live points. 
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Aside: Unweaving nested sampling runs}

  \begin{itemize}
    \pause\item The reverse is also true.
    \pause\item Given a nested sampling run with $\nlive$ points, there is a unique way of separating it into $\nlive$ single-point runs (threads).
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{PolyChord 2.0}
  \framesubtitle{Handling correlations}

  \begin{itemize}
    \pause\item Unweave the run into $\nlive$ threads.
    \pause\item Each thread is a ``true'' nested sampling run, although threads are correlated.
    \pause\item Can use traditional techniques on threads to quantify correlation 
      \begin{itemize}
        \pause\item Batch means
        \item Jacknifing
        \item Bootstrapping
      \end{itemize}
    \pause\item With this in hand, can produce correct inferences from correlated runs.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{PolyChord 2.0 vs.\ MultiNest}
  \framesubtitle{Gaussian likelihood}
  \includegraphics[width=\textwidth]{figures/polychord_vs_multinest_1.pdf}
\end{frame}


\begin{frame}
  \frametitle{Correlated distributions}
  \begin{itemize}
    \pause\item Correlated distributions are hard
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Correlated distributions}

  \includegraphics[width=\textwidth]{figures/contour_transform}

\end{frame}



\section{Examples}

\begin{frame}
  \frametitle{Object detection}
  \framesubtitle{Toy problem}

  \centerline{%
  \includegraphics[width=0.7\textwidth]{figures/object_detection_image}
}

\end{frame}

\begin{frame}
  \frametitle{Object detection}
  \framesubtitle{Evidences}

  \begin{itemize}
      \pause
    \item $\log\ev$ ratio: $-251:-156:-114:-117:-136$
      \pause
    \item odds ratio: $10^{-60}:10^{-19}:1:0.04:10^{-10}$
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{PolyChord in action}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}


%\begin{frame}
%  \frametitle{Planck data}
%  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
%  \begin{itemize}
%    \item<2-> Temperature data TT+lowP
%    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
%    \item<4-> Marginalised plots of $\PR(k)$
%    \item<5->
%      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
%  \end{itemize}
%\end{frame}



\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knots}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{8 internal knots}
  \frametitle<10>{Bayes Factors}
  \frametitle<11>{Marginalised plot}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.9\textwidth]{figures/0TT_fgivenx}
    \includegraphics<2>[width=0.9\textwidth]{figures/1TT_fgivenx}
    \includegraphics<3>[width=0.9\textwidth]{figures/2TT_fgivenx}
    \includegraphics<4>[width=0.9\textwidth]{figures/3TT_fgivenx}
    \includegraphics<5>[width=0.9\textwidth]{figures/4TT_fgivenx}
    \includegraphics<6>[width=0.9\textwidth]{figures/5TT_fgivenx}
    \includegraphics<7>[width=0.9\textwidth]{figures/6TT_fgivenx}
    \includegraphics<8>[width=0.9\textwidth]{figures/7TT_fgivenx}
    \includegraphics<9>[width=0.9\textwidth]{figures/8TT_fgivenx}
    \includegraphics<10>[width=0.9\textwidth]{figures/Bayes_TT.pdf}
    \includegraphics<11>[width=0.9\textwidth]{figures/combined_fgivenx.pdf}

  \end{center}
\end{frame}


\begin{frame}
  \frametitle{Affine invariance}
  \begin{itemize}
    \pause\item The optimal exploration technique is be affine invariant.
    \pause\item Treat distribution $\prob(\mathbf{x})$ and $\prob(R \mathbf{x})$ the same.
    \pause\item No need to worry about correlations.
    \pause\item Good example: Now highly successful emcee (MCMC hammer).
      \begin{itemize}
        \item Important: emcee is not unique (or necessarily best)
      \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Skillings affine invariant ideas}
  \framesubtitle{Leapfrog}
\foreach \pagenum in {1,...,6} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/leapfrog}
}
\end{frame}

\begin{frame}
  \frametitle{Skillings affine invariant ideas}
  \framesubtitle{Parallel walk}
\foreach \pagenum in {1,...,6} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/parallel_walk}
}
\end{frame}

\begin{frame}
  \frametitle{Affine invariance}
  \framesubtitle{Subspace collapse}
  \begin{itemize}
    \pause\item The main problem that besets these techniques is ``subspace collapse''.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Subspace collapse}
  \framesubtitle{Leapfrog}
  \includegraphics[width=\textwidth,page=6]{figures/leapfrog}
\end{frame}

\begin{frame}
  \frametitle{Subspace collapse}
  \includegraphics[width=\textwidth,page=7]{figures/parallel_walk}
\end{frame}

\begin{frame}
  \frametitle{Subspace collapse}
  \framesubtitle{Solution}
  \begin{itemize}
    \pause\item Need to use $\sim\mathcal{O}(D)$ points to avoid this.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Skillings affine invariant ideas}
  \framesubtitle{Guided walk}
\foreach \pagenum in {1,...,8} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/guided_walk}
}
\end{frame}

\begin{frame}
  \frametitle{Affine invariant}
  \framesubtitle{Other variations}
  \begin{itemize}
    \pause\item Generalise guided walk to $D$ dimensions (slice through the mean of $D$ other points).
    \pause\item Slice through a ``random'' linear combination of $D$ points.
    \pause\item Slice through a ``random'' linear combination of all points
    \pause\item There are lots of variations: This is an underused area of the field.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{PolyChord 2.0}
  \begin{itemize}
    \pause\item Using intermediate points so $\sim\mathcal{O}(D^3)\to\sim\mathcal{O}(D^2)$.
    \pause\item Unweaving runs to quantify correlations
    \pause\item Affine invariant sampling
  \end{itemize}
\end{frame}

\end{document}
```
# branch: remotes/origin/aachen_2017
```tex
\documentclass[%
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}

% todo 
% sampling figure
% mcmc for ligo case
% trim talk
% mention bayesian vs frequentist


\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\mathrm{knots}}
\newcommand{\nlive}{n_\mathrm{live}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}





\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{Modern Bayesian Inference}
\subtitle{Theory and Practice}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{July 4, 2017}



\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\section{Bayesian Inference}
\begin{frame}
    \frametitle{Introduction}
    \begin{itemize}
        \pause\item Statistics $\equiv$ Inference 
        \pause\item How to extract information about scientific models from data.
        \pause\item Most cosmologists work in a {\em Bayesian\/} framework of inference, although {\em Frequentist\/} methods are also sometimes used.
        \pause\item {\bf Bayesians use Probability Distributions to quantify uncertainty.}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Probability distributions}

    \begin{itemize}
        \pause\item As scientists, we are used to seeing error bars on results.
        \pause\item Age of the universe ({\em Planck\/}): 
        \pause \[13.73\pm 0.12\:\text{billion years old.}\]
        \pause\item Masses of LIGO GW150914 binary merger: 
        \[m_1 = 36.4^{+5.5}_{-4.9}\:M_\odot,\qquad m_2 = 30.9^{+4.8}_{-4.4}\:M_\odot \]
        \pause\item These are called {\em credible intervals}, state that we are e.g.\ $90\%$ confident of the value lying in this range.
        \pause\item More importantly, these are {\em summary statistics}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{LIGO binary merger}
    \begin{columns}
        \begin{column}{0.65\textwidth}
            \includegraphics[width=\textwidth]{./figures/ligo_m1_m2.pdf}
        \end{column}
        \begin{column}{0.35\textwidth}
            ${m_1 = 36.4^{+5.5}_{-4.9}\:M_\odot}$, ${m_2 = 30.9^{+4.8}_{-4.4}\:M_\odot}$
            \begin{itemize}
                \pause\item Summary statistics summarise a full probability distribution.
                \pause\item One goal of inference is to produce these probability distributions.
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Extended example of inference: LIGO}
    \begin{itemize}
        \item We will introduce the key concepts by discussing an extended example of the inference process.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Theory}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics[width=\textwidth]{./figures/ligo_schematic.png}
\end{frame}

\begin{frame}
    \frametitle{The model $M$}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics[width=\textwidth]{./figures/ligo_model.pdf}
\end{frame}

\begin{frame}
    \frametitle{The parameters $\Theta$ of the model $M$}
    \framesubtitle{Extended example of inference: LIGO}
    Theoretical signal depends on:
    \begin{itemize}
        \pause\item $m_1, m_2$: mass of binary
        \pause\item $\theta, \phi$: sky location
        \pause\item $r$: luminosity distance 
        \pause\item $\Phi_c, t_c$: phase and time of coalescence
        \pause\item $i, \theta_\mathrm{sky}$: inclination and angle on sky (orbital parameters)
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The data $D$}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics<1>[width=\textwidth]{./figures/ligo_data.pdf}
    \includegraphics<2>[width=\textwidth]{./figures/ligo_actual.png}
\end{frame}

\begin{frame}
    \frametitle<1>{The Likelihood: well matched}
    \frametitle<2>{The Likelihood: coalescence off}
    \frametitle<3>{The Likelihood: too large luminosity distance}
    \frametitle<4>{The Likelihood: incorrect inclination}
    \frametitle<5>{The Likelihood: 'Correct parameters'}
    \framesubtitle{Extended example of inference: LIGO}
    \only<1>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood.pdf}}
    \only<2>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood_t.pdf}}
    \only<3>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood_r.pdf}}
    \only<4>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood_i.pdf}}
    \only<5>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood.pdf}}
\end{frame}

\begin{frame}
    \frametitle{The likelihood $L(\Theta)$}
    \begin{itemize}
        \item<1-> We assume that observed signal is combination of theoretical signal and Gaussian noise $O = S + N$.
        \item<2-> Elementary probability tells us:
            \begin{itemize}
                \item<3-> $(t_i,h_i\pm\sigma_i)$: strain observed 
                \item<4-> $h(t;\Theta)$: Theoretical strain 
            \end{itemize}
            \only<5->{%
            \[ 
                \only<7->{L(\Theta) =}
                P(\only<3-5>{h_i}\only<6->{D}|\Theta,M) = 
                \only<6->{\prod_i}
                \frac{1}{\sqrt{2\pi}\sigma_i} \exp{\left( -\frac{[h_i-h(t_i;\Theta)]^2}{2\sigma_i^2} \right)} 
            \]
            }
        \item<8-> We normally work with log-likelihoods, which turn $\prod\to\sum$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Bayes' Theorem}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{itemize}
        \item Likelihood \pause$\equiv$ Probability of data, given model parameters: 
            \pause\[L(\Theta) \equiv P(D|\Theta,M)\] 
        \pause \item What we want: 
        \item {\em Posterior} \pause$\equiv$ Probability of model parameters, given data:
            \pause\[\mathcal{P}(\Theta) \equiv P(\Theta|D,M)\] 
            \pause \item Use Bayes theorem:
            \pause \[ P(\Theta|D,M) = \frac{P(D|\Theta, M) P(\Theta|M)}{P(D|M)}\]
            \pause \[ \mathcal{P}(\Theta) = \frac{L(\Theta) \pi(\Theta)}{Z}\]
            \pause \[ \text{Posterior} = \frac{\text{Likelihood $\times$ Prior}}{\text{Evidence}}\]
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Prior $\pi(\Theta)$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{align}
        P(\Theta|D,M) &= \frac{P(D|\Theta, M) P(\Theta|M)}{P(D|M)}\nonumber\\
        \mathcal{P}(\Theta) &= \frac{L(\Theta) \pi(\Theta)}{Z}\nonumber
    \end{align}
    \begin{itemize}
        \pause\item Prior: $\pi(\Theta) \equiv P(\Theta|M)$.
        \pause\item Represents initial degree of knowledge about the system:
        \pause\item e.g. $\pi(\theta,\phi)\: d\theta d\phi = -\frac{1}{2}\sin\theta\: d\theta d\phi$
        \pause\item Most Bayesian approaches are sensitive to this, and rightly so.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Evidence $Z$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{align}
        P(\Theta|D,M) &= \frac{P(D|\Theta, M) P(\Theta|M)}{P(D|M)}\nonumber\\
        \mathcal{P}(\Theta) &= \frac{L(\Theta) \pi(\Theta)}{Z}\nonumber
    \end{align}
    \begin{itemize}
        \item<1-> Evidence: $Z \equiv P(D|M) 
            \only<2>{=\int P(D|\Theta,M)P(\Theta|M) d\Theta.}
            \only<3->{=\int L(\Theta)\pi(\Theta) d\Theta.}$
        \item<4-> Normalising constant.
        \item<5-> Difficult to compute.
        \item<6-> Still extremely important.
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{itemize}
        \item Cannot plot the full posterior distribution:
            \[\mathcal{P}(\Theta) \equiv P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\mathrm{sky}|D,M)\]
        \item Can plot 1D and 2D {\em marginalised\/} distributions e.g:
            \begin{align}
            &P(m_1,m_2|D,M)=\nonumber\\&\int P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\mathrm{sky}|D,M) \,d\theta \,d\phi \,dr \,d\Phi_c \,d t_c \,d i \,d\theta_\mathrm{sky}\nonumber
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}

	\begin{columns}
	\begin{column}{0.65\textwidth}
        \only<1->{\includegraphics[width=\textwidth]{./figures/ligo_m1_m2.pdf}}
	\end{column}
	\begin{column}{0.35\textwidth}
		\begin{itemize}
          \item<2-> May do this for each pair of parameters
          \item<3-> Generates a {\em triangle plot}
		\end{itemize}
	\end{column}
	\end{columns}

\end{frame}

\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}
	\begin{columns}
	\begin{column}{0.7\textwidth}
        \only<1->{\includegraphics[width=\textwidth]{./figures/ligo_full.pdf}}
	\end{column}
	\begin{column}{0.3\textwidth}
		\begin{itemize}
          \item<2-> Does give insight
          \item<3-> Not the full picture
		\end{itemize}
	\end{column}
	\end{columns}
\end{frame}

\begin{frame}
    \frametitle{Evidences and model comparison}
    \framesubtitle{Extended example of inference: LIGO}

    \begin{itemize}
        \pause\item Up until now, we have discussed {\em Parameter estimation\/}: inferring what data tell us about parameters $\Theta$ of a model $M$.
        \pause\item Scientifically speaking, this is only half the story.
        \pause\item In general, we will have several competing models that describe the data, and we want to know which is the ``best''.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Bayes' Theorem (again)}

    \pause
    What does data tell us about our model $M_i$?
    \pause
    \[\prob(M_i|D) = \frac{\prob(D|M_i) \prob(M_i) }{ \prob(D) }\] 

    \pause
    \[\prob(M_i|D) = \frac{Z_i \: \mu_i}{\sum_k Z_k \: \mu_k}\] 

    \begin{itemize}
    \pause \item Evidences allow us to determine what weighting we should give to each model in light of the data.\\
    \pause \item e.g.\ EOBNR (Effective One Body) vs IMRPhenom (Inspiral-merger-ringdown).\\
    \pause \item Or testing modified theories of gravity.
    \end{itemize}
    

    \pause
    \textbf{Model averaging:}
    \begin{itemize}
        \item Multiple models with posterior on the same parameter: ${\prob(y|M_i,D)}$
    \end{itemize}
    \[\prob(y|D) = \sum_i \prob(y|M_i,D) \prob(M_i|D)\]

\end{frame}


\begin{frame}
    \frametitle{Parameter estimation}
    \framesubtitle{Another example.}

    \[\lik(\Theta) = P(D|\Theta,M)\]
    \begin{align}
        \onslide<2->{D =& \{C_\ell\only<6->{^\mathrm{(Planck)}}\}} 
        \onslide<15->{+\{\mathrm{LSS}\}} 
        \onslide<16->{+\{\text{``Big Data''}\}}
        \nonumber\\
        \onslide<3->{M =& \Lambda\mathrm{CDM}} 
        \onslide<9->{+ \mathrm{extensions} }
        \nonumber\\
        \onslide<4->{\Theta =& \Theta_{\Lambda \mathrm{CDM}}} \onslide<7->{+ \Theta_\mathrm{Planck}} \onslide<10->{+ \Theta_\mathrm{extensions}}\nonumber\\
        \onslide<5->{\Theta_{\Lambda \mathrm{CDM}} =& ( \Omega_b h^2, \Omega_c h^2, 100\theta_{MC}, \tau, {\rm{ln}}(10^{10} A_s), n_s) \nonumber\\}
        \onslide<8->{\Theta_\mathrm{Planck} =& (y_{\rm cal}, A^{CIB}_{217}, \xi^{tSZ-CIB}, A^{tSZ}_{143}, A^{PS}_{100}, A^{PS}_{143}, A^{PS}_{143\times 217}, A^{PS}_{217},\nonumber\\
        &A^{kSZ}, A^{{\rm dust}TT}_{100}, A^{{\rm dust}TT}_{143}, A^{{\rm dust}TT}_{143\times 217}, A^{{\rm dust}TT}_{217}, c_{100}, c_{217}) \nonumber\\}
        \onslide<11->{\Theta_\mathrm{extensions} =& (
                n_{\rm run}
                \only<12->{,n_{\rm run,run}}
                \only<13->{,w}
                \only<14->{,\Sigma m_\nu, m_{\nu,{\rm{sterile}}}^{\rm{eff}}}
        ) \nonumber}
    \end{align}

    \begin{itemize}
        \item<17->{Parameter estimation: $L, \pi \to \mathcal{P}$: model parameters}
        \item<17->{Model comparison: $L, \pi \to Z$: how good model is}
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Sampling}
    \framesubtitle{How to describe a high-dimensional posterior}

	\begin{columns}
	\begin{column}{0.5\textwidth}
		\begin{itemize}
          \item<3-> In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item<4-> {\em Sampling\/} the posterior is an excellent compression scheme.
		\end{itemize}
	\end{column}
	\begin{column}{0.5\textwidth}
        \only<2->{\includegraphics[width=\textwidth]{./figures/ligo_m1_m2.pdf}}
	\end{column}
	\end{columns}
 
\end{frame}

\begin{frame}
  \frametitle{Sampling}
  \foreach \pagenum in {1,...,4} {%
      \includegraphics<\pagenum>[height=\textheight,page=\pagenum]{figures/samples}
  }
\end{frame}

\begin{frame}
    \frametitle{Why do sampling?}
    \framesubtitle{Marginalisation over the posterior}

    \begin{itemize}
        \item<1-> Set of $N$ samples $S = \{\Theta^{(i)}: i=1,\ldots N:\: \Theta^{(i)}\sim\mathcal{P}\}$
        \item<2-> Mean mass: \[
                \bar{m}_1 \equiv\langle m_1\rangle_\mathcal{P} \approx 
                \only<2-6>{\frac{1}{N}\sum_{i=1}^N m_1^{(i)}}
                \only<7->{\frac{\sum_{i=1}^N w^{(i)} m_1^{(i)}}{\sum_{i=1}^N w^{(i)}}}
            \]
        \item<3-> Mass covariance: \[
                \mathrm{Cov}(m_1,m_2) \approx 
                \only<3-6>{\frac{1}{N}\sum_{i=1}^N (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}
                \only<7->{\frac{\sum_{i=1}^N (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}{\sum_{i=1}^N w^{(i)}}}
            \]
        \item<4-> Marginalised samples: Just ignore the other coordinates.
        \item<5-> N.B. Typically have {\em weighted\/} samples
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Parameter estimation}
    \begin{itemize}
        \pause\item The name of the game is therefore drawing samples $S$ from the posterior $\mathcal{P}$ with the minimum number of likelihood calls.
        \pause\item Gridding is doomed to failure in high dimensions.
        \pause\item Enter Metropolis Hastings.
    \end{itemize}
\end{frame}



\section{Metropolis Hastings}

\begin{frame}
    \frametitle{Current sampling approaches}
    \begin{enumerate}
        \pause\item Metropolis Hastings.
        \pause\item Hamiltonian Monte-Carlo (HMC).
        \pause\item Ensemble sampling (e.g.\ emcee).
    \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Hastings} 
  \begin{itemize}
      \pause
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \begin{enumerate}
          \pause
        \item Pick random direction
          \pause
        \item Choose step length
          \pause
        \item If uphill, make step\ldots
          \pause
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Hastings} 
  \includegraphics[width=\textwidth]{movies/MCMC_0.pdf}
\end{frame}
\begin{frame}
  \frametitle{Metropolis Hastings} 
  \movie[width=\textwidth,height=0.52\textwidth]{}{movies/MCMC.mp4}
\end{frame}
\begin{frame}
  \frametitle{Metropolis Hastings} 
  \includegraphics[width=\textwidth]{movies/MCMC_1.pdf}
\end{frame}


%\begin{frame}
%  \frametitle{Metropolis Hastings} 
%  \framesubtitle{More formally:}
%  \begin{description}
%      \item<1->[Step 0]{Choose initial set of parameters $\Theta^{(0)}$, set $w^{(0)}=1$}
%      \item<2->[Step i]
%          \begin{itemize}
%              \item<3-> Propose new point $\Theta^{\mathrm{new}}$ from some proposal distribution $Q(\Theta^{\mathrm{new}};\Theta^{(i)})$
%                  \begin{itemize}
%                      \item<4-> e.g:\: a point in a random direction, distance $d$ away from $\Theta^{(i)}$.
%                      \item<5-> $Q$ is typically symmetric.
%                  \end{itemize}
%              \item<6-> ``Accept'' this point with probability $\propto \mathcal{P}(\Theta^{(\mathrm{new})})/\mathcal{P}(\Theta^{(i)}) \times Q(\Theta^{(i)}) / Q(\Theta^{(\mathrm{new})}) $
%              \item<7-> if accepted, $\Theta^{(i+1)} = \Theta^{(\mathrm{new})}$, $i+=1$
%              \item<8-> Otherwise, reject, $w^{(i)}+=1$ and repeat.
%          \end{itemize}
%  \end{description}
%\end{frame}


\begin{frame}
  \frametitle{Metropolis Hastings} 
  \framesubtitle{Struggles with\ldots}
  \begin{enumerate}
      \pause\item Burn in
      \pause\item Multimodality
      \pause\item Correlated Peaks
      \pause\item Phase transitions
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte-Carlo} 
  \begin{itemize}
      \pause\item Key idea: Treat $\log L(\Theta)$ as a potential energy
      \pause\item Guide walker under ``force'': \[F(\Theta) =\nabla \log L(\Theta)\]
      \pause\item Walker is naturally ``guided'' uphill
      \pause\item Conserved quantities mean efficient acceptance ratios.
      \pause\item stan is a fully fledged, rapidly developing programming language with HMC as a default sampler.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Ensemble sampling} 
  \begin{itemize}
      \pause\item Instead of one walker, evolve a set of $n$ walkers.
      \pause\item Can use information present in ensemble to guide proposals.
      \pause\item emcee: affine invariant proposals.
      \pause\item emcee is not the only (or even best) affine invariant approach.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The fundamental issue with all of the above} 

  \begin{itemize}
    \item They don't give you evidences!
  \end{itemize}

  \begin{align}
    \ev 
    &= \prob(D|M) 
    \nonumber\\
    \pause&= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
    \pause&= \left\langle \lik \right\rangle_\prior
    \nonumber
  \end{align}
  
  \begin{itemize}
    \pause\item MCMC fundamentally explores the posterior, and cannot average over the prior.
    \pause\item Simulated annealing gives one possibility for computing evidences.
    \begin{itemize}
        \pause\item Inspired by thermodynamics.
        \pause\item Suffers from similar issues to MCMC.
        \pause\item Unclear how to choose correct annealing schedule
    \end{itemize}
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}

\begin{frame}
  \frametitle{What is nested sampling?}
  \begin{itemize}
    \pause\item Nested sampling is an alternative way of sampling posteriors. 
    \pause\item Uses ensemble sampling to compress prior to posterior.
    \pause\item In doing so, it circumvents many issues (dimensionality, topology, geometry) that beset standard approaches.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to traditional MCMC!} 

  \pause
  New procedure: 

  \pause
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      \pause
    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\prior$). 
      \pause
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new uniform sample with higher likelihood
  \end{description}

  \pause
  Requires one to be able to uniformly within a region, subject to a {\em hard likelihood constraint}.

\end{frame}



\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}


%\begin{frame}
%  \frametitle{Nested sampling}
%  \includegraphics[width=\textwidth]{movies/NS_0.pdf}
%\end{frame}
%\begin{frame}
%  \frametitle{Nested sampling}
%  \movie[width=\textwidth,height=0.52\textwidth]{}{movies/NS.mp4}
%\end{frame}
%\begin{frame}
%  \frametitle{Nested sampling}
%  \includegraphics[width=\textwidth]{movies/NS_1.pdf}
%\end{frame}




%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%\foreach \pagenum in {1,...,9} {%
%  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/areas}
%}
% 
%\end{frame}
%
%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%
%  \begin{itemize}
%    \item<2-> approximate compression:
%  \end{itemize}
%  \[ 
%    \onslide<2->{\Delta \log X \sim -\frac{1}{n}} 
%    \onslide<3->{\pm \frac{1}{n}} 
%  \]
%  \[ 
%    \onslide<4->{\log X_i \sim -\frac{i}{n}}
%    \onslide<5->{\pm \frac{\sqrt{i}}{n}} 
%  \]
%  \onslide<6->{%
%  \begin{itemize}
%    \item<6-> \# of steps to get to $H$:
%  \end{itemize}
%  \[ i_H \sim n H \]
%  }
%  \onslide<7->{%
%  \begin{itemize}
%    \item estimate of volume at $H$:
%  \end{itemize}
%  \[ \log X_H \approx -H \pm \sqrt{\frac{H}{n}} \]
%}
%
%  \onslide<8->{%
%  \begin{itemize}
%    \item estimate of evidence error:
%  \end{itemize}
%  \[ \log\ev \approx \sum w_i \lik_i  \pm \sqrt{\frac{H}{n}} \]
%}
%
%\end{frame}


\begin{frame}
  \frametitle{Nested sampling} 

  \begin{itemize}
    \pause\item The set of dead points are posterior samples with an appropriate weighting factor
    \pause\item They can also be used to calculate evidences, since it sequentially updates the priors.
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  \pause
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      \pause
    \item Most of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm.
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling within an iso-likelihood contour}
  \framesubtitle{Previous attempts}


  \begin{description}
    \pause\item[Rejection Sampling] MultiNest; F.\ Feroz \& M.\ Hobson (2009).
      \begin{itemize}
        \pause\item Suffers in high dimensions
      \end{itemize}
      \pause\item[Hamiltonian] M.J. Betancourt (2010) 
      \pause\item[Galilean] F.\ Feroz \& J.\ Skilling (2013) 
      \begin{itemize}
        \pause\item Requires gradients and tuning
      \end{itemize}
      \pause\item[Diffusive Nested Sampling] B.\ Brewer et al.\ (2009,2016).
      \begin{itemize}
        \pause\item Very promising
        \pause\item Still needs tuning.
      \end{itemize}
      \pause\item[Slice Sampling] PolyChord; Handley et al.\ (2015).
      \begin{itemize}
          \pause\item Current ``state-of-the-art''.
          \pause\item PolyChord 2.0 imminent.
      \end{itemize}
  \end{description}

\end{frame}


\section{Examples}
\begin{frame}
  \frametitle{Object detection}
  \framesubtitle{Toy problem}

  \centerline{%
  \includegraphics<1>[width=\textwidth]{figures/object_detection_image}
  \includegraphics<2>[width=\textwidth]{figures/object_detection_image_no_noise}
}

\end{frame}

\begin{frame}
  \frametitle{PolyChord in action}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}


%\begin{frame}
%  \frametitle{Planck data}
%  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
%  \begin{itemize}
%    \item<2-> Temperature data TT+lowP
%    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
%    \item<4-> Marginalised plots of $\PR(k)$
%    \item<5->
%      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
%  \end{itemize}
%\end{frame}



\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knots}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{8 internal knots}
  \frametitle<10>{Bayes Factors}
  \frametitle<11>{Marginalised plot}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.9\textwidth]{figures/0TT_fgivenx}
    \includegraphics<2>[width=0.9\textwidth]{figures/1TT_fgivenx}
    \includegraphics<3>[width=0.9\textwidth]{figures/2TT_fgivenx}
    \includegraphics<4>[width=0.9\textwidth]{figures/3TT_fgivenx}
    \includegraphics<5>[width=0.9\textwidth]{figures/4TT_fgivenx}
    \includegraphics<6>[width=0.9\textwidth]{figures/5TT_fgivenx}
    \includegraphics<7>[width=0.9\textwidth]{figures/6TT_fgivenx}
    \includegraphics<8>[width=0.9\textwidth]{figures/7TT_fgivenx}
    \includegraphics<9>[width=0.9\textwidth]{figures/8TT_fgivenx}
    \includegraphics<10>[width=0.9\textwidth]{figures/Bayes_TT.pdf}
    \includegraphics<11>[width=0.9\textwidth]{figures/combined_fgivenx.pdf}

  \end{center}
\end{frame}

\begin{frame}
    \frametitle{Dark energy equation of state reconstruction}
    \begin{itemize}
        \pause\item Same thing, but for Dark energy equation of state $w(z)$ (quintessence).
        \pause\item Data used is Planck 2015, BOSS DR 11, JLA supernovae and BOSS Ly$\alpha$ data
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle<1>{Flat, variable $w$}
    \frametitle<2>{Tilted}
    \frametitle<3>{1 internal node}
    \frametitle<4>{2 internal nodes}
    \frametitle<5>{3 internal nodes}
    \frametitle<6>{Marginalised plot - just extension models}
    \frametitle<7>{Marginalised plot - including LCDM}
    \framesubtitle{Dark energy equation of state reconstruction}

    \begin{center}
        \includegraphics<1>[width=0.9\textwidth]{figures/wCDM_1000Nlive_wgiivenz.pdf}
        \includegraphics<2>[width=0.9\textwidth]{figures/tCDM_100Nlive_wgiivenz.pdf}
        \includegraphics<3>[width=0.9\textwidth]{figures/1CDM_1000Nlive_wgiivenz.pdf}
        \includegraphics<4>[width=0.9\textwidth]{figures/2CDM_1000Nlive_wgiivenz.pdf}
        \includegraphics<5>[width=0.9\textwidth]{figures/3CDM_1000Nlive_wgiivenz.pdf}
        \includegraphics<6>[width=0.9\textwidth]{figures/extensionModels_1000Nlive_wgivenz.pdf}
        \includegraphics<7>[width=0.9\textwidth]{figures/allModels_1000Nlive_wgivenz.pdf}
    \end{center}
\end{frame}


\begin{frame}
    \frametitle{Useful links}
    \begin{description}
        \item[My email:] wh260@cam.ac.uk
        \item[My room:] Room 104, Tuesday-Thursday this week
        \item[PolyChord:] ccpforge.cse.rl.ac.uk/gf/project/polychord
        \item[MultiNest:] ccpforge.cse.rl.ac.uk/gf/project/multinest
        \item[Stan:] mc-stan.org/
        \item[emcee:] dan.iel.fm/emcee/current/
    \end{description}
\end{frame}

\end{document}
```
# branch: remotes/origin/flatiron_2017
```tex
\documentclass[%
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}



\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\mathrm{knots}}
\newcommand{\nlive}{n_\mathrm{live}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}





\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{PolyChord 2.0}
\subtitle{Advances in nested sampling with astrophysical applications}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{\today}



\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\section{Parameter estimation \& model comparison}
%\begin{frame}
%  \frametitle{Notation} 
%
%  \begin{itemize}
%      \pause
%    \item Data: $D$
%      \pause
%    \item Model: $M$
%      \pause
%    \item Parameters: $\Theta$
%      \pause
%    \item Likelihood: $\prob(D|\Theta,M)=\lik(\Theta)$
%      \pause
%    \item Posterior: $\prob(\Theta|D,M)=\posterior(\Theta)$
%      \pause
%    \item Prior: $\prob(\Theta|M)=\prior(\Theta)$
%      \pause
%    \item Evidence: $\prob(D|M)=\ev$
%  \end{itemize}
%
%
%\end{frame}

\begin{frame}
  \frametitle{What is nested sampling?}
  \begin{itemize}
    \pause\item Nested sampling is an alternative way of sampling posteriors. 
    \pause\item Uses ensemble sampling to compress prior to posterior.
    \pause\item In doing so, it circumvents many issues (dimensionality, topology, geometry) that beset standard approaches.
  \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \begin{enumerate}
        \pause\item Background theory
        \pause\item Review existing sampling approaches
        \pause\item Nested Sampling \& Historical implementations.
        \pause\item PolyChord
        \pause\item Applications
    \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Bayes' theorem}
  \framesubtitle{Parameter estimation}

  What does data $D$ tell us about the params $\Theta$ of our model $M$?
  \pause
  \[\prob(\Theta|D,M) = \frac{\prob(D|\Theta,M) \prob(\Theta|M) }{ \prob(D|M) }\] 
  \pause
  \[\text{Posterior } = \frac{\text{Likelihood} \times \text{Prior} } {\text{Evidence} }\] 
  \pause
  \[ \posterior(\Theta)= \frac{\lik(\Theta) \prior(\Theta)} {\ev}\] 

\end{frame}
\begin{frame}
    \frametitle{Parameter estimation}
    \framesubtitle{A concrete example.}

    \[\lik(\Theta) = P(D|\Theta,M)\]
    \begin{align}
        \onslide<2->{D =& \{C_\ell\only<6->{^\mathrm{(Planck)}}\}} 
        \onslide<15->{+\{\mathrm{LSS}\}} 
        \onslide<16->{+\{\text{``Big Data''}\}}
        \nonumber\\
        \onslide<3->{M =& \Lambda\mathrm{CDM}} 
        \onslide<9->{+ \mathrm{extensions} }
        \nonumber\\
        \onslide<4->{\Theta =& \Theta_{\Lambda \mathrm{CDM}}} \onslide<7->{+ \Theta_\mathrm{Planck}} \onslide<10->{+ \Theta_\mathrm{extensions}}\nonumber\\
        \onslide<5->{\Theta_{\Lambda \mathrm{CDM}} =& ( \Omega_b h^2, \Omega_c h^2, 100\theta_{MC}, \tau, {\rm{ln}}(10^{10} A_s), n_s) \nonumber\\}
        \onslide<8->{\Theta_\mathrm{Planck} =& (y_{\rm cal}, A^{CIB}_{217}, \xi^{tSZ-CIB}, A^{tSZ}_{143}, A^{PS}_{100}, A^{PS}_{143}, A^{PS}_{143\times 217}, A^{PS}_{217},\nonumber\\
        &A^{kSZ}, A^{{\rm dust}TT}_{100}, A^{{\rm dust}TT}_{143}, A^{{\rm dust}TT}_{143\times 217}, A^{{\rm dust}TT}_{217}, c_{100}, c_{217}) \nonumber\\}
        \onslide<11->{\Theta_\mathrm{extensions} =& (
                n_{\rm run}
                \only<12->{,n_{\rm run,run}}
                \only<13->{,w}
                \only<14->{,\Sigma m_\nu, m_{\nu,{\rm{sterile}}}^{\rm{eff}}}
        ) \nonumber}
    \end{align}

    \begin{itemize}
        \item<17->{Likelihoods can be quite complicated!}
        \item<18->{We  need advanced sampling approaches.}
    \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Bayes' theorem}
  \framesubtitle{Parameter estimation}

  What does data $D$ tell us about the params $\Theta$ of our model $M$?
  \[\prob(\Theta|D,M) = \frac{\prob(D|\Theta,M) \prob(\Theta|M) }{ \prob(D|M) }\] 
  \[\text{Posterior } = \frac{\text{Likelihood} \times \text{Prior} } {\text{Evidence} }\] 
  \[ \posterior(\Theta)= \frac{\lik(\Theta) \prior(\Theta)} {\ev}\] 
  \begin{description}
          \pause\item[Prior:] Usually uniform, but can be source of controversy.
      \pause\item[Evidence:] Just a normalising constant\pause?
  \end{description}

\end{frame}

\begin{frame}
  \frametitle{Bayes' theorem}
  \framesubtitle{Model comparison}
  \pause
  What does data tell us about our model $M_i$?
  \pause
    \[\prob(M_i|D) = \frac{\prob(D|M_i) \prob(M_i) }{ \prob(D) }\] 

  \pause
    \[\prob(M_i|D) = \frac{\ev_i \: \mu_i}{\sum_k \ev_k \: \mu_k}\] 

  \pause
  e.g.\ Should we include running? \pause Neutrinos? \pause Dark energy?

  \pause
  \textbf{Model averaging:}
  \begin{itemize}
        \item Multiple models with posterior on the same parameter: ${\prob(y|M_i,D)}$
  \end{itemize}
  \[\prob(y|D) = \sum_i \prob(y|M_i,D) \prob(M_i|D)\]


\end{frame}

%\begin{frame}
%  \frametitle{Parameter estimation \& model comparison} 
%  \framesubtitle{The challenge}
%
%  \pause
%  Both of these are challenging things to compute.
%  \begin{itemize}
%      \pause
%    \item Markov-Chain Monte-Carlo (MCMC) can solve the first of these (kind of)
%      \pause
%    \item Nested sampling (NS) promises to solve both simultaneously.
%  \end{itemize}
% 
%\end{frame}
%
\begin{frame}
  \frametitle{Parameter estimation} 
  \framesubtitle{Why do sampling?} 

	\begin{columns}
	\begin{column}{0.5\textwidth}
		\begin{itemize}
          \item<4-> In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item<5-> Describing an $N$-dimensional posterior fully is impossible.
          \item<6-> {\em Sampling\/} the posterior is an excellent compression scheme.
		\end{itemize}
	\end{column}
	\begin{column}{0.5\textwidth}
        \only<2>{\includegraphics[width=\textwidth]{./figures/ligo_m1_m2.pdf}}
        \only<3->{\includegraphics[width=\textwidth]{./figures/ligo_dist_thetajn.pdf}}	
	\end{column}
	\end{columns}
 
\end{frame}


\section{Metropolis Hastings}

\begin{frame}
    \frametitle{Current sampling approaches}
    \begin{enumerate}
        \pause\item Metropolis Hastings.
        \pause\item Hamiltonian Monte-Carlo (HMC).
        \pause\item Ensemble sampling (e.g.\ emcee).
    \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Hastings} 
  \begin{itemize}
      \pause
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \pause
    \item Explore the space via a biased random walk.
      \begin{enumerate}
          \pause
        \item Pick random direction
          \pause
        \item Choose step length
          \pause
        \item If uphill, make step\ldots
          \pause
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Hastings} 
  \includegraphics[width=\textwidth]{movies/MCMC_0.pdf}
\end{frame}
\begin{frame}
  \frametitle{Metropolis Hastings} 
  \movie[width=\textwidth,height=0.52\textwidth]{}{movies/MCMC.mp4}
\end{frame}
\begin{frame}
  \frametitle{Metropolis Hastings} 
  \includegraphics[width=\textwidth]{movies/MCMC_1.pdf}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Hastings} 
  \framesubtitle{Struggles with\ldots}
  \begin{enumerate}
      \pause\item Burn in
      \pause\item Multimodality
      \pause\item Correlated Peaks
      \pause\item Phase transitions
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte-Carlo} 
  \begin{itemize}
      \pause\item Key idea: Treat $\log L(\Theta)$ as a potential energy
      \pause\item Guide walker under ``force'': \[F(\Theta) =\nabla \log L(\Theta)\]
      \pause\item Walker is naturally ``guided'' uphill
      \pause\item Conserved quantities mean efficient acceptance ratios.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Hamiltonian Monte-Carlo} 
  \framesubtitle{Problems}
  \pause
  \begin{itemize}
      \item ``Uphill'' is not covariant.
  \end{itemize}
  \pause
  \includegraphics[width=\textwidth]{figures/gradients.pdf}
  \pause
  \begin{itemize}
      \item Requires gradients (autograd -- python)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Ensemble sampling} 
  \begin{itemize}
      \pause\item Instead of one walker, evolve a set of $n$ walkers.
      \pause\item Can use information present in ensemble to guide proposals.
      \pause\item emcee: affine invariant proposals.
      \pause\item emcee is not the only (or even best) affine invariant approach.
  \end{itemize}
\end{frame}

%\begin{frame}
%  \frametitle{When MCMC fails} 
%  \framesubtitle{Burn in} 
%  \movie[width=\textwidth,height=0.52\textwidth]{%
%  \includegraphics[width=\textwidth]{movies/MCMC_burn_in_0.pdf}}{movies/MCMC_burn_in.mp4}
%\end{frame}
%\begin{frame}
%  \frametitle{When MCMC fails} 
%  \framesubtitle{Burn in} 
%  \includegraphics[width=\textwidth]{movies/MCMC_burn_in_1.pdf}
%\end{frame}
%
%\begin{frame}
%  \frametitle{When MCMC fails} 
%  \framesubtitle{Tuning the proposal distribution} 
%  \movie[width=\textwidth,height=0.52\textwidth]{%
%  \includegraphics[width=\textwidth]{movies/MCMC_proposal_0.pdf}}{movies/MCMC_proposal.mp4}
%\end{frame}
%\begin{frame}
%  \frametitle{When MCMC fails} 
%  \framesubtitle{Tuning the proposal distribution} 
%  \includegraphics[width=\textwidth]{movies/MCMC_proposal_1.pdf}
%\end{frame}
%
%\begin{frame}
%  \frametitle{When MCMC fails} 
%  \framesubtitle{Multimodality} 
%  \movie[width=\textwidth,height=0.52\textwidth]{%
%  \includegraphics[width=\textwidth]{movies/MCMC_multimodal_0.pdf}}{movies/MCMC_multimodal.mp4}
%\end{frame}
%\begin{frame}
%  \frametitle{When MCMC fails} 
%  \framesubtitle{Multimodality} 
%  \includegraphics[width=\textwidth]{movies/MCMC_multimodal_1.pdf}
%\end{frame}
%
%\begin{frame}
%  \frametitle{When MCMC fails} 
%  \framesubtitle{Phase transitions} 
%
%  \includegraphics[width=\textwidth]{figures/phase_transition}
% 
%\end{frame}

\begin{frame}
  \frametitle{The real reason these all fail} 

  \begin{itemize}
    \item<2-> MCMC does not give you evidences!
  \end{itemize}

  \begin{align}
    \onslide<3->{%
    \ev 
    &= \prob(D|M) 
    \nonumber\\
  }
    \onslide<4->{%
    &= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
  }
    \onslide<5->{%
    &= \left\langle \lik \right\rangle_\prior
    \nonumber
  }
  \end{align}
  
  \begin{itemize}
    \item<6-> MCMC fundamentally explores the posterior, and cannot average over the prior.
    \item<7-> Simulated annealing gives one possibility for computing evidences.
    \begin{itemize}
        \item<8-> Inspired by thermodynamics.
        \item<8-> Suffers from similar issues to MCMC.
        \item<8-> Unclear how to choose correct annealing schedule
    \end{itemize}
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}
\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to traditional MCMC!} 

  \pause
  New procedure: 

  \pause
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      \pause
    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\prior$). 
      \pause
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new uniform sample with higher likelihood
  \end{description}

  \pause
  Requires one to be able to uniformly within a region, subject to a {\em hard likelihood constraint}.

\end{frame}



\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}


%\begin{frame}
%  \frametitle{Nested sampling}
%  \includegraphics[width=\textwidth]{movies/NS_0.pdf}
%\end{frame}
%\begin{frame}
%  \frametitle{Nested sampling}
%  \movie[width=\textwidth,height=0.52\textwidth]{}{movies/NS.mp4}
%\end{frame}
%\begin{frame}
%  \frametitle{Nested sampling}
%  \includegraphics[width=\textwidth]{movies/NS_1.pdf}
%\end{frame}



\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}
  \foreach \pagenum in {1,...,16} {%
      \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/lesbesgue}
  }
\end{frame}



\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{Exponential volume contraction} 
  
  \begin{itemize}
      \pause
    \item At each iteration, the likelihood contour will shrink in volume by  $\approx 1/n$.
      \pause
    \item Nested sampling zooms in to the peak of the posterior {\em exponentially}.
      \pause
    \item 
      \begin{equation}
          \ev \approx \sum_i \Delta\lik_i X_{i}
      \end{equation}
      \pause
    \item 
      \begin{equation}
        X_{i+1} \approx \frac{n}{n+1}X_i, \qquad X_{0} = 1
      \end{equation}
  \end{itemize}
  
\end{frame}

%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%\foreach \pagenum in {1,...,9} {%
%  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/areas}
%}
% 
%\end{frame}
%
%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%
%  \begin{itemize}
%    \item<2-> approximate compression:
%  \end{itemize}
%  \[ 
%    \onslide<2->{\Delta \log X \sim -\frac{1}{n}} 
%    \onslide<3->{\pm \frac{1}{n}} 
%  \]
%  \[ 
%    \onslide<4->{\log X_i \sim -\frac{i}{n}}
%    \onslide<5->{\pm \frac{\sqrt{i}}{n}} 
%  \]
%  \onslide<6->{%
%  \begin{itemize}
%    \item<6-> \# of steps to get to $H$:
%  \end{itemize}
%  \[ i_H \sim n H \]
%  }
%  \onslide<7->{%
%  \begin{itemize}
%    \item estimate of volume at $H$:
%  \end{itemize}
%  \[ \log X_H \approx -H \pm \sqrt{\frac{H}{n}} \]
%}
%
%  \onslide<8->{%
%  \begin{itemize}
%    \item estimate of evidence error:
%  \end{itemize}
%  \[ \log\ev \approx \sum w_i \lik_i  \pm \sqrt{\frac{H}{n}} \]
%}
%
%\end{frame}


\begin{frame}
  \frametitle{Nested sampling} 
  \framesubtitle{Parameter estimation} 

  \begin{itemize}
      \pause
    \item NS can also be used to sample the posterior
      \pause
    \item The set of dead points are posterior samples with an appropriate weighting factor
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  \pause
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      \pause
    \item Most of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm.
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling within an iso-likelihood contour}
  \framesubtitle{Previous attempts}


  \begin{description}
    \pause\item[Rejection Sampling] MultiNest; F.\ Feroz \& M.\ Hobson (2009).
      \begin{itemize}
        \pause\item Suffers in high dimensions
      \end{itemize}
      \pause\item[Hamiltonian] M.J. Betancourt (2010) 
      \pause\item[Galilean] F.\ Feroz \& J.\ Skilling (2013) 
      \begin{itemize}
        \pause\item Requires gradients and tuning
      \end{itemize}
      \pause\item[Diffusive Nested Sampling] B.\ Brewer et al.\ (2009,2016).
      \begin{itemize}
        \pause\item Very promising
        \pause\item Still needs tuning.
      \end{itemize}
    %\pause\item[Slice Sampling] R.\ Neal (2000), Aitken \& Akman (2013).
  \end{description}

\end{frame}

\section{PolyChord}
\begin{frame}
  \frametitle{``Hit and run'' slice sampling}
\foreach \pagenum in {1,...,32} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/contour_step}
}
\end{frame}


\begin{frame}
  \frametitle{Issues with Slice Sampling}
  \framesubtitle{Correlated distributions}

  \begin{itemize}
    \pause\item Need $N$ reasonably large $\sim\mathcal{O}(n_\mathrm{dims})$ so that $x_N$ is de-correlated from $x_1$.
    \pause\item Does not deal well with correlated distributions.
    \pause\item Need to ``tune'' $w$ parameter.
  \end{itemize}
\end{frame}

\begin{frame}
,   \frametitle{PolyChord 1.0's solution}
  \framesubtitle{Correlated distributions}

  \includegraphics[width=\textwidth]{figures/contour_transform}

\end{frame}

\begin{frame}
    \frametitle{PolyChord 1.0's solutions}
  \framesubtitle{Correlated distributions}

  \begin{itemize}
    \pause\item We make an affine transformation to remove degeneracies, and ``whiten'' the space.
    \pause\item Samples remain uniformly sampled
    \pause\item We use the covariance matrix of the live points and all inter-chain points 
    \pause\item Cholesky decomposition is the required skew transformation
    \pause\item $w=1$ in this transformed space
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Issues with Slice Sampling}
  \framesubtitle{Multimodality}

  \begin{enumerate}
    \pause\item Although it satisfies detailed balance practically this isn't good enough.
    \pause\item Affine transformation is useless.
  \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{PolyChord 1.0's solutions}
  \framesubtitle{Multimodality}

  \begin{enumerate}
    \pause\item Identifies separate modes via clustering algorithm on live points.
    \pause\item Evolves these modes ``semi-independently''
  \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{PolyChord 1.0's Additions}

  \begin{itemize}
    \pause\item Parallelised up to number of live points with openMPI.
    \pause\item Implemented in CosmoMC, as ``CosmoChord'', with fast-slow parameters.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{PolyChord vs.\ MultiNest}
  \framesubtitle{Gaussian likelihood}
  \includegraphics[width=\textwidth]{figures/polychord_vs_multinest.pdf}
\end{frame}


\begin{frame}
  \frametitle{PolyChord 1.0}

  \begin{itemize}
    \pause\item Well tested.
    \pause\item arXiv:1502.01856
    \pause\item arXiv:1506.00171
    \pause\item \url{ccpforge.cse.rl.ac.uk/gf/project/polychord/}
  \end{itemize}
\end{frame}

\section{PolyChord 2.0}

\begin{frame}
  \frametitle{PolyChord}
  \framesubtitle{Scaling with dimensionality}
  \begin{itemize}
    \pause\item Perfect nested sampling has $N_\lik \sim \mathcal{O}(D^2)$
    \pause\item PolyChord 1.0 has $N_\lik \sim \mathcal{O}(D^3)$
      \begin{itemize}
        \pause\item Need $\sim \mathcal{O}(D)$ to de-correlate at each step
        \pause\item Forced to throw $\sim \mathcal{O}(D)$ inter-chain points away.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{PolyChord 2.0}
  \framesubtitle{Inter-chain evaluations}
  \begin{itemize}
    \pause\item Traditional nested sampling requires the ability to easily generate a single sample at each iteration.
    \pause\item MCMC-like approaches generate many (correlated) samples
    \pause\item At each iteration:
      \begin{itemize}
        \pause\item Throw away $n_\mathrm{chain}$ samples.
        \pause\item Generate $n_\mathrm{chain}$ new (correlated) samples to replace them.
      \end{itemize}
    \pause\item If $n_\mathrm{chain}\sim\mathcal{O}(D)$ (as required), then overall $\sim\mathcal{O}(D^3)\to\sim\mathcal{O}(D^2)$.
    \pause\item Need to be able to quantify degree of correlation for correct inference.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Aside: Merging nested sampling runs}

  \begin{itemize}
    \pause\item In his original paper, John Skilling noted that nested sampling runs can be merged.
    \pause\item Take two complete nested sampling runs generated by $\nlive^{(1)}$ and $\nlive^{(2)}$ live points.
    \pause\item Combining the two runs in likelihood order gives a new run generated by $\nlive^{(1)}+\nlive^{(2)}$ live points. 
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Aside: Unweaving nested sampling runs}

  \begin{itemize}
    \pause\item The reverse is also true.
    \pause\item Given a nested sampling run with $\nlive$ points, there is a unique way of separating it into $\nlive$ single-point runs (threads).
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{PolyChord 2.0}
  \framesubtitle{Handling correlations}

  \begin{itemize}
    \pause\item Unweave the run into $\nlive$ threads.
    \pause\item Each thread is a ``true'' nested sampling run, although threads are correlated.
    \pause\item Can use traditional techniques on threads to quantify correlation 
      \begin{itemize}
        \pause\item Batch means
        \item Jacknifing
        \item Bootstrapping
      \end{itemize}
    \pause\item With this in hand, can produce correct inferences from correlated runs.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{PolyChord 2.0 vs.\ MultiNest}
  \framesubtitle{Gaussian likelihood}
  \includegraphics[width=\textwidth]{figures/polychord_vs_multinest_1.pdf}
\end{frame}

\section{Examples}

\begin{frame}
  \frametitle{Object detection}
  \framesubtitle{Toy problem}

  \centerline{%
  \includegraphics<1>[width=\textwidth]{figures/object_detection_image}
  \includegraphics<2>[width=\textwidth]{figures/object_detection_image_no_noise}
}

\end{frame}

\begin{frame}
  \frametitle{PolyChord in action}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}


%\begin{frame}
%  \frametitle{Planck data}
%  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
%  \begin{itemize}
%    \item<2-> Temperature data TT+lowP
%    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
%    \item<4-> Marginalised plots of $\PR(k)$
%    \item<5->
%      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
%  \end{itemize}
%\end{frame}



\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knots}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{8 internal knots}
  \frametitle<10>{Bayes Factors}
  \frametitle<11>{Marginalised plot}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.9\textwidth]{figures/0TT_fgivenx}
    \includegraphics<2>[width=0.9\textwidth]{figures/1TT_fgivenx}
    \includegraphics<3>[width=0.9\textwidth]{figures/2TT_fgivenx}
    \includegraphics<4>[width=0.9\textwidth]{figures/3TT_fgivenx}
    \includegraphics<5>[width=0.9\textwidth]{figures/4TT_fgivenx}
    \includegraphics<6>[width=0.9\textwidth]{figures/5TT_fgivenx}
    \includegraphics<7>[width=0.9\textwidth]{figures/6TT_fgivenx}
    \includegraphics<8>[width=0.9\textwidth]{figures/7TT_fgivenx}
    \includegraphics<9>[width=0.9\textwidth]{figures/8TT_fgivenx}
    \includegraphics<10>[width=0.9\textwidth]{figures/Bayes_TT.pdf}
    \includegraphics<11>[width=0.9\textwidth]{figures/combined_fgivenx.pdf}

  \end{center}
\end{frame}

\begin{frame}
    \frametitle{Dark energy equation of state reconstruction}
    \begin{itemize}
        \pause\item Same thing, but for Dark energy equation of state $w(z)$ (quintessence).
        \pause\item Data used is Planck 2015, BOSS DR 11, JLA supernovae and BOSS Ly$\alpha$ data
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle<1>{Flat, variable $w$}
    \frametitle<2>{Tilted}
    \frametitle<3>{1 internal node}
    \frametitle<4>{2 internal nodes}
    \frametitle<5>{3 internal nodes}
    \frametitle<6>{Marginalised plot - just extension models}
    \frametitle<7>{Marginalised plot - including LCDM}
    \framesubtitle{Dark energy equation of state reconstruction}

    \begin{center}
        \includegraphics<1>[width=0.9\textwidth]{figures/wCDM_1000Nlive_wgiivenz.pdf}
        \includegraphics<2>[width=0.9\textwidth]{figures/tCDM_100Nlive_wgiivenz.pdf}
        \includegraphics<3>[width=0.9\textwidth]{figures/1CDM_1000Nlive_wgiivenz.pdf}
        \includegraphics<4>[width=0.9\textwidth]{figures/2CDM_1000Nlive_wgiivenz.pdf}
        \includegraphics<5>[width=0.9\textwidth]{figures/3CDM_1000Nlive_wgiivenz.pdf}
        \includegraphics<6>[width=0.9\textwidth]{figures/extensionModels_1000Nlive_wgivenz.pdf}
        \includegraphics<7>[width=0.9\textwidth]{figures/allModels_1000Nlive_wgivenz.pdf}
    \end{center}
\end{frame}



\begin{frame}
  \frametitle{PolyChord 2.0}
  \begin{itemize}
    \pause\item Using intermediate points so $\sim\mathcal{O}(D^3)\to\sim\mathcal{O}(D^2)$.
    \pause\item Unweaving runs to quantify correlations.
    \pause\item Affine invariant sampling.
  \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Future work}
  \begin{enumerate}
    \pause\item Parallelisation
    \pause\item Affine invariant mode detection.
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Affine invariance}
  \begin{itemize}
    \pause\item The optimal exploration technique is be affine invariant.
    \pause\item Treat distribution $\prob(\mathbf{x})$ and $\prob(R \mathbf{x})$ the same.
    \pause\item No need to worry about correlations.
    \pause\item Good example: Now highly successful emcee (MCMC hammer).
      \begin{itemize}
        \item Important: emcee is not unique (or necessarily best)
      \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Skillings affine invariant ideas}
  \framesubtitle{Leapfrog}
\foreach \pagenum in {1,...,6} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/leapfrog}
}
\end{frame}

\begin{frame}
  \frametitle{Skillings affine invariant ideas}
  \framesubtitle{Parallel walk}
\foreach \pagenum in {1,...,6} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/parallel_walk}
}
\end{frame}

\begin{frame}
  \frametitle{Affine invariance}
  \framesubtitle{Subspace collapse}
  \begin{itemize}
    \pause\item The main problem that besets these techniques is ``subspace collapse''.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Subspace collapse}
  \framesubtitle{Leapfrog}
  \includegraphics[width=\textwidth,page=5]{figures/leapfrog}
\end{frame}

\begin{frame}
  \frametitle{Subspace collapse}
  \framesubtitle{Parallel walk}
  \includegraphics[width=\textwidth,page=7]{figures/parallel_walk}
\end{frame}

\begin{frame}
  \frametitle{Subspace collapse}
  \framesubtitle{Solution}
  \begin{itemize}
    \pause\item Need to use $\sim\mathcal{O}(D)$ points to avoid this.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Skillings affine invariant ideas}
  \framesubtitle{Guided walk}
\foreach \pagenum in {1,...,8} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/guided_walk}
}
\end{frame}

\begin{frame}
  \frametitle{Affine invariant}
  \framesubtitle{Other variations}
  \begin{itemize}
    \pause\item Generalise guided walk to $D$ dimensions (slice through the mean of $D$ other points).
    \pause\item Slice through a ``random'' linear combination of $D$ points.
    \pause\item Slice through a ``random'' linear combination of all points
    \pause\item There are lots of variations: This is an underused area of the field.
  \end{itemize}
\end{frame}

\end{document}
```
# branch: remotes/origin/madrid_2017
```tex
\documentclass[%
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}

% todo 
% - Ligo actual data
% -define IMRPhenom, EOBNR


\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\mathrm{knots}}
\newcommand{\nlive}{n_\mathrm{live}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}





\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{Statistics}
\subtitle{The IFT School on Cosmology Tools}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{March 13, 2017}



\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\section{Bayesian Inference}
\begin{frame}
    \frametitle{Introduction}
    \begin{itemize}
        \pause\item Statistics $\equiv$ Inference \pause $\equiv$ Machine Learning/AI.
        \pause\item How to extract information about scientific models from data.
        \pause\item Most cosmologists work in a {\em Bayesian\/} framework of inference, although {\em Frequentist\/} methods are also sometimes used.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Probability distributions}

    \begin{itemize}
        \pause\item As scientists, we are used to seeing error bars on results.
        \pause\item Age of the universe ({\em Planck\/}): 
        \pause \[13.73\pm 0.12\:\text{billion years old.}\]
        \pause\item Masses of LIGO GW150914 binary merger: 
        \[m_1 = 39.4^{+5.5}_{-4.9}\:M_\odot,\qquad m_2 = 30.9^{+4.8}_{-4.4}\:M_\odot \]
        \pause\item These are called {\em credible intervals}, state that we are e.g.\ $90\%$ confident of the value lying in this range.
        \pause\item More importantly, these are {\em summary statistics}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{LIGO binary merger}
    \begin{columns}
        \begin{column}{0.65\textwidth}
            \includegraphics[width=\textwidth]{./figures/ligo_m1_m2.pdf}
        \end{column}
        \begin{column}{0.35\textwidth}
            \begin{itemize}
                \pause\item Summary statistics summarise a full probability distribution.
                \pause\item One goal of inference is to produce these probability distributions.
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Extended example of inference: LIGO}
    \begin{itemize}
        \item We will introduce the key concepts by discussing an extended example of the inference process.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Theory}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics[width=\textwidth]{./figures/ligo_schematic.png}
\end{frame}

\begin{frame}
    \frametitle{The model $M$}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics[width=\textwidth]{./figures/ligo_model.pdf}
\end{frame}

\begin{frame}
    \frametitle{The parameters $\Theta$ of the model $M$}
    \framesubtitle{Extended example of inference: LIGO}
    Theoretical signal depends on:
    \begin{itemize}
        \pause\item $m_1, m_2$: mass of binary
        \pause\item $\theta, \phi$: sky location
        \pause\item $r$: luminosity distance 
        \pause\item $\Phi_c, t_c$: phase and time of coalescence
        \pause\item $i, \theta_\mathrm{sky}$: inclination and angle on sky (orbital parameters)
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The data $D$}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics<1>[width=\textwidth]{./figures/ligo_data.pdf}
    \includegraphics<2>[width=\textwidth]{./figures/ligo_actual.png}
\end{frame}

\begin{frame}
    \frametitle{The likelihood $L(\Theta)$}
    \begin{itemize}
        \item<1-> We assume that observed signal is combination of theoretical signal and Gaussian noise $O = S + N$.
        \item<2-> Elementary probability tells us:
            \only<3->{%
            \[ 
                \only<7->{L(\Theta) =}
                P(\only<3-5>{h_i}\only<6->{D}|\Theta,M) = 
                \only<6->{\prod_i}
                \frac{1}{\sqrt{2\pi}\sigma_i} \exp{\left( -\frac{[h_i-h(t_i;\Theta)]^2}{2\sigma_i^2} \right)} 
            \]
            }
            \begin{itemize}
                \item<4-> $(t_i,h_i\pm\sigma_i)$: strain observed 
                \item<5-> $h(t;\Theta)$: Theoretical strain 
            \end{itemize}
        \item<8-> We normally work with log-likelihoods, which turn $\prod\to\sum$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle<1>{The Likelihood: well matched}
    \frametitle<2>{The Likelihood: coalescence off}
    \frametitle<3>{The Likelihood: too large luminosity distance}
    \frametitle<4>{The Likelihood: incorrect inclination}
    \framesubtitle{Extended example of inference: LIGO}
    \only<1>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood.pdf}}
    \only<2>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood_t.pdf}}
    \only<3>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood_r.pdf}}
    \only<4>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood_i.pdf}}
\end{frame}

\begin{frame}
    \frametitle{Bayes' Theorem}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{itemize}
        \item Likelihood \pause$\equiv$ Probability of data, given model parameters: 
            \pause\[L(\Theta) \equiv P(D|\Theta,M)\] 
        \pause \item What we want: 
        \item {\em Posterior} \pause$\equiv$ Probability of model parameters, given data:
            \pause\[\mathcal{P}(\Theta) \equiv P(\Theta|D,M)\] 
            \pause \item Use Bayes theorem:
            \pause \[ P(\Theta|D,M) = \frac{P(D|\Theta, M) P(\Theta|M)}{P(D|M)}\]
            \pause \[ \mathcal{P}(\Theta) = \frac{L(\Theta) \pi(\Theta)}{Z}\]
            \pause \[ \text{Posterior} = \frac{\text{Likelihood $\times$ Prior}}{\text{Evidence}}\]
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Prior $\pi(\Theta)$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{align}
        P(\Theta|D,M) &= \frac{P(D|\Theta, M) P(\Theta|M)}{P(D|M)}\nonumber\\
        \mathcal{P}(\Theta) &= \frac{L(\Theta) \pi(\Theta)}{Z}\nonumber
    \end{align}
    \begin{itemize}
        \pause\item Prior: $\pi(\Theta) \equiv P(\Theta|M)$.
        \pause\item Represents initial degree of knowledge about the system:
        \pause\item e.g. $\pi(\theta,\phi)\: d\theta d\phi = -\frac{1}{2}\sin\theta\: d\theta d\phi$
        \pause\item Most Bayesian approaches are sensitive to this, and rightly so.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Evidence $Z$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{align}
        P(\Theta|D,M) &= \frac{P(D|\Theta, M) P(\Theta|M)}{P(D|M)}\nonumber\\
        \mathcal{P}(\Theta) &= \frac{L(\Theta) \pi(\Theta)}{Z}\nonumber
    \end{align}
    \begin{itemize}
        \item<1-> Evidence: $Z \equiv P(D|M) 
            \only<2>{=\int P(D|\Theta,M)P(\Theta|M) d\Theta.}
            \only<3->{=\int L(\Theta)\pi(\Theta) d\Theta.}$
        \item<4-> Normalising constant.
        \item<5-> Difficult to compute.
        \item<6-> Still extremely important.
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{itemize}
        \item Cannot plot the full posterior distribution:
            \[\mathcal{P}(\Theta) \equiv P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\mathrm{sky}|D,M)\]
        \item Can plot 1D and 2D {\em marginalised\/} distributions e.g:
            \begin{align}
            &P(m_1,m_2|D,M)=\nonumber\\&\int P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\mathrm{sky}|D,M) \,d\theta \,d\phi \,dr \,d\Phi_c \,d t_c \,d i \,d\theta_\mathrm{sky}\nonumber
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}

	\begin{columns}
	\begin{column}{0.65\textwidth}
        \only<1->{\includegraphics[width=\textwidth]{./figures/ligo_m1_m2.pdf}}
	\end{column}
	\begin{column}{0.35\textwidth}
		\begin{itemize}
          \item<2-> May do this for each pair of parameters
          \item<3-> Generates a {\em triangle plot}
		\end{itemize}
	\end{column}
	\end{columns}

\end{frame}

\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}
	\begin{columns}
	\begin{column}{0.7\textwidth}
        \only<1->{\includegraphics[width=\textwidth]{./figures/ligo_full.pdf}}
	\end{column}
	\begin{column}{0.3\textwidth}
		\begin{itemize}
          \item<2-> Does give insight
          \item<3-> Not the full picture
		\end{itemize}
	\end{column}
	\end{columns}
\end{frame}

\begin{frame}
    \frametitle{Evidences and model comparison}
    \framesubtitle{Extended example of inference: LIGO}

    \begin{itemize}
        \pause\item Up until now, we have discussed {\em Parameter estimation\/}: inferring what data tell us about parameters $\Theta$ of a model $M$.
        \pause\item Scientifically speaking, this is only half the story.
        \pause\item In general, we will have several competing models that describe the data, and we want to know which is the ``best''.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Bayes' Theorem (again)}

    \pause
    What does data tell us about our model $M_i$?
    \pause
    \[\prob(M_i|D) = \frac{\prob(D|M_i) \prob(M_i) }{ \prob(D) }\] 

    \pause
    \[\prob(M_i|D) = \frac{Z_i \: \mu_i}{\sum_k Z_k \: \mu_k}\] 

    \begin{itemize}
    \pause \item Evidences allow us to determine what weighting we should give to each model in light of the data.\\
    \pause \item e.g.\ EOBNR (Effective One Body) vs IMRPhenom (Inspiral-merger-ringdown).\\
    \pause \item Or testing modified theories of gravity.
    \end{itemize}
    

    \pause
    \textbf{Model averaging:}
    \begin{itemize}
        \item Multiple models with posterior on the same parameter: ${\prob(y|M_i,D)}$
    \end{itemize}
    \[\prob(y|D) = \sum_i \prob(y|M_i,D) \prob(M_i|D)\]

\end{frame}


\begin{frame}
    \frametitle{Parameter estimation}
    \framesubtitle{Another example.}

    \[\lik(\Theta) = P(D|\Theta,M)\]
    \begin{align}
        \onslide<2->{D =& \{C_\ell\only<6->{^\mathrm{(Planck)}}\}} 
        \onslide<15->{+\{\mathrm{LSS}\}} 
        \onslide<16->{+\{\text{``Big Data''}\}}
        \nonumber\\
        \onslide<3->{M =& \Lambda\mathrm{CDM}} 
        \onslide<9->{+ \mathrm{extensions} }
        \nonumber\\
        \onslide<4->{\Theta =& \Theta_{\Lambda \mathrm{CDM}}} \onslide<7->{+ \Theta_\mathrm{Planck}} \onslide<10->{+ \Theta_\mathrm{extensions}}\nonumber\\
        \onslide<5->{\Theta_{\Lambda \mathrm{CDM}} =& ( \Omega_b h^2, \Omega_c h^2, 100\theta_{MC}, \tau, {\rm{ln}}(10^{10} A_s), n_s) \nonumber\\}
        \onslide<8->{\Theta_\mathrm{Planck} =& (y_{\rm cal}, A^{CIB}_{217}, \xi^{tSZ-CIB}, A^{tSZ}_{143}, A^{PS}_{100}, A^{PS}_{143}, A^{PS}_{143\times 217}, A^{PS}_{217},\nonumber\\
        &A^{kSZ}, A^{{\rm dust}TT}_{100}, A^{{\rm dust}TT}_{143}, A^{{\rm dust}TT}_{143\times 217}, A^{{\rm dust}TT}_{217}, c_{100}, c_{217}) \nonumber\\}
        \onslide<11->{\Theta_\mathrm{extensions} =& (
                n_{\rm run}
                \only<12->{,n_{\rm run,run}}
                \only<13->{,w}
                \only<14->{,\Sigma m_\nu, m_{\nu,{\rm{sterile}}}^{\rm{eff}}}
        ) \nonumber}
    \end{align}

    \begin{itemize}
        \item<17->{Parameter estimation: $L, \pi \to \mathcal{P}$: model parameters}
        \item<17->{Model comparison: $L, \pi \to Z$: how good model is}
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Sampling}
    \framesubtitle{How to describe a high-dimensional posterior}

	\begin{columns}
	\begin{column}{0.5\textwidth}
		\begin{itemize}
          \item<3-> In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item<4-> {\em Sampling\/} the posterior is an excellent compression scheme.
		\end{itemize}
	\end{column}
	\begin{column}{0.5\textwidth}
        \only<2->{\includegraphics[width=\textwidth]{./figures/ligo_m1_m2.pdf}}
	\end{column}
	\end{columns}
 
\end{frame}

\begin{frame}
    \frametitle{Why do sampling?}
    \framesubtitle{Marginalisation over the posterior}

    \begin{itemize}
        \item<1-> Set of $N$ samples $S = \{\Theta^{(i)}: i=1,\ldots N:\: \Theta^{(i)}\sim\mathcal{P}\}$
        \item<2-> Mean mass: \[
                \bar{m}_1 \equiv\langle m_1\rangle_\mathcal{P} \approx 
                \only<2-6>{\frac{1}{N}\sum_{i=1}^N m_1^{(i)}}
                \only<7->{\frac{\sum_{i=1}^N w^{(i)} m_1^{(i)}}{\sum_{i=1}^N w^{(i)}}}
            \]
        \item<3-> Mass covariance: \[
                \mathrm{Cov}(m_1,m_2) \approx 
                \only<3-6>{\frac{1}{N}\sum_{i=1}^N (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}
                \only<7->{\frac{\sum_{i=1}^N (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}{\sum_{i=1}^N w^{(i)}}}
            \]
        \item<4-> Marginalised samples: Just ignore the other coordinates.
        \item<5-> N.B. Typically have {\em weighted\/} samples
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Parameter estimation}
    \begin{itemize}
        \pause\item The name of the game is therefore drawing samples $S$ from the posterior $\mathcal{P}$ with the minimum number of likelihood calls.
        \pause\item Gridding is doomed to failure in high dimensions.
        \pause\item Enter Metropolis Hastings.
    \end{itemize}
\end{frame}



\section{Metropolis Hastings}

\begin{frame}
    \frametitle{Current sampling approaches}
    \begin{enumerate}
        \pause\item Metropolis Hastings.
        \pause\item Hamiltonian Monte-Carlo (HMC).
        \pause\item Ensemble sampling (e.g.\ emcee).
    \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Hastings} 
  \begin{itemize}
      \pause
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \begin{enumerate}
          \pause
        \item Pick random direction
          \pause
        \item Choose step length
          \pause
        \item If uphill, make step\ldots
          \pause
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Hastings} 
  \includegraphics[width=\textwidth]{movies/MCMC_0.pdf}
\end{frame}
\begin{frame}
  \frametitle{Metropolis Hastings} 
  \movie[width=\textwidth,height=0.52\textwidth]{}{movies/MCMC.mp4}
\end{frame}
\begin{frame}
  \frametitle{Metropolis Hastings} 
  \includegraphics[width=\textwidth]{movies/MCMC_1.pdf}
\end{frame}


\begin{frame}
  \frametitle{Metropolis Hastings} 
  \framesubtitle{More formally:}
  \begin{description}
      \item<1->[Step 0]{Choose initial set of parameters $\Theta^{(0)}$, set $w^{(0)}=1$}
      \item<2->[Step i]
          \begin{itemize}
              \item<3-> Propose new point $\Theta^{\mathrm{new}}$ from some proposal distribution $Q(\Theta^{\mathrm{new}};\Theta^{(i)})$
                  \begin{itemize}
                      \item<4-> e.g:\: a point in a random direction, distance $d$ away from $\Theta^{(i)}$.
                      \item<5-> $Q$ is typically symmetric.
                  \end{itemize}
              \item<6-> ``Accept'' this point with probability $\propto \mathcal{P}(\Theta^{(\mathrm{new})})/\mathcal{P}(\Theta^{(i)}) \times Q(\Theta^{(i)}) / Q(\Theta^{(\mathrm{new})}) $
              \item<7-> if accepted, $\Theta^{(i+1)} = \Theta^{(\mathrm{new})}$, $i+=1$
              \item<8-> Otherwise, reject, $w^{(i)}+=1$ and repeat.
          \end{itemize}
  \end{description}
\end{frame}

\begin{frame}
\end{frame}


\begin{frame}
  \frametitle{Metropolis Hastings} 
  \framesubtitle{Struggles with\ldots}
  \begin{enumerate}
      \pause\item Burn in
      \pause\item Multimodality
      \pause\item Correlated Peaks
      \pause\item Phase transitions
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte-Carlo} 
  \begin{itemize}
      \pause\item Key idea: Treat $\log L(\Theta)$ as a potential energy
      \pause\item Guide walker under ``force'': \[F(\Theta) =\nabla \log L(\Theta)\]
      \pause\item Walker is naturally ``guided'' uphill
      \pause\item Conserved quantities mean efficient acceptance ratios.
      \pause\item stan is a fully fledged, rapidly developing programming language with HMC as a default sampler.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Ensemble sampling} 
  \begin{itemize}
      \pause\item Instead of one walker, evolve a set of $n$ walkers.
      \pause\item Can use information present in ensemble to guide proposals.
      \pause\item emcee: affine invariant proposals.
      \pause\item emcee is not the only (or even best) affine invariant approach.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The fundamental issue with all of the above} 

  \begin{itemize}
    \item They don't give you evidences!
  \end{itemize}

  \begin{align}
    \ev 
    &= \prob(D|M) 
    \nonumber\\
    \pause&= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
    \pause&= \left\langle \lik \right\rangle_\prior
    \nonumber
  \end{align}
  
  \begin{itemize}
    \pause\item MCMC fundamentally explores the posterior, and cannot average over the prior.
    \pause\item Simulated annealing gives one possibility for computing evidences.
    \begin{itemize}
        \pause\item Inspired by thermodynamics.
        \pause\item Suffers from similar issues to MCMC.
        \pause\item Unclear how to choose correct annealing schedule
    \end{itemize}
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}

\begin{frame}
  \frametitle{What is nested sampling?}
  \begin{itemize}
    \pause\item Nested sampling is an alternative way of sampling posteriors. 
    \pause\item Uses ensemble sampling to compress prior to posterior.
    \pause\item In doing so, it circumvents many issues (dimensionality, topology, geometry) that beset standard approaches.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to traditional MCMC!} 

  \pause
  New procedure: 

  \pause
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      \pause
    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\prior$). 
      \pause
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new uniform sample with higher likelihood
  \end{description}

  \pause
  Requires one to be able to uniformly within a region, subject to a {\em hard likelihood constraint}.

\end{frame}



\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}


%\begin{frame}
%  \frametitle{Nested sampling}
%  \includegraphics[width=\textwidth]{movies/NS_0.pdf}
%\end{frame}
%\begin{frame}
%  \frametitle{Nested sampling}
%  \movie[width=\textwidth,height=0.52\textwidth]{}{movies/NS.mp4}
%\end{frame}
%\begin{frame}
%  \frametitle{Nested sampling}
%  \includegraphics[width=\textwidth]{movies/NS_1.pdf}
%\end{frame}




%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%\foreach \pagenum in {1,...,9} {%
%  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/areas}
%}
% 
%\end{frame}
%
%\begin{frame}
%  \frametitle{Estimating evidences} 
%  \framesubtitle{Evidence error} 
%
%
%  \begin{itemize}
%    \item<2-> approximate compression:
%  \end{itemize}
%  \[ 
%    \onslide<2->{\Delta \log X \sim -\frac{1}{n}} 
%    \onslide<3->{\pm \frac{1}{n}} 
%  \]
%  \[ 
%    \onslide<4->{\log X_i \sim -\frac{i}{n}}
%    \onslide<5->{\pm \frac{\sqrt{i}}{n}} 
%  \]
%  \onslide<6->{%
%  \begin{itemize}
%    \item<6-> \# of steps to get to $H$:
%  \end{itemize}
%  \[ i_H \sim n H \]
%  }
%  \onslide<7->{%
%  \begin{itemize}
%    \item estimate of volume at $H$:
%  \end{itemize}
%  \[ \log X_H \approx -H \pm \sqrt{\frac{H}{n}} \]
%}
%
%  \onslide<8->{%
%  \begin{itemize}
%    \item estimate of evidence error:
%  \end{itemize}
%  \[ \log\ev \approx \sum w_i \lik_i  \pm \sqrt{\frac{H}{n}} \]
%}
%
%\end{frame}


\begin{frame}
  \frametitle{Nested sampling} 

  \begin{itemize}
    \pause\item The set of dead points are posterior samples with an appropriate weighting factor
    \pause\item They can also be used to calculate evidences, since it sequentially updates the priors.
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  \pause
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      \pause
    \item Most of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm.
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling within an iso-likelihood contour}
  \framesubtitle{Previous attempts}


  \begin{description}
    \pause\item[Rejection Sampling] MultiNest; F.\ Feroz \& M.\ Hobson (2009).
      \begin{itemize}
        \pause\item Suffers in high dimensions
      \end{itemize}
      \pause\item[Hamiltonian] M.J. Betancourt (2010) 
      \pause\item[Galilean] F.\ Feroz \& J.\ Skilling (2013) 
      \begin{itemize}
        \pause\item Requires gradients and tuning
      \end{itemize}
      \pause\item[Diffusive Nested Sampling] B.\ Brewer et al.\ (2009,2016).
      \begin{itemize}
        \pause\item Very promising
        \pause\item Still needs tuning.
      \end{itemize}
      \pause\item[Slice Sampling] PolyChord; Handley et al.\ (2015).
      \begin{itemize}
          \pause\item Current ``state-of-the-art''.
          \pause\item PolyChord 2.0 imminent.
      \end{itemize}
  \end{description}

\end{frame}


\section{Examples}
\begin{frame}
  \frametitle{Object detection}
  \framesubtitle{Toy problem}

  \centerline{%
  \includegraphics<1>[width=\textwidth]{figures/object_detection_image}
  \includegraphics<2>[width=\textwidth]{figures/object_detection_image_no_noise}
}

\end{frame}

\begin{frame}
  \frametitle{PolyChord in action}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}


%\begin{frame}
%  \frametitle{Planck data}
%  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
%  \begin{itemize}
%    \item<2-> Temperature data TT+lowP
%    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
%    \item<4-> Marginalised plots of $\PR(k)$
%    \item<5->
%      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
%  \end{itemize}
%\end{frame}



\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knots}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{8 internal knots}
  \frametitle<10>{Bayes Factors}
  \frametitle<11>{Marginalised plot}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.9\textwidth]{figures/0TT_fgivenx}
    \includegraphics<2>[width=0.9\textwidth]{figures/1TT_fgivenx}
    \includegraphics<3>[width=0.9\textwidth]{figures/2TT_fgivenx}
    \includegraphics<4>[width=0.9\textwidth]{figures/3TT_fgivenx}
    \includegraphics<5>[width=0.9\textwidth]{figures/4TT_fgivenx}
    \includegraphics<6>[width=0.9\textwidth]{figures/5TT_fgivenx}
    \includegraphics<7>[width=0.9\textwidth]{figures/6TT_fgivenx}
    \includegraphics<8>[width=0.9\textwidth]{figures/7TT_fgivenx}
    \includegraphics<9>[width=0.9\textwidth]{figures/8TT_fgivenx}
    \includegraphics<10>[width=0.9\textwidth]{figures/Bayes_TT.pdf}
    \includegraphics<11>[width=0.9\textwidth]{figures/combined_fgivenx.pdf}

  \end{center}
\end{frame}

\begin{frame}
    \frametitle{Dark energy equation of state reconstruction}
    \begin{itemize}
        \pause\item Same thing, but for Dark energy equation of state $w(z)$ (quintessence).
        \pause\item Data used is Planck 2015, BOSS DR 11, JLA supernovae and BOSS Ly$\alpha$ data
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle<1>{Flat, variable $w$}
    \frametitle<2>{Tilted}
    \frametitle<3>{1 internal node}
    \frametitle<4>{2 internal nodes}
    \frametitle<5>{3 internal nodes}
    \frametitle<6>{Marginalised plot - just extension models}
    \frametitle<7>{Marginalised plot - including LCDM}
    \framesubtitle{Dark energy equation of state reconstruction}

    \begin{center}
        \includegraphics<1>[width=0.9\textwidth]{figures/wCDM_1000Nlive_wgiivenz.pdf}
        \includegraphics<2>[width=0.9\textwidth]{figures/tCDM_100Nlive_wgiivenz.pdf}
        \includegraphics<3>[width=0.9\textwidth]{figures/1CDM_1000Nlive_wgiivenz.pdf}
        \includegraphics<4>[width=0.9\textwidth]{figures/2CDM_1000Nlive_wgiivenz.pdf}
        \includegraphics<5>[width=0.9\textwidth]{figures/3CDM_1000Nlive_wgiivenz.pdf}
        \includegraphics<6>[width=0.9\textwidth]{figures/extensionModels_1000Nlive_wgivenz.pdf}
        \includegraphics<7>[width=0.9\textwidth]{figures/allModels_1000Nlive_wgivenz.pdf}
    \end{center}
\end{frame}


\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}
  \foreach \pagenum in {1,...,16} {%
      \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/lesbesgue}
  }
\end{frame}



\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{Exponential volume contraction} 
  
  \begin{itemize}
      \pause
    \item At each iteration, the likelihood contour will shrink in volume by  $\approx 1/n$.
      \pause
    \item Nested sampling zooms in to the peak of the posterior {\em exponentially}.
      \pause
    \item 
      \begin{equation}
          \ev \approx \sum_i \Delta\lik_i X_{i}
      \end{equation}
      \pause
    \item 
      \begin{equation}
        X_{i+1} \approx \frac{n}{n+1}X_i, \qquad X_{0} = 1
      \end{equation}
  \end{itemize}
  
\end{frame}
\end{document}
```
# branch: remotes/origin/asterics_2018
```tex
\documentclass[%
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
%\usepackage[colorlinks=false,allbordercolors={0 0 0},pdfborderstyle={/S/U/W 1}]{hyperref}

\usepackage{cambridge_lecture}
\hypersetup{colorlinks=false,pdfborderstyle={/S/U/W 1}}

% todo 
% - Ligo actual data
% -define IMRPhenom, EOBNR


\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\text{knots}}
\newcommand{\nlive}{n_\text{live}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

%Nested Sampling: an efficient and robust Bayesian inference tool for
%astrophysics and cosmology
%
%Nested sampling is an alternative MCMC technique for integrating and exploring
%probability distributions. It has become widely adopted in the field of
%cosmology as the de-facto tool for computing Bayesian evidences and sampling
%challenging a-priori unknown parameter spaces.
%
%In this talk, I will give an introduction to the principles of Bayesian model
%comparison and parameter estimation, an explanation of the theory of nested
%sampling, a survey of the current state-of-the art (MultiNest, PolyChord,
%DNest and Dynesty) and the future of the field. Throughout I will illustrate
%with examples of it's application in cosmology and astrophysics, ranging from
%inflationary physics to exoplanets.





\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{Bayesian statistics}
\subtitle{Third ASTERICS-OBELICS Workshop}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Kavli Institute for Cosmology\\
Cavendish Laboratory (Astrophysics Group) \\
University of Cambridge
}
\date{October 24, 2018}

\include{include/beamer_commands}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Inference in cosmology: parameter estimation}
    \begin{figright}[0.4]{./figures/age_size.pdf}
        \begin{itemize}
            \item Cosmologists infer universe parameters from data
            \item Bayesian framework: Use probability distributions to quantify errors
            \item Inferences depend on models ($\Lambda$CDM)
            \item \href{https://arxiv.org/abs/1807.06209}{arXiv:1807.06209}
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
  \frametitle{Inference in cosmology: model comparison}
    \begin{figright}[0.4]{./figures/age_size_curved.pdf}
        \includegraphics[width=\textwidth]{./figures/shape-of-the-universe}
        \begin{itemize}
            \item Green model includes curvature (c$\Lambda$CDM)
            \item Age and size now correlated
            \item Measurement less precise
            \item Flat is better with 2:1 odds against curvature
        \end{itemize}
    \end{figright}
\end{frame}

\section{Fitting a line to data}
\begin{frame}
    \frametitle{Motivating example: Fitting a line to data}
    \begin{figright}[0.4]{./figures/data_points.pdf}
        \begin{itemize}
            \item We have noisy data $D$
            \item We wish to fit a model $M$
            \item Functional form $y=f_M(x;\theta)$
            \item For example:
                \begin{align}
                     f_\text{linear}(x;\theta)&=a x + b       \nonumber\\
                     f_\text{quadratic}(x;\theta)&=a x^2 + b  \nonumber
                \end{align}
            \item Model parameters $\theta= (a,b)$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ best-fit}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff.pdf}
        \begin{itemize}
            \item For each parameter set $\theta$:
                \[
                    \chi^2(\theta) = \sum_i \left|y_i - f(x_i;\theta)\right|^2
                \]
            \item Minimise $\chi^2$ wrt $\theta$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ with non-uniform data errors}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data.pdf}
        \begin{itemize}
            \item If data have non-uniform errors:
                \[
                    \chi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|^2}{\sigma_i^2}
                \]
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Problems with $\chi^2$}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff_2.pdf}
        \begin{itemize}
            \item Why square the errors? -- could take absolute:
                \[
                    \psi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|}{\sigma_i}
                \]
            \item How do we differentiate between models, e.g. quadratic vs curved
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Probability distributions}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.6]{./figures/data_diff_1.pdf}
        \begin{itemize}
            \item The probability of observing a datum:
                \[
                    P(y_i | \theta,M) = \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right)
                \]
            \item The probability of observing the data:
                \begin{align}
                    P(D | \theta,M) &= \prod_i \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right) \nonumber\\
                    &=  \frac{1}{\prod_i\sqrt{2\pi}\sigma_i}\exp\sum_i{-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}} \nonumber\\
                    &\propto e^{-\chi^2(\theta)/2}
                    \nonumber
                \end{align}
        \end{itemize}
    \end{figright}
\end{frame}



\begin{frame}
    \frametitle{Maximum likelihood}
    \framesubtitle{Fitting lines to data}
    \begin{figleft}[0.6]{./figures/data_diff.pdf}
        \begin{itemize}
            \item Minimising $\chi^2(\theta)$  is equivalent to maximising $P(D|\theta,M) \propto e^{-\chi^2(\theta)/2}$
            \item $P(D|\theta,M)$ is called the Likelihood $L=L(\theta)$ of the parameters $\theta$
            \item ``Least squares'' $\equiv$ ``maximum likelihood'' \\(if data are gaussian).
            \item \href{https://arxiv.org/abs/1809.04598}{arXiv:1809.04598}
        \end{itemize}
    \end{figleft}
\end{frame}

\begin{frame}
    \frametitle{Bayesian inference}
    \begin{itemize}
        \item Likelihood $L=P(D|\theta,M)$ is undeniably correct.
        \item Frequentists construct inference techniques purely from this function.
        \item The trend is cosmology is to work with a Bayesian approach.
        \item What we want are things like $P(\theta|D,M)$ and $P(M|D)$.
        \item To invert the conditionals, we need Bayes theorem:
            \begin{align}
                P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
                P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Terminology}
    \framesubtitle{Bayesian inference}
    \begin{align}
        P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
        \text{Posterior} &= \frac{\text{Likelihood}\times\text{Prior}}{\text{Evidence}} \nonumber
    \end{align}
    \begin{align}
        P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber\\
        \text{Model probability} &= \frac{\text{Evidence}\times\text{Model Prior}}{\text{Normalisation}} \nonumber
    \end{align}
\end{frame}

\begin{frame}
    \frametitle{The prior}
    \framesubtitle{Example: Biased coins}
    \begin{itemize}
        \item Need to define the \textbf{Prior} $P(\theta)$ --- probability of the bias, given no data
        \item Represents our knowledge of parameters before the data -- subjective
        \item Frequentists view this as a flaw in Bayesian inference. 
        \item Bayesians view this as an advantage
        \item Fundamental rule of Inference:\pause\\
            \vfill
            \begin{center}
                \Large You cannot extract information from data\\ without making assumptions 
            \end{center}
            \vfill
        \item All Bayesians do is make them explicit
        \item Any method that claims it is ``objective'' is simply hiding them
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Parameter estimation}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.3]{./figures/parameters.pdf}
        \begin{itemize}
            \item We may use $P(\theta|D,M)$ to inspect whether a model looks reasonable
        \end{itemize}
        \includegraphics[width=\textwidth]{./figures/data.pdf}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Predictive posterior}
    \begin{figright}[0.25]{./figures/fgivenx.pdf}
        More useful to plot:
        \begin{align}
        &P(y|x) = \nonumber\\
        &\int P(y|x,\theta) P(\theta) d\theta \nonumber
        \end{align}
        (all conditioned on $D,M$)
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Model comparison}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.33]{./figures/evidences_lin.pdf}
        \begin{itemize}
            \item We may use the Bayesian evidence $Z$ to determine whether a model is reasonable.
            \item $Z = P(D|M) = \int P(D|M,\theta)P(\theta|M)d\theta$
            \item The evidence quantifies Occam's razor, penalising over-fitted models with too many parameters.
            \item Normally assume uniform model priors $Z \propto P(M|D)P(M)$.
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Line fitting (context)}
    \begin{figright}[0.5]{./figures/supernovae.pdf}
        \begin{itemize}
            \item Whilst this model seems a little trite\ldots
            \item\ldots determining polynomial indices \\$\equiv$ determining cosmological material content:
        \end{itemize}
    \end{figright}
        \[
            {\left( \frac{H}{H_0} \right)}^2 = 
            \Omega_\text{r} {\left( \frac{a_0}{a} \right)}^4+
            \Omega_\text{m} {\left( \frac{a_0}{a} \right)}^3+
            \Omega_k {\left( \frac{a_0}{a} \right)}^2+
            \Omega_\Lambda
            \]
\end{frame}



\begin{frame}
    \frametitle{Quantifying error with Probability}

    \begin{figright}[0.5]{./figures/ligo_m1_m2.pdf}
        \begin{itemize}
            \item As scientists, we are used to seeing error bars on results.
            \item Masses of LIGO GW150914 binary merger: 
            \[m_1 = 39.4^{+5.5}_{-4.9}\:M_\odot\]
            \[ m_2 = 30.9^{+4.8}_{-4.4}\:M_\odot \]
            \item These are called {\em credible intervals}, state that we are e.g.\ $90\%$ confident of the value lying in this range.
            \item More importantly, these are {\em summary statistics}.
        \end{itemize}
    \end{figright}
\end{frame}


\begin{frame}
    \frametitle{Sampling}
    \framesubtitle{How to describe a high-dimensional posterior}

    \begin{figright}[0.4]{./figures/age_size_curved.pdf}
		\begin{itemize}
          \item In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item Gridding is doomed to failure for $D\gtrsim4$.
          \item {\em Sampling\/} the posterior is an excellent compression scheme.
          \item Name of the game: \\ Constructing algorithms to generate samples with a minimum number of likelihood calls
		\end{itemize}
	\end{figright}
\end{frame}

\begin{frame}
    \frametitle{Sampling}
    \framesubtitle{How to describe a high-dimensional posterior}

    \begin{figright}[0.4]{./figures/age_size_curved_samples.pdf}
		\begin{itemize}
          \item In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item Gridding is doomed to failure for $D\gtrsim4$.
          \item {\em Sampling\/} the posterior is an excellent compression scheme.
          \item Name of the game: \\ Constructing algorithms to generate samples with a minimum number of likelihood calls
		\end{itemize}
	\end{figright}
\end{frame}



%\section{Metropolis Hastings}
%
%
\begin{frame}
    \frametitle{Sampling algorithms: Metropolis Hastings} 
    \begin{itemize}

        \item Turn the $N$-dimensional problem into a one-dimensional one.
            \begin{enumerate}
                \item Propose random step to new point $x_{i} \to x_{i+1}$
                \item If uphill [$P(x_{i+1})>P(x_i)$], make step\ldots
                \item \ldots otherwise make step with probability $\propto P(x_{i+1})/P(x_i)$. 
            \end{enumerate}
        \item Theorem: set of steps $\{x_i : i=1\ldots N\}$ are samples from posterior $P$
        \item \href{https://chi-feng.github.io/mcmc-demo/app.html\#RandomWalkMH,banana}{chi-feng.github.io/mcmc-demo/app.html\#RandomWalkMH,banana}
    \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte-Carlo} 
  \begin{itemize}
      \item Key idea: Treat $\log L(\Theta)$ as a potential energy
      \item Guide walker under force: \[F(\Theta) =\nabla \log L(\Theta)\]
      \item Walker is naturally guided uphill
      \item Conserved quantities mean efficient acceptance ratios.
      \item Allows sampling in millions of dimensions.
      \item stan is a fully fledged probabilistic programming language for HMC (\href{https://www.jstatsoft.org/article/view/v076i01}{10.18637/jss.v076.i01}).
      \item \href{https://chi-feng.github.io/mcmc-demo/app.html\#HamiltonianMC,donut}{chi-feng.github.io/mcmc-demo/app.html\#HamiltonianMC,donut}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Ensemble sampling} 
  \begin{itemize}
      \item Instead of one walker, evolve a set of $n$ walkers.
      \item Can use information present in ensemble to guide proposals.
      \item emcee: affine invariant proposals \href{https://arxiv.org/abs/1202.3665}{arXiv:1202.3665}
      \item \href{https://chi-feng.github.io/mcmc-demo/app.html\#SVGD,banana}{chi-feng.github.io/mcmc-demo/app.html\#SVGD,banana}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to traditional MCMC} 

  \begin{itemize}
    \item Uses ensemble sampling to compress prior to posterior.
    \item Allows you to compute evidences, partition functions and Kullback-Liebler divergences.
  \end{itemize}
  
  New procedure: 

  
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
    \item[$S_0$:] Generate $n$ samples uniformly over the space . 
    \item[$S_{n+1}$:] Delete the lowest probability sample in $S_{n}$, and replace it with a new sample with higher probability
  \end{description}
  
  Requires one to be able to uniformly within a region, subject to a {\em hard probability constraint}.
  \begin{description}
      \item[MultiNest] Rejection sampling $D<20$ (\href{https://arxiv.org/abs/0809.3437}{arXiv:0809.3437})
      \item[PolyChord] Slice sampling $D\lesssim 1000$ (\href{https://arxiv.org/abs/1506.00171}{arXiv:1506.00171}) 
  \end{description}

\end{frame}

\begin{frame}
  \frametitle{Sampling algorithms: summary}
  \begin{description}[Hamiltonian Monte Carlo]
      \item[Metropolis Hastings] Easy to implement, requires manual tuning \& insight into the problem
      \item[emcee] Fire-and-forget, easy python implementation
      \item[Hamiltonian Monte Carlo] Allows sampling in extremely high dimensions, requires gradients, self-tuning. Need to learn stan programming language.
      \item[Nested Sampling] Allows evidence calculation in moderately high dimensions, self-tuning. Need to install MultiNest and/or PolyChord packages.
  \end{description}
\end{frame}

\begin{frame}
    \frametitle{Further Reading}
    \begin{itemize}
        \item Data Analysis: A Bayesian Tutorial (Sivia \& Skilling)
        \item Information theory, inference \& learning algorithms (\href{http://www.inference.org.uk/itila/book.html}{MacKay})
        \item Bayesian methods in cosmology \href{https://arxiv.org/abs/0803.4089}{arXiv.org:0803.4089}
        \item Bayesian sparse reconstruction \href{https://arxiv.org/abs/1809.04598}{arXiv:1809.04598}
        \item Hamiltonian monte carlo \href{https://arxiv.org/abs/1701.02434}{arXiv:1701.02434}
        \item Nested sampling \href{https://projecteuclid.org/euclid.ba/1340370944}{euclid.ba/1340370944}
    \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Example: Exoplanets}
  \includegraphics[width=\textwidth]{figures/rv_full.pdf}
\end{frame}
\begin{frame}
  \frametitle{Example: Exoplanets}
  \begin{itemize}
      \item Simple radial velocity model
          \begin{equation}
              \nu(t;\theta) = \sum_{p=1}^N K_p \sin(\omega_p t + \phi_p)\nonumber
          \end{equation}
      \item Fit each model to data.
      \item Posteriors on model parameters $[(K_p,\omega_p,\phi_p),p=1\cdots N]$ quantify knowledge of system characteristics.
      \item Evidences of models determine relative likelihood of number of planets in system
      \item \href{https://arxiv.org/abs/1806.00518}{arXiv:1806.00518}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Example: function $\PR(k)$ reconstruction}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}
%
%
%
\begin{frame}
  \frametitle<1>{no tilt}
  \frametitle<2>{tilted}
  \frametitle<3>{1 internal knot}
  \frametitle<4>{2 internal knots}
  \frametitle<5>{3 internal knots}
  \frametitle<6>{4 internal knots}
  \frametitle<7>{5 internal knots}
  \frametitle<8>{6 internal knots}
  \frametitle<9>{7 internal knots}
  \frametitle<10>{Bayes Factors}
  \frametitle<11>{Marginalised plot}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.6\textwidth]{figures/pps_1}
    \includegraphics<2>[width=0.6\textwidth]{figures/pps_2}
    \includegraphics<3>[width=0.6\textwidth]{figures/pps_3}
    \includegraphics<4>[width=0.6\textwidth]{figures/pps_4}
    \includegraphics<5>[width=0.6\textwidth]{figures/pps_5}
    \includegraphics<6>[width=0.6\textwidth]{figures/pps_6}
    \includegraphics<7>[width=0.6\textwidth]{figures/pps_7}
    \includegraphics<8>[width=0.6\textwidth]{figures/pps_8}
    \includegraphics<9>[width=0.6\textwidth]{figures/pps_9}
    \includegraphics<10>[width=0.49\textwidth]{figures/pps_dkl}
    \includegraphics<10>[width=0.49\textwidth]{figures/pps_evidence}
    \includegraphics<11>[width=0.6\textwidth]{figures/pps.pdf}

  \end{center}
\end{frame}

\end{document}
```
# branch: remotes/origin/consistency_2018
```tex
\documentclass[%
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}
\usepackage{mathtools}
\newtheorem*{theorem*}{Theorem}


\title{Planck, inflation and the future of inflationary constraints}
\subtitle{Consistency of Cosmological Datasets: Evidence for new Physics?}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
    Astrophysics Group \\
    Cavendish Laboratory \\
    University of Cambridge
}
\date{1\textsuperscript{st} June 2018}

\include{beamer_commands}

\begin{document}

\begin{frame}
    \titlepage{}
\end{frame}

\begin{frame}
    \frametitle{The pipeline from inflation to the CMB}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/p2001dcd6g16001.jpg}
            \begin{minipage}{0.7\textwidth}
                \include{asns_compact}
            \end{minipage}
            \includegraphics[width=0.7\textwidth]{./figures/700px-A15_TT.png}
        \end{column}
        \hfill
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item Inflationary model
                \item Primordial power spectrum $\mathcal{P}_\mathcal{R}$
                \item $(A_s,n_s,r)$
                \item Boltzmann solver \& line-of-site projection
                \item $(\Omega_b h^2,\Omega_c h^2,\theta_\ast,\tau)$
                \item CMB power spectrum $C_\ell$
            \end{itemize}
        \end{column}
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Constraining inflation (Planck 2015)}
    \framesubtitle{Linking theorists and observers}
    \includegraphics[width=\textwidth]{./figures/V18TTvsTTTEEEvs2013_120mmR.pdf}
\end{frame}


\begin{frame}
    \frametitle{COBE}
    \centering
    \includegraphics[width=0.7\textwidth]{./figures/COBE.pdf}
\end{frame}
\begin{frame}
    \frametitle{COBE, MAXIMA, BOOMERANG, VSA, DASI, CBI}
    \centering
    \includegraphics[width=0.7\textwidth]{./figures/pre_WMAP.pdf}
\end{frame}
\begin{frame}
    \frametitle{WMAP}
    \centering
    \includegraphics[width=0.7\textwidth]{./figures/WMAP.pdf}
\end{frame}
\begin{frame}
    \frametitle{Planck 2013}
    \centering
    \includegraphics[width=0.7\textwidth]{./figures/planck_2013.pdf}
\end{frame}

\begin{frame}
    \frametitle{The Future}
    \centering
    \begin{figleft}[0.6]{./figures/combined.pdf}
        \begin{itemize}
            \item Lyth bound:  $r>0.01 \Rightarrow \Delta\phi > M_\mathrm{p}$
            \item LiteBird: $r\sim \mathcal{O}(10^{-3})$, $r\sim \mathcal{O}(N^{-2})$ $50<N<60$
            \item Compare specific inflationary models.
        \end{itemize}
    \end{figleft}
\end{frame}


\begin{frame}
    \frametitle{The importance of $\tau$}
    \centering
    \includegraphics[width=0.7\textwidth]{./figures/tau_triangle_both.pdf}
\end{frame}

\begin{frame}
    \frametitle{Primordial power spectrum reconstruction}
    \centering
    \begin{minipage}{0.7\textwidth}
        \include{asns}
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Primordial power spectrum reconstruction}
    \centering
    \begin{minipage}{0.7\textwidth}
        \include{reconstruction}
    \end{minipage}
\end{frame}

\begin{frame}
    \frametitle{Primordial power spectrum reconstruction with CORE}
    \centering
    \includegraphics[width=0.49\textwidth]{./figures/posterior_monodromy_2.jpg}
    \includegraphics[width=0.49\textwidth]{./figures/posterior_cs_2.pdf}
\end{frame}

\begin{frame}
    \frametitle{The future of inflation observation}
    \centering
    \begin{figleft}[0.6]{./figures/pps.png}
        \begin{itemize}
            \item $r\stackrel{?}{\ne}0$ --- LiteBird
            \item better $\tau\Rightarrow$ feature finding --- CORE (CMB Bharat?) --- PICO?
            \item Spectral distortions --- PRISTINE?
            \item Alternative models to relieve tensions?
        \end{itemize}
    \end{figleft}
\end{frame}


\begin{frame}
    \frametitle{Alternative models: Kinetic dominance}
    \framesubtitle{``Just enough inflation''}
    \begin{figleft}[0.6]{./figures/wh293_100_v1.pdf}
        \begin{itemize}
            \item Only $\sim\mathcal{O}(60)$ efolds of total inflation 
            \item Preceded by generic phase of $\dot{\phi}^2\gg V(\phi)$
            \item Natural mechanism for generating low-$\ell$ supression
            \item Possibility of fitting $\ell\sim30$ feature
            \item Spatial curvature provides stronger justification.
        \end{itemize}
    \end{figleft}
\end{frame}

\begin{frame}
    \frametitle{The DES Bayes factor}
    \begin{figleft}[0.5]{./figures/Lahav_DES_DESI__Cambridge_29May2018.pdf}
        \begin{itemize}
            \item Contains everything it should, in a dimensionally consistent way:
                \begin{equation}
                    R = \frac{P(D_{pl},D_{des}|M)}{P(D_{pl}|M)P(D_{des}|M)}  \nonumber
                \end{equation}
            \item $R\sim 10$ default prior, $R\sim 0.1$ for narrower prior.
        \end{itemize}
        \begin{equation}
            R = \frac{P(D_{pl}|D_{des},M)}{P(D_{pl}|M)} = \frac{P(D_{pl}|M_{OL})}{P(D_{pl}|M_{GPE})} \nonumber
        \end{equation}
    \end{figleft}
    \begin{itemize}
        \item Represents Bayesian confidence in ability to combine the data
        \item The fact that there are physically reasonable priors which make $R<1$ means that as a Bayesian I am very suspicious of this combination.
        \item Conditioned on $\Lambda$CDM, these data are inconsistent.
    \end{itemize}
\end{frame}

%\begin{frame}
%    \frametitle{<++>}
%    \framesubtitle{<++>}
%
%	\begin{figright}{<++>}
%	\end{figright}
% 
%\end{frame}



\end{document}
```
# branch: remotes/origin/cosmotools_2018
```tex
\documentclass[%
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}

% todo 
% - Ligo actual data
% -define IMRPhenom, EOBNR


\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\text{knots}}
\newcommand{\nlive}{n_\text{live}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}





\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{Statistics}
\subtitle{Aachen Cosmotools 2018}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{April 24, 2018}

\include{include/beamer_commands}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\include{include/further_reading}
\begin{frame}
    \frametitle{Introduction}
    \begin{itemize}
        \item Statistics $\equiv$ Inference $\equiv$ Machine Learning/AI.
        \item How to extract information about scientific models from data.
        \item Most cosmologists work in a {\em Bayesian\/} framework of inference, although {\em Frequentist\/} methods are also sometimes used.
    \end{itemize}
\end{frame}

\section{Fitting a line to data}
\begin{frame}
    \frametitle{Motivating example}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_points.pdf}
        \begin{itemize}
            \item We have noisy data $D$
            \item We wish to fit a model $M$
            \item Functional form $y=f_M(x;\theta)=a x + b$
            \item e.g:
                \begin{align}
                     f_\text{linear}(x;\theta)&=a x + b       \nonumber\\
                     f_\text{quadratic}(x;\theta)&=a x^2 + b  \nonumber
                \end{align}
            \item Model parameters $\theta= (a,b)$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ best-fit}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff.pdf}
        \begin{itemize}
            \item For each parameter set $\theta$:
                \[
                    \chi^2(\theta) = \sum_i \left|y_i - f(x_i;\theta)\right|^2
                \]
            \item Minimise $\chi^2$ wrt $\theta$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ with non-uniform data errors}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data.pdf}
        \begin{itemize}
            \item If data have non-uniform errors:
                \[
                    \chi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|^2}{\sigma_i^2}
                \]
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Problems with $\chi^2$}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff_2.pdf}
        \begin{itemize}
            \item How do we differentiate between models
            \item Why square the errors? -- could take absolute:
                \[
                    \psi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|}{\sigma_i}
                \]
            \item Where does this even come from?
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Multivariate probability}
    \begin{itemize}
        \item Marginalisation:
            \begin{equation*}
                P(x) = \int P(x,y) dy
            \end{equation*}
        \item Conditioning:
            \begin{equation*}
                P(y|x) = \frac{P(x,y)}{P(x)} = \frac{P(x,y)}{\int P(x,y) dy}
            \end{equation*}
        \item De-Conditioning:
            \begin{equation*}
                P(x|y) P(y) = P(x,y)
            \end{equation*}
        \item Bayes theorem:
            \begin{equation*}
                P(y|x) = \frac{P(x|y) P(y)}{P(x)}
            \end{equation*}
            \begin{center}
                ``To flip a conditional $P(x|y)$, you first de-condition on $y$,\\ and then re-condition on $x$.''
            \end{center}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Probability distributions}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.6]{./figures/data_diff_1.pdf}
        \begin{itemize}
            \item The probability of observing a datum:
                \[
                    P(y_i | \theta,M) = \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right)
                \]
            \item The probability of observing the data:
                \begin{align}
                    P(D | \theta,M) &= \prod_i \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right) \nonumber\\
                    &=  \frac{1}{\prod_i\sqrt{2\pi}\sigma_i}\exp\sum_i{-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}} \nonumber\\
                    &\propto e^{-\chi^2(\theta)/2}
                    \nonumber
                \end{align}
        \end{itemize}
    \end{figright}
\end{frame}



\begin{frame}
    \frametitle{Maximum likelihood}
    \framesubtitle{Fitting lines to data}
    \begin{figleft}[0.6]{./figures/data_diff.pdf}
        \begin{itemize}
            \item Minimising $\chi^2(\theta)$  is equivalent to maximising $P(D|\theta,M) \propto e^{-\chi^2(\theta)/2}$
            \item $P(D|\theta,M)$ is called the Likelihood $L=L(\theta)$ of the parameters $\theta$
            \item ``Least squares'' $\equiv$ ``maximum likelihood'' \\(if data are gaussian).
        \end{itemize}
    \end{figleft}
\end{frame}

\begin{frame}
    \frametitle{Bayesian inference}
    \begin{itemize}
        \item Likelihood $L=P(D|\theta,M)$ is undeniably correct.
        \item Frequentists construct inference techniques purely from this function.
        \item The trend is cosmology is to work with a Bayesian approach.
        \item What we want are things like $P(\theta|D,M)$ and $P(M|D)$.
        \item To invert the conditionals, we need Bayes theorem:
            \begin{align}
                P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
                P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Terminology}
    \framesubtitle{Bayesian inference}
    \begin{align}
        P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
        \text{Posterior} &= \frac{\text{Likelihood}\times\text{Prior}}{\text{Evidence}} \nonumber
    \end{align}
    \begin{align}
        P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber\\
        \text{Model probability} &= \frac{\text{Evidence}\times\text{Model Prior}}{\text{Normalisation}} \nonumber
    \end{align}
\end{frame}

\begin{frame}
    \frametitle{The prior}
    \framesubtitle{Example: Biased coins}
    \begin{itemize}
        \item Need to define the \textbf{Prior} $P(\theta)$ --- probability of the bias, given no data
        \item Represents our knowledge of parameters before the data -- subjective
        \item Frequentists view this as a flaw in Bayesian inference. 
        \item Bayesians view this as an advantage
        \item Fundamental rule of Inference:\pause\\
            \vfill
            \begin{center}
                \Large You cannot extract information from data\\ without making assumptions 
            \end{center}
            \vfill
        \item All Bayesians do is make them explicit
        \item Any method that claims it is ``objective'' is simply hiding them
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Parameter estimation}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.3]{./figures/parameters.pdf}
        \begin{itemize}
            \item We may use $P(\theta|D,M)$ to inspect whether a model looks reasonable
        \end{itemize}
        \includegraphics[width=\textwidth]{./figures/data.pdf}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Predictive posterior}
    \begin{figright}[0.4]{./figures/fgivenx.pdf}
        More useful to plot:
        \begin{align}
        &P(y|x) = \nonumber\\
        &\int P(y|x,\theta) P(\theta) d\theta \nonumber
        \end{align}
        (all conditioned on $D,M$)
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Model comparison}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.33]{./figures/evidences_log.pdf}
        \begin{itemize}
            \item We may use $Z=P(D|M)$ to determine whether a model is reasonable.
        \end{itemize}
    \end{figright}
\end{frame}
\begin{frame}
    \frametitle{Model comparison}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.33]{./figures/evidences_lin.pdf}
        \begin{itemize}
            \item We may use $Z=P(D|M)$ to determine whether a model is reasonable.
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Line fitting (context)}
    \begin{figright}[0.5]{./figures/supernovae.pdf}
        \begin{itemize}
            \item Whilst this model seems a little trite\ldots
            \item\ldots determining polynomial indices \\$\equiv$ determining cosmological material content:
        \end{itemize}
    \end{figright}
        \[
            {\left( \frac{H}{H_0} \right)}^2 = 
            \Omega_\text{r} {\left( \frac{a_0}{a} \right)}^4+
            \Omega_\text{m} {\left( \frac{a_0}{a} \right)}^3+
            \Omega_k {\left( \frac{a_0}{a} \right)}^2+
            \Omega_\Lambda
            \]
\end{frame}



\begin{frame}
    \frametitle{Probability distributions}

    \begin{itemize}
        \item As scientists, we are used to seeing error bars on results.
        \item Age of the universe ({\em Planck\/}): 
         \[13.73\pm 0.12\:\text{billion years old.}\]
        \item Masses of LIGO GW150914 binary merger: 
        \[m_1 = 39.4^{+5.5}_{-4.9}\:M_\odot,\qquad m_2 = 30.9^{+4.8}_{-4.4}\:M_\odot \]
        \item These are called {\em credible intervals}, state that we are e.g.\ $90\%$ confident of the value lying in this range.
        \item More importantly, these are {\em summary statistics}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{LIGO binary merger}
    \begin{columns}
        \begin{column}{0.65\textwidth}
            \includegraphics[width=\textwidth]{./figures/ligo_m1_m2.pdf}
        \end{column}
        \begin{column}{0.35\textwidth}
            \begin{itemize}
                \item Summary statistics summarise a full probability distribution.
                \item One goal of inference is to produce these probability distributions.
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Theory}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics[width=\textwidth]{./figures/ligo_schematic.png}
\end{frame}

\begin{frame}
    \frametitle{The model $M$}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics[width=\textwidth]{./figures/ligo_model.pdf}
\end{frame}

\begin{frame}
    \frametitle{The parameters $\Theta$ of the model $M$}
    \framesubtitle{Extended example of inference: LIGO}
    Theoretical signal depends on:
    \begin{itemize}
        \item $m_1, m_2$: mass of binary
        \item $\theta, \phi$: sky location
        \item $r$: luminosity distance 
        \item $\Phi_c, t_c$: phase and time of coalescence
        \item $i, \theta_\text{sky}$: inclination and angle on sky (orbital parameters)
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The data $D$}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics<1>[width=\textwidth]{./figures/ligo_data.pdf}
    \includegraphics<2>[width=\textwidth]{./figures/ligo_actual.png}
\end{frame}

\begin{frame}
    \frametitle<1>{The Likelihood: well matched}
    \frametitle<2>{The Likelihood: coalescence off}
    \frametitle<3>{The Likelihood: too large luminosity distance}
    \frametitle<4>{The Likelihood: incorrect inclination}
    \framesubtitle{Extended example of inference: LIGO}
    \only<1>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood.pdf}}
    \only<2>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood_t.pdf}}
    \only<3>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood_r.pdf}}
    \only<4>{\includegraphics[width=\textwidth]{./figures/ligo_likelihood_i.pdf}}
\end{frame}



\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{itemize}
        \item Cannot plot the full posterior distribution:
            \[\mathcal{P}(\Theta) \equiv P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\text{sky}|D,M)\]
        \item Can plot 1D and 2D {\em marginalised\/} distributions e.g:
            \begin{align}
            &P(m_1,m_2|D,M)=\nonumber\\&\int P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\text{sky}|D,M) \,d\theta \,d\phi \,dr \,d\Phi_c \,d t_c \,d i \,d\theta_\text{sky}\nonumber
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}

    \begin{figleft}[0.65]{./figures/ligo_m1_m2.pdf}
        \begin{itemize}
            \item May do this for each pair of parameters
            \item Generates a {\em triangle plot}
        \end{itemize}
    \end{figleft}

\end{frame}

\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{figleft}[0.65]{./figures/ligo_full.pdf}
		\begin{itemize}
          \item Does give insight
          \item Not the full picture
		\end{itemize}
    \end{figleft}
\end{frame}

\begin{frame}
    \frametitle{Evidences and model comparison}
    \framesubtitle{Extended example of inference: LIGO}

    \begin{itemize}
        \item Up until now, we have discussed {\em Parameter estimation\/}: inferring what data tell us about parameters $\Theta$ of a model $M$.
        \item Scientifically speaking, this is only half the story.
        \item In general, we will have several competing models that describe the data, and we want to know which is the ``best''.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Parameter estimation}
    \framesubtitle{Another example.}

    \[\lik(\Theta) = P(D|\Theta,M)\]
    \begin{align}
        \onslide<2->{D =& \{C_\ell\only<6->{^\text{(Planck)}}\}} 
        \onslide<15->{+\{\text{LSS}\}} 
        \onslide<16->{+\{\text{``Big Data''}\}}
        \nonumber\\
        \onslide<3->{M =& \Lambda\text{CDM}} 
        \onslide<9->{+ \text{extensions} }
        \nonumber\\
        \onslide<4->{\Theta =& \Theta_{\Lambda \text{CDM}}} \onslide<7->{+ \Theta_\text{Planck}} \onslide<10->{+ \Theta_\text{extensions}}\nonumber\\
        \onslide<5->{\Theta_{\Lambda \text{CDM}} =& ( \Omega_b h^2, \Omega_c h^2, 100\theta_{MC}, \tau, {\rm{ln}}(10^{10} A_s), n_s) \nonumber\\}
        \onslide<8->{\Theta_\text{Planck} =& (y_{\rm cal}, A^{CIB}_{217}, \xi^{tSZ-CIB}, A^{tSZ}_{143}, A^{PS}_{100}, A^{PS}_{143}, A^{PS}_{143\times 217}, A^{PS}_{217},\nonumber\\
        &A^{kSZ}, A^{{\rm dust}TT}_{100}, A^{{\rm dust}TT}_{143}, A^{{\rm dust}TT}_{143\times 217}, A^{{\rm dust}TT}_{217}, c_{100}, c_{217}) \nonumber\\}
        \onslide<11->{\Theta_\text{extensions} =& (
                n_{\rm run}
                \only<12->{,n_{\rm run,run}}
                \only<13->{,w}
                \only<14->{,\Sigma m_\nu, m_{\nu,{\rm{sterile}}}^{\rm{eff}}}
        ) \nonumber}
    \end{align}

    \begin{itemize}
        \item<17->{Parameter estimation: $L, \pi \to \mathcal{P}$: model parameters}
        \item<17->{Model comparison: $L, \pi \to Z$: how good model is}
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Sampling}
    \framesubtitle{How to describe a high-dimensional posterior}

	\begin{figright}{./figures/ligo_m1_m2.pdf}
		\begin{itemize}
          \item In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item {\em Sampling\/} the posterior is an excellent compression scheme.
		\end{itemize}
	\end{figright}
 
\end{frame}
%
\begin{frame}
    \frametitle{Why do sampling?}
    \framesubtitle{Marginalisation over the posterior}

    \begin{itemize}
        \item Set of $N$ samples $S = \{\Theta^{(i)}: i=1,\ldots N:\: \Theta^{(i)}\sim\mathcal{P}\}$
        \item Mean mass: \[
                \bar{m}_1 \equiv\langle m_1\rangle_\mathcal{P} \approx 
                \only<1>{\frac{1}{N}\sum_{i=1}^N m_1^{(i)}}
                \only<2>{\frac{\sum_{i=1}^N w^{(i)} m_1^{(i)}}{\sum_{i=1}^N w^{(i)}}}
            \]
        \item Mass covariance: \[
                \mathrm{Cov}(m_1,m_2) \approx 
                \only<1>{\frac{1}{N}\sum_{i=1}^N (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}
                \only<2>{\frac{\sum_{i=1}^N (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}{\sum_{i=1}^N w^{(i)}}}
            \]
        \item Marginalised samples: Just ignore the other coordinates.
        \item N.B. Typically have {\em weighted\/} samples
    \end{itemize}
\end{frame}
%
\begin{frame}
    \frametitle{Parameter estimation}
    \begin{itemize}
        \item The name of the game is therefore drawing samples $S$ from the posterior $\mathcal{P}$ with the minimum number of likelihood calls.
        \item Gridding is doomed to failure in high dimensions.
        \item Enter Metropolis Hastings.
    \end{itemize}
\end{frame}



\section{Metropolis Hastings}

\begin{frame}
    \frametitle{Current sampling approaches}
    \begin{enumerate}
        \item Metropolis Hastings.
        \item Hamiltonian Monte-Carlo (HMC).
        \item Ensemble sampling (e.g.\ emcee).
    \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Hastings} 
  \begin{itemize}
      
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \begin{enumerate}
          
        \item Pick random direction
          
        \item Choose step length
          
        \item If uphill, make step\ldots
          
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Hastings} 
  \includegraphics[width=\textwidth]{movies/MCMC_0.pdf}
\end{frame}
\begin{frame}
  \frametitle{Metropolis Hastings} 
  \movie[width=\textwidth,height=0.52\textwidth]{}{movies/MCMC.mp4}
\end{frame}
\begin{frame}
  \frametitle{Metropolis Hastings} 
  \includegraphics[width=\textwidth]{movies/MCMC_1.pdf}
\end{frame}




\begin{frame}
  \frametitle{Metropolis Hastings} 
  \framesubtitle{Struggles with\ldots}
  \pause
  \begin{enumerate}
      \item Burn in
      \item Multimodality
      \item Correlated Peaks
      \item Phase transitions
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte-Carlo} 
  \begin{itemize}
      \item Key idea: Treat $\log L(\Theta)$ as a potential energy
      \item Guide walker under ``force'': \[F(\Theta) =\nabla \log L(\Theta)\]
      \item Walker is naturally ``guided'' uphill
      \item Conserved quantities mean efficient acceptance ratios.
      \item stan is a fully fledged, rapidly developing programming language with HMC as a default sampler.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Ensemble sampling} 
  \begin{itemize}
      \item Instead of one walker, evolve a set of $n$ walkers.
      \item Can use information present in ensemble to guide proposals.
      \item emcee: affine invariant proposals.
      \item emcee is not the only (or even best) affine invariant approach.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The fundamental issue with all of the above} 

  \begin{itemize}
    \item They don't give you evidences!
  \end{itemize}

  \begin{align}
    \ev 
    &= \prob(D|M) 
    \nonumber\\
    &= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
    &= \left\langle \lik \right\rangle_\prior
    \nonumber
  \end{align}
  
  \begin{itemize}
    \item MCMC fundamentally explores the posterior, and cannot average over the prior.
    \item Simulated annealing gives one possibility for computing evidences.
    \begin{itemize}
        \item Inspired by thermodynamics.
        \item Suffers from similar issues to MCMC.
        \item Unclear how to choose correct annealing schedule
    \end{itemize}
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}

\begin{frame}
  \frametitle{What is nested sampling?}
  \begin{itemize}
    \item Nested sampling is an alternative way of sampling posteriors. 
    \item Uses ensemble sampling to compress prior to posterior.
    \item In doing so, it circumvents many issues (dimensionality, topology, geometry) that beset standard approaches.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to traditional MCMC!} 

  
  New procedure: 

  
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      
    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\prior$). 
      
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new uniform sample with higher likelihood
  \end{description}

  
  Requires one to be able to uniformly within a region, subject to a {\em hard likelihood constraint}.

\end{frame}



\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}


%%\begin{frame}
%%  \frametitle{Nested sampling}
%%  \includegraphics[width=\textwidth]{movies/NS_0.pdf}
%%\end{frame}
%%\begin{frame}
%%  \frametitle{Nested sampling}
%%  \movie[width=\textwidth,height=0.52\textwidth]{}{movies/NS.mp4}
%%\end{frame}
%%\begin{frame}
%%  \frametitle{Nested sampling}
%%  \includegraphics[width=\textwidth]{movies/NS_1.pdf}
%%\end{frame}
%
%
%
%
%%\begin{frame}
%%  \frametitle{Estimating evidences} 
%%  \framesubtitle{Evidence error} 
%%
%%\foreach \pagenum in {1,...,9} {%
%%  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/areas}
%%}
%% 
%%\end{frame}
%%
%%\begin{frame}
%%  \frametitle{Estimating evidences} 
%%  \framesubtitle{Evidence error} 
%%
%%
%%  \begin{itemize}
%%    \item<2-> approximate compression:
%%  \end{itemize}
%%  \[ 
%%    \onslide<2->{\Delta \log X \sim -\frac{1}{n}} 
%%    \onslide<3->{\pm \frac{1}{n}} 
%%  \]
%%  \[ 
%%    \onslide<4->{\log X_i \sim -\frac{i}{n}}
%%    \onslide<5->{\pm \frac{\sqrt{i}}{n}} 
%%  \]
%%  \onslide<6->{%
%%  \begin{itemize}
%%    \item<6-> \# of steps to get to $H$:
%%  \end{itemize}
%%  \[ i_H \sim n H \]
%%  }
%%  \onslide<7->{%
%%  \begin{itemize}
%%    \item estimate of volume at $H$:
%%  \end{itemize}
%%  \[ \log X_H \approx -H \pm \sqrt{\frac{H}{n}} \]
%%}
%%
%%  \onslide<8->{%
%%  \begin{itemize}
%%    \item estimate of evidence error:
%%  \end{itemize}
%%  \[ \log\ev \approx \sum w_i \lik_i  \pm \sqrt{\frac{H}{n}} \]
%%}
%%
%%\end{frame}
%
%
\begin{frame}
  \frametitle{Nested sampling} 

  \begin{itemize}
    \item The set of dead points are posterior samples with an appropriate weighting factor
    \item They can also be used to calculate evidences, since it sequentially updates the priors.
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item Most of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm.
  \end{itemize}
 
\end{frame}


\begin{frame}
  \frametitle{Sampling within an iso-likelihood contour}
  \framesubtitle{Previous attempts}


  \begin{description}
    \item[Rejection Sampling] MultiNest; F.\ Feroz \& M.\ Hobson (2009).
      \begin{itemize}
        \item Suffers in high dimensions
      \end{itemize}
      \item[Hamiltonian] M.J. Betancourt (2010) 
      \item[Galilean] F.\ Feroz \& J.\ Skilling (2013) 
      \begin{itemize}
        \item Requires gradients and tuning
      \end{itemize}
      \item[Diffusive Nested Sampling] B.\ Brewer et al.\ (2009,2016).
      \begin{itemize}
        \item Very promising
        \item Still needs tuning.
      \end{itemize}
      \item[Slice Sampling] PolyChord; Handley et al.\ (2015).
      \begin{itemize}
          \item Current ``state-of-the-art''.
          \item PolyChord 2.0 imminent.
      \end{itemize}
  \end{description}

\end{frame}

%
%\section{Examples}
%\begin{frame}
%  \frametitle{Object detection}
%  \framesubtitle{Toy problem}
%
%  \centerline{%
%  \includegraphics<1>[width=\textwidth]{figures/object_detection_image}
%  \includegraphics<2>[width=\textwidth]{figures/object_detection_image_no_noise}
%}
%
%\end{frame}
%
\begin{frame}
  \frametitle{PolyChord in action}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}
%
%
%%\begin{frame}
%%  \frametitle{Planck data}
%%  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
%%  \begin{itemize}
%%    \item<2-> Temperature data TT+lowP
%%    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
%%    \item<4-> Marginalised plots of $\PR(k)$
%%    \item<5->
%%      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
%%  \end{itemize}
%%\end{frame}
%
%
%
\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knots}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{8 internal knots}
  \frametitle<10>{Bayes Factors}
  \frametitle<11>{Marginalised plot}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.8\textwidth]{figures/0TT_fgivenx}
    \includegraphics<2>[width=0.8\textwidth]{figures/1TT_fgivenx}
    \includegraphics<3>[width=0.8\textwidth]{figures/2TT_fgivenx}
    \includegraphics<4>[width=0.8\textwidth]{figures/3TT_fgivenx}
    \includegraphics<5>[width=0.8\textwidth]{figures/4TT_fgivenx}
    \includegraphics<6>[width=0.8\textwidth]{figures/5TT_fgivenx}
    \includegraphics<7>[width=0.8\textwidth]{figures/6TT_fgivenx}
    \includegraphics<8>[width=0.8\textwidth]{figures/7TT_fgivenx}
    \includegraphics<9>[width=0.8\textwidth]{figures/8TT_fgivenx}
    \includegraphics<10>[width=0.8\textwidth]{figures/Bayes_TT.pdf}
    \includegraphics<11>[width=0.8\textwidth]{figures/combined_fgivenx.pdf}

  \end{center}
\end{frame}
\begin{frame}
  \frametitle<1>{COBE (pre-2002)}
  \frametitle<2>{COBE et al (2002)}
  \frametitle<3>{WMAP (2012)}
  \frametitle<4>{Planck (2013)}
  \frametitle<5>{Planck (2015)}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.6\textwidth]{figures/cobe}
    \includegraphics<2>[width=0.6\textwidth]{figures/pre_WMAP}
    \includegraphics<3>[width=0.6\textwidth]{figures/WMAP}
    \includegraphics<4>[width=0.6\textwidth]{figures/planck_2013}
    \includegraphics<5>[width=0.6\textwidth]{figures/planck_2015}

  \end{center}
\end{frame}

%
%\begin{frame}
%    \frametitle{Dark energy equation of state reconstruction}
%    \begin{itemize}
%        \item Same thing, but for Dark energy equation of state $w(z)$ (quintessence).
%        \item Data used is Planck 2015, BOSS DR 11, JLA supernovae and BOSS Ly$\alpha$ data
%    \end{itemize}
%\end{frame}
%\begin{frame}
%    \frametitle<1>{Flat, variable $w$}
%    \frametitle<2>{Tilted}
%    \frametitle<3>{1 internal node}
%    \frametitle<4>{2 internal nodes}
%    \frametitle<5>{3 internal nodes}
%    \frametitle<6>{Marginalised plot - just extension models}
%    \frametitle<7>{Marginalised plot - including LCDM}
%    \framesubtitle{Dark energy equation of state reconstruction}
%
%    \begin{center}
%        \includegraphics<1>[width=0.9\textwidth]{figures/wCDM_1000Nlive_wgiivenz.pdf}
%        \includegraphics<2>[width=0.9\textwidth]{figures/tCDM_100Nlive_wgiivenz.pdf}
%        \includegraphics<3>[width=0.9\textwidth]{figures/1CDM_1000Nlive_wgiivenz.pdf}
%        \includegraphics<4>[width=0.9\textwidth]{figures/2CDM_1000Nlive_wgiivenz.pdf}
%        \includegraphics<5>[width=0.9\textwidth]{figures/3CDM_1000Nlive_wgiivenz.pdf}
%        \includegraphics<6>[width=0.9\textwidth]{figures/extensionModels_1000Nlive_wgivenz.pdf}
%        \includegraphics<7>[width=0.9\textwidth]{figures/allModels_1000Nlive_wgivenz.pdf}
%    \end{center}
%\end{frame}
%
%
%\begin{frame}
%  \frametitle{Nested Sampling}
%  \framesubtitle{Calculating evidences}
%  \foreach \pagenum in {1,...,16} {%
%      \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/lesbesgue}
%  }
%\end{frame}
%
%
%
%\begin{frame}
%  \frametitle{Nested Sampling} 
%  \framesubtitle{Exponential volume contraction} 
%  
%  \begin{itemize}
%      
%    \item At each iteration, the likelihood contour will shrink in volume by  $\approx 1/n$.
%      
%    \item Nested sampling zooms in to the peak of the posterior {\em exponentially}.
%      
%    \item 
%      \begin{equation}
%          \ev \approx \sum_i \Delta\lik_i X_{i}
%      \end{equation}
%      
%    \item 
%      \begin{equation}
%        X_{i+1} \approx \frac{n}{n+1}X_i, \qquad X_{0} = 1
%      \end{equation}
%  \end{itemize}
%  
%\end{frame}

\include{include/further_reading}

\end{document}
```
# branch: remotes/origin/darkmachines_bambi_2018
```tex
\documentclass[%
    handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}

\include{include/beamer_commands}

\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}


\title{BAMBI\@: Resurrection}
\subtitle{Blind Accelerated Multimodal Bayesian Inference}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{Nov 13, 2018}


\begin{document}

\begin{frame}
    \titlepage{}
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\section{Background}

\begin{frame}
    \frametitle{Background}
    \begin{itemize}
        \item Key papers in historical order:
            \begin{description}
                \item[MultiNest] \href{https://arxiv.org/abs/0809.3437}{arXiv:0809.3437} 
                \item[BAMBI]     \href{https://arxiv.org/abs/1110.2997}{arXiv:1110.2997} 
                \item[SkyNet]    \href{https://arxiv.org/abs/1309.0790}{arXiv:1309.0790}
                \item[PolyChord] \href{https://arxiv.org/abs/1506.00171}{arXiv:1506.00171} 
            \end{description}
        \item First widely-successful nested sampling algorithm MultiNest.
        \item BAMBI aims to speed up MultiNest using neural networks.
        \item Neural network approach is developed into stand-alone code SkyNet.
        \item MultiNest is updated from version 1 to version 3.
        \item Higher-dimensional nested sampling available in PolyChord.
    \end{itemize}
\end{frame}


\section{Nested Sampling summary}

\begin{frame}
  \frametitle{Nested Sampling} 
  \begin{itemize}
      \item Completely new approach to sampling:
  \end{itemize}

  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
    \item[$S_0$:] Generate $n$ samples uniformly over the space.
    \item[$S_{n+1}$:] Delete the lowest probability sample in $S_{n}$, and replace it with a new sample with higher probability
  \end{description}

  \begin{itemize}
      \item This generates a \emph{run} of discarded points.
      \item $n\sim\mathcal{O}(10s-1000s)$
      \item Requires one to be able to uniformly within a region, subject to a {\em hard probability constraint}.
      \item John Skilling's original paper: \href{https://projecteuclid.org/euclid.ba/1340370944}{euclid.ba/1340370944}
  \end{itemize}
\end{frame}

\begin{multifig}{figures/nested_sampling.pdf}
\end{multifig}

\begin{frame}
  \frametitle{How is Nested Sampling used?} 
  \begin{itemize}
      \item Nested sampling generates a \emph{run} of discarded points
      \item These points can be weighted in post-processing to give:
          \begin{itemize}
              \item Posterior samples
              \item Bayesian Evidence (marginal likelihoods)
              \item Kullback Liebler divergence
              \item Partition function
          \end{itemize}
      \item This is possible because the nested sampling scheme is a probabilistic integrator, allowing one to estimate the \emph{density of states}.
  \end{itemize}
\end{frame}

\section{BAMBI}
\begin{frame}
  \frametitle{Neural networks to speed up likelihood calls} 
  \begin{dfigright}[0.65]{./figures/NS1.pdf}{./figures/Artificial_neural_network.png}
    \begin{itemize}
        \item Likelihood calls can often be slow (seconds to minutes in cosmology and particle physics).
        \item Use trained neural network as fast proxy for the likelihood
    \end{itemize}
\end{dfigright}

\end{frame}

\begin{frame}
  \frametitle{BAMBI} 
  \begin{dfigright}[0.65]{./figures/NS1.pdf}{./figures/Artificial_neural_network.png}
    \begin{enumerate}
        \item Every \texttt{updInt/2} iterations, use the last \texttt{updInt} discarded points to train a neural network\label{it:loop1}
        \item Split points $80:20$ training:validation
        \item If accurate network is obtained, replace log-likelihood with NN\@.
        \item Periodically check whether NN remains accurate, and re-train if necesssary
    \end{enumerate}
    \begin{itemize}
        \item Do not use NN in place of likelihood if outside range of training data.
        \item Re-train if most new samples outside 95\% central percentile of training data likelihoods.
    \end{itemize}
\end{dfigright}

\end{frame}

\begin{frame}
  \frametitle{BAMBI\@: good ideas} 
  \begin{dfigright}[0.65]{./figures/NS1.pdf}{./figures/Artificial_neural_network.png}
    \begin{itemize}
        \item At end of procedure, one is left with a set of overlapping NNs.
        \item Each NN capable of predicting the log-likelihood across a certain range.
        \item NNs can be used for further analyses
        \item set of NNs are particularly well-suited to describing posterior peak
    \end{itemize}
\end{dfigright}
\end{frame}

\section{BAMBI\@: Resurrection}
\begin{frame}
  \frametitle{Problems with current BAMBI code} 
    \begin{itemize}
        \item Out-of-date: 
        \item Hard-coded into old MultiNest (v1). 
        \item NN training technology has advanced since 2013.
        \item MultiNest will not be suitable for high-dimensional problems, even with fast NN-proxy likelihoods.
    \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{PyBAMBI} 
    \begin{itemize}
        \item Use latest advances in neural networks.
        \item Use latest versions of PyMultiNest and PyPolyChord.
        \item Use dumper functions to keep everything in python.
        \item Use \href{https://keras.io/}{Keras, TensorFlow and Theano} for NN framework.
    \end{itemize}
\end{frame}

\end{document}
```
# branch: remotes/origin/darkmachines_ns_2018
```tex
\documentclass[%
    handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}

\include{include/beamer_commands}

\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}


\title{Nested Sampling}
\subtitle{An efficient and robust Bayesian inference tool\\ for cosmology and particle physics }
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{Nov 6, 2018}


\begin{document}

\begin{frame}
    \titlepage{}
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\section{What is nested sampling}
\begin{frame}
    \frametitle{What do I mean by sampling?}
\begin{figright}[0.6]{./figures/MCMCTraversal.png}
    \begin{itemize}
        \item Sampling is the process of generating $D$-dimensional points $\theta=(\theta_1,\ldots,\theta_D)$ drawn from probability distribution $P(\theta)$.
        \item $P(\theta)$ is a-priori unknown, and may be expensive to evaluate.
        \item The name of the game is use as few calls to $P$ as possible.
        \item Points need not be independent, and indeed normally only need $\sim\mathcal{O}(12)$ for most inference purposes.
    \end{itemize}
\end{figright}

\end{frame}

\begin{frame}
    \frametitle{Challenges in sampling techniques}
\begin{figright}[0.4]{./figures/Himmelblau_contour.png}
    \begin{itemize}
        \item Multimodality
        \item Burn-in
        \item Convergence diagnosis
        \item Correlation/Degeneracy 
        \item Parallelisation
        \item Phase-transitions
        \item High dimensions
    \end{itemize}
\end{figright}

\end{frame}

\begin{frame}
  \frametitle{Nested Sampling} 
  \begin{itemize}
      \item Completely new approach to sampling:
  \end{itemize}

  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
    \item[$S_0$:] Generate $n$ samples uniformly over the space.
    \item[$S_{n+1}$:] Delete the lowest probability sample in $S_{n}$, and replace it with a new sample with higher probability
  \end{description}

  \begin{itemize}
      \item This generates a \emph{run} of discarded points.
      \item $n\sim\mathcal{O}(10s-1000s)$
      \item Requires one to be able to uniformly within a region, subject to a {\em hard probability constraint}.
      \item John Skilling's original paper: \href{https://projecteuclid.org/euclid.ba/1340370944}{euclid.ba/1340370944}
  \end{itemize}
\end{frame}

\begin{multifig}{figures/nested_sampling.pdf}
\end{multifig}

\begin{frame}
  \frametitle{How is Nested Sampling used?} 
  \begin{itemize}
      \item Nested sampling generates a \emph{run} of discarded points
      \item These points can be weighted in post-processing to give:
          \begin{itemize}
              \item Posterior samples
              \item Bayesian Evidence (marginal likelihoods)
              \item Kullback Liebler divergence
              \item Partition function
          \end{itemize}
      \item This is possible because the nested sampling scheme is a probabilistic integrator, allowing one to estimate the \emph{density of states}.
  \end{itemize}
\end{frame}


\begin{multifig}{figures/lesbesgue.pdf}
    \frametitle{Probabilistic integration}
\end{multifig}

\begin{frame}
    \frametitle{Estimating the density of states} 

    \begin{itemize}
        \item If number of live points $n=100$, each uniformly sampled point sits in a shell $\approx 1\%$ of volume of outer most contour
        \item At each iteration, contour shrinks in volume by  $\approx 1/n$.
            \begin{equation}
                \ev \approx \sum_i \Delta\lik_i X_{i},\qquad
                X_{i+1} \approx \frac{n}{n+1}X_i, \qquad X_{0} = 1
                \nonumber
            \end{equation}
        \item Nested sampling zooms in to the peak of the posterior {\em exponentially}.
        \item In fact, we perform precise inference on the volumes:
            \begin{equation}
                P(X_{i+1}|X_{i}) = n{[X_{i+1}/X_{i}]}^{n-1}
                \nonumber
            \end{equation}
        \item Posterior weights are $\mathcal{P}_i = \mathcal{L}_i\times (X_i-X_{i-1})$
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key advantages of nested sampling} 
    \begin{itemize}
        \item The density of states (prior volume estimation) is the missing piece in inference, normally avoided/cancelled in traditional methods.
        \item Allows numerical computation of Bayesian Evidence \& KL divergence.
        \item At each iteration, the set of live points enables self-tuning:
            \begin{itemize}
                \item Clustering (handling multi-modality)
                \item Correlation estimation (constructing proposal distributions)
            \end{itemize}
        \item The sampling process is athermal, and invariant under monotonic tranformations of the sampled distribution $L(\theta)\to f(L(\theta))$:
            \begin{equation}
                E(\theta) = -\log L(\theta) \qquad P(\theta) = \frac{1}{Z(\beta)}e^{-\beta E(\theta)}
            \end{equation}
        \item By appropriate re-weighting, we can post-process the posterior samples to be at any temperature.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{How do we perform nested sampling?} 
    \begin{itemize}
        \item Requires one to be able to uniformly within a region, subject to a {\em hard probability constraint}.
        \item Two main codes that implement traditional NS\@:
            \begin{description}
                \item[MultiNest] \href{https://arxiv.org/abs/0809.3437}{arXiv:0809.3437} 
                \item[PolyChord] \href{https://arxiv.org/abs/1506.00171}{arXiv:1506.00171} 
            \end{description}
        \item Alternative frameworks:
            \begin{description}
                \item[Diffusive NS] \href{https://arxiv.org/abs/0912.2380}{arXiv:0912.2380} 
                \item[SE NS]\href{https://arxiv.org/abs/1402.6306}{arXiv:1402.6306} 
                \item[Dynamic NS] \href{https://arxiv.org/abs/1704.03459}{arXiv:1704.03459} 
            \end{description}
    \end{itemize}

\end{frame}


\section{MultiNest}
\begin{multifig}{figures/multinest.pdf}
    \frametitle{MultiNest}
\end{multifig}
\begin{frame}
    \frametitle{MultiNest} 
    \begin{itemize}
        \item \href{https://arxiv.org/abs/0809.3437}{arXiv:0809.3437} 
        \item Uses a set of overlapping ellipsoids to approximate the shape of the contours
        \item Rejection samples from this representation.
        \item Maximally efficient in low dimensions\ldots
        \item \ldots exponentially bad in high dimensions.
        \item Transition is distribution-dependent (as low as $5$ or as high as $60$).
        \item John Skilling originally anticipated/advocated using MCMC-style chain-based approach.
    \end{itemize}
\end{frame}

\section{PolyChord}
\begin{multifig}{figures/polychord.pdf}
    \frametitle{PolyChord}
\end{multifig}

\begin{frame}
    \frametitle{PolyChord} 
    \begin{itemize}
        \item \href{https://arxiv.org/abs/1506.00171}{arXiv:1506.00171}
        \item Uses slice sampling to generate new points: \href{https://projecteuclid.org/euclid.aos/1056562461}{euclid.aos/1056562461} 
        \item Each step requires $\sim\mathcal{O}(3-5)$ evaluations, need $N\sim\mathcal{O}(D)$ steps per chain to decorrelate from start point.
        \item Worse than MultiNest in low dimensions, exponentially more efficient in high dimensions.
        \item Chain-based $\Rightarrow$ allows exploitation of fast-slow hierarchy.
        \item Under active maintenance/development
    \end{itemize}
\end{frame}

\section{Advances in Nested Sampling}
\begin{frame}
    \frametitle{Further advances in nested sampling}
    \begin{itemize}
        \item Dynamic nested sampling
            \begin{itemize}
                \item \href{https://arxiv.org/abs/1704.03459}{arXiv:1704.03459} 
                \item Allows one to refine a run and generate more points, e.g.\ in the posterior bulk, or prior tails.
            \end{itemize}
        \item Consistency checking
            \begin{itemize}
                \item \href{https://arxiv.org/abs/1804.06406}{arXiv:1804.06406} 
                \item Unweaving runs allows for cross-checking and testing for imperfect contour sampling.
            \end{itemize}
        \item Diffusive nested sampling
            \begin{itemize}
                \item \href{https://arxiv.org/abs/0912.2380}{arXiv:0912.2380} 
                \item Fuzzy contours represent an alternative approach to nested sampling
            \end{itemize}
        \item PolyChord 2.0
            \begin{itemize}
                \item \href{https://arxiv.org/abs/0912.2380}{arXiv:0912.2380} 
                \item Under active develoment, promises $\sim\mathcal{O}(D)$ speed-up
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Takeaway points}
    \begin{itemize}
        \item Nested sampling is far more than a posterior sampler.
        \item To do nested sampling in high dimensions, you cannot use MultiNest.
        \item Fast-slow hierarchies have proven extremely useful in speeding up Planck, AMI and DES analyses.
        \item PolyChord is available on GitHub: \href{https://github.com/PolyChord/PolyChordLite}{github.com/PolyChord/PolyChordLite}
        \item PolyChord interfaces are available for CosmoMC, cosmosis, MontePython and GAMBIT\@.
    \end{itemize}
\end{frame}

\end{document}
```
# branch: remotes/origin/esac_2018
```tex
\documentclass[%
    %handout
]{beamer}
\usepackage{cambridge_lecture}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\include{include/beamer_commands}

\title{Inflation, curvature and kinetic dominance}
%\subtitle{<++>}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
\includegraphics[width=0.2\textwidth]{kicc.png}\\
Kavli Institute for Cosmology \\
\& Cavendish Astrophysics \\
University of Cambridge
}
\date{3\textsuperscript{rd} December 2018}

\newcommand{\Rk}{\mathcal{R}_\K^{\phantom{\ast}}}
\newcommand{\K}{\mathbf{k}}
\newcommand{\prm}[1]{{{#1}^{\prime}}}
\newcommand{\dprm}[1]{{#1}^{\prime\prime}}
\renewcommand{\d}[2][]{\operatorname{d}^{#1}\!{#2}}

\begin{document}

\begin{frame}
    \titlepage{}
\end{frame}

\begin{frame}
    \frametitle{Lightning review of inflation}
    \begin{itemize}
        \item Inflation explains observed present-day flatness and homogeneity.
        \item A primordial accelerated phase $\ddot{a}>0$ shrinks horizon $1/aH$.
    \end{itemize}
    \begin{figright}[0.45]{./figures/SmallField.pdf}
    \begin{itemize}
        \item Fill the universe with a homogeneous scalar field $\phi$:
        \begin{align*}                                                 
            H^2 &= \frac{1}{3}\left( \frac{1}{2}\dot\phi^2 + V(\phi) \right) -\frac{K}{a^2},\\
            0 &= \ddot\phi + 3 H \dot\phi + \prm{V}(\phi).
        \end{align*}
    \end{itemize}
    \end{figright}
    \begin{itemize}
        \item Slow roll solutions have $\dot\phi^2 \ll V(\phi)\Rightarrow {H\approx H_{*}} \Rightarrow a\propto e^{H_{*} t}$.
        \item Small $\delta H$ generates $\mathcal{P} = A_s {\left(k/k_*\right)}^{n_s-1}$ with $n_s\ne 1$.
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{The problem with eternal inflation}
    \begin{itemize}
        \item The canonical view of inflation has an initially {\em eternal\/} exponential expansion phase $a\propto e^{H_{*} t}$ as $t\to-\infty$.
        \item This viewpoint is only compatible with the flat case ($K=0$).
        \item In flat case, there is a rescaling symmetry $a\to \alpha a$.
        \item In curved case ($K\ne0$), $a$ is physically interpretable as (pseudo) radius.
        \item Inflation is limited by domination of curvature energy density $-\frac{K}{a^2}$.
        \item If one invokes inflation to flatten the universe, you cannot assume it is flat initially.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Primordial horizon evolution with curvature}
    \framesubtitle{Analytic approximation}
    \begin{figright}[0.45]{./figures/rho.pdf}
        \begin{itemize}
            \item Equation of motion of horizon:
                \begin{equation*}
                    \frac{\d{}}{\d{N}}\log \frac{1}{aH} = -1 - \frac{K}{{(aH)}^2} + \frac{\dot{\phi}^2}{2H^2}.
                \end{equation*}
            \item Assuming slow-roll: $\dot{\phi}^2 \ll H^2$
                \begin{equation*}
                    \log\frac{1}{aH} = -\frac{1}{2}\log{\left( e^{N-N_\mathrm{start}} - K \right)}.
                \end{equation*}
            \item Closed: Limit amount of inflation
            \item Open: Limit on Horizon size
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Primordial horizon evolution with curvature}
    \framesubtitle{Numerics}
    \includegraphics[width=\textwidth]{./figures/horizon_kinetic.pdf}
    \begin{itemize}
        \item Evolution for closed and open cases, such that $N_*=50$.
        \item Inflation preceded by a kinetically dominated phase $\dot{\phi}^2\gg V(\phi)$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Primordial vs present-day curvature}
    \includegraphics[width=\textwidth]{./figures/horizon_history.pdf}
\end{frame}

\begin{frame}
    \frametitle{Kinetically dominated power spectra}
    \framesubtitle{primordial power spectrum}
    \begin{figright}[0.3]{./figures/pps_Nstar.pdf}
        \begin{itemize}
            \item Eternal inflating models have $\mathcal{P} = A_s {\left(k/k_*\right)}^{n_s-1}$
            \item Finite amount of inflation introduces cutoff and oscillations. 
            \item Hergt et al 2018 (arXiv:1809.07737)
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Kinetically dominated power spectra}
    \framesubtitle{CMB power spectrum}
    \includegraphics[width=\textwidth]{./figures/cmb_KD_Ndagg.pdf}
    \begin{itemize}
        \item Flat case can reproduce suppression of power
        \item Oscillations have wrong location to explain $\ell \sim 30$ feature.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kinetically dominated power spectra}
    \framesubtitle{Parameter estimation and model comparison}
    \includegraphics[width=0.6\textwidth]{./figures/cosmo_nrNN.pdf}\includegraphics[width=0.4\textwidth]{./figures/cosmo_evidences.pdf}
\end{frame}

\begin{frame}
    \frametitle{Power spectra with primordial curvature.}
    \begin{figright}[0.3]{./figures/dCl.pdf}
    \begin{itemize}
        \item Primordial curvature is able to move oscillation to correct location
        \item Preliminary results: Need full constraint pipeline.
        \item Discretised PPS in closed case.
    \end{itemize}
    \end{figright}
\end{frame}



\begin{frame}
    \frametitle{Computing the primordial power spectrum}
    \begin{itemize}
        \item Comoving curvature perturbation $\Rk$, Power spectrum $\mathcal{P}(k) \propto |\Rk|^2$
        \item Mukhanov-Sasaki equation:
    \end{itemize}
\only<1>{%
\begin{equation*}
    0=
    \dprm{\Rk}
    +2\frac{\prm{z}}{z}\prm{\Rk}
    +\K^2\Rk
\end{equation*}
}
\only<2->{%
\begin{equation*}
    0=
    \dprm{\Rk}
    +\left[2\frac{\prm{z}}{z} +  2K\mathcal{E}\frac{\mathcal{H}  - \frac{\prm{z}}{z} }{\K^2 + K\mathcal{E}}\right]\prm{\Rk}
    +\left[\K^2 + K \frac{\K^2 - K\mathcal{E}- \frac{2\K^2}{\mathcal{H}} \frac{\prm{z}}{z}}{\K^2 + K\mathcal{E}}\right]\Rk
\end{equation*}
}
\onslide<2->{%
    \begin{equation*}
    \mathcal{E} = \mathcal{H}/\dot{\phi}^2
    \end{equation*}
\begin{align*}
    \K^2 &= k(k+2)-3 & K>0 \\
    \K^2 &= k^2 & K=0 \\
    \K^2 &= k^2+3 & K<0
\end{align*}
}

\end{frame}

\begin{frame}
    \frametitle{Problems to be overcome}
    \begin{itemize}
        \item How to set initial condition on $\Rk$ for low-$\K$ modes?
            \begin{itemize}
                \item Usually do so by invoking Bunch-Davies vacuum, which is tied to eternal inflation
                \item Both when and how they are set becomes important
            \end{itemize}
        \item Computing the MS equation numerically becomes bottleneck in computation: Need faster integrators.
        \item Must take care with properly discretised spectra
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Runge-Kutta-Wentzel-Kramers-Brillioun methods}
    \begin{figright}[0.3]{./figures/RKWKB.pdf}
        \begin{itemize}
            \item Rapid solving of equations with oscillatory solutions.
            \item Runge-Kutta based on Taylor series
            \item Replace polynomials with oscillating solutions (e.g.\ Airy, Bessel or WKB).
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Further reading}
    \begin{itemize}
        \item Kinetic initial conditions: Handley et al.\ 2015 (arXiv:1401.2253)
        \item Quantum Kinetic Dominance: Handley et al.\ 2016 (arXiv:1607.04148)
        \item Kinetic dominance: Hergt et al.\ 2018 (arXiv:1809.07185)
        \item Kinetic constraints: Hergt et al.\ 2018 (arXiv:1809.07737)
        \item Mukhanov-Sasaki evolution: Haddadin et al..\ 2018 (arXiv:1809.11095)
    \end{itemize}
\end{frame}

\end{document}
```
# branch: remotes/origin/icic_2018
```tex
\documentclass[%
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}

% todo 
% - Ligo actual data
% -define IMRPhenom, EOBNR


\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\text{knots}}
\newcommand{\nlive}{n_\text{live}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

%Nested Sampling: an efficient and robust Bayesian inference tool for
%astrophysics and cosmology
%
%Nested sampling is an alternative MCMC technique for integrating and exploring
%probability distributions. It has become widely adopted in the field of
%cosmology as the de-facto tool for computing Bayesian evidences and sampling
%challenging a-priori unknown parameter spaces.
%
%In this talk, I will give an introduction to the principles of Bayesian model
%comparison and parameter estimation, an explanation of the theory of nested
%sampling, a survey of the current state-of-the art (MultiNest, PolyChord,
%DNest and Dynesty) and the future of the field. Throughout I will illustrate
%with examples of it's application in cosmology and astrophysics, ranging from
%inflationary physics to exoplanets.





\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{Nested Sampling}
\subtitle{An efficient and robust Bayesian inference tool\\ for astrophysics and cosmology }
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{May 9, 2018}

\include{include/beamer_commands}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\section{Fitting a line to data}
\begin{frame}
    \frametitle{Motivating example}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_points.pdf}
        \begin{itemize}
            \item We have noisy data $D$
            \item We wish to fit a model $M$
            \item Functional form $y=f_M(x;\theta)$
            \item For example:
                \begin{align}
                     f_\text{linear}(x;\theta)&=a x + b       \nonumber\\
                     f_\text{quadratic}(x;\theta)&=a x^2 + b  \nonumber
                \end{align}
            \item Model parameters $\theta= (a,b)$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ best-fit}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff.pdf}
        \begin{itemize}
            \item For each parameter set $\theta$:
                \[
                    \chi^2(\theta) = \sum_i \left|y_i - f(x_i;\theta)\right|^2
                \]
            \item Minimise $\chi^2$ wrt $\theta$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ with non-uniform data errors}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data.pdf}
        \begin{itemize}
            \item If data have non-uniform errors:
                \[
                    \chi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|^2}{\sigma_i^2}
                \]
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Problems with $\chi^2$}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff_2.pdf}
        \begin{itemize}
            \item How do we differentiate between models
            \item Why square the errors? -- could take absolute:
                \[
                    \psi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|}{\sigma_i}
                \]
            \item Where does this approach even come from?
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Multivariate probability}
    \begin{itemize}
        \item Marginalisation:
            \begin{equation*}
                P(x) = \int P(x,y) dy
            \end{equation*}
        \item Conditioning:
            \begin{equation*}
                P(y|x) = \frac{P(x,y)}{P(x)} = \frac{P(x,y)}{\int P(x,y) dy}
            \end{equation*}
        \item De-Conditioning:
            \begin{equation*}
                P(x|y) P(y) = P(x,y)
            \end{equation*}
        \item Bayes theorem:
            \begin{equation*}
                P(y|x) = \frac{P(x|y) P(y)}{P(x)}
            \end{equation*}
            \begin{center}
                ``To flip a conditional $P(x|y)$, you first de-condition on $y$,\\ and then re-condition on $x$.''
            \end{center}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Probability distributions}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.6]{./figures/data_diff_1.pdf}
        \begin{itemize}
            \item The probability of observing a datum:
                \[
                    P(y_i | \theta,M) = \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right)
                \]
            \item The probability of observing the data:
                \begin{align}
                    P(D | \theta,M) &= \prod_i \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right) \nonumber\\
                    &=  \frac{1}{\prod_i\sqrt{2\pi}\sigma_i}\exp\sum_i{-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}} \nonumber\\
                    &\propto e^{-\chi^2(\theta)/2}
                    \nonumber
                \end{align}
        \end{itemize}
    \end{figright}
\end{frame}



\begin{frame}
    \frametitle{Maximum likelihood}
    \framesubtitle{Fitting lines to data}
    \begin{figleft}[0.6]{./figures/data_diff.pdf}
        \begin{itemize}
            \item Minimising $\chi^2(\theta)$  is equivalent to maximising $P(D|\theta,M) \propto e^{-\chi^2(\theta)/2}$
            \item $P(D|\theta,M)$ is called the Likelihood $L=L(\theta)$ of the parameters $\theta$
            \item ``Least squares'' $\equiv$ ``maximum likelihood'' \\(if data are gaussian).
        \end{itemize}
    \end{figleft}
\end{frame}

\begin{frame}
    \frametitle{Bayesian inference}
    \begin{itemize}
        \item Likelihood $L=P(D|\theta,M)$ is undeniably correct.
        \item Frequentists construct inference techniques purely from this function.
        \item The trend is cosmology is to work with a Bayesian approach.
        \item What we want are things like $P(\theta|D,M)$ and $P(M|D)$.
        \item To invert the conditionals, we need Bayes theorem:
            \begin{align}
                P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
                P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Terminology}
    \framesubtitle{Bayesian inference}
    \begin{align}
        P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
        \text{Posterior} &= \frac{\text{Likelihood}\times\text{Prior}}{\text{Evidence}} \nonumber
    \end{align}
    \begin{align}
        P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber\\
        \text{Model probability} &= \frac{\text{Evidence}\times\text{Model Prior}}{\text{Normalisation}} \nonumber
    \end{align}
\end{frame}

\begin{frame}
    \frametitle{The prior}
    \framesubtitle{Example: Biased coins}
    \begin{itemize}
        \item Need to define the \textbf{Prior} $P(\theta)$ --- probability of the bias, given no data
        \item Represents our knowledge of parameters before the data -- subjective
        \item Frequentists view this as a flaw in Bayesian inference. 
        \item Bayesians view this as an advantage
        \item Fundamental rule of Inference:\pause\\
            \vfill
            \begin{center}
                \Large You cannot extract information from data\\ without making assumptions 
            \end{center}
            \vfill
        \item All Bayesians do is make them explicit
        \item Any method that claims it is ``objective'' is simply hiding them
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Parameter estimation}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.3]{./figures/parameters.pdf}
        \begin{itemize}
            \item We may use $P(\theta|D,M)$ to inspect whether a model looks reasonable
        \end{itemize}
        \includegraphics[width=\textwidth]{./figures/data.pdf}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Predictive posterior}
    \begin{figright}[0.4]{./figures/fgivenx.pdf}
        More useful to plot:
        \begin{align}
        &P(y|x) = \nonumber\\
        &\int P(y|x,\theta) P(\theta) d\theta \nonumber
        \end{align}
        (all conditioned on $D,M$)
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Model comparison}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.33]{./figures/evidences_log.pdf}
        \begin{itemize}
            \item We may use the Bayezian evidence $Z$ to determine whether a model is reasonable.
            \item $Z = P(D|M) = \int P(D|M,\theta)P(\theta|M)d\theta$
            \item Normally assume uniform model priors $Z \propto P(M|D)P(M)$.
        \end{itemize}
    \end{figright}
\end{frame}
\begin{frame}
    \frametitle{Model comparison}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.33]{./figures/evidences_lin.pdf}
        \begin{itemize}
            \item We may use the Bayezian evidence $Z$ to determine whether a model is reasonable.
            \item $Z = P(D|M) = \int P(D|M,\theta)P(\theta|M)d\theta$
            \item Normally assume uniform model priors $Z \propto P(M|D)P(M)$.
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Line fitting (context)}
    \begin{figright}[0.5]{./figures/supernovae.pdf}
        \begin{itemize}
            \item Whilst this model seems a little trite\ldots
            \item\ldots determining polynomial indices \\$\equiv$ determining cosmological material content:
        \end{itemize}
    \end{figright}
        \[
            {\left( \frac{H}{H_0} \right)}^2 = 
            \Omega_\text{r} {\left( \frac{a_0}{a} \right)}^4+
            \Omega_\text{m} {\left( \frac{a_0}{a} \right)}^3+
            \Omega_k {\left( \frac{a_0}{a} \right)}^2+
            \Omega_\Lambda
            \]
\end{frame}



\begin{frame}
    \frametitle{Quantifying error with Probability}

    \begin{itemize}
        \item As scientists, we are used to seeing error bars on results.
        \item Age of the universe ({\em Planck\/}): 
         \[13.73\pm 0.12\:\text{billion years old.}\]
        \item Masses of LIGO GW150914 binary merger: 
        \[m_1 = 39.4^{+5.5}_{-4.9}\:M_\odot,\qquad m_2 = 30.9^{+4.8}_{-4.4}\:M_\odot \]
        \item These are called {\em credible intervals}, state that we are e.g.\ $90\%$ confident of the value lying in this range.
        \item More importantly, these are {\em summary statistics}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{LIGO binary merger}
    \begin{columns}
        \begin{column}{0.65\textwidth}
            \includegraphics[width=\textwidth]{./figures/ligo_m1_m2.pdf}
        \end{column}
        \begin{column}{0.35\textwidth}
            \begin{itemize}
                \item Summary statistics summarise a full probability distribution.
                \item One goal of inference is to produce these probability distributions.
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Theory}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics[width=\textwidth]{./figures/ligo_schematic.png}
\end{frame}

\begin{frame}
    \frametitle{The parameters $\Theta$ of the model $M$}
    \framesubtitle{Extended example of inference: LIGO}
    Theoretical signal depends on:
    \begin{itemize}
        \item $m_1, m_2$: mass of binary
        \item $\theta, \phi$: sky location
        \item $r$: luminosity distance 
        \item $\Phi_c, t_c$: phase and time of coalescence
        \item $i, \theta_\text{sky}$: inclination and angle on sky (orbital parameters)
    \end{itemize}
\end{frame}



\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{itemize}
        \item Cannot plot the full posterior distribution:
            \[\mathcal{P}(\Theta) \equiv P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\text{sky}|D,M)\]
        \item Can plot 1D and 2D {\em marginalised\/} distributions e.g:
            \begin{align}
            &P(m_1,m_2|D,M)=\nonumber\\&\int P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\text{sky}|D,M) \,d\theta \,d\phi \,dr \,d\Phi_c \,d t_c \,d i \,d\theta_\text{sky}\nonumber
            \end{align}
        \item May do this for each pair of parameters
        \item Generates a {\em triangle plot}
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{figleft}[0.65]{./figures/ligo_full.pdf}
		\begin{itemize}
          \item Does give insight
          \item Not the full picture
		\end{itemize}
    \end{figleft}
\end{frame}


\begin{frame}
    \frametitle{Sampling}
    \framesubtitle{How to describe a high-dimensional posterior}

	\begin{figright}{./figures/ligo_m1_m2.pdf}
		\begin{itemize}
          \item In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item Gridding is doomed to failure for $D\gtrsim4$.
          \item {\em Sampling\/} the posterior is an excellent compression scheme.
		\end{itemize}
	\end{figright}
 
\end{frame}
%
\begin{frame}
    \frametitle{Why do sampling?}
    \framesubtitle{Marginalisation over the posterior}

    \begin{itemize}
        \item Set of $N$ samples $S = \{\Theta^{(i)}: i=1,\ldots N:\: \Theta^{(i)}\sim\mathcal{P}\}$
        \item Mean mass: \[
                \bar{m}_1 \equiv\langle m_1\rangle_\mathcal{P}
                \only<1>{\equiv \int m_1 P(\theta|D,M) d\theta }
                \only<2>{\approx \frac{1}{N}\sum_{i=1}^N m_1^{(i)}}
                \only<3>{\approx \frac{\sum_{i=1}^N w^{(i)} m_1^{(i)}}{\sum_{i=1}^N w^{(i)}}}
            \]
        \item Mass covariance: \[
                \mathrm{Cov}(m_1,m_2)
            \only<1>{\equiv \int (m_1-\bar{m}_1)(m_2-\bar{m}_2) P(\theta|D,M) d\theta }
                \only<2>{\approx \frac{1}{N}\sum_{i=1}^N (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}
                \only<3>{\approx \frac{\sum_{i=1}^N w^{(i)} (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}{\sum_{i=1}^N w^{(i)}}}
            \]
        \item Marginalised samples: Just ignore the other coordinates.
        \item N.B. Typically have {\em weighted\/} samples
    \end{itemize}
\end{frame}
%
\begin{frame}
    \frametitle{Parameter estimation}
    \begin{itemize}
        \item The name of the game is therefore drawing samples $S$ from the posterior $\mathcal{P}$ with the minimum number of likelihood calls.
        \item Gridding is doomed to failure in high dimensions.
        \item Enter Metropolis Hastings.
    \end{itemize}
\end{frame}



\section{Metropolis Hastings}


\begin{frame}
  \frametitle{Metropolis Hastings} 
  \begin{itemize}
      
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \begin{enumerate}
        \item Propose random step
        \item If uphill, make step\ldots
          
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Metropolis Hastings} 
  \includegraphics[width=\textwidth]{movies/MCMC_0.pdf}
\end{frame}
\begin{frame}
  \frametitle{Metropolis Hastings} 
  \movie[width=\textwidth,height=0.52\textwidth]{}{movies/MCMC.mp4}
\end{frame}
\begin{frame}
  \frametitle{Metropolis Hastings} 
  \includegraphics[width=\textwidth]{movies/MCMC_1.pdf}
\end{frame}




\begin{frame}
  \frametitle{Metropolis Hastings} 
  \framesubtitle{Struggles with\ldots}
  \pause
  \begin{enumerate}
      \item Burn in
      \item Multimodality
      \item Correlated Peaks
      \item Phase transitions
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte-Carlo} 
  \begin{itemize}
      \item Key idea: Treat $\log L(\Theta)$ as a potential energy
      \item Guide walker under ``force'': \[F(\Theta) =\nabla \log L(\Theta)\]
      \item Walker is naturally ``guided'' uphill
      \item Conserved quantities mean efficient acceptance ratios.
      \item stan is a fully fledged, rapidly developing programming language with HMC as a default sampler.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Ensemble sampling} 
  \begin{itemize}
      \item Instead of one walker, evolve a set of $n$ walkers.
      \item Can use information present in ensemble to guide proposals.
      \item emcee: affine invariant proposals.
      \item emcee is not the only (or even best) affine invariant approach.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The fundamental issue with all of the above} 

  \begin{itemize}
    \item They don't give you evidences!
  \begin{align}
    \ev 
    &= \prob(D|M) 
    \nonumber\\
    &= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
    &= \left\langle \lik \right\rangle_\prior
    \nonumber
  \end{align}
    \item MCMC fundamentally explores the posterior, and cannot average over the prior.
    \item Thermodynamic annealing 
    \begin{itemize}
        \item Suffers from same tuning issues as MCMC
    \end{itemize}
    \item Nearest neighbor volume estimation (Heavens arXiv:1704.03472)
    \begin{itemize}
        \item Does not scale to high dimensions $D\gtrsim15$.
    \end{itemize}
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}

\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to traditional MCMC!} 

  \begin{itemize}
    \item Nested sampling is a completely different way of sampling. 
    \item Uses ensemble sampling to compress prior to posterior.
  \end{itemize}
  
  New procedure: 

  
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      
    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\prior$). 
      
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new uniform sample with higher likelihood
  \end{description}

  
  Requires one to be able to uniformly within a region, subject to a {\em hard likelihood constraint}.

\end{frame}



\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}

\begin{frame}
  \frametitle{Nested sampling} 

  \begin{itemize}
    \item The set of dead points are posterior samples with an appropriate weighting factor
    \item They can also be used to calculate evidences, since it sequentially updates the priors.
  \end{itemize}
 
\end{frame}

\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}
  \foreach \pagenum in {1,...,16} {%
      \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/lesbesgue}
  }
\end{frame}



\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item Most of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm.
  \end{itemize}
 
\end{frame}

\begin{frame}
\frametitle{MultiNest }
  \framesubtitle{arXiv:0809.3437 arXiv:0704.3704 arXiv:1306.2144}
  \includegraphics[width=\textwidth]{figures/multinest.pdf}
\end{frame}

\begin{frame}
  \frametitle{PolyChord}
  \framesubtitle{arXiv:1502.01856 arXiv:1506.00171}
  \includegraphics[width=\textwidth]{figures/polychord.png}
\end{frame}

\begin{frame}
  \frametitle{Diffusive nested sampling}
  \framesubtitle{arXiv:0912.2380}
  \includegraphics[width=\textwidth]{figures/dnest.pdf}
\end{frame}

\begin{frame}
\frametitle{PolyChord vs MultiNest}
\begin{itemize}
    \item MultiNest excels in low dimensions $D<10-20$.
    \item PolyChord can go up to $\sim 150$.
    \item Crossover is problem dependent
    \item PolyChord can also exploit fast-slow hierarchy
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Exoplanets}
  \framesubtitle{Nested sampling in action}
  \includegraphics[width=\textwidth]{figures/rv_full.pdf}
\end{frame}

\begin{frame}
  \frametitle{Exoplanets}
  \framesubtitle{Nested sampling in action}
  \begin{itemize}
      \item Simple radial velocity model
          \begin{equation}
              \nu(t;\theta) = \sum_{p=1}^N K_p \sin(\omega_p t + \phi_p)\nonumber
          \end{equation}
      \item Fit each model to data.
      \item Posteriors on model parameters $[(K_p,\omega_p,\phi_p),p=1\cdots N]$ quantify knowledge of system characteristics.
      \item Evidences of models determine relative likelihood of number of planets in system
  \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Cosmology}
    \framesubtitle{Another example.}

    \[\lik(\Theta) = P(D|\Theta,M)\]
    \begin{align}
        \onslide<2->{D =& \{C_\ell\only<6->{^\text{(Planck)}}\}} 
        \onslide<15->{+\{\text{LSS}\}} 
        \onslide<16->{+\{\text{``Big Data''}\}}
        \nonumber\\
        \onslide<3->{M =& \Lambda\text{CDM}} 
        \onslide<9->{+ \text{extensions} }
        \nonumber\\
        \onslide<4->{\Theta =& \Theta_{\Lambda \text{CDM}}} \onslide<7->{+ \Theta_\text{Planck}} \onslide<10->{+ \Theta_\text{extensions}}\nonumber\\
        \onslide<5->{\Theta_{\Lambda \text{CDM}} =& ( \Omega_b h^2, \Omega_c h^2, 100\theta_{MC}, \tau, {\rm{ln}}(10^{10} A_s), n_s) \nonumber\\}
        \onslide<8->{\Theta_\text{Planck} =& (y_{\rm cal}, A^{CIB}_{217}, \xi^{tSZ-CIB}, A^{tSZ}_{143}, A^{PS}_{100}, A^{PS}_{143}, A^{PS}_{143\times 217}, A^{PS}_{217},\nonumber\\
        &A^{kSZ}, A^{{\rm dust}TT}_{100}, A^{{\rm dust}TT}_{143}, A^{{\rm dust}TT}_{143\times 217}, A^{{\rm dust}TT}_{217}, c_{100}, c_{217}) \nonumber\\}
        \onslide<11->{\Theta_\text{extensions} =& (
                n_{\rm run}
                \only<12->{,n_{\rm run,run}}
                \only<13->{,w}
                \only<14->{,\Sigma m_\nu, m_{\nu,{\rm{sterile}}}^{\rm{eff}}}
        ) \nonumber}
    \end{align}

    \begin{itemize}
        \item<17->{Parameter estimation: $L, \pi \to \mathcal{P}$: model parameters}
        \item<17->{Model comparison: $L, \pi \to Z$: how good model is}
    \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Nested Sampling in action}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}
%
%
%%\begin{frame}
%%  \frametitle{Planck data}
%%  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
%%  \begin{itemize}
%%    \item<2-> Temperature data TT+lowP
%%    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
%%    \item<4-> Marginalised plots of $\PR(k)$
%%    \item<5->
%%      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
%%  \end{itemize}
%%\end{frame}
%
%
%
\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knots}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{8 internal knots}
  \frametitle<10>{Bayes Factors}
  \frametitle<11>{Marginalised plot}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.8\textwidth]{figures/0TT_fgivenx}
    \includegraphics<2>[width=0.8\textwidth]{figures/1TT_fgivenx}
    \includegraphics<3>[width=0.8\textwidth]{figures/2TT_fgivenx}
    \includegraphics<4>[width=0.8\textwidth]{figures/3TT_fgivenx}
    \includegraphics<5>[width=0.8\textwidth]{figures/4TT_fgivenx}
    \includegraphics<6>[width=0.8\textwidth]{figures/5TT_fgivenx}
    \includegraphics<7>[width=0.8\textwidth]{figures/6TT_fgivenx}
    \includegraphics<8>[width=0.8\textwidth]{figures/7TT_fgivenx}
    \includegraphics<9>[width=0.8\textwidth]{figures/8TT_fgivenx}
    \includegraphics<10>[width=0.8\textwidth]{figures/Bayes_TT.pdf}
    \includegraphics<11>[width=0.8\textwidth]{figures/combined_fgivenx.pdf}

  \end{center}
\end{frame}
\begin{frame}
  \frametitle<1>{COBE (pre-2002)}
  \frametitle<2>{COBE et al (2002)}
  \frametitle<3>{WMAP (2012)}
  \frametitle<4>{Planck (2013)}
  \frametitle<5>{Planck (2015)}
  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}


  \begin{center}
    \includegraphics<1>[width=0.6\textwidth]{figures/cobe}
    \includegraphics<2>[width=0.6\textwidth]{figures/pre_WMAP}
    \includegraphics<3>[width=0.6\textwidth]{figures/WMAP}
    \includegraphics<4>[width=0.6\textwidth]{figures/planck_2013}
    \includegraphics<5>[width=0.6\textwidth]{figures/planck_2015}

  \end{center}
\end{frame}

\begin{frame}
    \frametitle{Unweaving runs}
    \framesubtitle{Advances in nested sampling}
    \begin{itemize}
        \item John Skilling noted that two nested sampling runs can be combined in likelihood order to produce a valid run with a larger number of live points.
        \item The reverse is also true (Higson 1704.03459).
        \item In general, a run with $n$ live points can be ``unweaved'' into $n$ runs with a single live point.
        \item Useful for providing convergence diagnostics and better parameter estimation.
    \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Dynesty}
  \framesubtitle{Advances in nested sampling (arXiv:1704.03459)}
  \includegraphics[width=\textwidth]{figures/dynesty.pdf}
  The number of live points can be varied dynamically in order to oversample regions of interest
\end{frame}

\begin{frame}
    \frametitle{Multi-temperature sampling}
    \framesubtitle{Advances in nested sampling}
    \begin{itemize}
        \item By compressing from prior to posterior, Nested Sampling's weighted samples are fundamentally different from traditional MCMC.
        \item Nested sampling tails and peaks equally.
        \item We can define the ``temperature'' of a distribution in analogy with thermodnyamics:
            \begin{equation}
                \log L \sim E \Rightarrow P \propto e^{-\beta E} = e^{-E/kT},\quad \beta = 1\nonumber
            \end{equation}
        \item Sampling at different temperatures can be useful for exploring tails.
        \item Nested sampling runs give you the full partition function $\log Z(\beta)$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Nested importance sampling}
    \framesubtitle{Future research}
    \begin{itemize}
        \item Much of the time spent in a nested sampling run is spent ``compressing the tails''.
        \item Sometimes we have a-priori good knowledge of the posterior bulk (analagous to an MCMC proposal distribution).
        \begin{align}
            Z_0 &= \int L(\theta) \pi_0(\theta) d\theta, \qquad
            Z_1 = \int L(\theta) \pi_1(\theta) d\theta \nonumber\\
            &= \int L(\theta)\pi_1(\theta) \frac{\pi_0(\theta)}{\pi_1(\theta)} d\theta
            = \left\langle \frac{\pi_0(\theta)}{\pi_1(\theta)} \right\rangle_{P_1}  \nonumber
        \end{align}
        \item This importance weighting only works if you have a lot of tail samples.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$N$-$\sigma$ contours}
    \framesubtitle{Future research}
    \begin{itemize}
        \item Traditional posterior samples only allow you to plot contours out to 2-3$\sigma$.
        \item Nested sampling fully samples the tails, so in theory one could do $20\sigma$ contours.
        \item Requires further thought in alternatives to kernel density estimation.
    \end{itemize}
\end{frame}

%\begin{frame}
%    \frametitle{PolyChord 2.0}
%    \framesubtitle{Advances in nested sampling}
%\end{frame}

\include{include/further_reading}

\end{document}
```
# branch: remotes/origin/maxent_2018
```tex
\documentclass[%
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}
\usepackage{mathtools}
\newtheorem*{theorem*}{Theorem}


\title{Derived parameters with specified distributions}
\subtitle{Maximum entropy prior choices}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{16\textsuperscript{th} May 2018}

\include{include/beamer_commands}

\begin{document}

\begin{frame}
    \titlepage{}
\end{frame}

\begin{frame}
    \frametitle{Bayesian inference}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{itemize}
                \item Model parameters $x$ describing data $D$:
                    \begin{align}
                        P(x|D) &= \frac{P(D|x) P(x)}{P(D)}
                        \nonumber\\
                        \text{Posterior} &= 
                        \frac{\text{Likelihood}\times \text{Prior}}{\text{Evidence}}
                        \nonumber
                    \end{align}
                \item Need Prior distribution $P(x)$
                \item Chosen to reflect initial knowledge, without data
                \item Harder to do with modern inference techniques:
                    \begin{itemize}
                        \item Non-parametric (model-independent) reconstructions
                        \item Hierarchical models
                    \end{itemize}
            \end{itemize}
        \end{column}
        \begin{column}{0.4\textwidth}
            \include{include/reconstruction}
            \includegraphics[width=0.95\textwidth]{figures/hierarchical.pdf}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Prior construction}
    \framesubtitle{The principle of maximum entropy}
    \begin{itemize}
        \item We may wish to construct a prior ``assuming the least information''
        \item One way to quantify this is using the Shannon entropy:
            \begin{equation}
                H(\Omega) 
                = \sum_{E\in \Omega} P(E) \log \frac{1}{P(E)}
                \nonumber
            \end{equation}
        \item Shannon information 
            \begin{equation}
                \mathcal{I}(E) = \log \frac{1}{P(E)},
                \qquad
                H = \left\langle\log \mathcal{I}(E)\right\rangle_{E\in \Omega}
                \nonumber
            \end{equation}
        \item We construct priors by minimising $H$, subject to knowledge constraints
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Maximum entropy prior examples}
    \begin{itemize}
        \item Known mean $\mu$ and variance $\sigma$ $\Rightarrow$ Gaussian:
            \begin{equation}
                P(x) = \tfrac{1}{\sqrt{2\pi}\sigma}\exp\left[-\tfrac{{(x-\mu)}^2}{2\sigma^2}\right]
                \nonumber
            \end{equation}
        \item Known mean $x_0$ and positive $x>0$ $\Rightarrow$ Exponential:
            \begin{equation}
                P(x) = \tfrac{1}{x_0}\exp\left[-x/x_0\right]
                \nonumber
            \end{equation}
        \item Positive $x>0$ $\Rightarrow$ Logarithmic (improper):
            \begin{equation}
                P(x) \propto 1/x
                \nonumber
            \end{equation}
        \item Nothing  $\Rightarrow$ Uniform (improper):
            \begin{equation}
                P(x) \propto 1
                \nonumber
            \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The importance of plotting priors}
    \framesubtitle{VSA cosmological parameters (astro-ph:0212497)}

    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}
            \item<1-> Consider constraint on Hubble parameter $h$ $(H_0 = 100h\frac{\mathrm{kms}^{-1}}{\mathrm{Mpc}})$
            \item<3-> $h$-constraint {\em gets worse\/} with data
            \item<3-> Lesson: It is essential to plot priors and posteriors together.
            \item<3-> Particularly relevent for new data with weak constraints (e.g. EoR)
        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \includegraphics<1>[width=\textwidth]{figures/vsa_prior}
            \includegraphics<2->[width=\textwidth]{figures/vsa_posterior}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Non-parametric reconstructions}
    \framesubtitle{Example: cosmic reionisation history from CMB (Millea \& Bouchet 1804.08476)}
    \includegraphics[width=\textwidth]{figures/reio_hists_prior}
    \begin{itemize}
        \item Aim to reconstruct reionisation history $x_\mathrm{e}(z)$ from Planck data 
        \item Model-independent/non-parametric
        \item Optical depth $\tau = \int \frac{n_\mathrm{H}(z){(1+z)}^2}{H(z)}x_\mathrm{e}(z) dz $
        \item Reconstruction introduces non-trivial prior on derived parameter $\tau$
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Tau prior and posterior}
    \framesubtitle{Example: cosmic reionisation history from CMB (Millea \& Bouchet 1804.08476)}
    \centerline{%
        \includegraphics[width=0.45\textwidth]{figures/implicit_tau_priors}
        \includegraphics[width=0.45\textwidth]{figures/data_simlow}
    }
\end{frame}

\begin{frame}
    \frametitle{Reionisation posterior}
    \framesubtitle{Example: cosmic reionisation history from CMB (Millea \& Bouchet 1804.08476)}
    \centerline{%
        \includegraphics[width=0.8\textwidth]{figures/reio_hists_simlow}
    }
\end{frame}

\begin{frame}
    \frametitle{Derived parameter priors}
    \framesubtitle{Simplified example}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}
            \item Uniform distribution $q(a,b)$
            \item $\Rightarrow$ triangular distribution on $a+b$.
            \item Remove this effect by dividing out this distribution:
                \begin{equation}
                    p(a,b) = \frac{q(a,b)}{q(a+b)}
                    \nonumber
                \end{equation}
            %\item<2-> $p(a,b)$ is adjusted to counterbalance tails of $q(a+b)$.
        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \includegraphics<1>[width=\textwidth]{figures/q}
            \includegraphics<2->[width=\textwidth]{figures/p}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
\frametitle{General result: This is maximum entropy}
\framesubtitle{Handley \& Millea 1804.08143}
\begin{theorem*}
    If one has a distribution on parameters $x$ with probability density function $q(x)$ along with a derived parameter $f$ defined by a function $f=f(x)$, then the maximum entropy distribution $p(x)$ relative to $q(x)$ satisfying the constraint that $f$ is distributed with probability density function to $r(f)$ is:
\begin{equation}
    p(x)  = \frac{q(x)r(f(x))}{P(f(x)|q)},\label{eqn:final_answer}
    \nonumber
\end{equation}
where $P(f|q)$ is the probability density for the distribution induced by $q$ on $f=f(x)$.
\end{theorem*}
\end{frame}

\begin{frame}
    \frametitle{Derived parameter priors}
    \framesubtitle{Neutrino example}
	\begin{figright}{figures/neutrino_masses}
        \begin{itemize}
            \item Initial spherical log-gaussian $q$
            \item $\Rightarrow$ non-trivial shifted distribution on mass sum $m_1+m_2+m_3$
            \item Apply maxent prior forcing this distribution back to center
            \item Creates heavy tail previously ruled out by $q$.
        \end{itemize}
	\end{figright}
\end{frame}

%\begin{frame}
%    \frametitle{<++>}
%    \framesubtitle{<++>}
%\end{frame}

%\begin{frame}
%    \frametitle{<++>}
%    \framesubtitle{<++>}
%
%	\begin{figright}{<++>}
%	\end{figright}
% 
%\end{frame}


\include{include/further_reading}

\end{document}
```
# branch: remotes/origin/curvature_portsmouth_2019_old
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{<+Title+>}
\subtitle{<+subtitle+>}
\date{<+Date+>}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{<+Frame title+>}
    <+Content+>
\end{frame}

\end{document}
\documentclass[%
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}

\include{include/beamer_commands}
\usepackage{layouts}



\title{Curvature tension}
\subtitle{Evidence for a closed universe (?)}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
    Astrophysics Group, Cavendish Laboratory, University of Cambridge\\
    Kavli Institute for Cosmology, Cambridge\\
    Gonville \& Caius college, Cambridge\\
}
\date{25\textsuperscript{th} August 2019}

% Abstract
% --------
% Observational cosmology is close to breaking point. There is now a 4.9-sigma
% tension between the expansion rate $H_0$ as inferred by CMB modelling and
% $H_0$ as measured by supernovae observers. Other parameter tensions
% potentially exist between alternative combinations of data, such as a DES and
% Planck ($S_8$ and $\Omega_m$), or Planck CMB and lensing ($\Omega_K$). In
% these cases it is less obvious how to quantify the level of tension in terms
% of ``sigma'', either due to the high dimensionality of the space, or the
% non-gaussianity of the distributions. Diagnosing the location and level of
% these tensions could prove critical in determining their cause.
%
% In this talk I will discuss recent research by Pablo Lemos and myself into
% novel statistical ways to measure these tensions, how to quantify the number
% of parameters a given experiment constrains, the calibration of the DES
% evidence ratio and the problems with principle component analysis (PCA).


\begin{document}

\begin{frame}
    \titlepage{}
    \begin{center}
        Handley \arxiv{1908.09139}, \arxiv{1907.08524}\\
        Handley \& Lemos \arxiv{1902.04029}, \arxiv{1903.06682}

    \vspace{10pt}
    \url{github.com/williamjameshandley/CosmoChord}
    \url{github.com/williamjameshandley/anesthetic}
    \end{center}
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \framesubtitle{A talk of two halves}
    \begin{description}
        \item[First half:] Inference, quantifying tensions and observing curvature
        \item[Second half:] Theory and predictions of primordially curved universes
    \end{description}
\end{frame}

\begin{frame}
    \frametitle{The Hubble $H_0$ tension}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}
            \item CMB cosmologists (Planck) infer \\ $H_0=67\pm0.5\text{ km s}^{-1} \text{Mpc}^{-1}$
            \item Supernovae data  ($SH_0ES$) measure \\ $H_0=74\pm1.4 $
            \item $>4\sigma$ discrepancy could be due to:
                \begin{itemize}
                    \item Systematic error
                    \item Problem with standard model of cosmology ($\Lambda$CDM)
                \end{itemize}
            \item Inconsistent datasets shouldn't be combined
        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{overlayarea}{\textwidth}{0.5\textheight}
            \includegraphics<1>{./plots/H0.pdf}
            \includegraphics<2>{./plots/H0_combined.pdf}
            \end{overlayarea}
        \end{column}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{$\sigma_8$-$\Omega_m$ tension}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}
            \item<1-> Matter density $\Omega_m$ and RMS matter fluctuations $\sigma_8$ are constrained by 
            \item<1-> BAO and Planck look consistent
            \item<2-> DES is less clear
            \item<2-> How do you define a tension in terms of ``sigma'' for this case?
        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{overlayarea}{\textwidth}{0.5\textheight}
            \includegraphics<1>{./plots/BAO_planck.pdf}
            \includegraphics<2>{./plots/DES_planck.pdf}
            \end{overlayarea}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{$\Omega_K$ tension}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}
            \item Models with spatial curvature $\Omega_K$.
            \item Best-kept secret of Planck: only $1/10,000$ MCMC samples $\Omega_K>0$.
            \item How consistent do Planck and CMB lensing look?
            \item \arxiv{1908.09139}
        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{overlayarea}{\textwidth}{0.5\textheight}
            \includegraphics<1>{./plots/curvature.pdf}
            \includegraphics<2>{./plots/H0_combined.pdf}
            \includegraphics<3>{./plots/curvature_lensing.pdf}
            \end{overlayarea}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Quantifying tension}
    \framesubtitle{Gaussians}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            For 1D Gaussian distributions, tension is pretty easy to define:
            \[
                X = \frac{|\mu_A - \mu_B|}{\sqrt{\sigma_A^2 + \sigma_B^2}},
            \]
         where $\mu$ and $\sigma$ are the respective parameter means and standard deviations.
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{overlayarea}{\textwidth}{0.65\textheight}
                \includegraphics{./plots/H0.pdf}
            \end{overlayarea}
        \end{column}
    \end{columns}

    The multivariate $d$-dimensional equivalent to this tension would be:
    \[
        X^2_d = {(\mu_A - \mu_B)}^T{(\Sigma_A + \Sigma_B)}^{-1}(\mu_A-\mu_B),
    \]
    where $\Sigma$ is in general a covariance matrix.
\end{frame}

\begin{frame}
    \frametitle{Quantifying tension}
    \framesubtitle{non-Gaussians}
    \begin{figright}[0.4]{./figures/all_components.pdf}
        \begin{itemize}
            \item Things become less clear when distributions become ``banana like'' (\arxiv{1906.11628}),
        or worse, multimodal.
        \end{itemize}
    \end{figright}
    \begin{itemize}
        \item Many attempts to generalise the Gaussian case result in a parameterisation-dependent quantity.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Quantifying tension}
    \framesubtitle{High-dimensional spaces}
    \centerline{%
    \includegraphics[width=0.49\textwidth]{./figures/tension.pdf}
    \includegraphics[width=0.49\textwidth]{./figures/tension_rotated.pdf}
    }
    \begin{itemize}
        \item In high dimensions, things can look good when projected into 2D.
        \item We need a systematic way of seeking out tension, without relying on inspired choices of parameters to reveal them
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{The DES evidence ratio $R$}
    \begin{itemize}
        \item The Dark Energy Survey (\arxiv{1708.01530}) quantifies tension between two datasets $A$ and $B$ using the Bayes ratio:
            \[
                R = \frac{\mathcal{Z}_{AB}}{\mathcal{Z}_A \mathcal{Z}_B}
            \]
            where $\mathcal{Z}$ is the Bayesian evidence.
        \item Many attractive properties:
            \begin{itemize}
                \item Symmetry
                \item Parameterisation independence
                \item Dimensional consistency
                \item Use of well-defined Bayesian quantities
            \end{itemize}
        \item What does it mean?
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The meaning of the DES evidence ratio $R$}
    \begin{itemize}
        \item The Dark Energy Survey collaboration (\arxiv{1708.01530}) quantify tension between two datasets $A$ and $B$ using the Bayes ratio:
            \[
                R = \frac{\mathcal{Z}_{AB}}{\mathcal{Z}_A \mathcal{Z}_B} = \frac{P(A\cap B)}{P(A)P(B)} = \frac{P(A|B)}{P(A)} = \frac{P(B|A)}{P(B)}
            \]
        \item $R$ gives the relative change in our confidence in data $A$ in light of having seen $B$ (and vice-versa).
        \item $R>1$ implies we have more confidence in $A$ having received $B$.
        \item Like evidences, it is prior-dependent
        \item Increasing prior widths $\Rightarrow$ increasing confidence.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The DES evidence ratio $R$: Prior dependency}
    {\includegraphics[trim=0.6in 0.3in 0in 0in]{./plots/prior_dependency.pdf}}
    \begin{itemize}
        \item What does it mean if increasing prior widths $\Rightarrow$ increasing confidence? 
        \item Wide priors mean {\em a-priori\/} the parameters could land anywhere.
        \item We should be proportionally more reassured when they land close to one another if the priors are wide
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{How do we deal with the prior dependency in $R$?}
    \begin{description}
        \item[Option 1] Take the Bayesian route, accept the prior dependency, and spend time trying to justify why a given set of priors are ``physical''.
        \item[Option 2] Try to find a principled way of removing this prior dependency
    \end{description}
    \begin{itemize}
        \item One of the critical observations is that one can only hide tension by widening priors. Narrowing them will only ever show tension if it is present.
        \item If we could define ``Narrowest reasonable priors'' and find that $R<1$, then this would indicate tension.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$R$: a Gaussian example}
    \begin{itemize}
        \item Given two Gaussians with parameter means $\mu_A,\mu_B$ and parameter covariances $\Sigma_A,\Sigma_B$ and a prior with volume $V_\pi$:
            \begin{align}
                \log R =& -\frac{1}{2} (\mu_A-\mu_B){(\Sigma_{A}+\Sigma_{B})}^{-1}(\mu_A-\mu_B)\nonumber\\
                & + \log V_\pi\nonumber -\log\sqrt{|2\pi(\Sigma_{A}+\Sigma_{B})|} 
            \end{align}
        \item Like evidence, $R$ composed of ``Goodness of fit'', and ``Occam factor''.
        \item Ideally want would remove this Occam factor (ratio of prior to posterior volume).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{KL divergence $\mathcal{D}$, Information $\mathcal{I}$, suspiciousness $S$}
    \begin{itemize}
        \item The KL divergence quantifies the compression from prior to posterior:
            \[
                \mathcal{D} = \int P(\theta|D) \log \frac{P(\theta|D)}{P(\theta)} d\theta = \left\langle\log\frac{\text{Posterior}}{\text{Prior}}\right\rangle_\text{Posterior}
            \]
        \item It bears many similarities to an Occam factor, for a Gaussian:
            \[
                \mathcal{D} =   \log V_\pi - \log \sqrt{|2\pi\Sigma|} - \frac{1}{2}d
            \]
        \item Can define equivalent of $R$ for KL divergence, the information ratio $\mathcal{I}$
            \begin{align}
                \log R &= \mathcal{Z}_{AB} -\mathcal{Z}_A - \mathcal{Z}_B \nonumber\\
                \log \mathcal{I} &= \mathcal{D}_A + \mathcal{D}_B - \mathcal{D}_{AB} \nonumber
            \end{align}
        \item Subtracting the two removes prior dependency, giving suspiciousness:
            \[
                \log S = \log R - \log \mathcal{I}
            \]
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Suspiciousness $S$}
    \begin{itemize}
        \item For a Gaussian:
            \[
                \log S = \frac{d}{2}  -\frac{1}{2} (\mu_A-\mu_B){(\Sigma_{A}+\Sigma_{B})}^{-1}(\mu_A-\mu_B).
            \]
        \item We thus find that our original idea for tension $X^2_d=d-2\log S$.
        \item However $S$ is composed of evidences $\mathcal{Z}$ and KL divergences $\mathcal{D}$, which are Gaussian-independent concepts.
        \item The only thing remaining to determine is $d$, the ``number of parameters''.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Dimensionality $d$}
    \centerline{%
    \includegraphics[width=0.49\textwidth]{./figures/dimensions_1.pdf}
    \includegraphics[width=0.49\textwidth]{./figures/dimensions_2.pdf}
    }
    \begin{itemize}
        \item Intuition should tell us that the $d$ we need is the effective number of parameters (i.e.\ should not include unconstrained ones).
        \item Like the evidence, or the KL divergence, this ``Model dimensionality'' should be a sought-after inference quantity.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Dimensionality $\tilde{d}$}
    \begin{itemize}
        \item KL divergence is the mean of the Shannon information $I$:
            \begin{align}
                \mathcal{D} &= \int P(\theta|D) \log \frac{P(\theta|D)}{P(\theta)} d\theta = \left\langle\log\frac{\text{Posterior}}{\text{Prior}}\right\rangle_\text{Posterior}\nonumber\\
                I &= \log\frac{\text{Posterior}}{\text{Prior}}\nonumber
            \end{align}
        \item Model dimensionality proportional to variance of Shannon information:
            \[
                \frac{\tilde{d}}{2} = \text{var}\left(\frac{\text{Posterior}}{\text{Prior}}\right)_\text{Posterior}
            \]
        \item Examples from real data:
            \begin{align}
                \tilde{d}_\text{Planck} &= 15.8 \pm  0.3 &(21) \nonumber\\
                \tilde{d}_\text{DES} &= 14.0 \pm  0.3 &(26) \nonumber\\
                \tilde{d}_\text{BAO} &= 2.95 \pm  0.07 &(6) \nonumber\\
                \tilde{d}_\text{S$H_0$ES} &= 0.93 \pm  0.03 &(6) \nonumber
            \end{align}
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Headline results: $\Lambda$CDM}
    \begin{itemize}
        \item Can calibrate $X^2_d$ as on the same scale as $\chi^2_d$ to give a $p$-value-like quantity, termed ``Tension probability'' $p$, or $\sigma$
            \begin{align}
                \text{Planck 2015 vs BAO}:&      &p&=  42 \pm     4 \%        &\sigma& = 0.8\pm0.1\nonumber\\
                \text{Planck 2015 vs DES}:&      &p&=   3.2 \pm     1.0 \%    &\sigma& = 2.1\pm0.1\nonumber\\
                \text{Planck 2015 vs S$H_0$ES}:& &p&=   0.25 \pm     0.17 \%  &\sigma& = 3.1\pm0.3 \nonumber\\
                \text{Planck 2018 vs S$H_0$ES}:& &p&=   0.001 \pm     0.0001 \%  &\sigma& = 4.42\pm0.03\nonumber
            \end{align}
        \item Under this metric, S$H_0$ES is unambiguously inconsistent. BAO is consistent, and $DES$ is inconsistent, but only just. This is pleasingly similar to ones intuition.
		\item \arxiv{1902.04029}
    \end{itemize}
\end{frame}



\begin{frame}
    \frametitle{Headline results: $K\Lambda$CDM $\equiv\Lambda$CDM + $\Omega_K$}
    \begin{itemize}
        \item Cosmologies with curvature have different tension (\arxiv{1908.09139}):
            \begin{align}
                \text{Planck vs CMB lensing}:&      &p&=  1.3\pm 0.3 \% &\sigma& =2.5\pm0.1\nonumber\\
                \text{Planck vs BAO}:&      &p&=   0.25\pm 0.05 \% &\sigma& = 3.0 \pm 0.1 \nonumber
            \end{align}
		\includegraphics[width=\textwidth]{figures/omegak_H0.pdf}
        \item One should be suspicious about combining CMB lensing and Planck in curved models.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Evidence: Is the universe curved?}
    \includegraphics{figures/evidences.pdf}
    \begin{itemize}
        \item We only think the universe is flat because Planck+lensing and/or BAO tell us so, but these datasets are in tension.
        \item Planck alone tells us the universe is closed at 50:1
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{What about $A_\mathrm{lens}$?}
    \begin{itemize}
        \item The Planck papers do discuss this, but phrase in terms of $A_\mathrm{lens}$
        \item $A_\mathrm{lens}$ is a ``fudge factor'' that artificially increases the effective smoothing by lensing on the primordial power spectrum if $A_\mathrm{lens}\ne 1$
        \item Planck CMB prefers $A_\mathrm{lens}\sim1.2$, lensing wants $A_\mathrm{lens}\sim1$ (can also be defined as a $\sim2\sigma$ tension)
        \item It is the same effect that causes the preference for closed universes.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{How do we fix curvature tension?}
    \begin{itemize}
        \item There could be several explanations:
        \item Systematic in Planck CMB
            \begin{itemize}
                \item Efstathiou \& Gratton's new likelihood looks like it may relax these tensions a little (results pending release of likelihood)
            \end{itemize}
        \item Systematic in Planck lensing
            \begin{itemize}
                \item The lensing likelihood includes a fiducial $\Lambda$CDM model.
                \item {\em In theory\/} this shouldn't matter, but re-doing the analysis with a different fiducial model is a mammoth task.
            \end{itemize}
        \item Statistical fluctuation?
            \begin{itemize}
                \item With tensions at $2$--$3\sigma$ there is always a possibility that we just got unlucky with our universe.
                \item There are a lot of these $2$--$3\sigma$ tensions in the Planck data. 
                \item If there were an explanation that resolved multiple tensions simultaneously then this could be decicive evidence for a new concordance model.
            \end{itemize}
        \item It may be that we need to wait for the next generation CMB experiment to resolve the issue (SO, LiteBird, PICO?, CORE?)
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Theory of curved universes}
    \begin{itemize}
        \item We generally work with flat cosmologies for a few reason
            \begin{enumerate}
                \item Data tell us it is (or does it?)
                \item Theory of inflation tells us it is (somewhat circular reasoning)
                \item The theory is \textbf{much} easier in flat cosmologies
            \end{enumerate}
        \item I've been interested in the effect of curvature at the start of inflation for a while
        \item Even small amounts of curvature can have observational effects on the primordial power spectrum
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{History of curved universes}
    \includegraphics[width=\textwidth]{figures/history.pdf}
\end{frame}

\begin{frame}
    \frametitle{The primordial power spectrum}
    \begin{itemize}
        \item Inflationary theory interacts primarily with observation via the primordial power spectrum of curvature perturbations $\mathcal{P}_\mathcal{R}(k)$ (along with the tensor to scalar ratio $r$)
        \item $\Lambda$CDM assumes a near scale-invariant power spectrum 
            \[ \mathcal{P}_\mathcal{R}(k) = A_s {\left( \frac{k}{k_s} \right)}^{n_s-1} \]
        \item Inflation theories generally predict values of $(r,n_s)$ which can be compared with the phenomenological fit.
        \item $A_s$ is constrained by (amongst other things) the amplitude of the cosmic microwave background power spectrum
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{Predictions from inflation}
    \includegraphics[width=\textwidth]{figures/inflation.pdf}
\end{frame}

\begin{frame}
    \frametitle{Primordial power spectrum computation}
    \begin{itemize}
        \item Background variables:
            \[ H^2 = \frac{1}{3}\left( \frac{1}{2}\dot\phi^2 + V(\phi) \right), \qquad 0=\ddot{\phi} + 3 H \dot{\phi} + \frac{d}{d\phi}V(\phi) \]
        \item Mukhanov variable $v= z\mathcal{R}$ ($z=a \dot{\phi}/H$) evolves according to:
            \[ 0 = v^{\prime\prime} + \left(k^2 - \frac{z^{\prime\prime}}{z}\right) v 
            \quad\Leftrightarrow\quad 0 = \mathcal{R}^{\prime\prime} + 2\frac{z^\prime}{z} \mathcal{R}^\prime + k^2 \mathcal{R} \]
    In terms of cosmic time $t$:
    \[ 
    0 = \ddot{\mathcal{R}} + \left(2\frac{z^\prime}{z}+H\right) \dot{\mathcal{R}} + k^2 \mathcal{R} 
\]
        \item During inflation $\eta\to\eta_*$ friction term $\to\infty$, mode freezes.
        \item Primordial power spectrum is $P_\mathcal{R}(k)\propto\lim_{\eta\to\eta_*} |\mathcal{R}|^2$
        \item Initial conditions for evolution set by quantum mechanics (more later)
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Mukhanov Sazaki equation in curved universes}

    \begin{itemize}
        \item In flat space:
        \begin{gather}
            H^2 = \frac{1}{3}\left( \frac{1}{2}\dot\phi^2 + V(\phi) \right), \qquad 0=\ddot{\phi} + 3 H \dot{\phi} + \frac{d}{d\phi}V(\phi), \qquad z=\frac{a \dot{\phi}}{H} \nonumber\\
            0 = \ddot{\mathcal{R}} + \left(2\frac{z^\prime}{z}+H\right) \dot{\mathcal{R}} + k^2 \mathcal{R} \nonumber
        \end{gather}
    \item With curvature ($K=-1,0,+1$):
        \begin{gather}
            H^2 = \frac{1}{3}\left( \frac{1}{2}\dot\phi^2 + V(\phi) \right) - \frac{K}{a^2}, \qquad 0=\ddot{\phi} + 3 H \dot{\phi} + \frac{d}{d\phi}V(\phi) \qquad z=\frac{a \dot{\phi}}{H}\nonumber\\
            \hspace{-10pt}0=\ddot{\mathcal{R}} + \frac{\left( H + \frac{2\dot z}{z} \right)\mathcal{D}^2 - \frac{3K H z^2}{2a^2}}{\mathcal{D}^2 - \frac{Kz^2}{2a^2}}\:\dot{\mathcal{R}}
            +\frac{K\left(1+\frac{z^2}{2a^2} - \frac{2}{H}\frac{\dot{z}}{z}\right)\mathcal{D}^2 + \frac{K^2z^2}{2a^2}-\mathcal{D}^4}{a^2(\mathcal{D}^2 - \frac{Kz^2}{2a^2})}\:\mathcal{R}\nonumber\\
            \mathcal{D}^2 = \nabla^2 +3K = -k^2 + 3K \nonumber
        \end{gather}
    \item  \arxiv{1907.08524}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Primordial power spectrum}
    \includegraphics[width=\textwidth]{figures/BD.pdf}
\end{frame}

\begin{frame}
    \frametitle{Initial conditions in curved cosmologies}
    \begin{itemize}
        \item In the flat case, usuallyinitial conditions via ``Bunch Davies vacuum''  to initialise Mukhanov evolution:
            \[ v = \frac{1}{\sqrt{2k}} e^{-i k\eta}\]
        \item These are only valid for modes deep in the horizon $k\gg aH$, where the spacetime is de-Sitter (effectively Minkowski)
        \item In the curved case, the large spatial modes have $k\sim aH$
        \item Need new theory of quantum initial conditions for rapidly changing spacetimes.
        \item Instead of defining vacuum as ``particle-less'' (a relative concept), could define it in terms of ``lowest energy''
        \item Minimising the renormalised stress-energy yields observationally distinct initial conditions \arxiv{1607.04148}.
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Bunch Davies initial conditions}
    \includegraphics[width=\textwidth]{figures/BD.pdf}
\end{frame}


\begin{frame}
    \frametitle{Renormalised stress-energy initial conditions}
    \includegraphics[width=\textwidth]{figures/RST.pdf}
\end{frame}


\begin{frame}
    \frametitle{Conclusions}
    \begin{itemize}
        \item The suspiciousness $S$ has become one of the standards for measuring tensions multiple simultaneous parameters.
        %\item The ``inference triple'' of $\mathcal{Z},\mathcal{D},\tilde{d}$ should be considered in all model comparison analyses (computed \texttt{anesthetic} package).
        \item Significant tensions remain for cosmologies that involve curvature, comparable in scale with other tensions that are discussed.
        \item Curvature+inflation yields a rich set of theoretical problems with potentially observable consequences.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Extra slides}
\end{frame}



%\begin{frame}
%    \frametitle{Bayesian evidence $\mathcal{Z}$}
%    \begin{itemize}
%        \item Bayes theorem for parameter estimation:
%    \[
%    P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)} \quad\longrightarrow\quad \text{Posterior} = \frac{\text{Likelihood}\times\text{Prior}}{\text{Evidence}}
%    \]
%        \item Normalising constant $\equiv$ Bayesian evidence $\equiv$ $P(D)$ is hard to compute:
%    \[
%        P(D) = \int P(D|\theta)P(\theta) d\theta = \left\langle \text{Likelihood} \right\rangle_\text{Prior}
%    \]
%        \item Traditionally used to compare models using the same data
%        \item For DES, it is used to compare different data with the same model.
%        \item Computed using nested sampling (\texttt{MultiNest}, \texttt{PolyChord}, \texttt{dynesty}), simulated annealing (\texttt{emcee}), or from MCMC using \texttt{MCEvidence}.
%    \end{itemize}
%\end{frame}

\begin{frame}
    \frametitle{Bayesian evidence $\mathcal{Z}$: Prior dependency}
    \begin{itemize}
        \item Bayesian evidences are prior dependent:
            \[
                \mathcal{Z} = \int P(D|\theta)P(\theta) d\theta \approx \langle\text{Likelihood}\rangle_\text{Posterior} \times \frac{\text{Posterior volume}}{\text{Prior volume}}
            \]
        \item They balance ``goodness of fit'' via likelihood with ``complexity'' through Occam penalty.
        \item Models that include too many fine-tuned parameters are disfavoured, unless they provide a much better fit.
        \item Corollary: Unconstrained parameters are not penalised.
        \item Widen prior $\Rightarrow$ reduce evidence \\ (providing prior does not cut into posterior).
        \item Bayesians vs Frequentists $\leftrightarrow$ Feature vs Bug.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The problem with Principle Component Analysis}

    \begin{figright}[0.6]{./plots/pca.pdf}
        \begin{itemize}
            \item Compute eigenvectors and eigenvalues of covariance matrix.
            \item These aim to describe ``directions'' in parameter space
            \item This procedure is not covariant:
        \end{itemize}
    \end{figright}
    \begin{quote}
        "Principal Component Analysis" is a dimensionally invalid method that gives people a delusion that they are doing something useful with their data. If you change the units that one of the variables is measured in, it will change all the "principal components"! It's for that reason that I made no mention of PCA in my book. I am not a slavish conformist, regurgitating whatever other people think should be taught. I think before I teach. David J C MacKay.

        \end{quote}
\end{frame}



\end{document}
```
# branch: remotes/origin/cambridge_cdt_2019
```tex
\documentclass[%
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}

% todo 
% - Ligo actual data
% -define IMRPhenom, EOBNR


\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\text{knots}}
\newcommand{\nlive}{n_\text{live}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

% Title:
% Nested Sampling: an efficient and robust Bayesian inference tool for Machine
% Learning and Data Science
%
% Abstract:
% Nested sampling is an MCMC technique for integrating and exploring probability
% distributions. It has become widely adopted in the field of cosmology as a
% powerful tool for computing Bayesian evidences and sampling challenging
% a-priori unknown parameter spaces.
% 
% In this talk, I will give an introduction to the principles of Bayesian model
% comparison and parameter estimation, an explanation of the theory of nested
% sampling, a survey of the current state-of-the art (MultiNest, PolyChord,
% DNest and Dynesty) and the future of the field. I will illustrate with
% applications in CMB and 21cm Cosmology, Bayesian Sparse Reconstruction and
% Bayesian Neural Networks.





\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{Nested Sampling}
\subtitle{An efficient and robust Bayesian inference tool\\ for Machine Learning and Data Science}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Kavli Institute for Cosmology \\
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{November 14\textsuperscript{th}, 2019}

\include{include/beamer_commands}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\section{Fitting a line to data}
\begin{frame}
    \frametitle{Motivating example}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_points.pdf}
        \begin{itemize}
            \item We have noisy data $D$
            \item We wish to fit a model $M$
            \item Functional form $y=f_M(x;\theta)$
            \item For example:
                \begin{align}
                     f_\text{linear}(x;\theta)&=a x + b       \nonumber\\
                     f_\text{quadratic}(x;\theta)&=a x^2 + b  \nonumber
                \end{align}
            \item Model parameters $\theta= (a,b)$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ best-fit}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff.pdf}
        \begin{itemize}
            \item For each parameter set $\theta$:
                \[
                    \chi^2(\theta) = \sum_i \left|y_i - f(x_i;\theta)\right|^2
                \]
            \item Minimise $\chi^2$ wrt $\theta$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ with non-uniform data errors}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data.pdf}
        \begin{itemize}
            \item If data have non-uniform errors:
                \[
                    \chi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|^2}{\sigma_i^2}
                \]
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Problems with $\chi^2$}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff_2.pdf}
        \begin{itemize}
            \item How do we differentiate between models
            \item Why square the errors? -- could take absolute:
                \[
                    \psi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|}{\sigma_i}
                \]
            \item Where does this approach even come from?
        \end{itemize}
    \end{figright}
\end{frame}


\begin{frame}
    \frametitle{Probability distributions}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.6]{./figures/data_diff_1.pdf}
        \begin{itemize}
            \item The probability of observing a datum:
                \[
                    P(y_i | \theta,M) = \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right)
                \]
            \item The probability of observing the data:
                \begin{align}
                    P(D | \theta,M) &= \prod_i \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right) \nonumber\\
                    &=  \frac{1}{\prod_i\sqrt{2\pi}\sigma_i}\exp\sum_i{-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}} \nonumber\\
                    &\propto e^{-\chi^2(\theta)/2}
                    \nonumber
                \end{align}
        \end{itemize}
    \end{figright}
\end{frame}



\begin{frame}
    \frametitle{Maximum likelihood}
    \framesubtitle{Fitting lines to data}
    \begin{figleft}[0.6]{./figures/data_diff.pdf}
        \begin{itemize}
            \item Minimising $\chi^2(\theta)$  is equivalent to maximising $P(D|\theta,M) \propto e^{-\chi^2(\theta)/2}$
            \item $P(D|\theta,M)$ is called the Likelihood $L=L(\theta)$ of the parameters $\theta$
            \item ``Least squares'' $\equiv$ ``maximum likelihood'' \\(if data are gaussian).
        \end{itemize}
    \end{figleft}
\end{frame}

\begin{frame}
    \frametitle{Bayesian inference}
    \begin{itemize}
        \item Likelihood $L=P(D|\theta,M)$ is undeniably correct.
        \item Frequentists construct inference techniques purely from this function.
        \item The trend is cosmology is to work with a Bayesian approach.
        \item What we want are things like $P(\theta|D,M)$ and $P(M|D)$.
        \item To invert the conditionals, we need Bayes theorem:
            \begin{align}
                P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
                P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Terminology}
    \framesubtitle{Bayesian inference}
    \begin{align}
        P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
        \text{Posterior} &= \frac{\text{Likelihood}\times\text{Prior}}{\text{Evidence}} \nonumber
    \end{align}
    \begin{align}
        P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber\\
        \text{Model probability} &= \frac{\text{Evidence}\times\text{Model Prior}}{\text{Normalisation}} \nonumber
    \end{align}
\end{frame}


\begin{frame}
    \frametitle{Multivariate probability}
    \begin{itemize}
        \item Marginalisation:
            \begin{equation*}
                P(x) = \int P(x,y) dy
            \end{equation*}
        \item Conditioning:
            \begin{equation*}
                P(y|x) = \frac{P(x,y)}{P(x)} = \frac{P(x,y)}{\int P(x,y) dy}
            \end{equation*}
        \item De-Conditioning:
            \begin{equation*}
                P(x|y) P(y) = P(x,y)
            \end{equation*}
        \item Bayes theorem:
            \begin{equation*}
                P(y|x) = \frac{P(x|y) P(y)}{P(x)}
            \end{equation*}
            \begin{center}
                ``To flip a conditional $P(x|y)$, you first de-condition on $y$,\\ and then re-condition on $x$.''
            \end{center}
    \end{itemize}
\end{frame}


%\begin{frame}
%    \frametitle{The prior}
%    \framesubtitle{Example: Biased coins}
%    \begin{itemize}
%        \item Need to define the \textbf{Prior} $P(\theta)$ --- probability of the bias, given no data
%        \item Represents our knowledge of parameters before the data -- subjective
%        \item Frequentists view this as a flaw in Bayesian inference. 
%        \item Bayesians view this as an advantage
%        \item Fundamental rule of Inference:\pause\\
%            \vfill
%            \begin{center}
%                \Large You cannot extract information from data\\ without making assumptions 
%            \end{center}
%            \vfill
%        \item All Bayesians do is make them explicit
%        \item Any method that claims it is ``objective'' is simply hiding them
%    \end{itemize}
%\end{frame}

\begin{frame}
    \frametitle{Parameter estimation}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.3]{./figures/parameters.pdf}
        \begin{itemize}
            \item We may use $P(\theta|D,M)$ to inspect whether a model looks reasonable
        \end{itemize}
        \includegraphics[width=\textwidth]{./figures/data.pdf}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Predictive posterior}
    \begin{figright}[0.4]{./figures/fgivenx.pdf}
        More useful to plot:
        \begin{align}
        &P(y|x) = \nonumber\\
        &\int P(y|x,\theta) P(\theta) d\theta \nonumber
        \end{align}
        (all conditioned on $D,M$)
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Model comparison}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.33]{./figures/evidences_log.pdf}
        \begin{itemize}
            \item We may use the Bayesian evidence $Z$ to determine whether a model is reasonable.
            \item $Z = P(D|M) = \int P(D|M,\theta)P(\theta|M)d\theta$
            \item Normally assume uniform model priors $Z \propto P(M|D)P(M)$.
        \end{itemize}
    \end{figright}
\end{frame}
\begin{frame}
    \frametitle{Model comparison}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.33]{./figures/evidences_lin.pdf}
        \begin{itemize}
            \item We may use the Bayesian evidence $Z$ to determine whether a model is reasonable.
            \item $Z = P(D|M) = \int P(D|M,\theta)P(\theta|M)d\theta$
            \item Normally assume uniform model priors $Z \propto P(M|D)P(M)$.
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Line fitting (context)}
    \begin{figright}[0.5]{./figures/supernovae.pdf}
        \begin{itemize}
            \item Whilst this model seems a little trite\ldots
            \item\ldots determining polynomial indices \\$\equiv$ determining cosmological material content:
        \end{itemize}
    \end{figright}
        \[
            {\left( \frac{H}{H_0} \right)}^2 = 
            \Omega_\text{r} {\left( \frac{a_0}{a} \right)}^4+
            \Omega_\text{m} {\left( \frac{a_0}{a} \right)}^3+
            \Omega_k {\left( \frac{a_0}{a} \right)}^2+
            \Omega_\Lambda
            \]
\end{frame}



\begin{frame}
    \frametitle{Quantifying error with Probability}

    \begin{itemize}
        \item As scientists, we are used to seeing error bars on results.
        \item Age of the universe ({\em Planck\/}): 
         \[13.73\pm 0.12\:\text{billion years old.}\]
        \item Masses of LIGO GW150914 binary merger: 
        \[m_1 = 39.4^{+5.5}_{-4.9}\:M_\odot,\qquad m_2 = 30.9^{+4.8}_{-4.4}\:M_\odot \]
        \item These are called {\em credible intervals}, state that we are e.g.\ $66\%$ confident of the value lying in this range.
        \item More importantly, these are {\em summary statistics}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{LIGO binary merger}
    \begin{columns}
        \begin{column}{0.65\textwidth}
            \includegraphics[width=\textwidth]{./figures/ligo_m1_m2.pdf}
        \end{column}
        \begin{column}{0.35\textwidth}
            \begin{itemize}
                \item Summary statistics summarise a full probability distribution.
                \item One goal of inference is to produce these probability distributions.
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Theory}
    \framesubtitle{Extended example of inference: LIGO}
    \includegraphics[width=\textwidth]{./figures/ligo_schematic.png}
\end{frame}

\begin{frame}
    \frametitle{The parameters $\Theta$ of the model $M$}
    \framesubtitle{Extended example of inference: LIGO}
    Theoretical signal depends on:
    \begin{itemize}
        \item $m_1, m_2$: mass of binary
        \item $\theta, \phi$: sky location
        \item $r$: luminosity distance 
        \item $\Phi_c, t_c$: phase and time of coalescence
        \item $i, \theta_\text{sky}$: inclination and angle on sky (orbital parameters)
    \end{itemize}
\end{frame}



\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{itemize}
        \item Cannot plot the full posterior distribution:
            \[\mathcal{P}(\Theta) \equiv P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\text{sky}|D,M)\]
        \item Can plot 1D and 2D {\em marginalised\/} distributions e.g:
            \begin{align}
            &P(m_1,m_2|D,M)=\nonumber\\&\int P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\text{sky}|D,M) \,d\theta \,d\phi \,dr \,d\Phi_c \,d t_c \,d i \,d\theta_\text{sky}\nonumber
            \end{align}
        \item May do this for each pair of parameters
        \item Generates a {\em triangle plot}
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Posterior $\mathcal{P}$}
    \framesubtitle{Extended example of inference: LIGO}
    \begin{figleft}[0.65]{./figures/ligo_full.pdf}
		\begin{itemize}
          \item Does give insight
          \item Not the full picture
		\end{itemize}
    \end{figleft}
\end{frame}


\begin{frame}
    \frametitle{Sampling}
    \framesubtitle{How to describe a high-dimensional posterior}

	\begin{figright}{./figures/ligo_m1_m2.pdf}
		\begin{itemize}
          \item In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item Gridding is doomed to failure for $D\gtrsim4$.
          \item {\em Sampling\/} the posterior is an excellent compression scheme.
		\end{itemize}
	\end{figright}
 
\end{frame}
%
\begin{frame}
    \frametitle{Why do sampling?}
    \framesubtitle{Marginalisation over the posterior}

    \begin{itemize}
        \item Set of $N$ samples $S = \{\Theta^{(i)}: i=1,\ldots N:\: \Theta^{(i)}\sim\mathcal{P}\}$
        \item Mean mass: \[
                \bar{m}_1 \equiv\langle m_1\rangle_\mathcal{P}
                \only<1>{\equiv \int m_1 P(\theta|D,M) d\theta }
                \only<2>{\approx \frac{1}{N}\sum_{i=1}^N m_1^{(i)}}
                \only<3>{\approx \frac{\sum_{i=1}^N w^{(i)} m_1^{(i)}}{\sum_{i=1}^N w^{(i)}}}
            \]
        \item Mass covariance: \[
                \mathrm{Cov}(m_1,m_2)
            \only<1>{\equiv \int (m_1-\bar{m}_1)(m_2-\bar{m}_2) P(\theta|D,M) d\theta }
                \only<2>{\approx \frac{1}{N}\sum_{i=1}^N (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}
                \only<3>{\approx \frac{\sum_{i=1}^N w^{(i)} (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}{\sum_{i=1}^N w^{(i)}}}
            \]
        \item Marginalised samples: Just ignore the other coordinates.
        \item N.B. Typically have {\em weighted\/} samples
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Cosmology in high dimensions}

    \[\lik(\Theta) = P(D|\Theta,M)\]
    \begin{align}
        \onslide<2->{D =& \{C_\ell\only<6->{^\text{(Planck)}}\}} 
        \onslide<15->{+\{\text{LSS}\}} 
        \onslide<16->{+\{\text{``Big Data''}\}}
        \nonumber\\
        \onslide<3->{M =& \Lambda\text{CDM}} 
        \onslide<9->{+ \text{extensions} }
        \nonumber\\
        \onslide<4->{\Theta =& \Theta_{\Lambda \text{CDM}}} \onslide<7->{+ \Theta_\text{Planck}} \onslide<10->{+ \Theta_\text{extensions}}\nonumber\\
        \onslide<5->{\Theta_{\Lambda \text{CDM}} =& ( \Omega_b h^2, \Omega_c h^2, 100\theta_{MC}, \tau, {\rm{ln}}(10^{10} A_s), n_s) \nonumber\\}
        \onslide<8->{\Theta_\text{Planck} =& (y_{\rm cal}, A^{CIB}_{217}, \xi^{tSZ-CIB}, A^{tSZ}_{143}, A^{PS}_{100}, A^{PS}_{143}, A^{PS}_{143\times217}, A^{PS}_{217}, A^{kSZ}, \nonumber\\& A^{{\rm dust}TT}_{100}, A^{{\rm dust}TT}_{143}, A^{{\rm dust}TT}_{143\times217}, A^{{\rm dust}TT}_{217}, A^{{\rm dust}TE}_{100}, A^{{\rm dust}TE}_{100\times143}, \nonumber\\& A^{{\rm dust}TE}_{100\times217}, A^{{\rm dust}TE}_{143}, A^{{\rm dust}TE}_{143\times217}, A^{{\rm dust}TE}_{217}, c_{100}, c_{217}) \nonumber\\}
        \onslide<11->{\Theta_\text{extensions} =& (
                n_{\rm run}
                \only<12->{,n_{\rm run,run}}
                \only<13->{,w}
                \only<14->{,\Sigma m_\nu, m_{\nu,{\rm{sterile}}}^{\rm{eff}}}
        ) \nonumber}
    \end{align}

    \begin{itemize}
        \item<17->{Parameter estimation: $L, \pi \to \mathcal{P}$: model parameters}
        \item<17->{Model comparison: $L, \pi \to Z$: how good model is}
    \end{itemize}

\end{frame}



%
\begin{frame}
    \frametitle{Parameter estimation}
    \begin{itemize}
        \item The name of the game is therefore drawing samples $S$ from the posterior $\mathcal{P}$ with the minimum number of likelihood calls.
        \item Gridding is doomed to failure in high dimensions.
        \item Enter Metropolis Hastings.
    \end{itemize}
\end{frame}



\section{Metropolis Hastings}


\begin{frame}
  \frametitle{Metropolis Hastings} 
  \begin{itemize}
      
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \begin{enumerate}
        \item Propose random step
        \item If uphill, make step\ldots
          
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
    \item \url{chi-feng.github.io/mcmc-demo/}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Metropolis Hastings} 
  \framesubtitle{Struggles with\ldots}
  \pause
  \begin{enumerate}
      \item Burn in
      \item Multimodality
      \item Correlated Peaks
      \item Phase transitions
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte-Carlo} 
  \begin{itemize}
      \item Key idea: Treat $\log L(\Theta)$ as a potential energy
      \item Guide walker under ``force'': \[F(\Theta) =\nabla \log L(\Theta)\]
      \item Walker is naturally ``guided'' uphill
      \item Conserved quantities mean efficient acceptance ratios.
      \item stan is a fully fledged, rapidly developing programming language with HMC as a default sampler.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Ensemble sampling} 
  \begin{itemize}
      \item Instead of one walker, evolve a set of $n$ walkers.
      \item Can use information present in ensemble to guide proposals.
      \item emcee: affine invariant proposals.
      \item emcee is not the only (or even best) affine invariant approach.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The fundamental issue with all of the above} 

  \begin{itemize}
    \item They don't give you evidences!
  \begin{align}
    \ev 
    &= \prob(D|M) 
    \nonumber\\
    &= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
    &= \left\langle \lik \right\rangle_\prior
    \nonumber
  \end{align}
    \item MCMC fundamentally explores the posterior, and cannot average over the prior.
    \item Thermodynamic annealing 
    \begin{itemize}
        \item Suffers from same tuning issues as MCMC
    \end{itemize}
    \item Nearest neighbor volume estimation (Heavens arXiv:1704.03472)
    \begin{itemize}
        \item Does not scale to high dimensions $D\gtrsim10$.
    \end{itemize}
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}

\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to traditional MCMC!} 

  \begin{itemize}
    \item Nested sampling is a completely different way of sampling. 
    \item Uses ensemble sampling to compress prior to posterior.
  \end{itemize}
  
  New procedure: 

  
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      
    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\prior$). 
      
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new uniform sample with higher likelihood
  \end{description}

  
  Requires one to be able to uniformly within a region, subject to a {\em hard likelihood constraint}.

\end{frame}



\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}

\begin{frame}
  \frametitle{Nested sampling} 

  \begin{itemize}
    \item The set of dead points are posterior samples with an appropriate weighting factor
    \item They can also be used to calculate evidences, since it sequentially updates the priors.
    \item The current set of live points is useful for performing clustering and constructing new proposed points.
    \item Algorithm terminates when prior has been compressed onto (and past) the posterior bulk (typical set).
  \end{itemize}
 
\end{frame}

\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}
  \foreach \pagenum in {1,...,16} {%
      \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/lesbesgue}
  }
\end{frame}

\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{Exponential volume contraction} 
  
  \begin{itemize}
    \item At each iteration, the likelihood contour will shrink in volume by  $\approx 1/n$.
    \item Nested sampling zooms in to the peak of the posterior {\em exponentially}.
      \begin{equation}
          \ev \approx \sum_i \Delta\lik_i X_{i}, \qquad
        X_{i+1} \approx \frac{n}{n+1}X_i, \qquad X_{0} = 1 \nonumber
      \end{equation}
    \item Although this is only approximate, we can quantify the error \\
      \begin{equation}
          P(X_{i+1}|X_i) = \frac{1}{n X_i}{\left( \frac{X_{i+1}}{X_i} \right)}^{n-1}[0<X_i<X_{i+1}]
          \nonumber
      \end{equation}
    \item Integral can be expressed in one of two ways
      \begin{equation}
          \ev \approx \sum_i \Delta\lik_i X_{i} = \sum_i \lik_i \Delta X_{i} \nonumber
      \end{equation}

  \end{itemize}
  
\end{frame}



\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item Most of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm.
    \item \url{https://projecteuclid.org/euclid.ba/1340370944}
  \end{itemize}
 
\end{frame}

\begin{frame}
\frametitle{MultiNest }
  \framesubtitle{arXiv:0809.3437 arXiv:0704.3704 arXiv:1306.2144}
  \includegraphics[width=\textwidth]{figures/multinest.pdf}
\end{frame}

\begin{frame}
  \frametitle{PolyChord}
  \framesubtitle{arXiv:1502.01856 arXiv:1506.00171}
  \includegraphics[width=\textwidth]{figures/polychord.png}
\end{frame}

\begin{frame}
  \frametitle{Diffusive nested sampling}
  \framesubtitle{arXiv:0912.2380}
  \includegraphics[width=\textwidth]{figures/dnest.pdf}
\end{frame}

\begin{frame}
\frametitle{PolyChord vs MultiNest}
\begin{itemize}
    \item MultiNest excels in low dimensions $D<10-20$.
    \item PolyChord can go up to $\sim 150$.
    \item Crossover is problem dependent
    \item PolyChord can also exploit fast-slow hierarchy
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Exoplanets}
  \framesubtitle{Nested sampling in action (arXiv:1806.00518, Hall, Walker-Smith, Handley, Queloz)}
  \includegraphics[width=\textwidth]{figures/rv_full.pdf}
\end{frame}

\begin{frame}
  \frametitle{Exoplanets}
  \framesubtitle{Nested sampling in action}
  \begin{itemize}
      \item Simple radial velocity model
          \begin{equation}
              \nu(t;\theta) = \sum_{p=1}^N K_p \sin(\omega_p t + \phi_p)\nonumber
          \end{equation}
      \item Fit each model to data.
      \item Posteriors on model parameters $[(K_p,\omega_p,\phi_p),p=1\cdots N]$ quantify knowledge of system characteristics.
      \item Evidences of models determine relative likelihood of number of planets in system
      \item This is an application where phase transitions matter
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{21cm cosmology}
  \framesubtitle{Nested sampling in action (Paper coming soon, Anstey, de Lera Acedo \& Handley)}
    \begin{figright}[0.3]{./figures/21cm.pdf}
        \begin{itemize}
            \item Search for signal  $T=T_\mathrm{fg} + T_\mathrm{21cm}$ 
            \item Fit parameterised models with/without $T_\mathrm{21cm}$
            \item Compare evidences for signal detection
            \item Use evidences to quantify complexity of beam/sky models
        \end{itemize}
    \end{figright}
\end{frame}


\begin{frame}
  \frametitle{Primordial power spectrum $\PR(k)$ reconstruction}
  \framesubtitle{Nested Sampling in action (arXiv:1908.00906)}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}
%
%
%%\begin{frame}
%%  \frametitle{Planck data}
%%  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
%%  \begin{itemize}
%%    \item<2-> Temperature data TT+lowP
%%    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
%%    \item<4-> Marginalised plots of $\PR(k)$
%%    \item<5->
%%      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
%%  \end{itemize}
%%\end{frame}
%
%
%
\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knot}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{Bayes Factors}
  \frametitle<10>{Marginalised plot}
  %\framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}



  \begin{center}
    \includegraphics<1>[width=0.7\textwidth]{figures/pps_both_1}
    \includegraphics<2>[width=0.7\textwidth]{figures/pps_both_2}
    \includegraphics<3>[width=0.7\textwidth]{figures/pps_both_3}
    \includegraphics<4>[width=0.7\textwidth]{figures/pps_both_4}
    \includegraphics<5>[width=0.7\textwidth]{figures/pps_both_5}
    \includegraphics<6>[width=0.7\textwidth]{figures/pps_both_6}
    \includegraphics<7>[width=0.7\textwidth]{figures/pps_both_7}
    \includegraphics<8>[width=0.7\textwidth]{figures/pps_both_8}
    \includegraphics<9>[width=0.7\textwidth]{figures/pps_evidence}
    \includegraphics<10>[width=0.7\textwidth]{figures/pps_both}

  \end{center}
\end{frame}
\begin{frame}
  \frametitle<1>{COBE (pre-2002)}
  \frametitle<2>{COBE et al (2002)}
  \frametitle<3>{WMAP (2012)}
  \frametitle<4>{Planck (2013)}
  \frametitle<5>{Planck (2015)}
  \frametitle<6>{Planck (2018)}


  \begin{center}
    \includegraphics<1>[width=0.7\textwidth]{figures/cobe}
    \includegraphics<2>[width=0.7\textwidth]{figures/pre_WMAP}
    \includegraphics<3>[width=0.7\textwidth]{figures/WMAP}
    \includegraphics<4>[width=0.7\textwidth]{figures/planck_2013}
    \includegraphics<5>[width=0.7\textwidth]{figures/planck_2015}
    \includegraphics<6>[width=0.7\textwidth]{figures/pps}

  \end{center}
\end{frame}

\begin{frame}
    \frametitle{Bayesian neural networks}
    \framesubtitle{Sparse reconstruction (arXiv:1809.04598)}

    \begin{figright}[0.6]{./figures/Colored_neural_network.pdf}
        \begin{itemize}
            \item Neural networks require:
                \begin{itemize}
                    \item Training to find weights
                    \item Choice of architecture/topology
                \end{itemize}
            \item Bayesian NNs treat training as a model fitting problem
            \item Compute posterior of weights (parameter estimation)
            \item Use evidence to determine best architecture (model comparison)
            \item Paper coming soon (Javid, Handley, Lasenby \& Hobson)
                \begin{itemize}
                    \item Bayesian evidences correlate with out-of-sample performance
                    \item Can be used to determine width and number of hidden layers
                \end{itemize}
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Unweaving runs}
    \framesubtitle{Advances in nested sampling}
    \begin{itemize}
        \item John Skilling noted that two nested sampling runs can be combined in likelihood order to produce a valid run with a larger number of live points.
        \item The reverse is also true (arXiv:1704.03459).
        \item In general, a run with $n$ live points can be ``unweaved'' into $n$ runs with a single live point.
        \item Useful for providing convergence diagnostics and better parameter estimation (arXiv:1804.06406).
    \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Dynamic nested sampling}
  \framesubtitle{Advances in nested sampling (arXiv:1704.03459,  dynesty: arXiv:1904.02180)}
  \includegraphics[width=\textwidth]{figures/dynesty.pdf}
  The number of live points can be varied dynamically in order to oversample regions of interest
\end{frame}

\begin{frame}
    \frametitle{Other uses of nested sampling}
    \begin{figright}[0.4]{./figures/s8.pdf}
        \begin{itemize}
        \item Nested sampling estimates the density of states $\Delta X_i$, and hence gives you access to a lot more than just posterior samples
        \item Kullback-Liebler divergence (arXiv:1607.00270)
        \item Bayesian model dimensionality (arXiv:1903.06682)
        \item Suspiciousness \& Tension quantification (arXiv:1902.04029)
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Multi-temperature sampling}
    \begin{itemize}
        \item By compressing from prior to posterior, Nested Sampling's weighted samples are fundamentally different from traditional MCMC.
        \item Nested sampling tails and peaks equally.
        \item We can define the ``temperature'' of a distribution in analogy with thermodnyamics:
            \begin{equation}
                \log L \sim E \Rightarrow P \propto e^{-\beta E} = e^{-E/kT},\quad \beta = 1\nonumber
            \end{equation}
        \item Sampling at different temperatures can be useful for exploring tails.
        \item Nested sampling runs give you the full partition function 
            \begin{equation}
                \log Z(\beta) \approx \sum_i \lik_i^{\beta} \Delta X_{i} \nonumber
            \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Nested importance sampling}
    \framesubtitle{Future research}
    \begin{itemize}
        \item Much of the time spent in a nested sampling run is spent ``compressing the tails''.
        \item Posterior-repartitioned nested sampling gives one way of speeding this up (arXiv:1908.04655)
        \item Sometimes we have a-priori good knowledge of the posterior bulk (analagous to an MCMC proposal distribution).
        \begin{align}
            Z_0 &= \int L(\theta) \pi_0(\theta) d\theta, \qquad
            Z_1 = \int L(\theta) \pi_1(\theta) d\theta \nonumber\\
            &= \int L(\theta)\pi_1(\theta) \frac{\pi_0(\theta)}{\pi_1(\theta)} d\theta
            = \left\langle \frac{\pi_0(\theta)}{\pi_1(\theta)} \right\rangle_{P_1}  \nonumber
        \end{align}
        \item This importance weighting only works if you have a lot of tail samples.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$N$-$\sigma$ contours}
    \framesubtitle{Future research}
    \begin{itemize}
        \item Traditional posterior samples only allow you to plot contours out to 2-3$\sigma$.
        \item Nested sampling fully samples the tails, so in theory one could do $20\sigma$ contours.
        \item Requires further thought in alternatives to kernel density estimation.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Things every nested sampling user should know}
        \begin{itemize}
            \item ``Burn in'' can take a while, and results are not informative until then.
            \item Reducing the stopping criterion does not appreciably change run-time, but does reduce reliability.
            \item Run time is linear in the number of live points, so reduce this for exploratory runs $\sim\mathcal{O}(10)$, but increase to $\sim\mathcal{O}(1000)$ for production-ready runs.
            \item Most nested sampling algorithms are intensely parallelisable, and work best in pure MPI mode (no openMP).
        \end{itemize}
\end{frame}

%\begin{frame}
%    \frametitle{PolyChord 2.0}
%    \framesubtitle{Advances in nested sampling}
%\end{frame}

\begin{frame}
    \frametitle{Key software}
    \begin{description}
        \item[MultiNest] \url{github.com/farhanferoz/MultiNest}
        \item[PolyChord] \url{github.com/PolyChord/PolyChordLite}
        \item[DNest] \url{github.com/eggplantbren/DNest3}
        \item[dynesty] \url{github.com/joshspeagle/dynesty}
        \item[anesthetic] nested sampling visualisation \\ \url{github.com/williamjameshandley/anesthetic}
        \item[fgivenx] posterior plotting of functions \\ \url{github.com/williamjameshandley/fgivenx}
        \item[cosmology] Implemented as an alternative sampler in CosmoMC, MontePython, cosmosis, cobaya \& GAMBIT
    \end{description}
\end{frame}

\begin{frame}
    \frametitle{Summary}
    \begin{itemize}
        \item Nested sampling is a rich framework for performing the full pipeline of Bayesian inference
        \item Plenty of further work to do on the underlying theory
            \begin{itemize}
                \item (If any students/postdocs are interested, I have a large stack of projects waiting to be explored)
            \end{itemize}
        \item Some understanding is required in order to operate \& get the most from nested sampling chains.
    \end{itemize}
\end{frame}

\include{include/further_reading}

\end{document}
```
# branch: remotes/origin/flatiron_2019
```tex
\documentclass[%
    9pt,
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{tikz}
\usepackage{layouts}

\usepackage{cambridge_lecture}
\hypersetup{colorlinks=false}



\title{Compromise-free Bayesian sparse reconstruction}
\subtitle{Higson, Handley, Hobson, Lasenby (\href{https://arxiv.org/abs/1809.04598}{arxiv:1809.04598})}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
    \includegraphics[width=0.2\textwidth]{kicc.png}\\
    Kavli Institute for Cosmology\\
    Cavendish Laboratory (Astrophysics Group) \\
    University of Cambridge
}
\date{March 19\textsuperscript{th} 2019}

\include{beamer_commands}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\bg}[1]{\mathbf{#1}}
\newcommand{\red}[1]{\textcolor{cambblue}{#1}}
\newcommand{\blue}[1]{\textcolor{red}{#1}}
\newcommand{\black}[1]{\textcolor{black}{#1}}
\newcommand{\rr}{\Rightarrow}
\newcommand{\arxiv}[1]{\href{https://arxiv.org/abs/#1}{arXiv:#1}}

\begin{document}

\begin{frame}
    \titlepage{}
\end{frame}



%\begin{frame}
%  \frametitle{Mixture Modelling}
%  \begin{itemize}
%      \item Fit $D$ data points $(\mathbf{x}_d, y_d)$ with some function $y=f(\mathbf{x};\theta)$ with free parameters $\theta$.
%  \end{itemize}
%  \includegraphics{./bsr/results_plots/multi_gg_1d_gg_1d_3_True_1_1000_100_5runs.pdf}
%  \begin{itemize}
%      \item Model function $f(\mathbf{x};\theta)$ as a sum of $N$ basis functions $\varphi(x)$ with weights $a_i$ and location/shape parameters $p_i$, so $\theta=(N,\mathbf{a},\
%  \end{itemize}<++>
%\end{frame}

\begin{frame}
    \frametitle{Bayesian inference}
    \begin{itemize}
        \item Bayes theorem for parameter estimation
            \begin{gather}
                \Pr(D|\theta,M) \times \Pr(\theta|M) = \Pr(\theta|D,M) \times \Pr(D|M) \nonumber\\
                    \mathcal{L}\times\pi = \mathcal{P}\times \mathcal{Z} \qquad
                \mathrm{Likelihood}\times \mathrm{Prior} = \mathrm{Posterior}\times \mathrm{Evidence}\nonumber
            \end{gather}
                \vspace{-15pt}
        \item Bayes theorem for model comparison
            \[
                \Pr(M_i|D) = \frac{\Pr(D|M_i)\Pr(M_i)}{\sum_j \Pr(D|M_j)\Pr(M_j)} \equiv \frac{\mathcal{Z}_i\times\Pi_i}{\sum_j \mathcal{Z}_j\Pi_j}
            \]
            \vspace{-10pt}
        \item Model marginalisation
            \[
                \Pr(\alpha|D) = \sum_j \Pr(\alpha|M_j,D) \Pr(M_j|D) \equiv \sum_j \mathcal{P}_j(\alpha) \times \Pi_j
            \]
            \vspace{-5pt}
        \item Bayesian inference depends on parameter and model priors $\pi(\theta|M)$ and $\Pi(M)$, e.g:
            \begin{itemize}
                \item If the prior adjusts the shape of the likelihood
                \item If the prior changes its width
            \end{itemize}
        \item My definition of \red{Bayesianism} vs \red{Frequentism} is whether you consider this prior dependency a \red{feature} or a \red{bug}.
        \item Other important quantities: Shannon information $\mathcal{I}$, Kullback-Leibler divergence $\mathcal{D}$ and Bayesian model dimensionality $d$ 
            \[
                \mathcal{I}(\theta) = \log \frac{\mathcal{P}}{\pi} \qquad
                \mathcal{D} = \int \mathcal{P} \log \frac{\mathcal{P}}{\pi} d\theta \equiv \left\langle \log \frac{P}{\pi}\right\rangle_\mathcal{P} \equiv \left\langle \mathcal{I} \right\rangle_\mathcal{P} \qquad
                \frac{d}{2} = \left\langle (\mathcal{I}-\mathcal{D})^2\right\rangle_\mathcal{P} \hfill
            \] 
            Handley \& Lemos: \arxiv{1902.04029},   \arxiv{1903.06682}
            
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Bayesian approach to sparse regression}
    \begin{itemize}
        \item Fit $D$ data points $(\vect{x}_d,y_d)$ with some function $y=f(\vect{x};\bg{\theta})$ with free parameters $\bg{\theta}$
    \end{itemize}

    \begin{columns}
        \begin{column}{0.25\textwidth}
            \includegraphics[width=\textwidth]{figures/data_points.pdf}
        \end{column}
        \begin{column}{0.25\textwidth}
            \includegraphics[width=\textwidth]{figures/fgivenx.pdf}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item If $\vect{x}$ is \red{2-dimensional} 
                    $\rr$ \red{image reconstruction}, etc.
            \end{itemize}

        \end{column}
    \end{columns}

    \begin{itemize}
        \item Model function $f(\vect{x};\bg{\theta})$ as \red{sum} of $N$
            \red{basis functions} $\varphi^{(T)}$ of type $T$,
            with \red{weights} $a_i$ and \red{location/shape} parameters $p_i$, 
            so $\bg{\theta} = (T, N, \vect{a}, \vect{p}_1,\ldots,\vect{p}_N)$:
            \[
                f(\vect{x};\bg{\theta}) 
%= f(\vect{x};T, N, \vect{a}, \vect{p}_1,\ldots,\vect{p}_N) 
                =
                \sum_{i=1}^N a_i\varphi^{(T)}(\vect{x};\vect{p}_i)
            \]
        \item Explore (by sampling) posterior with \red{variable (effective) dimensionality}:
            \[
                \Pr(T,N,\vect{a},\{\vect{p}_i\}|\vect{y})
                \propto \underbrace{\Pr(\vect{y}|T,N,\vect{a},\{\vect{p}_i\})}_{\mbox{likelihood}}
                \underbrace{\Pr(\vect{a}|T)\Pr(\{\vect{p}_i\}|T,N)
                \Pr(N|T)\Pr(T)}_{\mbox{prior}}
            \]
        \item Full \red{posterior} of fit $\Pr(\bg{f}|\bg{y}) = {\displaystyle \int \Pr(\bg{f}|\bg{\theta})\,\Pr(\bg{\theta}|\bg{y})\,d\bg{\theta}}$
            \ldots and that's it!
    \end{itemize}




\end{frame}

\begin{frame}
    \frametitle{Desirable properties}

    $\Pr(T,N,\vect{a},\{\vect{p}_i\}|\vect{y})
    \propto \underbrace{\Pr(\vect{y}|T,N,\vect{a},\{\vect{p}_i\})}_{\mbox{likelihood}}
    \underbrace{\Pr(\vect{a}|T)\Pr(\{\vect{p}_i\}|T,N)
    \Pr(N|T)\Pr(T)}_{\mbox{prior}}$

%$\Pr(T,N,\vect{a},\{\vect{p}_i\}|\vect{y})
%\propto \Pr(\vect{y}|T,N,\vect{a},\{\vect{p}_i\})
%\Pr(\vect{a}|T)\Pr(\{\vect{p}_i\}|T,N)
%\Pr(N|T)\Pr(T)$

    \begin{itemize}
        \item  \red{Full posterior} on parameters 
            (rather than simply optimising) $\rr$ quantify \red{uncertainties}

        \item Bayesian approach $\rr$ naturally penalises \red{overcomplex} models 

        \item \red{Sparsity} can be further \red{enforced directly} by
            $\Pr(N)$ and \red{marginalised over}

        \item No \red{regularisation parameter} to be chosen
            (unlike $L_p$-norm regularisation, etc.)

        \item \red{Variable number} of basis functions with \red{variable positions}

        \item Basis functions \red{families/shapes} determined
            (dictionary learning) or \red{marginalised over}


        \item Can impose \red{arbitrary constraints} on reconstruction 
            (not just positivity)

        \item Accommodates \red{any noise type}, e.g. Gaussian,
            Poisson, etc. (extra \red{hyperparameters})


        \item Accommodates arbitrary \red{missing} and/or \red{irregular} data 

    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Practical considerations}

    $\Pr(T,N,\vect{a},\{\vect{p}_i\}|\vect{y})
    \propto \underbrace{\Pr(\vect{y}|T,N,\vect{a},\{\vect{p}_i\})}_{\mbox{likelihood}}
    \underbrace{\Pr(\vect{a}|T)\Pr(\{\vect{p}_i\}|T,N)
    \Pr(N|T)\Pr(T)}_{\mbox{prior}}$

    \begin{itemize}
        \item \red{Transdimensional sampling}
            (RJMCMC) costly $\Rightarrow$ use \red{product-space} approach
            \newline -- consider \red{hypermodel} $H$ with space $\bg{\theta}$ of
            \red{fixed dimensionality} $T_{\rm max} \times N_{\rm max}$
            \newline -- integer parameters $(T,N)$ enumerate models $H_M$ within $H$
            \newline -- for each sampled $(T,N)$-values, \red{partition} $\bg{\theta}$ into
            parameters used by $H_M$ and others
            \newline -- latter set of parameters \red{ignored} (not passed by
            `wrapper' to likelihood for $H_M$)

        \item \red{Marginalisation} over $\vect{a}$ and
            $\{\vect{p_i}\}$ $\Rightarrow$ posterior $\Pr(T,N|\vect{y})$ (\red{recovers
            PORs})
            \[
                \red{
                    {\cal P}_{(T,N)}^{(T',N')} = \ln\left[\frac{\Pr(T',N'|\vect{y})}{\Pr(T,N|\vect{y})}\right]
                }
            \]
        \item
            i.e.\ Bayesian model selection 
            \red{without evidences!} (can also use \red{`vanilla'} method)

        \item \blue{But\ldots} 
            \newline -- posterior $\Pr(T,N,\vect{a},\{\vect{p}_i\}|\vect{y})$ 
            dimensionality $N_{\rm dim} \sim 10^3-10^4$ for \red{small images} 
            \newline -- posterior is \red{highly multimodal} with \red{strong degeneracies}
            (certainly non-convex!)
            \newline -- \red{categorical/integer} parameters $T,N$ $\Rightarrow$ cannot use
            \red{gradients}


            $\blue{\Rightarrow}$ use \blue{(dynamic) nested sampling} to explore posterior
            with \red{(dy)PolyChord}

        \item Computationally \red{demanding}, but now
            \red{possible} (proof of principle) \ldots 
    \end{itemize}

\end{frame}

% Possible slide on nested sampling
\begin{frame}
    \frametitle{Nested sampling}
    \begin{itemize}
        \item Want to compute evidence, which is high-dimensional integral over parameter space $\theta$. Define prior volume $X$ as fraction of prior above contour $\mathcal{L}(\theta)\ge \mathcal{L}$
            \[\mathcal{Z} = \int \mathcal{L}(\theta) \pi(\theta) d\theta = \int \mathcal{L}(X) dX, \qquad X(\mathcal{L}) = \int_{\mathcal{L}(\theta)\ge\mathcal{L}} \pi(\theta) d\theta \]
        \item Nested sampling procedure:
            \begin{enumerate}
                \item Draw $N$ ``live'' points from the prior $\pi(\theta)$ and compute likelihoods.
                \item Remove lowest live point, replace with one drawn from prior at higher likelihood.\label{step2}
                \item Repeat step \ref{step2} until live points occupy a small enough prior volume.
            \end{enumerate}
        \item Procedure allows one to estimate prior volumes probabilistically, as volume contracts by factor $\approx\frac{N}{N+1}$ at each step.
        \item Compute evidence from $M$ discarded points via trapezium rule:
            \[ \mathcal{Z} \approx \sum_{i=0}^{M} \mathcal{L}_i \times \frac{1}{2}(X_{i-1}-X_{i+1}), \qquad X_0=1, \quad X_N =0, \quad X_i= t_i X_{i-1}, \quad \Pr(t_i) = N t^{N-1}  \]
        \item Generates posteriors as by-product with weights $w_i = \frac{1}{\mathcal{Z}}\mathcal{L}_i \times \frac{1}{2}(X_{i-1}-X_{i+1})$
        \item Step \ref{step2} is by far the hardest step.
            \begin{description}
                \item[MultiNest] Ellipsoidal based rejection sampling
                \item[Galilean] Gradient-based HMC-like algorithm
                \item[Diffusive NS \& Dynesty] User-based choice
                \item[PolyChord] Slice-sampling 
            \end{description}

    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Simple basis functions}

    \begin{itemize}
        \item 1-d generalised Gaussians $\varphi^{\rm (g)}(x;\bg{p}) = \varphi^{\rm (g)}(x;\mu,\sigma,\beta) = e^{-(|x-\mu|/\sigma)^\beta}$ (GGMM)
        \item 1-d tanh functions  $\varphi^{\rm (t)}(x;\bg{p}) = \varphi^{\rm (t)}(x;w,b) = \tanh(wx+b)$ (TMM)
            \vfill
            \includegraphics[width=0.49\textwidth]{BSR/diagrams/gg_demo}\hfill
            \includegraphics[width=0.49\textwidth]{BSR/diagrams/ta_demo}
        \item Easily extended to \red{higher dimensions} (including
            \red{anisotropic scaling} and \red{rotation})
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Simple 1-D examples: generalised Gaussians data}

    \includegraphics[width=0.59\textwidth]{BSR/results_plots/multi_gg_1d_gg_1d_2_True_1_1000_100_5runs}\hfill
    \includegraphics[width=0.35\textwidth]{BSR/results_plots/odds_gg_1d_gg_1d_2_5runs}
    \includegraphics[width=0.59\textwidth]{BSR/results_plots/multi_gg_1d_gg_1d_3_True_1_1000_100_5runs}\hfill
    \includegraphics[width=0.35\textwidth]{BSR/results_plots/odds_gg_1d_gg_1d_3_5runs}
    \includegraphics[width=0.59\textwidth]{BSR/results_plots/multi_adfam_gg_ta_1d_gg_1d_1_True_1_1000_100_5runs}\hfill
    \includegraphics[width=0.35\textwidth]{BSR/results_plots/odds_adfam_gg_ta_1d_gg_1d_1_5runs}
\end{frame}

\begin{frame}
    \frametitle{Simple 2-D examples: generalised Gaussians data}

\includegraphics[width=0.59\textwidth]{BSR/results_plots/multi_gg_2d_gg_2d_1_True_1_2000_250_5runs.pdf}\hfill
\includegraphics[width=0.35\textwidth]{BSR/results_plots/odds_gg_2d_gg_2d_1_5runs.pdf}
\includegraphics[width=0.59\textwidth]{BSR/results_plots/multi_gg_2d_gg_2d_2_True_1_2000_250_5runs.pdf}\hfill
\includegraphics[width=0.35\textwidth]{BSR/results_plots/odds_gg_2d_gg_2d_2_5runs.pdf}
\includegraphics[width=0.59\textwidth]{BSR/results_plots/multi_gg_2d_gg_2d_3_True_1_2000_250_5runs.pdf}\hfill
\includegraphics[width=0.35\textwidth]{BSR/results_plots/odds_gg_2d_gg_2d_3_5runs.pdf}
\end{frame}

\begin{frame}
    \frametitle{HST eXtreme Deep Field images: generalised Gaussians fit}

\includegraphics[width=0.59\textwidth]{BSR/results_plots/multi_gg_2d_get_image_1_True_1_2000_250_5runs.pdf}\quad 
\includegraphics[width=0.35\textwidth]{BSR/results_plots/odds_gg_2d_get_image_1_5runs.pdf}                       
\includegraphics[width=0.59\textwidth]{BSR/results_plots/multi_gg_2d_get_image_2_True_1_2000_250_5runs.pdf}\quad 
\includegraphics[width=0.35\textwidth]{BSR/results_plots/odds_gg_2d_get_image_2_5runs.pdf}                       
\includegraphics[width=0.59\textwidth]{BSR/results_plots/multi_gg_2d_get_image_3_True_1_2000_250_5runs.pdf}\quad 
\includegraphics[width=0.35\textwidth]{BSR/results_plots/odds_gg_2d_get_image_3_5runs.pdf}                       
\end{frame}

\begin{frame}
\frametitle{Bayesian neural networks}

\begin{itemize}
\item Consider \red{feed-forward NN}, $d$-dimensional input $\vect{x}$, one hidden layer with $N$ nodes:\\[0.4cm]
%
\begin{columns}
	\begin{column}{0.4\textwidth}
		\includegraphics[width=\textwidth]{BSR/diagrams/nn_diagram}
	\end{column}
\begin{column}{0.6\textwidth}
\begin{eqnarray*}
a^{[1]}_j & = & \phi^{[1]}\left(\sum_{i=1}^d x_i w^{[1]}_{ji} +
b_j^{[1]}\right)\\
\hat{y}_j = a^{[2]}_j & = & \phi^{[2]}\left(\sum_{i=1}^N a_i^{[1]}w^{[2]}_{ji} + b_j^{[2]}\right)
\end{eqnarray*}
\end{column}
\end{columns}

\item If activation functions $\phi^{[1]}(x)=\tanh x$ and $\phi^{[2]}(x)=x$, and $b_j^{[2]}=0$
\newline $\Rightarrow$ consider (noisy) \red{data points} 
$(\vect{x}^{(t)},y^{(t)})$ $(t=1,2,\ldots,T)$ as training set
\newline\phantom{$\rr$} 
with \red{objective function} equal to \red{likelihood}
$\Pr(\bg{y}|\hat{\bg{y}})$ (noise model)\\[0.2cm]
%from $d$-dimensional function $f(\vect{x})$ (with noise on output)
%\newline 
$\Rightarrow$ \red{regression problem} with $N$ 
adaptive \red{tanh basis functions} (or
$\mbox{sig}(x)$ or $\mbox{max}(0,x)$)
%
\[
\red{
\hat{y}(\vect{x}) = f(\vect{x}) = \sum_{j=1}^N a^{[1]}_j \tanh
\left(\sum_{i=1}^d x_iw^{[1]}_{ji} +
b_j^{[1]}\right)\qquad\black{\mbox{(Activation function MM)}}
}
\]
%
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Bayesian neural networks architecture}

\begin{itemize}

\item \red{In general}: NN can have $L$ \red{hidden layers} with
\red{nodes} $N^{[1]},\ldots,N^{[L]}$ \& many \red{outputs}
\item output(s) no longer \red{direct sum(s)} of inputs but \red{method
still applicable}
\item can \red{determine integer parameters}
$(L,\{N^{[l]}\})$ and \red{activation type} $T$
\item \red{simultaneous} training of network \red{parameters},
\red{architecture} and \red{activation function}
\item \red{full joint posterior} distribution on \red{all
aspects} of NN

\end{itemize}

\end{frame}

\begin{frame}
    \frametitle{2-D examples: generalised Gaussians \& HST images}

\includegraphics[width=0.59\textwidth]{BSR/results_plots/multi_nn_2l_get_image_3_True_1_2000_250_5runs.pdf}\qquad 
\includegraphics[width=0.35\textwidth]{BSR/results_plots/odds_nn_2l_get_image_3_5runs.pdf}                        
\includegraphics[width=0.59\textwidth]{BSR/results_plots/multi_nn_2l_get_image_2_True_1_2000_250_5runs.pdf}\qquad 
\includegraphics[width=0.35\textwidth]{BSR/results_plots/odds_nn_2l_get_image_2_5runs.pdf}                        
\includegraphics[width=0.59\textwidth]{BSR/results_plots/multi_nn_adl_gg_2d_3_True_1_2000_250_5runs.pdf} 
\includegraphics[width=0.35\textwidth]{BSR/results_plots/odds_nn_adl_gg_2d_3_5runs.pdf} 
\end{frame}

\begin{frame}
    \frametitle{Bayesian inference from simulations}
    \framesubtitle{(a.k.a. Likelihood Free Inference)}

    \begin{itemize}
        \item In many cases, do not have access to likelihood $\Pr(D|\theta,M)$
        \item Can however simulate data $D=\phi(\theta,M)$
        \item Must compress data in order to avoid curse of dimensionality $t = t(D)$
        \item Massive compression: $\mathrm{dim}(t)=\mathrm{dim}(\theta)$ (Alsing et al \arxiv{1801.01497})
    \end{itemize}
    \begin{enumerate}
        \item Construct proxy joint/conditional distribution $p=\Pr(t,\theta|\eta)$ with nuisance $\eta$:
            \begin{itemize}
                \item Gaussian mixture model, $x=(t,\theta)$, $\eta = (N, A_1,\mu_1,\sigma_1,\ldots,A_N,\mu_N, \sigma_N)$: 
                    \[p(t,\theta|\eta) = \sum_{i=1}^N A_i\exp\left(-\frac{(x-\mu_i)^2}{2\sigma_i^2}\right) \quad \text{\black{(Alsing et al \arxiv{1801.01497})}}\]
                    \vspace{-5pt}
                \item Neural density estimator $x=(t,\theta)$, $\eta = (\mathrm{Architecture}, \mathbf{w})$ \\(Alsing et al \arxiv{1903.00007}): 
            \end{itemize}
        \item Compute example simulations $\{(t_i, \theta_i)\}$
        \item Fit proxy to simulations via $\eta$ given prior $\Pr(\eta)$, using likelihood
            \[\mathcal{L}(\eta) = \prod_i p(t_i,\theta_i|\eta) \]
            \vspace{-10pt}
        \item Marginalise over proxy (ignore $\eta$ column in samples), evaluated at observed data $D$
            \[\Pr(\theta,D) = \int p(\theta,t(D)|\eta)\Pr(\eta) d\eta \]
            \vspace{-10pt}
        \item Condition on data $D$, either analytically or via nested sampling
            \vspace{-3pt}
            \[\Pr(\theta|D) = \Pr(\theta,D)/\Pr(D) \qquad \Pr(D) = \int \Pr(\theta,D) d\theta\]
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Likelihood free inference: what's in a name?}
    \begin{itemize}
        \item The term ``Likelihood-free'' is a misnomer -- there is still very much a likelihood involved at the centre of the analysis, we just don't analytically compute it
        \item From the Bayesian viewpoint, in lieu of attempting an impossible calculation of a likelihood, we construct a proxy, and marginalise over our lack of knowledge.
        \item Before becoming involved in this hack week, I found the term LFI disconcerting.
        \item Alternative names
            \begin{itemize}
                \item Simulation-based inference
                \item Likelihood learning
            \end{itemize}
        \item Proposed Hack: Come up with a different name for those outside the field
    \end{itemize}
\end{frame}


\end{document}
```
# branch: remotes/origin/gambit_x_2019
```tex
\documentclass[%
    9pt,
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{graphbox}
\usepackage{tikz}
\usepackage{layouts}


\usepackage{cambridge_lecture}
\hypersetup{colorlinks=false}



\title{Likelihood-free inference}
\subtitle{GAMBIT X}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
    \includegraphics[width=0.2\textwidth]{kicc.png}\\
    Kavli Institute for Cosmology\\
    Cavendish Laboratory (Astrophysics Group) \\
    University of Cambridge
}
\date{June 6\textsuperscript{th} 2019}

\include{beamer_commands}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\bg}[1]{\mathbf{#1}}
\newcommand{\red}[1]{\textcolor{cambblue}{#1}}
\newcommand{\blue}[1]{\textcolor{red}{#1}}
\newcommand{\black}[1]{\textcolor{black}{#1}}
\newcommand{\rr}{\Rightarrow}
\newcommand{\arxiv}[1]{\href{https://arxiv.org/abs/#1}{arXiv:#1}}

\begin{document}

\begin{frame}
    \titlepage{}
\end{frame}

\begin{frame}
    \frametitle{Likelihood free inference: what's in a name?}
    \begin{itemize}
        \item The term ``Likelihood-free'' is a misnomer -- there is still very much a likelihood involved in the analysis
        \item LFI is a framework for situations where you don't know what the likelihood is, but can still simulate your system
            \begin{itemize}
                \item i.e.\ you are freed from having to write down the likelihood explicitly
            \end{itemize}
        \item Fundamental idea: construct a flexible proxy likelihood, and fit this to simulations.
        \item Fitted likelihood can then be used in both Frequentist and Bayesian analyses.
        \item Related to Approximate Bayesian Computation (ABC) but better.
        \item Key references from cosmology:
            \begin{itemize}
                \item \arxiv{1801.01497}
                \item \arxiv{1903.00007}
                \item \arxiv{1903.01473}
                \item \arxiv{1904.05364}
                \item Another paper coming soon: ``Compromise-free Likelihood-free inference''
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Motivating example: Cosmological large-scale structure}
  \begin{figleft}[0.7]{./figures/dm.jpeg}
      \begin{itemize}
          \item What is the likelihood $P(D|\theta)$ for large scale structure formation?
          \item Can simulate data $\hat{D}$ given set of cosmological parameters $\theta$.
          \item Image shows effect of varying the amount of dark matter in simulation
          \item Would like to compare simulation to actual data: $\chi^2(\theta) \sim |\hat{D}(\theta) - D |^2$
      \end{itemize}
  \end{figleft}
\end{frame}

\begin{frame}
  \frametitle{Two key problems}
  \[\chi^2(\theta) \sim |\hat{D}(\theta) - D |^2\]
  \begin{enumerate}
      \item Datasets are in general exponentially large
          \begin{itemize}
              \item The whole point of science is to compress data into models with a small number of parameters.
          \end{itemize}
      \item What is the correct metric to measure the difference between $\hat{D}$ and $D$?
          \begin{itemize}
              \item ABC works by rejection sampling up to some $\epsilon$ difference above to select the correct $\theta$.
          \end{itemize}
  \end{enumerate}
  The recent advance in LFI is a framework that solves both of these problems.
\end{frame}

\begin{frame}
  \frametitle{Solution 1: Massive compression}
      \begin{itemize}
          \item All we are interested in is ``What do the data $D$ tell me about the $n$ parameters of my model $\theta$?''
          \item In theory, the dataset $D$ should be compressible into $n$ numbers without losing information about $\theta$.
          \item For example, when inferring the underlying mean $\mu$ and variance $\sigma^2$ of some numbers $\{x_1,\cdots,x_d\}$, all you need is the sample mean $\bar{x}$ and sample variance $S^2$ (c.f.\ sufficient statistics).
      \end{itemize}
      {\Large
      $\theta = [\Omega_m, \Omega_b, \sigma_8]$
      \hfill
      \includegraphics[align=c,width=0.4\textwidth]{./figures/simulation.jpg}
      \hfill
      $\longrightarrow  D=[0.3, 0.4, 0.9]$
      }
      \begin{itemize}
          \item The advance in cosmology which made the recent work possible (\arxiv{1712.00012}) was to generalise Karhunen-Lo\'{e}ve  and MOPED to {\em score  compression}:
              \begin{itemize}
                  \item If you have an approximate likelihood $L(\theta)$, then $\nabla_\theta L$ is in some sense an optimal compression.
              \end{itemize}
          \item More compression schemes are possible: watch this space.
      \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Solution 2: Density estimation}
      \begin{itemize}
          \item One can generate a set of $K\sim1000$ training simulations $\{(D_1,\theta_1),\cdots,(D_K,\theta_K)\}$.
          \item Apply compression, and these are samples in a $2n$-dimensional space.
          \item Build a proxy joint distribution $P(\theta,D) = f(\theta, D; \alpha)$ with some free parameters $\alpha$:
          \item With this proxy joint, one has a likelihood of $\alpha$ defined by:
              \[
                  L(\alpha) = \prod_{i=1}^K f(\theta_i, D_i; \alpha)
              \]
          \item Use $L(\alpha)$ as a misfit function for maximising/marginalising $\alpha$.
      \end{itemize}
      \begin{columns}
        \begin{column}{0.4\textwidth}
              \begin{itemize}
                  \item Gaussian mixture models
                  \item Neural density estimators
                  \item \ldots
              \end{itemize}
        \end{column}
        \begin{column}{0.6\textwidth}
            \hfill
            \includegraphics[width=0.35\textwidth]{./figures/nn.png}
            \hfill
            \includegraphics[width=0.6\textwidth]{./figures/mixture_gaussian.jpeg}
            \hfill
        \end{column}
      \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The full framework}
    \begin{enumerate}
        \item In lieu of a likelihood, write data simulator $\hat{D}(\theta)$
        \item Generate training data $\{(D_1,\theta_1),\cdots,(D_K,\theta_K)\}$
        \item Choose massive compression scheme (e.g. score) and compress training data
        \item Choose proxy distribution $f(D, \theta; \alpha)$ (e.g. neural density estimator) and fit to training data to give e.g. $P(D,\theta) = f(D, \theta; \alpha_\mathrm{max})$
        \item Use trained joint for all your usual inference:
            \begin{itemize}
                \item Can calculate likelihood by inputting the actual data (in compressed form)
                \item For some proxy distributions (e.g. mixture models) you can get the evidence for free.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Cosmological examples}
    \includegraphics[width=0.59\textwidth]{./figures/tau.jpeg}
    \includegraphics[width=0.39\textwidth]{./figures/jla_contours_insert.pdf}
    My research:
    \begin{itemize}
        \item Instead of maximising wrt $\alpha$, in a Bayesian framework one should marginalise
        \item Can use Bayesian evidences to select the best proxy, and to pick/marginalise over the number of mixture components/neural network nodes.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Summary}
    \begin{itemize}
        \item Recent advances have brought LFI into the realm of ``possible'' with current technology
        \item In ten years time, with advances in both theory and computing power everyone will be doing this (think MCMC twenty years ago).
        \item GAMBIT should be thinking about incorporating these techniques over the next few years.
    \end{itemize}
\end{frame}

\end{document}
```
# branch: remotes/origin/portsmouth_2019
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{<+Title+>}
\subtitle{<+subtitle+>}
\date{<+Date+>}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{<+Frame title+>}
    <+Content+>
\end{frame}

\end{document}
\documentclass[%
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}

\include{include/beamer_commands}
\usepackage{layouts}



\title{Curvature tension}
\subtitle{Evidence for a closed universe (?)}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
    Astrophysics Group, Cavendish Laboratory, University of Cambridge\\
    Kavli Institute for Cosmology, Cambridge\\
    Gonville \& Caius college, Cambridge\\
}
\date{25\textsuperscript{th} August 2019}

% Abstract
% --------
% Observational cosmology is close to breaking point. There is now a 4.9-sigma
% tension between the expansion rate $H_0$ as inferred by CMB modelling and
% $H_0$ as measured by supernovae observers. Other parameter tensions
% potentially exist between alternative combinations of data, such as a DES and
% Planck ($S_8$ and $\Omega_m$), or Planck CMB and lensing ($\Omega_K$). In
% these cases it is less obvious how to quantify the level of tension in terms
% of ``sigma'', either due to the high dimensionality of the space, or the
% non-gaussianity of the distributions. Diagnosing the location and level of
% these tensions could prove critical in determining their cause.
%
% In this talk I will discuss recent research by Pablo Lemos and myself into
% novel statistical ways to measure these tensions, how to quantify the number
% of parameters a given experiment constrains, the calibration of the DES
% evidence ratio and the problems with principle component analysis (PCA).


\begin{document}

\begin{frame}
    \titlepage{}
    \begin{center}
        Handley \arxiv{1908.09139}, \arxiv{1907.08524}\\
        Handley \& Lemos \arxiv{1902.04029}, \arxiv{1903.06682}

    \vspace{10pt}
    \url{github.com/williamjameshandley/CosmoChord}
    \url{github.com/williamjameshandley/anesthetic}
    \end{center}
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \framesubtitle{A talk of two halves}
    \begin{description}
        \item[First half:] Inference, quantifying tensions and observing curvature
        \item[Second half:] Theory and predictions of primordially curved universes
    \end{description}
\end{frame}

\begin{frame}
    \frametitle{The Hubble $H_0$ tension}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}
            \item CMB cosmologists (Planck) infer \\ $H_0=67\pm0.5\text{ km s}^{-1} \text{Mpc}^{-1}$
            \item Supernovae data  ($SH_0ES$) measure \\ $H_0=74\pm1.4 $
            \item $>4\sigma$ discrepancy could be due to:
                \begin{itemize}
                    \item Systematic error
                    \item Problem with standard model of cosmology ($\Lambda$CDM)
                \end{itemize}
            \item Inconsistent datasets shouldn't be combined
        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{overlayarea}{\textwidth}{0.5\textheight}
            \includegraphics<1>{./plots/H0.pdf}
            \includegraphics<2>{./plots/H0_combined.pdf}
            \end{overlayarea}
        \end{column}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{$\sigma_8$-$\Omega_m$ tension}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}
            \item<1-> Matter density $\Omega_m$ and RMS matter fluctuations $\sigma_8$ are constrained by 
            \item<1-> BAO and Planck look consistent
            \item<2-> DES is less clear
            \item<2-> How do you define a tension in terms of ``sigma'' for this case?
        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{overlayarea}{\textwidth}{0.5\textheight}
            \includegraphics<1>{./plots/BAO_planck.pdf}
            \includegraphics<2>{./plots/DES_planck.pdf}
            \end{overlayarea}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{$\Omega_K$ tension}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}
            \item Models with spatial curvature $\Omega_K$.
            \item Best-kept secret of Planck: only $1/10,000$ MCMC samples $\Omega_K>0$.
            \item How consistent do Planck and CMB lensing look?
            \item \arxiv{1908.09139}
        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{overlayarea}{\textwidth}{0.5\textheight}
            \includegraphics<1>{./plots/curvature.pdf}
            \includegraphics<2>{./plots/H0_combined.pdf}
            \includegraphics<3>{./plots/curvature_lensing.pdf}
            \end{overlayarea}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Quantifying tension}
    \framesubtitle{Gaussians}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            For 1D Gaussian distributions, tension is pretty easy to define:
            \[
                X = \frac{|\mu_A - \mu_B|}{\sqrt{\sigma_A^2 + \sigma_B^2}},
            \]
         where $\mu$ and $\sigma$ are the respective parameter means and standard deviations.
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{overlayarea}{\textwidth}{0.65\textheight}
                \includegraphics{./plots/H0.pdf}
            \end{overlayarea}
        \end{column}
    \end{columns}

    The multivariate $d$-dimensional equivalent to this tension would be:
    \[
        X^2_d = {(\mu_A - \mu_B)}^T{(\Sigma_A + \Sigma_B)}^{-1}(\mu_A-\mu_B),
    \]
    where $\Sigma$ is in general a covariance matrix.
\end{frame}

\begin{frame}
    \frametitle{Quantifying tension}
    \framesubtitle{non-Gaussians}
    \begin{figright}[0.4]{./figures/all_components.pdf}
        \begin{itemize}
            \item Things become less clear when distributions become ``banana like'' (\arxiv{1906.11628}),
        or worse, multimodal.
        \end{itemize}
    \end{figright}
    \begin{itemize}
        \item Many attempts to generalise the Gaussian case result in a parameterisation-dependent quantity.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Quantifying tension}
    \framesubtitle{High-dimensional spaces}
    \centerline{%
    \includegraphics[width=0.49\textwidth]{./figures/tension.pdf}
    \includegraphics[width=0.49\textwidth]{./figures/tension_rotated.pdf}
    }
    \begin{itemize}
        \item In high dimensions, things can look good when projected into 2D.
        \item We need a systematic way of seeking out tension, without relying on inspired choices of parameters to reveal them
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{The DES evidence ratio $R$}
    \begin{itemize}
        \item The Dark Energy Survey (\arxiv{1708.01530}) quantifies tension between two datasets $A$ and $B$ using the Bayes ratio:
            \[
                R = \frac{\mathcal{Z}_{AB}}{\mathcal{Z}_A \mathcal{Z}_B}
            \]
            where $\mathcal{Z}$ is the Bayesian evidence.
        \item Many attractive properties:
            \begin{itemize}
                \item Symmetry
                \item Parameterisation independence
                \item Dimensional consistency
                \item Use of well-defined Bayesian quantities
            \end{itemize}
        \item What does it mean?
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The meaning of the DES evidence ratio $R$}
    \begin{itemize}
        \item The Dark Energy Survey collaboration (\arxiv{1708.01530}) quantify tension between two datasets $A$ and $B$ using the Bayes ratio:
            \[
                R = \frac{\mathcal{Z}_{AB}}{\mathcal{Z}_A \mathcal{Z}_B} = \frac{P(A\cap B)}{P(A)P(B)} = \frac{P(A|B)}{P(A)} = \frac{P(B|A)}{P(B)}
            \]
        \item $R$ gives the relative change in our confidence in data $A$ in light of having seen $B$ (and vice-versa).
        \item $R>1$ implies we have more confidence in $A$ having received $B$.
        \item Like evidences, it is prior-dependent
        \item Increasing prior widths $\Rightarrow$ increasing confidence.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The DES evidence ratio $R$: Prior dependency}
    {\includegraphics[trim=0.6in 0.3in 0in 0in]{./plots/prior_dependency.pdf}}
    \begin{itemize}
        \item What does it mean if increasing prior widths $\Rightarrow$ increasing confidence? 
        \item Wide priors mean {\em a-priori\/} the parameters could land anywhere.
        \item We should be proportionally more reassured when they land close to one another if the priors are wide
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{How do we deal with the prior dependency in $R$?}
    \begin{description}
        \item[Option 1] Take the Bayesian route, accept the prior dependency, and spend time trying to justify why a given set of priors are ``physical''.
        \item[Option 2] Try to find a principled way of removing this prior dependency
    \end{description}
    \begin{itemize}
        \item One of the critical observations is that one can only hide tension by widening priors. Narrowing them will only ever show tension if it is present.
        \item If we could define ``Narrowest reasonable priors'' and find that $R<1$, then this would indicate tension.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$R$: a Gaussian example}
    \begin{itemize}
        \item Given two Gaussians with parameter means $\mu_A,\mu_B$ and parameter covariances $\Sigma_A,\Sigma_B$ and a prior with volume $V_\pi$:
            \begin{align}
                \log R =& -\frac{1}{2} (\mu_A-\mu_B){(\Sigma_{A}+\Sigma_{B})}^{-1}(\mu_A-\mu_B)\nonumber\\
                & + \log V_\pi\nonumber -\log\sqrt{|2\pi(\Sigma_{A}+\Sigma_{B})|} 
            \end{align}
        \item Like evidence, $R$ composed of ``Goodness of fit'', and ``Occam factor''.
        \item Ideally want would remove this Occam factor (ratio of prior to posterior volume).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{KL divergence $\mathcal{D}$, Information $\mathcal{I}$, suspiciousness $S$}
    \begin{itemize}
        \item The KL divergence quantifies the compression from prior to posterior:
            \[
                \mathcal{D} = \int P(\theta|D) \log \frac{P(\theta|D)}{P(\theta)} d\theta = \left\langle\log\frac{\text{Posterior}}{\text{Prior}}\right\rangle_\text{Posterior}
            \]
        \item It bears many similarities to an Occam factor, for a Gaussian:
            \[
                \mathcal{D} =   \log V_\pi - \log \sqrt{|2\pi\Sigma|} - \frac{1}{2}d
            \]
        \item Can define equivalent of $R$ for KL divergence, the information ratio $\mathcal{I}$
            \begin{align}
                \log R &= \mathcal{Z}_{AB} -\mathcal{Z}_A - \mathcal{Z}_B \nonumber\\
                \log \mathcal{I} &= \mathcal{D}_A + \mathcal{D}_B - \mathcal{D}_{AB} \nonumber
            \end{align}
        \item Subtracting the two removes prior dependency, giving suspiciousness:
            \[
                \log S = \log R - \log \mathcal{I}
            \]
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Suspiciousness $S$}
    \begin{itemize}
        \item For a Gaussian:
            \[
                \log S = \frac{d}{2}  -\frac{1}{2} (\mu_A-\mu_B){(\Sigma_{A}+\Sigma_{B})}^{-1}(\mu_A-\mu_B).
            \]
        \item We thus find that our original idea for tension $X^2_d=d-2\log S$.
        \item However $S$ is composed of evidences $\mathcal{Z}$ and KL divergences $\mathcal{D}$, which are Gaussian-independent concepts.
        \item The only thing remaining to determine is $d$, the ``number of parameters''.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Dimensionality $d$}
    \centerline{%
    \includegraphics[width=0.49\textwidth]{./figures/dimensions_1.pdf}
    \includegraphics[width=0.49\textwidth]{./figures/dimensions_2.pdf}
    }
    \begin{itemize}
        \item Intuition should tell us that the $d$ we need is the effective number of parameters (i.e.\ should not include unconstrained ones).
        \item Like the evidence, or the KL divergence, this ``Model dimensionality'' should be a sought-after inference quantity.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Dimensionality $\tilde{d}$}
    \begin{itemize}
        \item KL divergence is the mean of the Shannon information $I$:
            \begin{align}
                \mathcal{D} &= \int P(\theta|D) \log \frac{P(\theta|D)}{P(\theta)} d\theta = \left\langle\log\frac{\text{Posterior}}{\text{Prior}}\right\rangle_\text{Posterior}\nonumber\\
                I &= \log\frac{\text{Posterior}}{\text{Prior}}\nonumber
            \end{align}
        \item Model dimensionality proportional to variance of Shannon information:
            \[
                \frac{\tilde{d}}{2} = \text{var}\left(\frac{\text{Posterior}}{\text{Prior}}\right)_\text{Posterior}
            \]
        \item Examples from real data:
            \begin{align}
                \tilde{d}_\text{Planck} &= 15.8 \pm  0.3 &(21) \nonumber\\
                \tilde{d}_\text{DES} &= 14.0 \pm  0.3 &(26) \nonumber\\
                \tilde{d}_\text{BAO} &= 2.95 \pm  0.07 &(6) \nonumber\\
                \tilde{d}_\text{S$H_0$ES} &= 0.93 \pm  0.03 &(6) \nonumber
            \end{align}
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Headline results: $\Lambda$CDM}
    \begin{itemize}
        \item Can calibrate $X^2_d$ as on the same scale as $\chi^2_d$ to give a $p$-value-like quantity, termed ``Tension probability'' $p$, or $\sigma$
            \begin{align}
                \text{Planck 2015 vs BAO}:&      &p&=  42 \pm     4 \%        &\sigma& = 0.8\pm0.1\nonumber\\
                \text{Planck 2015 vs DES}:&      &p&=   3.2 \pm     1.0 \%    &\sigma& = 2.1\pm0.1\nonumber\\
                \text{Planck 2015 vs S$H_0$ES}:& &p&=   0.25 \pm     0.17 \%  &\sigma& = 3.1\pm0.3 \nonumber\\
                \text{Planck 2018 vs S$H_0$ES}:& &p&=   0.001 \pm     0.0001 \%  &\sigma& = 4.42\pm0.03\nonumber
            \end{align}
        \item Under this metric, S$H_0$ES is unambiguously inconsistent. BAO is consistent, and $DES$ is inconsistent, but only just. This is pleasingly similar to ones intuition.
		\item \arxiv{1902.04029}
    \end{itemize}
\end{frame}



\begin{frame}
    \frametitle{Headline results: $K\Lambda$CDM $\equiv\Lambda$CDM + $\Omega_K$}
    \begin{itemize}
        \item Cosmologies with curvature have different tension (\arxiv{1908.09139}):
            \begin{align}
                \text{Planck vs CMB lensing}:&      &p&=  1.3\pm 0.3 \% &\sigma& =2.5\pm0.1\nonumber\\
                \text{Planck vs BAO}:&      &p&=   0.25\pm 0.05 \% &\sigma& = 3.0 \pm 0.1 \nonumber
            \end{align}
		\includegraphics[width=\textwidth]{figures/omegak_H0.pdf}
        \item One should be suspicious about combining CMB lensing and Planck in curved models.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Evidence: Is the universe curved?}
    \includegraphics{figures/evidences.pdf}
    \begin{itemize}
        \item We only think the universe is flat because Planck+lensing and/or BAO tell us so, but these datasets are in tension.
        \item Planck alone tells us the universe is closed at 50:1
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{What about $A_\mathrm{lens}$?}
    \begin{itemize}
        \item The Planck papers do discuss this, but phrase in terms of $A_\mathrm{lens}$
        \item $A_\mathrm{lens}$ is a ``fudge factor'' that artificially increases the effective smoothing by lensing on the primordial power spectrum if $A_\mathrm{lens}\ne 1$
        \item Planck CMB prefers $A_\mathrm{lens}\sim1.2$, lensing wants $A_\mathrm{lens}\sim1$ (can also be defined as a $\sim2\sigma$ tension)
        \item It is the same effect that causes the preference for closed universes.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{How do we fix curvature tension?}
    \begin{itemize}
        \item There could be several explanations:
        \item Systematic in Planck CMB
            \begin{itemize}
                \item Efstathiou \& Gratton's new likelihood looks like it may relax these tensions a little (results pending release of likelihood)
            \end{itemize}
        \item Systematic in Planck lensing
            \begin{itemize}
                \item The lensing likelihood includes a fiducial $\Lambda$CDM model.
                \item {\em In theory\/} this shouldn't matter, but re-doing the analysis with a different fiducial model is a mammoth task.
            \end{itemize}
        \item Statistical fluctuation?
            \begin{itemize}
                \item With tensions at $2$--$3\sigma$ there is always a possibility that we just got unlucky with our universe.
                \item There are a lot of these $2$--$3\sigma$ tensions in the Planck data. 
                \item If there were an explanation that resolved multiple tensions simultaneously then this could be decicive evidence for a new concordance model.
            \end{itemize}
        \item It may be that we need to wait for the next generation CMB experiment to resolve the issue (SO, LiteBird, PICO?, CORE?)
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Theory of curved universes}
    \begin{itemize}
        \item We generally work with flat cosmologies for a few reason
            \begin{enumerate}
                \item Data tell us it is (or does it?)
                \item Theory of inflation tells us it is (somewhat circular reasoning)
                \item The theory is \textbf{much} easier in flat cosmologies
            \end{enumerate}
        \item I've been interested in the effect of curvature at the start of inflation for a while
        \item Even small amounts of curvature can have observational effects on the primordial power spectrum
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{History of curved universes}
    \includegraphics[width=\textwidth]{figures/history.pdf}
\end{frame}

\begin{frame}
    \frametitle{The primordial power spectrum}
    \begin{itemize}
        \item Inflationary theory interacts primarily with observation via the primordial power spectrum of curvature perturbations $\mathcal{P}_\mathcal{R}(k)$ (along with the tensor to scalar ratio $r$)
        \item $\Lambda$CDM assumes a near scale-invariant power spectrum 
            \[ \mathcal{P}_\mathcal{R}(k) = A_s {\left( \frac{k}{k_s} \right)}^{n_s-1} \]
        \item Inflation theories generally predict values of $(r,n_s)$ which can be compared with the phenomenological fit.
        \item $A_s$ is constrained by (amongst other things) the amplitude of the cosmic microwave background power spectrum
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{Predictions from inflation}
    \includegraphics[width=\textwidth]{figures/inflation.pdf}
\end{frame}

\begin{frame}
    \frametitle{Primordial power spectrum computation}
    \begin{itemize}
        \item Background variables:
            \[ H^2 = \frac{1}{3}\left( \frac{1}{2}\dot\phi^2 + V(\phi) \right), \qquad 0=\ddot{\phi} + 3 H \dot{\phi} + \frac{d}{d\phi}V(\phi) \]
        \item Mukhanov variable $v= z\mathcal{R}$ ($z=a \dot{\phi}/H$) evolves according to:
            \[ 0 = v^{\prime\prime} + \left(k^2 - \frac{z^{\prime\prime}}{z}\right) v 
            \quad\Leftrightarrow\quad 0 = \mathcal{R}^{\prime\prime} + 2\frac{z^\prime}{z} \mathcal{R}^\prime + k^2 \mathcal{R} \]
    In terms of cosmic time $t$:
    \[ 
    0 = \ddot{\mathcal{R}} + \left(2\frac{z^\prime}{z}+H\right) \dot{\mathcal{R}} + k^2 \mathcal{R} 
\]
        \item During inflation $\eta\to\eta_*$ friction term $\to\infty$, mode freezes.
        \item Primordial power spectrum is $P_\mathcal{R}(k)\propto\lim_{\eta\to\eta_*} |\mathcal{R}|^2$
        \item Initial conditions for evolution set by quantum mechanics (more later)
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Mukhanov Sazaki equation in curved universes}

    \begin{itemize}
        \item In flat space:
        \begin{gather}
            H^2 = \frac{1}{3}\left( \frac{1}{2}\dot\phi^2 + V(\phi) \right), \qquad 0=\ddot{\phi} + 3 H \dot{\phi} + \frac{d}{d\phi}V(\phi), \qquad z=\frac{a \dot{\phi}}{H} \nonumber\\
            0 = \ddot{\mathcal{R}} + \left(2\frac{z^\prime}{z}+H\right) \dot{\mathcal{R}} + k^2 \mathcal{R} \nonumber
        \end{gather}
    \item With curvature ($K=-1,0,+1$):
        \begin{gather}
            H^2 = \frac{1}{3}\left( \frac{1}{2}\dot\phi^2 + V(\phi) \right) - \frac{K}{a^2}, \qquad 0=\ddot{\phi} + 3 H \dot{\phi} + \frac{d}{d\phi}V(\phi) \qquad z=\frac{a \dot{\phi}}{H}\nonumber\\
            \hspace{-10pt}0=\ddot{\mathcal{R}} + \frac{\left( H + \frac{2\dot z}{z} \right)\mathcal{D}^2 - \frac{3K H z^2}{2a^2}}{\mathcal{D}^2 - \frac{Kz^2}{2a^2}}\:\dot{\mathcal{R}}
            +\frac{K\left(1+\frac{z^2}{2a^2} - \frac{2}{H}\frac{\dot{z}}{z}\right)\mathcal{D}^2 + \frac{K^2z^2}{2a^2}-\mathcal{D}^4}{a^2(\mathcal{D}^2 - \frac{Kz^2}{2a^2})}\:\mathcal{R}\nonumber\\
            \mathcal{D}^2 = \nabla^2 +3K = -k^2 + 3K \nonumber
        \end{gather}
    \item  \arxiv{1907.08524}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Primordial power spectrum}
    \includegraphics[width=\textwidth]{figures/BD.pdf}
\end{frame}

\begin{frame}
    \frametitle{Initial conditions in curved cosmologies}
    \begin{itemize}
        \item In the flat case, usuallyinitial conditions via ``Bunch Davies vacuum''  to initialise Mukhanov evolution:
            \[ v = \frac{1}{\sqrt{2k}} e^{-i k\eta}\]
        \item These are only valid for modes deep in the horizon $k\gg aH$, where the spacetime is de-Sitter (effectively Minkowski)
        \item In the curved case, the large spatial modes have $k\sim aH$
        \item Need new theory of quantum initial conditions for rapidly changing spacetimes.
        \item Instead of defining vacuum as ``particle-less'' (a relative concept), could define it in terms of ``lowest energy''
        \item Minimising the renormalised stress-energy yields observationally distinct initial conditions \arxiv{1607.04148}.
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Bunch Davies initial conditions}
    \includegraphics[width=\textwidth]{figures/BD.pdf}
\end{frame}


\begin{frame}
    \frametitle{Renormalised stress-energy initial conditions}
    \includegraphics[width=\textwidth]{figures/RST.pdf}
\end{frame}


\begin{frame}
    \frametitle{Conclusions}
    \begin{itemize}
        \item The suspiciousness $S$ has become one of the standards for measuring tensions multiple simultaneous parameters.
        %\item The ``inference triple'' of $\mathcal{Z},\mathcal{D},\tilde{d}$ should be considered in all model comparison analyses (computed \texttt{anesthetic} package).
        \item Significant tensions remain for cosmologies that involve curvature, comparable in scale with other tensions that are discussed.
        \item Curvature+inflation yields a rich set of theoretical problems with potentially observable consequences.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Extra slides}
\end{frame}



%\begin{frame}
%    \frametitle{Bayesian evidence $\mathcal{Z}$}
%    \begin{itemize}
%        \item Bayes theorem for parameter estimation:
%    \[
%    P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)} \quad\longrightarrow\quad \text{Posterior} = \frac{\text{Likelihood}\times\text{Prior}}{\text{Evidence}}
%    \]
%        \item Normalising constant $\equiv$ Bayesian evidence $\equiv$ $P(D)$ is hard to compute:
%    \[
%        P(D) = \int P(D|\theta)P(\theta) d\theta = \left\langle \text{Likelihood} \right\rangle_\text{Prior}
%    \]
%        \item Traditionally used to compare models using the same data
%        \item For DES, it is used to compare different data with the same model.
%        \item Computed using nested sampling (\texttt{MultiNest}, \texttt{PolyChord}, \texttt{dynesty}), simulated annealing (\texttt{emcee}), or from MCMC using \texttt{MCEvidence}.
%    \end{itemize}
%\end{frame}

\begin{frame}
    \frametitle{Bayesian evidence $\mathcal{Z}$: Prior dependency}
    \begin{itemize}
        \item Bayesian evidences are prior dependent:
            \[
                \mathcal{Z} = \int P(D|\theta)P(\theta) d\theta \approx \langle\text{Likelihood}\rangle_\text{Posterior} \times \frac{\text{Posterior volume}}{\text{Prior volume}}
            \]
        \item They balance ``goodness of fit'' via likelihood with ``complexity'' through Occam penalty.
        \item Models that include too many fine-tuned parameters are disfavoured, unless they provide a much better fit.
        \item Corollary: Unconstrained parameters are not penalised.
        \item Widen prior $\Rightarrow$ reduce evidence \\ (providing prior does not cut into posterior).
        \item Bayesians vs Frequentists $\leftrightarrow$ Feature vs Bug.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The problem with Principle Component Analysis}

    \begin{figright}[0.6]{./plots/pca.pdf}
        \begin{itemize}
            \item Compute eigenvectors and eigenvalues of covariance matrix.
            \item These aim to describe ``directions'' in parameter space
            \item This procedure is not covariant:
        \end{itemize}
    \end{figright}
    \begin{quote}
        "Principal Component Analysis" is a dimensionally invalid method that gives people a delusion that they are doing something useful with their data. If you change the units that one of the variables is measured in, it will change all the "principal components"! It's for that reason that I made no mention of PCA in my book. I am not a slavish conformist, regurgitating whatever other people think should be taught. I think before I teach. David J C MacKay.

        \end{quote}
\end{frame}



\end{document}
```
# branch: remotes/origin/texas_2019
```tex
\documentclass[%
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usetikzlibrary{positioning}

\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}

% todo 
% - Ligo actual data
% -define IMRPhenom, EOBNR




\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{Quantised primordial power spectra}
\subtitle{\tiny PARALLEL SESSION: COSMOLOGY - EARLY UNIVERSE AND THE ORIGIN OF STRUCTURE (F)}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Kavli Institute for Cosmology \\
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{14:30, Monday 16\textsuperscript{th} December 2019}

\include{include/beamer_commands}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\begin{frame}
\frametitle{Quantised primordial power spectra}
\tableofcontents
\end{frame}

\section{Observational motivations for quantised primordial power spectra}
\begin{frame}
    \frametitle{Features in the Planck data?}
    \includegraphics[width=\textwidth]{coadded_TT}
\end{frame}

\begin{frame}
    \frametitle{Features in the Planck data?}
    Normalise residuals to have unit standard deviation:

    \includegraphics[width=\textwidth]{residuals}

    \begin{enumerate}
        \item low quadrupole ($\ell=2$) and suppression of power $\ell<30$.
        \item $\ell\sim20$ feature.
        \item (faint) oscillatory characteristics in high-$\ell$ residuals.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Quantised primordial power spectra}
    \begin{itemize}
        \item From inflation, primordial power spectrum $\mathcal{P}_\mathcal{R}(k)\approx A_s{\left( \frac{k}{k_*} \right)}^{n_s-1}$.
        \item What if, instead of a continuous set of wavevectors, only a discrete set are allowed?
        \item Practically amounts to changing transfer convolution: \[C_\ell \sim \int \mathcal{P}_\mathcal{R}(k) \Delta(k) dk \to \sum\limits_{k_i\in {\text{allowed } k}}\mathcal{P}_\mathcal{R}(k_i) \Delta(k_i)\].
        \item For example, allowed $k$ could have linear spacing $\Delta k$, starting at $k_0$.
        \item Quantisation occurs naturally in closed universes, but mechanisms exist for creating the same in flat or open universes.
        \item ``Improved cosmological fits with quantized primordial power spectra'' (Bartlett, Handley \& Lasenby, Jan 2020).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Quantised primordial power spectra}
        \includegraphics[width=\textwidth]{Quantised_Primordial}
\end{frame}



\begin{frame}
    \frametitle{Physical effects of quantisation}
    \centerline{%
        \includegraphics[width=0.95\textwidth]{all_delta_dl_fcb}
    }
\end{frame}

\begin{frame}
    \frametitle{Optimal quantised spectra, $\Delta\chi^2=-8.55$}
    \framesubtitle{Best-fit with Planck 2018 TT,TE,EE+low$\ell$+lowE+lensing likelihood}
    \centerline{%
        \includegraphics[width=0.9\textwidth]{contour_plot_twoscales}
    }

    Plot of residuals
\end{frame}

\begin{frame}
    \frametitle{Breakdown of fit}
    \includegraphics[width=\textwidth]{TT_linear_dl}

    \includegraphics[width=\textwidth]{chi2_contributions}

\end{frame}

\begin{frame}
    \frametitle{How good are these fits?}
    \begin{itemize}
        \item $\Delta\chi^2=-8.55$ is nominally quite an improvement on $\Lambda$CDM.
        \item This comes at the cost of two new parameters associated with the lowest wavenumber $k_0$ and quantisation spacing $\Delta k$.
        \item The parameters are also finely tuned, incurring an Occam penalty in the Bayesian evidence similar in degree to the improved fit.
        \item Ideally would have a models that are more predictive.
    \end{itemize}
    \includegraphics[width=\textwidth]{best_lowl}
\end{frame}

\section{Future conformal boundary theories}

\begin{frame}
    \frametitle{The future conformal boundary}

    \begin{itemize}
        \item Extrapolating our current cosmology, the universe ``ends'' in a Dark energy dominated phase.
        \item As $t\to\infty$, the universe enters a de-Sitter state $a\sim e^{H_\infty t}\to\infty$.
        \item This is a coordinate singularity, at a finite conformal time in the future.
        \item Evolutions of various cosmological components may be continued through this ``future conformal boundary''.
        \item Distinct from Penrose's conformal cyclic cosmologies (CCCs).
        \item ``Radiation, Cold Dark Matter Perturbations and the Future Conformal Boundary'' (Lasenby, Handley et al, Jan 2020).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Evolution through the future conformal boundary}
    \includegraphics[width=\textwidth]{evolution}
\end{frame}

\begin{frame}
    \frametitle{Consequences of the future conformal boundary}
    Theoretical observations:
    \begin{itemize}
        \item The scale factor $a$ is not a physical quantity.
        \item Metric: $dt^2 - a^2 dx^2 \Rightarrow$  physics is blind to the sign of the scale factor.
        \item $s = 1/a$ would be equally appropriate as a variable.
        \item $s$ remains finite and smooth through the boundary where $a\to\infty$.
    \end{itemize}
    \vspace{10pt}
    Physical consequences:
    \begin{itemize}
        \item For a perturbative approach to be valid, first-order perturbations must remain finite at all times.
        \item Requiring this at the beginning and end of the universe means only a discrete set of wavenumbers are allowed.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Allowed wavenumbers}
    \centerline{
    \includegraphics[width=0.95\textwidth]{first_allowed_pert}
    }
\end{frame}

\begin{frame}
    \frametitle{Poor predictions in practice}
    \begin{figright}[0.27]{all_delta_dl_fcb}
        \begin{itemize}
            \item $\Delta k = 0.272 \times 10^{-3}$\\
            \item $k_0=0.701 \times 10^{-3}$
            \item Worse fitting than continuous spectra
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{The future of future conformal boundary theories}
    Key points:
    \begin{itemize}
        \item The quantisations predicted by future conformal boundary theories are of the correct order of magnitude.
        \item Full analyticity is challenging for matter: Is $\rho_\mathrm{m}\sim a^{-3}\text{ or }|a|^{-3}$?
        \item Correcting this discrepancy may be the key to bringing these theories into observational consistency.
    \end{itemize}
    \vspace{10pt}
    Future work:
    \begin{itemize}
        \item How does inflation fit into this picture?
        \item Can a compensator field $\phi$ create a full analytic continuation?
    \end{itemize}
\end{frame}

\section{Kinetic initial conditions}
\begin{frame}
    \frametitle{Kinetic initial conditions}
    \begin{figright}[0.67]{background}
        \begin{itemize}
            \item If inflation starts late, effects of finite inflation may be observable.
            \item ``Just enough inflation'' theory: $N_\mathrm{tot}\sim N_\star+10$.
            \item Suppression of power and features in spectra.
            \item Pre-inflationary phase generically has $\dot\phi^2\gg V(\phi)$: kinetic dominance.
            \item Solutions and theory independent from $V(\phi)$.
            \item ``Kinetic initial conditions for inflation''\\ (Handley, Hobson \& Lasenby arXiv:1401.2253).
            \item ``A case for kinetically dominated initial conditions for inflation'' (Hergt, Handley, Hobson \& Lasenby arXiv:1809.07185).
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Kinetic initial conditions}
    \includegraphics[width=0.49\textwidth]{pps_Nstar}
    \includegraphics[width=0.49\textwidth]{pps_Ndagg}
    \includegraphics[width=0.49\textwidth]{hubble}
    \includegraphics[width=0.49\textwidth]{cmb_KD_Ndagg}
    \begin{itemize}
        \item Hergt, Handley, Hobson \& Lasenby arXiv:1809.07737
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Quantum initial conditions}
    \begin{itemize}
        \item Primordial power spectrum $\mathcal{P}_\mathcal{R}$ governed by freeze-out values of comoving curvature perturbation $\mathcal{R}$.
        \item Traditionally one sets initial conditions for $\mathcal{R}$ (or equivalently the Mukhanov variable $v=z\mathcal{R})$ via quantum mechanical considerations.
        \item For de-Sitter, vacuum clearly defined by Bunch-Davies prescription.
        \item In just enough inflation, for large modes (small $k$) the quantum mechanics is much less clear.
        \item ``Novel quantum initial conditions for inflation''\\
            (Handley, Lasenby \& Hobson arXiv:1607.04148).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Frozen initial conditions}

    \begin{figright}[0.3]{Exit}
        \begin{itemize}
            \item Proposal: 
                \begin{align}
                    \lim\limits_{t\to 0}&\:|\mathcal{R}|\sim\mathrm{const} \nonumber\\
                    \lim\limits_{t\to 0}&\:\dot{\mathcal{R}}=0 \nonumber
                \end{align}
            \item Perturbations remain valid.
            \item Cosine mode/real component is selected.
            \item Acoustic oscillations on horizon exit.
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Frozen power spectra}
    \includegraphics[width=0.49\textwidth]{quantised_pps}
    \includegraphics[width=0.49\textwidth]{quantised_pps_log}
    \begin{itemize}
        \item Pseudo quantised power spectrum $\mathcal{P}_\mathcal{R}\approx A_s{\left( \frac{k}{k_*} \right)}^{n_s-1}\times \cos^2(\omega k+\phi)$.
        \item $k_0$, $\Delta k$ are a function of when inflation starts.
        \item ``Rapid numerical solutions for the Mukhanov-Sazaki equation''\\(Haddadin \& Handley, arXiv:1809.11095v2 Jan 2020).
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Observational consequences}
    \centerline{%
        \includegraphics[width=0.95\textwidth]{haddadin}
    }
\end{frame}

\begin{frame}
    \frametitle{Further work}
    \begin{itemize}
        \item Full Bayesian parameter estimation and model comparison.
        \item Quantum consequences/interpretation of frozen initial conditions.
        \item Implications for the effects of the future conformal boundary?
        \item Currently being explored by Thomas Gessey-Jones (Part III student).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Summary}
    \begin{itemize}
        \item Quantised primordial power spectra have an intriguing observational motivation, able to simultaneously reproduce suppression of low-$\ell$ power, the $\ell\sim20$ feature and oscillations at high $\ell$.
        \item The future conformal boundary is a coordinate singularity, and naturally induces a quantised primordial power spectrum of the correct order of magnitude, but inconsistent with observations.
        \item Kinetic initial conditions/just enough inflation models combined with frozen initial conditions produce a pseudo-quantised spectrum which compellingly can recover the 'sweet spot' best-fit with a single additional cosmological parameter.
    \end{itemize}
\end{frame}

%\begin{frame}
%    \includegraphics[width=0.8\textwidth]{phi4o3}
%
%    primordial curvature as alternative motivation for just enough inflation
%
%    + other slides
%\end{frame}
%
%\begin{frame}
%    \includegraphics[width=\textwidth]{RST}
%
%    \begin{itemize}
%        \item ``Primordial power spectra for curved inflating universes''\\ Handley arXiv:1907.08524 
%    \end{itemize}
%\end{frame}

\end{document}
```
# branch: remotes/origin/ucl_2019
```tex
\documentclass[%
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}

\include{include/beamer_commands}
\usepackage{layouts}



\title{Quantifying cosmological tensions}
\subtitle{Interpreting the DES evidence ratio}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{24\textsuperscript{th} July 2019}

% Abstract
% --------
% Observational cosmology is close to breaking point. There is now a 4.9-sigma
% tension between the expansion rate $H_0$ as inferred by CMB modelling and
% $H_0$ as measured by supernovae observers. Other parameter tensions
% potentially exist between alternative combinations of data, such as a DES and
% Planck ($S_8$ and $\Omega_m$), or Planck CMB and lensing ($\Omega_K$). In
% these cases it is less obvious how to quantify the level of tension in terms
% of ``sigma'', either due to the high dimensionality of the space, or the
% non-gaussianity of the distributions. Diagnosing the location and level of
% these tensions could prove critical in determining their cause.
%
% In this talk I will discuss recent research by Pablo Lemos and myself into
% novel statistical ways to measure these tensions, how to quantify the number
% of parameters a given experiment constrains, the calibration of the DES
% evidence ratio and the problems with principle component analysis (PCA).


\begin{document}

\begin{frame}
    \titlepage{}
    \begin{center}
    Handley \& Lemos \arxiv{1902.04029}, \arxiv{1903.06682}

    \vspace{10pt}
    \url{github.com/williamjameshandley/anesthetic}
    \end{center}
\end{frame}

%\begin{frame}
%    \frametitle{Outline}
%    \tableofcontents
%\end{frame}

\begin{frame}
    \frametitle{The Hubble $H_0$ tension}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}
            \item CMB cosmologists (Planck) infer \\ $H_0=67\pm0.5\text{ km s}^{-1} \text{Mpc}^{-1}$
            \item Supernovae data  ($SH_0ES$) measure \\ $H_0=74\pm1.4 $
            \item $>4\sigma$ discrepancy could be due to:
                \begin{itemize}
                    \item Systematic error
                    \item Problem with standard model of cosmology ($\Lambda$CDM)
                \end{itemize}
            \item Inconsistent datasets shouldn't be combined
        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{overlayarea}{\textwidth}{0.5\textheight}
            \includegraphics<1>{./plots/H0.pdf}
            \includegraphics<2>{./plots/H0_combined.pdf}
            \end{overlayarea}
        \end{column}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{$\sigma_8$-$\Omega_m$ tension}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}
            \item<1-> Matter density $\Omega_m$ and RMS matter fluctuations $\sigma_8$ are constrained by 
            \item<1-> BAO and Planck look consistent
            \item<2-> DES is less clear
            \item<2-> How do you define a tension in terms of ``sigma'' for this case?
        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{overlayarea}{\textwidth}{0.5\textheight}
            \includegraphics<1>{./plots/BAO_planck.pdf}
            \includegraphics<2>{./plots/DES_planck.pdf}
            \end{overlayarea}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{$\Omega_K$ tension}
    \begin{columns}
        \begin{column}{0.5\textwidth}
        \begin{itemize}
            \item Models with spatial curvature $\Omega_K$.
            \item Best-kept secret of Planck: only $1/10,000$ MCMC samples $\Omega_K>0$.
            \item How consistent do Planck and CMB lensing look?
            \item Await likelihood release (beginning of next month)
        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{overlayarea}{\textwidth}{0.5\textheight}
            \includegraphics<1>{./plots/curvature.pdf}
            \includegraphics<2>{./plots/H0_combined.pdf}
            \end{overlayarea}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Quantifying tension}
    \framesubtitle{Gaussians}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            For 1D Gaussian distributions, tension is pretty easy to define:
            \[
                X = \frac{\mu_A - \mu_B}{\sqrt{\sigma_A^2 + \sigma_B^2}},
            \]
         where $\mu$ and $\sigma$ are the respective parameter means and standard deviations.
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{overlayarea}{\textwidth}{0.65\textheight}
                \includegraphics{./plots/H0.pdf}
            \end{overlayarea}
        \end{column}
    \end{columns}

    The multivariate $d$-dimensional equivalent to this tension would be:
    \[
        X^2_d = {(\mu_A - \mu_B)}^T{(\Sigma_A + \Sigma_B)}^{-1}(\mu_A-\mu_B),
    \]
    where $\Sigma$ is in general a covariance matrix.
\end{frame}

\begin{frame}
    \frametitle{Quantifying tension}
    \framesubtitle{non-Gaussians}
    \begin{figright}[0.4]{./figures/all_components.pdf}
        Things become less clear when distributions become ``banana like'' (\arxiv{1906.11628}),
        or worse, multimodal.
    \end{figright}
    Many attempts to generalise the Gaussian case result in a parameterisation-dependent quantity.
\end{frame}

\begin{frame}
    \frametitle{Quantifying tension}
    \framesubtitle{High-dimensional spaces}
    \centerline{%
    \includegraphics[width=0.49\textwidth]{./figures/tension.pdf}
    \includegraphics[width=0.49\textwidth]{./figures/tension_rotated.pdf}
    }
    \begin{itemize}
        \item In high dimensions, things can look good when projected into 2D.
        \item We need a systematic way of seeking out tension, without relying on inspired choices of parameters to reveal them
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{The DES evidence ratio $R$}
    \begin{itemize}
        \item The Dark Energy Survey (\arxiv{1708.01530}) quantifies tension between two datasets $A$ and $B$ using the Bayes ratio:
            \[
                R = \frac{\mathcal{Z}_{AB}}{\mathcal{Z}_A \mathcal{Z}_B}
            \]
            where $\mathcal{Z}$ is the Bayesian evidence.
        \item Many attractive properties:
            \begin{itemize}
                \item Symmetry
                \item Parameterisation independence
                \item Dimensional consistency
                \item Use of well-defined Bayesian quantities
            \end{itemize}
        \item What does it mean?
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Bayesian evidence $\mathcal{Z}$}
    \begin{itemize}
        \item Bayes theorem for parameter estimation:
    \[
    P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)} \quad\longrightarrow\quad \text{Posterior} = \frac{\text{Likelihood}\times\text{Prior}}{\text{Evidence}}
    \]
        \item Normalising constant $\equiv$ Bayesian evidence $\equiv$ $P(D)$ is hard to compute:
    \[
        P(D) = \int P(D|\theta)P(\theta) d\theta = \left\langle \text{Likelihood} \right\rangle_\text{Prior}
    \]
        \item Traditionally used to compare models using the same data
        \item For DES, it is used to compare different data with the same model.
        \item Computed using nested sampling (\texttt{MultiNest}, \texttt{PolyChord}, \texttt{dynesty}), simulated annealing (\texttt{emcee}), or from MCMC using \texttt{MCEvidence}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Bayesian evidence $\mathcal{Z}$: Prior dependency}
    \begin{itemize}
        \item Bayesian evidences are prior dependent:
            \[
                \mathcal{Z} = \int P(D|\theta)P(\theta) d\theta \approx \langle\text{Likelihood}\rangle_\text{Posterior} \times \frac{\text{Posterior volume}}{\text{Prior volume}}
            \]
        \item They balance ``goodness of fit'' via likelihood with ``complexity'' through Occam penalty.
        \item Models that include too many fine-tuned parameters are disfavoured, unless they provide a much better fit.
        \item Corollary: Unconstrained parameters are not penalised.
        \item Widen prior $\Rightarrow$ reduce evidence \\ (providing prior does not cut into posterior).
        \item Bayesians vs Frequentists $\leftrightarrow$ Feature vs Bug.
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{The meaning of the DES evidence ratio $R$}
    \begin{itemize}
        \item The Dark Energy Survey collaboration (\arxiv{1708.01530}) quantify tension between two datasets $A$ and $B$ using the Bayes ratio:
            \[
                R = \frac{\mathcal{Z}_{AB}}{\mathcal{Z}_A \mathcal{Z}_B} = \frac{P(A\cap B)}{P(A)P(B)} = \frac{P(A|B)}{P(A)} = \frac{P(B|A)}{P(B)}
            \]
        \item $R$ gives the relative change in our confidence in data $A$ in light of having seen $B$ (and vice-versa).
        \item $R>1$ implies we have more confidence in $A$ having received $B$.
        \item Like evidences, it is prior-dependent
        \item Increasing prior widths $\Rightarrow$ increasing confidence.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The DES evidence ratio $R$: Prior dependency}
    {\includegraphics[trim=0.6in 0.3in 0in 0in]{./plots/prior_dependency.pdf}}
    \begin{itemize}
        \item What does it mean if increasing prior widths $\Rightarrow$ increasing confidence? 
        \item Wide priors mean {\em a-priori\/} the parameters could land anywhere.
        \item We should be proportionally more reassured when they land close to one another if the priors are wide
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{How do we deal with the prior dependency in $R$?}
    \begin{description}
        \item[Option 1] Take the Bayesian route, accept the prior dependency, and spend time trying to justify why a given set of priors are ``physical''.
        \item[Option 2] Try to find a principled way of removing this prior dependency
    \end{description}
    \begin{itemize}
        \item One of the critical observations is that one can only hide tension by widening priors. Narrowing them will only ever show tension if it is present.
        \item If we could define ``Narrowest reasonable priors'' and find that $R<1$, then this would indicate tension.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$R$: a Gaussian example}
    \begin{itemize}
        \item Given two Gaussians with parameter means $\mu_A,\mu_B$ and parameter covariances $\Sigma_A,\Sigma_B$ and a prior with volume $V_\pi$:
            \begin{align}
                \log R =& -\frac{1}{2} (\mu_A-\mu_B){(\Sigma_{A}+\Sigma_{B})}^{-1}(\mu_A-\mu_B)\nonumber\\
                & + \log V_\pi\nonumber -\log\sqrt{|2\pi(\Sigma_{A}+\Sigma_{B})|} 
            \end{align}
        \item Like evidence, $R$ composed of ``Goodness of fit'', and ``Occam factor''.
        \item Ideally want would remove this Occam factor (ratio of prior to posterior volume).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{KL divergence $\mathcal{D}$, Information $\mathcal{I}$, suspiciousness $S$}
    \begin{itemize}
        \item The KL divergence quantifies the compression from prior to posterior:
            \[
                \mathcal{D} = \int P(\theta|D) \log \frac{P(\theta|D)}{P(\theta)} d\theta = \left\langle\log\frac{\text{Posterior}}{\text{Prior}}\right\rangle_\text{Posterior}
            \]
        \item It bears many similarities to an Occam factor, for a Gaussian:
            \[
                \mathcal{D} =   \log V_\pi - \log \sqrt{|2\pi\Sigma|} - \frac{1}{2}d
            \]
        \item Can define equivalent of $R$ for KL divergence, the information ratio $\mathcal{I}$
            \begin{align}
                \log R &= \mathcal{Z}_{AB} -\mathcal{Z}_A - \mathcal{Z}_B \nonumber\\
                \log \mathcal{I} &= \mathcal{D}_A + \mathcal{D}_B - \mathcal{D}_{AB} \nonumber
            \end{align}
        \item Subtracting the two removes prior dependency, giving suspiciousness:
            \[
                \log S = \log R - \log \mathcal{I}
            \]
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Suspiciousness $S$}
    \begin{itemize}
        \item For a Gaussian:
            \[
                \log S = \frac{d}{2}  -\frac{1}{2} (\mu_A-\mu_B){(\Sigma_{A}+\Sigma_{B})}^{-1}(\mu_A-\mu_B).
            \]
        \item We thus find that our original idea for tension $X^2_d=d-2\log S$.
        \item However $S$ is composed of evidences $\mathcal{Z}$ and KL divergences $\mathcal{D}$, which are Gaussian-independent concepts.
        \item The only thing remaining to determine is $d$, the ``number of parameters''.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Dimensionality $d$}
    \centerline{%
    \includegraphics[width=0.49\textwidth]{./figures/dimensions_1.pdf}
    \includegraphics[width=0.49\textwidth]{./figures/dimensions_2.pdf}
    }
    \begin{itemize}
        \item Intuition should tell us that the $d$ we need is the effective number of parameters (i.e.\ should not include unconstrained ones).
        \item Like the evidence, or the KL divergence, this ``Model dimensionality'' should be a sought-after inference quantity.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Dimensionality $\tilde{d}$}
    \begin{itemize}
        \item KL divergence is the mean of the Shannon information $I$:
            \begin{align}
                \mathcal{D} &= \int P(\theta|D) \log \frac{P(\theta|D)}{P(\theta)} d\theta = \left\langle\log\frac{\text{Posterior}}{\text{Prior}}\right\rangle_\text{Posterior}\nonumber\\
                I &= \log\frac{\text{Posterior}}{\text{Prior}}\nonumber
            \end{align}
        \item Model dimensionality proportional to variance of Shannon information:
            \[
                \frac{\tilde{d}}{2} = \text{var}\left(\frac{\text{Posterior}}{\text{Prior}}\right)_\text{Posterior}
            \]
        \item Examples from real data:
            \begin{align}
                \tilde{d}_\text{Planck} &= 15.8 \pm  0.3 &(21) \nonumber\\
                \tilde{d}_\text{DES} &= 14.0 \pm  0.3 &(26) \nonumber\\
                \tilde{d}_\text{BAO} &= 2.95 \pm  0.07 &(6) \nonumber\\
                \tilde{d}_\text{S$H_0$ES} &= 0.93 \pm  0.03 &(6) \nonumber
            \end{align}
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Headline results}
    \begin{itemize}
        \item Can calibrate $X^2_d$ as on the same scale as $\chi^2_d$ to give a $p$-value-like quantity, termed ``Tension probability'' $p$
            \begin{align}
                \text{Planck+BAO}:&      &p&=  42 \pm     4 \% \nonumber\\
                \text{Planck+DES}:&      &p&=   3.2 \pm     1.0 \% \nonumber\\
                \text{Planck+S$H_0$ES}:& &p&=   0.25 \pm     0.17 \% \nonumber
            \end{align}
        \item Under this metric, S$H_0$ES is unambiguously inconsistent, although not quite as brutal as $>4\sigma$. BAO is consistent, and $DES$ is inconsistent, but only just. This is pleasingly similar to ones intuition.
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Extensions}
    \begin{itemize}
        \item In light of these results, there are two natural questions to ask:
    \end{itemize}
    \begin{enumerate}
        \item $\tilde{d}_\text{BAO} = 2.95 \pm  0.07 $ out of a possible 6. Which $\sim 3$ are these?
        \item Is there a direction in parameter space which is ``most in tension''
    \end{enumerate}
    \begin{itemize}
        \item These are questions which people would usually answer with a PCA-type approach.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The problem with Principle Component Analysis}

    \begin{figright}[0.6]{./plots/pca.pdf}
        \begin{itemize}
            \item Compute eigenvectors and eigenvalues of covariance matrix.
            \item These aim to describe ``directions'' in parameter space
            \item This procedure is not covariant:
        \end{itemize}
    \end{figright}
    \begin{quote}
        "Principal Component Analysis" is a dimensionally invalid method that gives people a delusion that they are doing something useful with their data. If you change the units that one of the variables is measured in, it will change all the "principal components"! It's for that reason that I made no mention of PCA in my book. I am not a slavish conformist, regurgitating whatever other people think should be taught. I think before I teach. David J C MacKay.

        \end{quote}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \begin{itemize}
        \item The DES ratio $R$ is a principled thing to work with, but its prior dependency must be acknowledged
        \item Using KL divergences and model dimensionalities, $R$ may be calibrated into something akin to the tension we desire.
        \item The ``inference triple'' of $\mathcal{Z},\mathcal{D},\tilde{d}$ should be considered in all model comparison analyses.
        \item All three can be computed from nested sampling runs using the \texttt{anesthetic} package.
        \item Be careful when applying principle component analysis!
    \end{itemize}
\end{frame}
\end{document}
```
# branch: remotes/origin/adelaide_2020
```tex
\documentclass[%
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}

% todo 
% - Ligo actual data
% -define IMRPhenom, EOBNR


\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\text{knots}}
\newcommand{\nlive}{n_\text{live}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

% Nested sampling: an efficient and robust Bayesian inference tool for
% physics and machine learning.
% 
% Nested sampling is an alternative Markov-chain Monte-Carlo technique for
% integrating and exploring probability distributions. With publicly available
% implementations such as MultiNest, PolyChord and dynesty, nested sampling has
% become widely adopted in the fields of astronomy and particle physics as a
% powerful tool for computing Bayesian evidences and sampling challenging
% a-priori unknown parameter spaces.
% 
% In this talk I will give a user's guide to the theory of nested sampling in
% the context of Bayesian model comparison and parameter estimation, a survey of
% the current state of the art and the future of the field. I will illustrate
% with examples from cosmological and inflationary model comparison, primordial
% power spectrum reconstruction, 21cm cosmology, exoplanets and particle
% physics, as well as recent work in machine learning, Bayesian Neural Networks
% and likelihood-free inference.


\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{Nested Sampling}
\subtitle{An efficient and robust Bayesian inference tool\\ for physics and machine learning}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Kavli Institute for Cosmology \\
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{February 14\textsuperscript{th}, 2020}

\include{include/beamer_commands}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\section{Fitting a line to data}
\begin{frame}
    \frametitle{Motivating example}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_points.pdf}
        \begin{itemize}
            \item We have noisy data $D$
            \item We wish to fit a model $M$
            \item Functional form $y=f_M(x;\theta)$
            \item For example:
                \begin{align}
                     f_\text{linear}(x;\theta)&=a x + b       \nonumber\\
                     f_\text{quadratic}(x;\theta)&=a x^2 + b  \nonumber
                \end{align}
            \item Model parameters $\theta= (a,b)$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ best-fit}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff.pdf}
        \begin{itemize}
            \item For each parameter set $\theta$:
                \[
                    \chi^2(\theta) = \sum_i \left|y_i - f(x_i;\theta)\right|^2
                \]
            \item Minimise $\chi^2$ wrt $\theta$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ with non-uniform data errors}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data.pdf}
        \begin{itemize}
            \item If data have non-uniform errors:
                \[
                    \chi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|^2}{\sigma_i^2}
                \]
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Problems with $\chi^2$}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff_2.pdf}
        \begin{itemize}
            \item How do we differentiate between models
            \item Why square the errors? -- could take absolute:
                \[
                    \psi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|}{\sigma_i}
                \]
            \item Where does this approach even come from?
        \end{itemize}
    \end{figright}
\end{frame}


\begin{frame}
    \frametitle{Probability distributions}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.6]{./figures/data_diff_1.pdf}
        \begin{itemize}
            \item The probability of observing a datum:
                \[
                    P(y_i | \theta,M) = \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right)
                \]
            \item The probability of observing the data:
                \begin{align}
                    P(D | \theta,M) &= \prod_i \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right) \nonumber\\
                    &=  \frac{1}{\prod_i\sqrt{2\pi}\sigma_i}\exp\sum_i{-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}} \nonumber\\
                    &\propto e^{-\chi^2(\theta)/2}
                    \nonumber
                \end{align}
        \end{itemize}
    \end{figright}
\end{frame}



\begin{frame}
    \frametitle{Maximum likelihood}
    \framesubtitle{Fitting lines to data}
    \begin{figleft}[0.6]{./figures/data_diff.pdf}
        \begin{itemize}
            \item Minimising $\chi^2(\theta)$  is equivalent to maximising $P(D|\theta,M) \propto e^{-\chi^2(\theta)/2}$
            \item $P(D|\theta,M)$ is called the Likelihood $L=L(\theta)$ of the parameters $\theta$
            \item ``Least squares'' $\equiv$ ``maximum likelihood'' \\(if data are gaussian).
        \end{itemize}
    \end{figleft}
\end{frame}

\begin{frame}
    \frametitle{Bayesian inference}
    \begin{itemize}
        \item Likelihood $L=P(D|\theta,M)$ is undeniably correct.
        \item Frequentists construct inference techniques purely from this function.
        \item The trend is cosmology is to work with a Bayesian approach.
        \item What we want are things like $P(\theta|D,M)$ and $P(M|D)$.
        \item To invert the conditionals, we need Bayes theorems:
            \begin{align}
                P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
                P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Terminology}
    \framesubtitle{Bayesian inference}
    \begin{align}
        P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
        \text{Posterior} &= \frac{\text{Likelihood}\times\text{Prior}}{\text{Evidence}} \nonumber
    \end{align}
    \begin{align}
        P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber\\
        \text{Model probability} &= \frac{\text{Evidence}\times\text{Model Prior}}{\text{Normalisation}} \nonumber
    \end{align}
\end{frame}


\begin{frame}
    \frametitle{Multivariate probability}
    \begin{itemize}
        \item Marginalisation:
            \begin{equation*}
                P(x) = \int P(x,y) dy
            \end{equation*}
        \item Conditioning:
            \begin{equation*}
                P(y|x) = \frac{P(x,y)}{P(x)} = \frac{P(x,y)}{\int P(x,y) dy}
            \end{equation*}
        \item De-Conditioning:
            \begin{equation*}
                P(x|y) P(y) = P(x,y)
            \end{equation*}
        \item Bayes theorem:
            \begin{equation*}
                P(y|x) = \frac{P(x|y) P(y)}{P(x)}
            \end{equation*}
            \begin{center}
                ``To flip a conditional $P(x|y)$, you first de-condition on $y$,\\ and then re-condition on $x$.''
            \end{center}
    \end{itemize}
\end{frame}


%\begin{frame}
%    \frametitle{The prior}
%    \framesubtitle{Example: Biased coins}
%    \begin{itemize}
%        \item Need to define the \textbf{Prior} $P(\theta)$ --- probability of the bias, given no data
%        \item Represents our knowledge of parameters before the data -- subjective
%        \item Frequentists view this as a flaw in Bayesian inference. 
%        \item Bayesians view this as an advantage
%        \item Fundamental rule of Inference:\pause\\
%            \vfill
%            \begin{center}
%                \Large You cannot extract information from data\\ without making assumptions 
%            \end{center}
%            \vfill
%        \item All Bayesians do is make them explicit
%        \item Any method that claims it is ``objective'' is simply hiding them
%    \end{itemize}
%\end{frame}

\begin{frame}
    \frametitle{Parameter estimation}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.3]{./figures/parameters.pdf}
        \begin{itemize}
            \item We may use $P(\theta|D,M)$ to inspect whether a model looks reasonable
        \end{itemize}
        \includegraphics[width=\textwidth]{./figures/data.pdf}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Predictive posterior}
    \begin{figright}[0.4]{./figures/fgivenx.pdf}
        More useful to plot:
        \begin{align}
        &P(y|x) = \nonumber\\
        &\int P(y|x,\theta) P(\theta) d\theta \nonumber
        \end{align}
        (all conditioned on $D,M$)
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Model comparison}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.33]{./figures/evidences_log.pdf}
        \begin{itemize}
            \item We may use the Bayesian evidence $Z$ to determine whether a model is reasonable.
            \item $Z = P(D|M) = \int P(D|M,\theta)P(\theta|M)d\theta$
            \item Normally assume uniform model priors $Z \propto P(M|D)P(M)$.
        \end{itemize}
    \end{figright}
\end{frame}
\begin{frame}
    \frametitle{Model comparison}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.33]{./figures/evidences_lin.pdf}
        \begin{itemize}
            \item We may use the Bayesian evidence $Z$ to determine whether a model is reasonable.
            \item $Z = P(D|M) = \int P(D|M,\theta)P(\theta|M)d\theta$
            \item Normally assume uniform model priors $Z \propto P(M|D)P(M)$.
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Line fitting (context)}
    \begin{figright}[0.5]{./figures/supernovae.pdf}
        \begin{itemize}
            \item Whilst this model seems a little trite\ldots
            \item\ldots determining polynomial indices \\$\equiv$ determining cosmological material content:
        \end{itemize}
    \end{figright}
        \[
            {\left( \frac{H}{H_0} \right)}^2 = 
            \Omega_\text{r} {\left( \frac{a_0}{a} \right)}^4+
            \Omega_\text{m} {\left( \frac{a_0}{a} \right)}^3+
            \Omega_k {\left( \frac{a_0}{a} \right)}^2+
            \Omega_\Lambda
            \]
\end{frame}



%\begin{frame}
%    \frametitle{Quantifying error with Probability}
%
%    \begin{itemize}
%        \item As scientists, we are used to seeing error bars on results.
%        \item Age of the universe ({\em Planck\/}): 
%         \[13.73\pm 0.12\:\text{billion years old.}\]
%        \item Masses of LIGO GW150914 binary merger: 
%        \[m_1 = 39.4^{+5.5}_{-4.9}\:M_\odot,\qquad m_2 = 30.9^{+4.8}_{-4.4}\:M_\odot \]
%        \item These are called {\em credible intervals}, state that we are e.g.\ $66\%$ confident of the value lying in this range.
%        \item More importantly, these are {\em summary statistics}.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{LIGO binary merger}
%    \begin{columns}
%        \begin{column}{0.65\textwidth}
%            \includegraphics[width=\textwidth]{./figures/ligo_m1_m2.pdf}
%        \end{column}
%        \begin{column}{0.35\textwidth}
%            \begin{itemize}
%                \item Summary statistics summarise a full probability distribution.
%                \item One goal of inference is to produce these probability distributions.
%            \end{itemize}
%        \end{column}
%    \end{columns}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Theory}
%    \framesubtitle{Extended example of inference: LIGO}
%    \includegraphics[width=\textwidth]{./figures/ligo_schematic.png}
%\end{frame}
%
%\begin{frame}
%    \frametitle{The parameters $\Theta$ of the model $M$}
%    \framesubtitle{Extended example of inference: LIGO}
%    Theoretical signal depends on:
%    \begin{itemize}
%        \item $m_1, m_2$: mass of binary
%        \item $\theta, \phi$: sky location
%        \item $r$: luminosity distance 
%        \item $\Phi_c, t_c$: phase and time of coalescence
%        \item $i, \theta_\text{sky}$: inclination and angle on sky (orbital parameters)
%    \end{itemize}
%\end{frame}
%
%
%
%\begin{frame}
%    \frametitle{Posterior $\mathcal{P}$}
%    \framesubtitle{Extended example of inference: LIGO}
%    \begin{itemize}
%        \item Cannot plot the full posterior distribution:
%            \[\mathcal{P}(\Theta) \equiv P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\text{sky}|D,M)\]
%        \item Can plot 1D and 2D {\em marginalised\/} distributions e.g:
%            \begin{align}
%            &P(m_1,m_2|D,M)=\nonumber\\&\int P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\text{sky}|D,M) \,d\theta \,d\phi \,dr \,d\Phi_c \,d t_c \,d i \,d\theta_\text{sky}\nonumber
%            \end{align}
%        \item May do this for each pair of parameters
%        \item Generates a {\em triangle plot}
%    \end{itemize}
%\end{frame}




\begin{frame}
    \frametitle{Sampling}
    \framesubtitle{How to describe a high-dimensional posterior}

	\begin{figright}{./figures/ligo_m1_m2.pdf}
		\begin{itemize}
          \item In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item Gridding is doomed to failure for $D\gtrsim4$.
          \item {\em Sampling\/} the posterior is an excellent compression scheme.
		\end{itemize}
	\end{figright}
 
\end{frame}

\begin{frame}
    \frametitle{Sampling}
    \framesubtitle{How to describe a high-dimensional posterior}
    \centerline{%
        \includegraphics[height=0.65\textwidth]{./figures/ligo_full.pdf}
    }
\end{frame}
%
%\begin{frame}
%    \frametitle{Why do sampling?}
%    \framesubtitle{Marginalisation over the posterior}
%
%    \begin{itemize}
%        \item Set of $N$ samples $S = \{\Theta^{(i)}: i=1,\ldots N:\: \Theta^{(i)}\sim\mathcal{P}\}$
%        \item Mean mass: \[
%                \bar{m}_1 \equiv\langle m_1\rangle_\mathcal{P}
%                \only<1>{\equiv \int m_1 P(\theta|D,M) d\theta }
%                \only<2>{\approx \frac{1}{N}\sum_{i=1}^N m_1^{(i)}}
%                \only<3>{\approx \frac{\sum_{i=1}^N w^{(i)} m_1^{(i)}}{\sum_{i=1}^N w^{(i)}}}
%            \]
%        \item Mass covariance: \[
%                \mathrm{Cov}(m_1,m_2)
%            \only<1>{\equiv \int (m_1-\bar{m}_1)(m_2-\bar{m}_2) P(\theta|D,M) d\theta }
%                \only<2>{\approx \frac{1}{N}\sum_{i=1}^N (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}
%                \only<3>{\approx \frac{\sum_{i=1}^N w^{(i)} (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}{\sum_{i=1}^N w^{(i)}}}
%            \]
%        \item Marginalised samples: Just ignore the other coordinates.
%        \item N.B. Typically have {\em weighted\/} samples
%    \end{itemize}
%\end{frame}

\begin{frame}
    \frametitle{Cosmology in high dimensions}

    \[\lik(\Theta) = P(D|\Theta,M)\]
    \begin{align}
        \onslide<2->{D =& \{C_\ell\only<6->{^\text{(Planck)}}\}} 
        \onslide<15->{+\{\text{LSS}\}} 
        \onslide<16->{+\{\text{``Big Data''}\}}
        \nonumber\\
        \onslide<3->{M =& \Lambda\text{CDM}} 
        \onslide<9->{+ \text{extensions} }
        \nonumber\\
        \onslide<4->{\Theta =& \Theta_{\Lambda \text{CDM}}} \onslide<7->{+ \Theta_\text{Planck}} \onslide<10->{+ \Theta_\text{extensions}}\nonumber\\
        \onslide<5->{\Theta_{\Lambda \text{CDM}} =& ( \Omega_b h^2, \Omega_c h^2, 100\theta_{MC}, \tau, {\rm{ln}}(10^{10} A_s), n_s) \nonumber\\}
        \onslide<8->{\Theta_\text{Planck} =& (y_{\rm cal}, A^{CIB}_{217}, \xi^{tSZ-CIB}, A^{tSZ}_{143}, A^{PS}_{100}, A^{PS}_{143}, A^{PS}_{143\times217}, A^{PS}_{217}, A^{kSZ}, \nonumber\\& A^{{\rm dust}TT}_{100}, A^{{\rm dust}TT}_{143}, A^{{\rm dust}TT}_{143\times217}, A^{{\rm dust}TT}_{217}, A^{{\rm dust}TE}_{100}, A^{{\rm dust}TE}_{100\times143}, \nonumber\\& A^{{\rm dust}TE}_{100\times217}, A^{{\rm dust}TE}_{143}, A^{{\rm dust}TE}_{143\times217}, A^{{\rm dust}TE}_{217}, c_{100}, c_{217}) \nonumber\\}
        \onslide<11->{\Theta_\text{extensions} =& (
                n_{\rm run}
                \only<12->{,n_{\rm run,run}}
                \only<13->{,w}
                \only<14->{,\Sigma m_\nu, m_{\nu,{\rm{sterile}}}^{\rm{eff}}}
        ) \nonumber}
    \end{align}

    \begin{itemize}
        \item<17->{Parameter estimation: $L, \pi \to \mathcal{P}$: model parameters}
        \item<17->{Model comparison: $L, \pi \to Z$: how good model is}
    \end{itemize}

\end{frame}



%
\begin{frame}
    \frametitle{Parameter estimation}
    \begin{itemize}
        \item The name of the game is therefore drawing samples $S$ from the posterior $\mathcal{P}$ with the minimum number of likelihood calls.
        \item Gridding is doomed to failure in high dimensions.
        \item Enter Metropolis Hastings.
    \end{itemize}
\end{frame}



\section{Metropolis Hastings}


\begin{frame}
  \frametitle{Metropolis Hastings} 
  \begin{itemize}
      
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \begin{enumerate}
        \item Propose random step
        \item If uphill, make step\ldots
          
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
    \item \url{chi-feng.github.io/mcmc-demo/}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Metropolis Hastings} 
  \framesubtitle{Struggles with\ldots}
  \pause
  \begin{enumerate}
      \item Burn in
      \item Multimodality
      \item Correlated Peaks
      \item Phase transitions
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte-Carlo} 
  \begin{itemize}
      \item Key idea: Treat $\log L(\Theta)$ as a potential energy
      \item Guide walker under ``force'': \[F(\Theta) =\nabla \log L(\Theta)\]
      \item Walker is naturally ``guided'' uphill
      \item Conserved quantities mean efficient acceptance ratios.
      \item Mass matrix for kinetic term is a hidden tuning element.
      \item stan is a fully fledged, rapidly developing programming language with HMC as a default sampler.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Ensemble sampling} 
  \begin{itemize}
      \item Instead of one walker, evolve a set of $n$ walkers.
      \item Can use information present in ensemble to guide proposals.
      \item emcee: affine invariant proposals.
      \item emcee is not the only (or even best) affine invariant approach.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The fundamental issue with all of the above} 

  \begin{itemize}
    \item They don't give you evidences!
  \begin{align}
    \ev 
    &= \prob(D|M) 
    \nonumber\\
    &= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
    &= \left\langle \lik \right\rangle_\prior
    \nonumber
  \end{align}
    \item MCMC fundamentally explores the posterior, and cannot average over the prior.
    \item Thermodynamic annealing 
    \begin{itemize}
        \item Suffers from same tuning issues as MCMC
    \end{itemize}
    \item Nearest neighbor volume estimation (Heavens arXiv:1704.03472)
    \begin{itemize}
        \item Does not scale to high dimensions $D\gtrsim10$.
    \end{itemize}
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}

\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to traditional MCMC!} 

  \begin{itemize}
    \item Nested sampling is a completely different way of sampling. 
    \item Uses ensemble sampling to compress prior to posterior.
  \end{itemize}
  
  New procedure: 

  
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      
    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\prior$). 
      
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new uniform sample with higher likelihood
  \end{description}

  
  Requires one to be able to uniformly within a region, subject to a {\em hard likelihood constraint}.

\end{frame}



\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}

\begin{frame}
  \frametitle{Nested sampling} 

  \begin{itemize}
    \item The set of dead points are posterior samples with an appropriate weighting factor
    \item They can also be used to calculate evidences, since it sequentially updates the priors.
    \item The current set of live points is useful for performing clustering and constructing new proposed points.
    \item Algorithm terminates when prior has been compressed onto (and past) the posterior bulk (typical set).
  \end{itemize}
 
\end{frame}

\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}
  \foreach \pagenum in {1,...,16} {%
      \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/lesbesgue}
  }
\end{frame}

\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{Exponential volume contraction} 
  
  \begin{itemize}
    \item At each iteration, the likelihood contour will shrink in volume by  $\approx 1/n$.
    \item Nested sampling zooms in to the peak of the posterior {\em exponentially}.
      \begin{equation}
          \ev \approx \sum_i \Delta\lik_i X_{i}, \qquad
        X_{i+1} \approx \frac{n}{n+1}X_i, \qquad X_{0} = 1 \nonumber
      \end{equation}
    \item Although this is only approximate, we can quantify the error \\
      \begin{equation}
          P(X_{i+1}|X_i) = \frac{1}{n X_i}{\left( \frac{X_{i+1}}{X_i} \right)}^{n-1}[0<X_i<X_{i+1}]
          \nonumber
      \end{equation}
    \item Integral can be expressed in one of two ways
      \begin{equation}
          \ev \approx \sum_i \Delta\lik_i X_{i} = \sum_i \lik_i \Delta X_{i} \nonumber
      \end{equation}

  \end{itemize}
  
\end{frame}



\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item Most of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm.
    \item \url{https://projecteuclid.org/euclid.ba/1340370944}
  \end{itemize}
 
\end{frame}

\begin{frame}
\frametitle{MultiNest}
    \framesubtitle{arXiv:0809.3437 arXiv:0704.3704 arXiv:1306.2144, Feroz, Hobson}
  \includegraphics[width=\textwidth]{figures/multinest.pdf}
\end{frame}

\begin{frame}
  \frametitle{PolyChord}
  \framesubtitle{arXiv:1502.01856 arXiv:1506.00171, Handley, Hobson, Lasenby}
  \includegraphics[width=\textwidth]{figures/polychord.png}
\end{frame}

\begin{frame}
  \frametitle{NeuralNest}
  \framesubtitle{arXiv:1903.10860, Moss}
    \begin{columns}
        \begin{column}{0.55\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
        \end{column}
        \begin{column}{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Diffusive nested sampling}
  \framesubtitle{arXiv:0912.2380, Brewer}
  \includegraphics[width=\textwidth]{figures/dnest.pdf}
\end{frame}

\begin{frame}
\frametitle{PolyChord vs MultiNest}
\begin{itemize}
    \item MultiNest excels in low dimensions $D<10-20$.
    \item PolyChord can go up to $\sim 150$.
    \item Crossover is problem dependent
    \item PolyChord can also exploit fast-slow hierarchy
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Exoplanets}
  \framesubtitle{Nested sampling in action (arXiv:1806.00518, Hall, Walker-Smith, Handley, Queloz)}
  \includegraphics[width=\textwidth]{figures/rv_full.pdf}
\end{frame}

\begin{frame}
  \frametitle{Exoplanets}
  \framesubtitle{Nested sampling in action}
  \begin{itemize}
      \item Simple radial velocity model
          \begin{equation}
              \nu(t;\theta) = \sum_{p=1}^N K_p \sin(\omega_p t + \phi_p)\nonumber
          \end{equation}
      \item Fit each model to data.
      \item Posteriors on model parameters $[(K_p,\omega_p,\phi_p),p=1\cdots N]$ quantify knowledge of system characteristics.
      \item Evidences of models determine relative likelihood of number of planets in system
      \item This is an application where phase transitions matter
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{21cm cosmology}
  \framesubtitle{Nested sampling in action (Paper coming soon, Anstey, de Lera Acedo \& Handley)}
    \begin{figright}[0.3]{./figures/21cm.pdf}
        \begin{itemize}
            \item Search for signal  $T=T_\mathrm{fg} + T_\mathrm{21cm}$ 
            \item Fit parameterised models with/without $T_\mathrm{21cm}$
            \item Compare evidences for signal detection
            \item Use evidences to quantify complexity of beam/sky models
        \end{itemize}
    \end{figright}
\end{frame}


\begin{frame}
  \frametitle{Primordial power spectrum $\PR(k)$ reconstruction}
  \framesubtitle{Nested Sampling in action (arXiv:1908.00906)}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}
%
%
%%\begin{frame}
%%  \frametitle{Planck data}
%%  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
%%  \begin{itemize}
%%    \item<2-> Temperature data TT+lowP
%%    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
%%    \item<4-> Marginalised plots of $\PR(k)$
%%    \item<5->
%%      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
%%  \end{itemize}
%%\end{frame}
%
%
%
\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knot}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{Bayes Factors}
  \frametitle<10>{Marginalised plot}
  %\framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}



  \begin{center}
    \includegraphics<1>[width=0.7\textwidth]{figures/pps_both_1}
    \includegraphics<2>[width=0.7\textwidth]{figures/pps_both_2}
    \includegraphics<3>[width=0.7\textwidth]{figures/pps_both_3}
    \includegraphics<4>[width=0.7\textwidth]{figures/pps_both_4}
    \includegraphics<5>[width=0.7\textwidth]{figures/pps_both_5}
    \includegraphics<6>[width=0.7\textwidth]{figures/pps_both_6}
    \includegraphics<7>[width=0.7\textwidth]{figures/pps_both_7}
    \includegraphics<8>[width=0.7\textwidth]{figures/pps_both_8}
    \includegraphics<9>[width=0.7\textwidth]{figures/pps_evidence}
    \includegraphics<10>[width=0.7\textwidth]{figures/pps_both}

  \end{center}
\end{frame}
\begin{frame}
  \frametitle<1>{COBE (pre-2002)}
  \frametitle<2>{COBE et al (2002)}
  \frametitle<3>{WMAP (2012)}
  \frametitle<4>{Planck (2013)}
  \frametitle<5>{Planck (2015)}
  \frametitle<6>{Planck (2018)}


  \begin{center}
    \includegraphics<1>[width=0.7\textwidth]{figures/cobe}
    \includegraphics<2>[width=0.7\textwidth]{figures/pre_WMAP}
    \includegraphics<3>[width=0.7\textwidth]{figures/WMAP}
    \includegraphics<4>[width=0.7\textwidth]{figures/planck_2013}
    \includegraphics<5>[width=0.7\textwidth]{figures/planck_2015}
    \includegraphics<6>[width=0.7\textwidth]{figures/pps}

  \end{center}
\end{frame}

\begin{frame}
    \frametitle{Bayesian neural networks}
    \framesubtitle{Sparse reconstruction (arXiv:1809.04598)}

    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{itemize}
                \item Neural networks require:
                    \begin{itemize}
                        \item Training to find weights
                        \item Choice of architecture/topology
                    \end{itemize}
                \item Bayesian NNs treat training as a model fitting problem
                \item Compute posterior of weights (parameter estimation)
                \item Use evidence to determine best architecture (model comparison)
                \item ``Compromise-free Bayesian NNs''\\ (Javid, Handley, Lasenby \& Hobson)
                    \begin{itemize}
                        \item Bayesian evidences correlate with out-of-sample performance
                        \item Can be used to determine width and number of hidden layers
                    \end{itemize}
            \end{itemize}
        \end{column}
        \begin{column}{0.4\textwidth}
            \includegraphics[width=\textwidth]{./figures/Colored_neural_network.pdf}
            \includegraphics[width=\textwidth]{./figures/non_comb_mean_results_colour.pdf}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Other uses of nested sampling}
    \begin{columns}
        \begin{column}{0.55\textwidth}
        \begin{itemize}
        \item Nested sampling estimates the density of states $\Delta X_i$, and hence gives you access to a lot more than just posterior samples and evidences
        \item Kullback-Liebler divergence (arXiv:1607.00270)
        \item Bayesian model dimensionality (arXiv:1903.06682)
        \item Suspiciousness \& Tension quantification (arXiv:1902.04029)
        \item DES tension: $\sim2.3\sigma$
        \end{itemize}
        \end{column}
        \begin{column}{0.45\textwidth}
            \includegraphics[width=\textwidth]{./figures/H0.pdf}
            \includegraphics[width=\textwidth]{./figures/s8.pdf}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Curvature tension?}
    \begin{columns}
        \begin{column}{0.4\textwidth}
        \begin{itemize}
            \item Under the same measures of tension, CMB lensing is $2.5\sigma$ in tension with CMB TT,TE,EE
            \item Neglecting CMB lensing gives moderate preference for curvature
            \item Planck phrase these issues in terms of $A_L$
        \end{itemize}
        \end{column}
        \begin{column}{0.6\textwidth}
            \includegraphics[width=\textwidth]{./figures/omegak_H0_2.pdf}
            \includegraphics[width=\textwidth]{./figures/evidences.pdf}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT}
    \framesubtitle{Nested sampling in particle physics (arXiv:1705.07959)}
    \includegraphics[width=\textwidth]{./figures/mn_pp_1.pdf}

    \includegraphics[height=0.234\linewidth]{figures/plot_SingletDM_18_17_post2D_MN1504}\hspace{-3mm}%
    \includegraphics[height=0.234\linewidth]{figures/plot_SingletDM_18_17_post2D_MN1512}\hspace{-3mm}%
    \includegraphics[height=0.234\linewidth]{figures/plot_SingletDM_18_17_post2D_MN1506}\hspace{-3mm}%
    \includegraphics[height=0.234\linewidth]{figures/plot_SingletDM_18_17_post2D_MN1505}


\end{frame}

\begin{frame}
    \frametitle{GAMBIT}
    \framesubtitle{Nested sampling in particle physics (arXiv:1705.07959)}
    \includegraphics[width=\textwidth]{./figures/mn_pp_2.pdf}
    \begin{itemize}
        \item Nested sampling is not an efficient maximiser, it is a sampler.
        \item Tolerance will not get you proportionally closer to the peak, or proportionally faster to the typical set.
        \item Better procedure would be to use it to scout out multimodal distributions, and then launch diver from its final (mode-separated) samples.
        \item You don't need that many live points (run time is linear in live points).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Unweaving runs}
    \framesubtitle{Advances in nested sampling}
    \begin{itemize}
        \item John Skilling noted that two nested sampling runs can be combined in likelihood order to produce a valid run with a larger number of live points.
        \item The reverse is also true (arXiv:1704.03459).
        \item In general, a run with $n$ live points can be ``unweaved'' into $n$ runs with a single live point.
        \item Useful for providing convergence diagnostics and better parameter estimation (arXiv:1804.06406).
    \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Dynamic nested sampling}
  \framesubtitle{Advances in nested sampling (arXiv:1704.03459,  dynesty: arXiv:1904.02180)}
  \includegraphics[width=\textwidth]{figures/dynesty.pdf}
  The number of live points can be varied dynamically in order to oversample regions of interest
\end{frame}


\begin{frame}
    \frametitle{Multi-temperature sampling}
    \begin{itemize}
        \item By compressing from prior to posterior, Nested Sampling's weighted samples are fundamentally different from traditional MCMC.
        \item Nested sampling tails and peaks equally.
        \item We can define the ``temperature'' of a distribution in analogy with thermodnyamics:
            \begin{equation}
                \log L \sim E \Rightarrow P \propto e^{-\beta E} = e^{-E/kT},\quad \beta = 1\nonumber
            \end{equation}
        \item Sampling at different temperatures can be useful for exploring tails.
        \item Nested sampling runs give you the full partition function 
            \begin{equation}
                \log Z(\beta) \approx \sum_i \lik_i^{\beta} \Delta X_{i} \nonumber
            \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Nested importance sampling}
    \framesubtitle{Future research}
    \begin{itemize}
        \item Much of the time spent in a nested sampling run is spent ``compressing the tails''.
        \item Posterior-repartitioned nested sampling gives one way of speeding this up (arXiv:1908.04655)
        \item Sometimes we have a-priori good knowledge of the posterior bulk (analagous to an MCMC proposal distribution).
        \begin{align}
            Z_0 &= \int L(\theta) \pi_0(\theta) d\theta, \qquad
            Z_1 = \int L(\theta) \pi_1(\theta) d\theta \nonumber\\
            &= \int L(\theta)\pi_1(\theta) \frac{\pi_0(\theta)}{\pi_1(\theta)} d\theta
            = \left\langle \frac{\pi_0(\theta)}{\pi_1(\theta)} \right\rangle_{P_1}  \nonumber
        \end{align}
        \item This importance weighting only works if you have a lot of tail samples.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$N$-$\sigma$ contours}
    \framesubtitle{Future research}
    \begin{itemize}
        \item Traditional posterior samples only allow you to plot contours out to 2-3$\sigma$.
        \item Nested sampling fully samples the tails, so in theory one could do $20\sigma$ contours.
        \item Requires further thought in alternatives to kernel density estimation.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Likelihood free inference}
    \begin{itemize}
        \item How can we apply Bayesian inference if we don't know the likelihood, but can simulate the system?
        \item Learn approximation of likelihood from simulations by fitting $f(\theta;\alpha)\approx L(\theta) =P(D|\theta)$, $\alpha$ are hyperparameters, $D$ are massively compressed statistics.
        \item Current work in this field treats this as a ``training'' problem, using neural density estimators $f$.
        \item Better to fit for proxy hyperparameters $\alpha$ using full Bayesian approach 
        \item Currently investigating this with mixture modelling+sparse reconstruction
        \item ``Compromise-free Likelihood-free inference'' (Handley \& Alsing)
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Things every nested sampling user should know}
        \begin{itemize}
            \item ``Burn in'' can take a while, and results are not informative until then.
            \item Reducing the stopping criterion does not appreciably change run-time, or get you much futher from the peak, but does reduce reliability.
            \item Run time is linear in the number of live points, so reduce this for exploratory runs $\sim\mathcal{O}(10)$, but increase to $\sim\mathcal{O}(1000)$ for production-ready runs.
            \item Most nested sampling algorithms are intensely parallelisable, and work best in pure MPI mode (no openMP).
        \end{itemize}
\end{frame}

%\begin{frame}
%    \frametitle{PolyChord 2.0}
%    \framesubtitle{Advances in nested sampling}
%\end{frame}

\begin{frame}
    \frametitle{Key software}
    \begin{description}
        \item[MultiNest] \url{github.com/farhanferoz/MultiNest}
        \item[PolyChord] \url{github.com/PolyChord/PolyChordLite}
        \item[DNest] \url{github.com/eggplantbren/DNest3}
        \item[dynesty] \url{github.com/joshspeagle/dynesty}
        \item[anesthetic] nested sampling visualisation \\ \url{github.com/williamjameshandley/anesthetic}
        \item[fgivenx] posterior plotting of functions \\ \url{github.com/williamjameshandley/fgivenx}
        \item[cosmology] Implemented as an alternative sampler in CosmoMC, MontePython, cosmosis, cobaya \& GAMBIT
    \end{description}
\end{frame}

\begin{frame}
    \frametitle{Summary}
    \begin{itemize}
        \item Nested sampling is a rich framework for performing the full pipeline of Bayesian inference
        \item Plenty of further work to do on the underlying theory
        \item Some understanding is required in order to operate \& get the most from nested sampling chains.
    \end{itemize}
\end{frame}

\include{include/further_reading}

\end{document}
```
# branch: remotes/origin/oxford_2020
```tex
\documentclass[%
    %handout
]{beamer}
\usepackage{graphicx} % For including single page pdfs
\usepackage{bm}       % bold math
\usepackage{pgffor}   % for loop
\usepackage{tikz}
\usepackage{multimedia}
\usepackage{layouts}
\usepackage{hyperref}
\usepackage{cambridge_lecture}

% todo 
% - Ligo actual data
% -define IMRPhenom, EOBNR


\newcommand{\lik}{\mathcal{L}}
\newcommand{\posterior}{\mathcal{P}}
\newcommand{\prior}{\pi}
\newcommand{\ev}{\mathcal{Z}}

\newcommand{\prob}{\mathrm{P}}

\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\text{knots}}
\newcommand{\nlive}{n_\text{live}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

% Title:
% Nested Sampling: an efficient and robust Bayesian inference tool for Machine
% Learning and Data Science
%
% Abstract:
% Nested sampling is an MCMC technique for integrating and exploring probability
% distributions. It has become widely adopted in the field of cosmology as a
% powerful tool for computing Bayesian evidences and sampling challenging
% a-priori unknown parameter spaces.
% 
% In this talk, I will give an introduction to the principles of Bayesian model
% comparison and parameter estimation, an explanation of the theory of nested
% sampling, a survey of the current state-of-the art (MultiNest, PolyChord,
% DNest and Dynesty) and the future of the field. I will illustrate with
% applications in CMB and 21cm Cosmology, Bayesian Sparse Reconstruction and
% Bayesian Neural Networks.

% Nested sampling: an efficient and robust Bayesian inference tool for
% astrophysics and cosmology.
% 
% Nested sampling is an alternative MCMC technique for integrating and exploring
% probability distributions. With publicly available implementations such as
% MultiNest, PolyChord and dynesty, it has become widely adopted in the field of
% cosmology and astrophysics as a powerful tool for computing Bayesian evidences
% and sampling challenging a-priori unknown parameter spaces.
% 
% In this talk I will give a user's guide to the theory of nested sampling in
% the context of Bayesian model comparison and parameter estimation,
% a survey of the current state of the art and the future of the field. I will
% illustrate with examples from cosmological and inflationary model comparison,
% primordial power spectrum reconstruction, 21cm cosmology, exoplanets, and
% tension quantification, as well as recent work in sparse reconstruction,
% Bayesian Neural Networks and likelihood-free inference.

% - primordial power spectrum reconstruction, 
% - 21cm cosmology, 
% - exoplanets
% - Bayesian Neural Networks 

% - sparse reconstruction,
% - likelihood-free inference.


%TODO:
% - Re-order final sections
% - Add extra examples
% - Add extra examples


\setbeamertemplate{navigation symbols}{} % Turn off that bottom bar


\title{Nested Sampling}
\subtitle{An efficient and robust Bayesian inference tool\\ for astrophysics and cosmology}
\author[Handley] % (optional, for multiple authors)
{Will Handley\\ \small{wh260@cam.ac.uk}}
\institute[University of Cambridge] % (optional)
{%
Kavli Institute for Cosmology \\
Astrophysics Group \\
Cavendish Laboratory \\
University of Cambridge
}
\date{January 28\textsuperscript{th}, 2020}

\include{include/beamer_commands}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\section{Fitting a line to data}
\begin{frame}
    \frametitle{Motivating example}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_points.pdf}
        \begin{itemize}
            \item We have noisy data $D$
            \item We wish to fit a model $M$
            \item Functional form $y=f_M(x;\theta)$
            \item For example:
                \begin{align}
                     f_\text{linear}(x;\theta)&=a x + b       \nonumber\\
                     f_\text{quadratic}(x;\theta)&=a x^2 + b  \nonumber
                \end{align}
            \item Model parameters $\theta= (a,b)$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ best-fit}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff.pdf}
        \begin{itemize}
            \item For each parameter set $\theta$:
                \[
                    \chi^2(\theta) = \sum_i \left|y_i - f(x_i;\theta)\right|^2
                \]
            \item Minimise $\chi^2$ wrt $\theta$
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{$\chi^2$ with non-uniform data errors}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data.pdf}
        \begin{itemize}
            \item If data have non-uniform errors:
                \[
                    \chi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|^2}{\sigma_i^2}
                \]
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Problems with $\chi^2$}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.4]{./figures/data_diff_2.pdf}
        \begin{itemize}
            \item How do we differentiate between models
            \item Why square the errors? -- could take absolute:
                \[
                    \psi^2(\theta) = \sum_i \frac{\left|y_i - f(x_i;\theta)\right|}{\sigma_i}
                \]
            \item Where does this approach even come from?
        \end{itemize}
    \end{figright}
\end{frame}


\begin{frame}
    \frametitle{Probability distributions}
    \framesubtitle{Fitting lines to data}
    \begin{figright}[0.6]{./figures/data_diff_1.pdf}
        \begin{itemize}
            \item The probability of observing a datum:
                \[
                    P(y_i | \theta,M) = \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right)
                \]
            \item The probability of observing the data:
                \begin{align}
                    P(D | \theta,M) &= \prod_i \frac{1}{\sqrt{2\pi}\sigma_i}\exp\left({-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}}\right) \nonumber\\
                    &=  \frac{1}{\prod_i\sqrt{2\pi}\sigma_i}\exp\sum_i{-\frac{|y_i-f(x_i;\theta)|^2}{2\sigma_i^2}} \nonumber\\
                    &\propto e^{-\chi^2(\theta)/2}
                    \nonumber
                \end{align}
        \end{itemize}
    \end{figright}
\end{frame}



\begin{frame}
    \frametitle{Maximum likelihood}
    \framesubtitle{Fitting lines to data}
    \begin{figleft}[0.6]{./figures/data_diff.pdf}
        \begin{itemize}
            \item Minimising $\chi^2(\theta)$  is equivalent to maximising $P(D|\theta,M) \propto e^{-\chi^2(\theta)/2}$
            \item $P(D|\theta,M)$ is called the Likelihood $L=L(\theta)$ of the parameters $\theta$
            \item ``Least squares'' $\equiv$ ``maximum likelihood'' \\(if data are gaussian).
        \end{itemize}
    \end{figleft}
\end{frame}

\begin{frame}
    \frametitle{Bayesian inference}
    \begin{itemize}
        \item Likelihood $L=P(D|\theta,M)$ is undeniably correct.
        \item Frequentists construct inference techniques purely from this function.
        \item The trend is cosmology is to work with a Bayesian approach.
        \item What we want are things like $P(\theta|D,M)$ and $P(M|D)$.
        \item To invert the conditionals, we need Bayes theorems:
            \begin{align}
                P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
                P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber
            \end{align}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Terminology}
    \framesubtitle{Bayesian inference}
    \begin{align}
        P(\theta|D,M) &= \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)} \nonumber\\
        \text{Posterior} &= \frac{\text{Likelihood}\times\text{Prior}}{\text{Evidence}} \nonumber
    \end{align}
    \begin{align}
        P(M|D) &= \frac{P(D|M) P(M)}{P(D)} \nonumber\\
        \text{Model probability} &= \frac{\text{Evidence}\times\text{Model Prior}}{\text{Normalisation}} \nonumber
    \end{align}
\end{frame}


\begin{frame}
    \frametitle{Multivariate probability}
    \begin{itemize}
        \item Marginalisation:
            \begin{equation*}
                P(x) = \int P(x,y) dy
            \end{equation*}
        \item Conditioning:
            \begin{equation*}
                P(y|x) = \frac{P(x,y)}{P(x)} = \frac{P(x,y)}{\int P(x,y) dy}
            \end{equation*}
        \item De-Conditioning:
            \begin{equation*}
                P(x|y) P(y) = P(x,y)
            \end{equation*}
        \item Bayes theorem:
            \begin{equation*}
                P(y|x) = \frac{P(x|y) P(y)}{P(x)}
            \end{equation*}
            \begin{center}
                ``To flip a conditional $P(x|y)$, you first de-condition on $y$,\\ and then re-condition on $x$.''
            \end{center}
    \end{itemize}
\end{frame}


%\begin{frame}
%    \frametitle{The prior}
%    \framesubtitle{Example: Biased coins}
%    \begin{itemize}
%        \item Need to define the \textbf{Prior} $P(\theta)$ --- probability of the bias, given no data
%        \item Represents our knowledge of parameters before the data -- subjective
%        \item Frequentists view this as a flaw in Bayesian inference. 
%        \item Bayesians view this as an advantage
%        \item Fundamental rule of Inference:\pause\\
%            \vfill
%            \begin{center}
%                \Large You cannot extract information from data\\ without making assumptions 
%            \end{center}
%            \vfill
%        \item All Bayesians do is make them explicit
%        \item Any method that claims it is ``objective'' is simply hiding them
%    \end{itemize}
%\end{frame}

\begin{frame}
    \frametitle{Parameter estimation}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.3]{./figures/parameters.pdf}
        \begin{itemize}
            \item We may use $P(\theta|D,M)$ to inspect whether a model looks reasonable
        \end{itemize}
        \includegraphics[width=\textwidth]{./figures/data.pdf}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Predictive posterior}
    \begin{figright}[0.4]{./figures/fgivenx.pdf}
        More useful to plot:
        \begin{align}
        &P(y|x) = \nonumber\\
        &\int P(y|x,\theta) P(\theta) d\theta \nonumber
        \end{align}
        (all conditioned on $D,M$)
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Model comparison}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.33]{./figures/evidences_log.pdf}
        \begin{itemize}
            \item We may use the Bayesian evidence $Z$ to determine whether a model is reasonable.
            \item $Z = P(D|M) = \int P(D|M,\theta)P(\theta|M)d\theta$
            \item Normally assume uniform model priors $Z \propto P(M|D)P(M)$.
        \end{itemize}
    \end{figright}
\end{frame}
\begin{frame}
    \frametitle{Model comparison}
    \framesubtitle{Bayesian inference}
    \begin{figright}[0.33]{./figures/evidences_lin.pdf}
        \begin{itemize}
            \item We may use the Bayesian evidence $Z$ to determine whether a model is reasonable.
            \item $Z = P(D|M) = \int P(D|M,\theta)P(\theta|M)d\theta$
            \item Normally assume uniform model priors $Z \propto P(M|D)P(M)$.
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{Line fitting (context)}
    \begin{figright}[0.5]{./figures/supernovae.pdf}
        \begin{itemize}
            \item Whilst this model seems a little trite\ldots
            \item\ldots determining polynomial indices \\$\equiv$ determining cosmological material content:
        \end{itemize}
    \end{figright}
        \[
            {\left( \frac{H}{H_0} \right)}^2 = 
            \Omega_\text{r} {\left( \frac{a_0}{a} \right)}^4+
            \Omega_\text{m} {\left( \frac{a_0}{a} \right)}^3+
            \Omega_k {\left( \frac{a_0}{a} \right)}^2+
            \Omega_\Lambda
            \]
\end{frame}



%\begin{frame}
%    \frametitle{Quantifying error with Probability}
%
%    \begin{itemize}
%        \item As scientists, we are used to seeing error bars on results.
%        \item Age of the universe ({\em Planck\/}): 
%         \[13.73\pm 0.12\:\text{billion years old.}\]
%        \item Masses of LIGO GW150914 binary merger: 
%        \[m_1 = 39.4^{+5.5}_{-4.9}\:M_\odot,\qquad m_2 = 30.9^{+4.8}_{-4.4}\:M_\odot \]
%        \item These are called {\em credible intervals}, state that we are e.g.\ $66\%$ confident of the value lying in this range.
%        \item More importantly, these are {\em summary statistics}.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{LIGO binary merger}
%    \begin{columns}
%        \begin{column}{0.65\textwidth}
%            \includegraphics[width=\textwidth]{./figures/ligo_m1_m2.pdf}
%        \end{column}
%        \begin{column}{0.35\textwidth}
%            \begin{itemize}
%                \item Summary statistics summarise a full probability distribution.
%                \item One goal of inference is to produce these probability distributions.
%            \end{itemize}
%        \end{column}
%    \end{columns}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Theory}
%    \framesubtitle{Extended example of inference: LIGO}
%    \includegraphics[width=\textwidth]{./figures/ligo_schematic.png}
%\end{frame}
%
%\begin{frame}
%    \frametitle{The parameters $\Theta$ of the model $M$}
%    \framesubtitle{Extended example of inference: LIGO}
%    Theoretical signal depends on:
%    \begin{itemize}
%        \item $m_1, m_2$: mass of binary
%        \item $\theta, \phi$: sky location
%        \item $r$: luminosity distance 
%        \item $\Phi_c, t_c$: phase and time of coalescence
%        \item $i, \theta_\text{sky}$: inclination and angle on sky (orbital parameters)
%    \end{itemize}
%\end{frame}
%
%
%
%\begin{frame}
%    \frametitle{Posterior $\mathcal{P}$}
%    \framesubtitle{Extended example of inference: LIGO}
%    \begin{itemize}
%        \item Cannot plot the full posterior distribution:
%            \[\mathcal{P}(\Theta) \equiv P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\text{sky}|D,M)\]
%        \item Can plot 1D and 2D {\em marginalised\/} distributions e.g:
%            \begin{align}
%            &P(m_1,m_2|D,M)=\nonumber\\&\int P(m_1,m_2,\theta,\phi,r,\Phi_c, t_c, i, \theta_\text{sky}|D,M) \,d\theta \,d\phi \,dr \,d\Phi_c \,d t_c \,d i \,d\theta_\text{sky}\nonumber
%            \end{align}
%        \item May do this for each pair of parameters
%        \item Generates a {\em triangle plot}
%    \end{itemize}
%\end{frame}




\begin{frame}
    \frametitle{Sampling}
    \framesubtitle{How to describe a high-dimensional posterior}

	\begin{figright}{./figures/ligo_m1_m2.pdf}
		\begin{itemize}
          \item In high dimensions, posterior $\posterior$ occupies a vanishingly small region of the prior $\prior$.
          \item Gridding is doomed to failure for $D\gtrsim4$.
          \item {\em Sampling\/} the posterior is an excellent compression scheme.
		\end{itemize}
	\end{figright}
 
\end{frame}

\begin{frame}
    \frametitle{Sampling}
    \framesubtitle{How to describe a high-dimensional posterior}
    \centerline{%
        \includegraphics[height=0.65\textwidth]{./figures/ligo_full.pdf}
    }
\end{frame}
%
%\begin{frame}
%    \frametitle{Why do sampling?}
%    \framesubtitle{Marginalisation over the posterior}
%
%    \begin{itemize}
%        \item Set of $N$ samples $S = \{\Theta^{(i)}: i=1,\ldots N:\: \Theta^{(i)}\sim\mathcal{P}\}$
%        \item Mean mass: \[
%                \bar{m}_1 \equiv\langle m_1\rangle_\mathcal{P}
%                \only<1>{\equiv \int m_1 P(\theta|D,M) d\theta }
%                \only<2>{\approx \frac{1}{N}\sum_{i=1}^N m_1^{(i)}}
%                \only<3>{\approx \frac{\sum_{i=1}^N w^{(i)} m_1^{(i)}}{\sum_{i=1}^N w^{(i)}}}
%            \]
%        \item Mass covariance: \[
%                \mathrm{Cov}(m_1,m_2)
%            \only<1>{\equiv \int (m_1-\bar{m}_1)(m_2-\bar{m}_2) P(\theta|D,M) d\theta }
%                \only<2>{\approx \frac{1}{N}\sum_{i=1}^N (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}
%                \only<3>{\approx \frac{\sum_{i=1}^N w^{(i)} (m_1^{(i)}-\bar{m}_1)(m_2^{(i)}-\bar{m}_2)}{\sum_{i=1}^N w^{(i)}}}
%            \]
%        \item Marginalised samples: Just ignore the other coordinates.
%        \item N.B. Typically have {\em weighted\/} samples
%    \end{itemize}
%\end{frame}

\begin{frame}
    \frametitle{Cosmology in high dimensions}

    \[\lik(\Theta) = P(D|\Theta,M)\]
    \begin{align}
        \onslide<2->{D =& \{C_\ell\only<6->{^\text{(Planck)}}\}} 
        \onslide<15->{+\{\text{LSS}\}} 
        \onslide<16->{+\{\text{``Big Data''}\}}
        \nonumber\\
        \onslide<3->{M =& \Lambda\text{CDM}} 
        \onslide<9->{+ \text{extensions} }
        \nonumber\\
        \onslide<4->{\Theta =& \Theta_{\Lambda \text{CDM}}} \onslide<7->{+ \Theta_\text{Planck}} \onslide<10->{+ \Theta_\text{extensions}}\nonumber\\
        \onslide<5->{\Theta_{\Lambda \text{CDM}} =& ( \Omega_b h^2, \Omega_c h^2, 100\theta_{MC}, \tau, {\rm{ln}}(10^{10} A_s), n_s) \nonumber\\}
        \onslide<8->{\Theta_\text{Planck} =& (y_{\rm cal}, A^{CIB}_{217}, \xi^{tSZ-CIB}, A^{tSZ}_{143}, A^{PS}_{100}, A^{PS}_{143}, A^{PS}_{143\times217}, A^{PS}_{217}, A^{kSZ}, \nonumber\\& A^{{\rm dust}TT}_{100}, A^{{\rm dust}TT}_{143}, A^{{\rm dust}TT}_{143\times217}, A^{{\rm dust}TT}_{217}, A^{{\rm dust}TE}_{100}, A^{{\rm dust}TE}_{100\times143}, \nonumber\\& A^{{\rm dust}TE}_{100\times217}, A^{{\rm dust}TE}_{143}, A^{{\rm dust}TE}_{143\times217}, A^{{\rm dust}TE}_{217}, c_{100}, c_{217}) \nonumber\\}
        \onslide<11->{\Theta_\text{extensions} =& (
                n_{\rm run}
                \only<12->{,n_{\rm run,run}}
                \only<13->{,w}
                \only<14->{,\Sigma m_\nu, m_{\nu,{\rm{sterile}}}^{\rm{eff}}}
        ) \nonumber}
    \end{align}

    \begin{itemize}
        \item<17->{Parameter estimation: $L, \pi \to \mathcal{P}$: model parameters}
        \item<17->{Model comparison: $L, \pi \to Z$: how good model is}
    \end{itemize}

\end{frame}



%
\begin{frame}
    \frametitle{Parameter estimation}
    \begin{itemize}
        \item The name of the game is therefore drawing samples $S$ from the posterior $\mathcal{P}$ with the minimum number of likelihood calls.
        \item Gridding is doomed to failure in high dimensions.
        \item Enter Metropolis Hastings.
    \end{itemize}
\end{frame}



\section{Metropolis Hastings}


\begin{frame}
  \frametitle{Metropolis Hastings} 
  \begin{itemize}
      
    \item Turn the $N$-dimensional problem into a one-dimensional one.
      \begin{enumerate}
        \item Propose random step
        \item If uphill, make step\ldots
          
        \item \ldots otherwise sometimes make step. 
      \end{enumerate}
    \item \url{chi-feng.github.io/mcmc-demo/}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Metropolis Hastings} 
  \framesubtitle{Struggles with\ldots}
  \pause
  \begin{enumerate}
      \item Burn in
      \item Multimodality
      \item Correlated Peaks
      \item Phase transitions
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Hamiltonian Monte-Carlo} 
  \begin{itemize}
      \item Key idea: Treat $\log L(\Theta)$ as a potential energy
      \item Guide walker under ``force'': \[F(\Theta) =\nabla \log L(\Theta)\]
      \item Walker is naturally ``guided'' uphill
      \item Conserved quantities mean efficient acceptance ratios.
      \item Mass matrix for kinetic term is a hidden tuning element.
      \item stan is a fully fledged, rapidly developing programming language with HMC as a default sampler.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Ensemble sampling} 
  \begin{itemize}
      \item Instead of one walker, evolve a set of $n$ walkers.
      \item Can use information present in ensemble to guide proposals.
      \item emcee: affine invariant proposals.
      \item emcee is not the only (or even best) affine invariant approach.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The fundamental issue with all of the above} 

  \begin{itemize}
    \item They don't give you evidences!
  \begin{align}
    \ev 
    &= \prob(D|M) 
    \nonumber\\
    &= \int\prob(D|\Theta,M)\prob(\Theta|M) d\Theta 
    \nonumber\\
    &= \left\langle \lik \right\rangle_\prior
    \nonumber
  \end{align}
    \item MCMC fundamentally explores the posterior, and cannot average over the prior.
    \item Thermodynamic annealing 
    \begin{itemize}
        \item Suffers from same tuning issues as MCMC
    \end{itemize}
    \item Nearest neighbor volume estimation (Heavens arXiv:1704.03472)
    \begin{itemize}
        \item Does not scale to high dimensions $D\gtrsim10$.
    \end{itemize}
  \end{itemize}
 
\end{frame}

\section{Nested Sampling}

\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{John Skilling's alternative to traditional MCMC!} 

  \begin{itemize}
    \item Nested sampling is a completely different way of sampling. 
    \item Uses ensemble sampling to compress prior to posterior.
  \end{itemize}
  
  New procedure: 

  
  Maintain a set $S$ of $n$ samples, which are sequentially updated:

  \begin{description}
      
    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\prior$). 
      
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new uniform sample with higher likelihood
  \end{description}

  
  Requires one to be able to uniformly within a region, subject to a {\em hard likelihood constraint}.

\end{frame}



\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Graphical aid}
\foreach \pagenum in {1,...,38} {%
  \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/nested_sampling}
}
\end{frame}

\begin{frame}
  \frametitle{Nested sampling} 

  \begin{itemize}
    \item The set of dead points are posterior samples with an appropriate weighting factor
    \item They can also be used to calculate evidences, since it sequentially updates the priors.
    \item The current set of live points is useful for performing clustering and constructing new proposed points.
    \item Algorithm terminates when prior has been compressed onto (and past) the posterior bulk (typical set).
  \end{itemize}
 
\end{frame}

\begin{frame}
  \frametitle{Nested Sampling}
  \framesubtitle{Calculating evidences}
  \foreach \pagenum in {1,...,16} {%
      \includegraphics<\pagenum>[width=\textwidth,page=\pagenum]{figures/lesbesgue}
  }
\end{frame}

\begin{frame}
  \frametitle{Nested Sampling} 
  \framesubtitle{Exponential volume contraction} 
  
  \begin{itemize}
    \item At each iteration, the likelihood contour will shrink in volume by  $\approx 1/n$.
    \item Nested sampling zooms in to the peak of the posterior {\em exponentially}.
      \begin{equation}
          \ev \approx \sum_i \Delta\lik_i X_{i}, \qquad
        X_{i+1} \approx \frac{n}{n+1}X_i, \qquad X_{0} = 1 \nonumber
      \end{equation}
    \item Although this is only approximate, we can quantify the error \\
      \begin{equation}
          P(X_{i+1}|X_i) = \frac{1}{n X_i}{\left( \frac{X_{i+1}}{X_i} \right)}^{n-1}[0<X_i<X_{i+1}]
          \nonumber
      \end{equation}
    \item Integral can be expressed in one of two ways
      \begin{equation}
          \ev \approx \sum_i \Delta\lik_i X_{i} = \sum_i \lik_i \Delta X_{i} \nonumber
      \end{equation}

  \end{itemize}
  
\end{frame}



\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 

  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item Most of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm.
    \item \url{https://projecteuclid.org/euclid.ba/1340370944}
  \end{itemize}
 
\end{frame}

\begin{frame}
\frametitle{MultiNest}
    \framesubtitle{arXiv:0809.3437 arXiv:0704.3704 arXiv:1306.2144, Feroz, Hobson}
  \includegraphics[width=\textwidth]{figures/multinest.pdf}
\end{frame}

\begin{frame}
  \frametitle{PolyChord}
  \framesubtitle{arXiv:1502.01856 arXiv:1506.00171, Handley, Hobson, Lasenby}
  \includegraphics[width=\textwidth]{figures/polychord.png}
\end{frame}

\begin{frame}
  \frametitle{NeuralNest}
  \framesubtitle{arXiv:1903.10860, Moss}
    \begin{columns}
        \begin{column}{0.55\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
        \end{column}
        \begin{column}{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Diffusive nested sampling}
  \framesubtitle{arXiv:0912.2380, Brewer}
  \includegraphics[width=\textwidth]{figures/dnest.pdf}
\end{frame}

\begin{frame}
\frametitle{PolyChord vs MultiNest}
\begin{itemize}
    \item MultiNest excels in low dimensions $D<10-20$.
    \item PolyChord can go up to $\sim 150$.
    \item Crossover is problem dependent
    \item PolyChord can also exploit fast-slow hierarchy
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Exoplanets}
  \framesubtitle{Nested sampling in action (arXiv:1806.00518, Hall, Walker-Smith, Handley, Queloz)}
  \includegraphics[width=\textwidth]{figures/rv_full.pdf}
\end{frame}

\begin{frame}
  \frametitle{Exoplanets}
  \framesubtitle{Nested sampling in action}
  \begin{itemize}
      \item Simple radial velocity model
          \begin{equation}
              \nu(t;\theta) = \sum_{p=1}^N K_p \sin(\omega_p t + \phi_p)\nonumber
          \end{equation}
      \item Fit each model to data.
      \item Posteriors on model parameters $[(K_p,\omega_p,\phi_p),p=1\cdots N]$ quantify knowledge of system characteristics.
      \item Evidences of models determine relative likelihood of number of planets in system
      \item This is an application where phase transitions matter
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{21cm cosmology}
  \framesubtitle{Nested sampling in action (Paper coming soon, Anstey, de Lera Acedo \& Handley)}
    \begin{figright}[0.3]{./figures/21cm.pdf}
        \begin{itemize}
            \item Search for signal  $T=T_\mathrm{fg} + T_\mathrm{21cm}$ 
            \item Fit parameterised models with/without $T_\mathrm{21cm}$
            \item Compare evidences for signal detection
            \item Use evidences to quantify complexity of beam/sky models
        \end{itemize}
    \end{figright}
\end{frame}


\begin{frame}
  \frametitle{Primordial power spectrum $\PR(k)$ reconstruction}
  \framesubtitle{Nested Sampling in action (arXiv:1908.00906)}


  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{4}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{2}
    % start coordinate
      \def\xstart{2}
      \def\ystart{3}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{1}
    % end coordinate
      \def\xend{5.5}
      \def\yend{3}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{1.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2->{\movablevert{mn}};
      \onslide<2->{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3->{\movablecross{start}};
      \draw<3-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4->{\movablecross{mid}};
      \draw<4-> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5-> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5->{\movablecross{end}};
      \draw<5-> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }

\end{frame}
%
%
%%\begin{frame}
%%  \frametitle{Planck data}
%%  \framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}
%%  \begin{itemize}
%%    \item<2-> Temperature data TT+lowP
%%    \item<3-> Foreground $(14)$ \& cosmological $(4 +2*\Nknots-2)$  parameters
%%    \item<4-> Marginalised plots of $\PR(k)$
%%    \item<5->
%%      \[ \prob(\PR|k,\Nknots) = \int \delta(\PR-f(k;\theta))\posterior(\theta)d\theta \]
%%  \end{itemize}
%%\end{frame}
%
%
%
\begin{frame}
  \frametitle<1>{0 internal knots}
  \frametitle<2>{1 internal knot}
  \frametitle<3>{2 internal knots}
  \frametitle<4>{3 internal knots}
  \frametitle<5>{4 internal knots}
  \frametitle<6>{5 internal knots}
  \frametitle<7>{6 internal knots}
  \frametitle<8>{7 internal knots}
  \frametitle<9>{Bayes Factors}
  \frametitle<10>{Marginalised plot}
  %\framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}



  \begin{center}
    \includegraphics<1>[width=0.7\textwidth]{figures/pps_both_1}
    \includegraphics<2>[width=0.7\textwidth]{figures/pps_both_2}
    \includegraphics<3>[width=0.7\textwidth]{figures/pps_both_3}
    \includegraphics<4>[width=0.7\textwidth]{figures/pps_both_4}
    \includegraphics<5>[width=0.7\textwidth]{figures/pps_both_5}
    \includegraphics<6>[width=0.7\textwidth]{figures/pps_both_6}
    \includegraphics<7>[width=0.7\textwidth]{figures/pps_both_7}
    \includegraphics<8>[width=0.7\textwidth]{figures/pps_both_8}
    \includegraphics<9>[width=0.7\textwidth]{figures/pps_evidence}
    \includegraphics<10>[width=0.7\textwidth]{figures/pps_both}

  \end{center}
\end{frame}
\begin{frame}
  \frametitle<1>{COBE (pre-2002)}
  \frametitle<2>{COBE et al (2002)}
  \frametitle<3>{WMAP (2012)}
  \frametitle<4>{Planck (2013)}
  \frametitle<5>{Planck (2015)}
  \frametitle<6>{Planck (2018)}


  \begin{center}
    \includegraphics<1>[width=0.7\textwidth]{figures/cobe}
    \includegraphics<2>[width=0.7\textwidth]{figures/pre_WMAP}
    \includegraphics<3>[width=0.7\textwidth]{figures/WMAP}
    \includegraphics<4>[width=0.7\textwidth]{figures/planck_2013}
    \includegraphics<5>[width=0.7\textwidth]{figures/planck_2015}
    \includegraphics<6>[width=0.7\textwidth]{figures/pps}

  \end{center}
\end{frame}

\begin{frame}
    \frametitle{Bayesian neural networks}
    \framesubtitle{Sparse reconstruction (arXiv:1809.04598)}

    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{itemize}
                \item Neural networks require:
                    \begin{itemize}
                        \item Training to find weights
                        \item Choice of architecture/topology
                    \end{itemize}
                \item Bayesian NNs treat training as a model fitting problem
                \item Compute posterior of weights (parameter estimation)
                \item Use evidence to determine best architecture (model comparison)
                \item ``Compromise-free Bayesian NNs''\\ (Javid, Handley, Lasenby \& Hobson)
                    \begin{itemize}
                        \item Bayesian evidences correlate with out-of-sample performance
                        \item Can be used to determine width and number of hidden layers
                    \end{itemize}
            \end{itemize}
        \end{column}
        \begin{column}{0.4\textwidth}
            \includegraphics[width=\textwidth]{./figures/Colored_neural_network.pdf}
            \includegraphics[width=\textwidth]{./figures/non_comb_mean_results_colour.pdf}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Other uses of nested sampling}
    \begin{columns}
        \begin{column}{0.55\textwidth}
        \begin{itemize}
        \item Nested sampling estimates the density of states $\Delta X_i$, and hence gives you access to a lot more than just posterior samples and evidences
        \item Kullback-Liebler divergence (arXiv:1607.00270)
        \item Bayesian model dimensionality (arXiv:1903.06682)
        \item Suspiciousness \& Tension quantification (arXiv:1902.04029)
        \item DES tension: $\sim2.3\sigma$
        \end{itemize}
        \end{column}
        \begin{column}{0.45\textwidth}
            \includegraphics[width=\textwidth]{./figures/H0.pdf}
            \includegraphics[width=\textwidth]{./figures/s8.pdf}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Curvature tension?}
    \begin{columns}
        \begin{column}{0.4\textwidth}
        \begin{itemize}
            \item Under the same measures of tension, CMB lensing is $2.5\sigma$ in tension with CMB TT,TE,EE
            \item Neglecting CMB lensing gives moderate preference for curvature
            \item Planck phrase these issues in terms of $A_L$
        \end{itemize}
        \end{column}
        \begin{column}{0.6\textwidth}
            \includegraphics[width=\textwidth]{./figures/omegak_H0_2.pdf}
            \includegraphics[width=\textwidth]{./figures/evidences.pdf}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Likelihood free inference}
    \begin{itemize}
        \item How can we apply Bayesian inference if we don't know the likelihood, but can simulate the system?
        \item Learn approximation of likelihood from simulations by fitting $f(\theta;\alpha)\approx L(\theta) =P(D|\theta)$, $\alpha$ are hyperparameters, $D$ are massively compressed statistics.
        \item Current work in this field treats this as a ``training'' problem, using neural density estimators $f$.
        \item Better to fit for proxy hyperparameters $\alpha$ using full Bayesian approach 
        \item Currently investigating this with mixture modelling+sparse reconstruction
        \item ``Compromise-free Likelihood-free inference'' (Handley \& Alsing)
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Unweaving runs}
    \framesubtitle{Advances in nested sampling}
    \begin{itemize}
        \item John Skilling noted that two nested sampling runs can be combined in likelihood order to produce a valid run with a larger number of live points.
        \item The reverse is also true (arXiv:1704.03459).
        \item In general, a run with $n$ live points can be ``unweaved'' into $n$ runs with a single live point.
        \item Useful for providing convergence diagnostics and better parameter estimation (arXiv:1804.06406).
    \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Dynamic nested sampling}
  \framesubtitle{Advances in nested sampling (arXiv:1704.03459,  dynesty: arXiv:1904.02180)}
  \includegraphics[width=\textwidth]{figures/dynesty.pdf}
  The number of live points can be varied dynamically in order to oversample regions of interest
\end{frame}


\begin{frame}
    \frametitle{Multi-temperature sampling}
    \begin{itemize}
        \item By compressing from prior to posterior, Nested Sampling's weighted samples are fundamentally different from traditional MCMC.
        \item Nested sampling tails and peaks equally.
        \item We can define the ``temperature'' of a distribution in analogy with thermodnyamics:
            \begin{equation}
                \log L \sim E \Rightarrow P \propto e^{-\beta E} = e^{-E/kT},\quad \beta = 1\nonumber
            \end{equation}
        \item Sampling at different temperatures can be useful for exploring tails.
        \item Nested sampling runs give you the full partition function 
            \begin{equation}
                \log Z(\beta) \approx \sum_i \lik_i^{\beta} \Delta X_{i} \nonumber
            \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Nested importance sampling}
    \framesubtitle{Future research}
    \begin{itemize}
        \item Much of the time spent in a nested sampling run is spent ``compressing the tails''.
        \item Posterior-repartitioned nested sampling gives one way of speeding this up (arXiv:1908.04655)
        \item Sometimes we have a-priori good knowledge of the posterior bulk (analagous to an MCMC proposal distribution).
        \begin{align}
            Z_0 &= \int L(\theta) \pi_0(\theta) d\theta, \qquad
            Z_1 = \int L(\theta) \pi_1(\theta) d\theta \nonumber\\
            &= \int L(\theta)\pi_1(\theta) \frac{\pi_0(\theta)}{\pi_1(\theta)} d\theta
            = \left\langle \frac{\pi_0(\theta)}{\pi_1(\theta)} \right\rangle_{P_1}  \nonumber
        \end{align}
        \item This importance weighting only works if you have a lot of tail samples.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$N$-$\sigma$ contours}
    \framesubtitle{Future research}
    \begin{itemize}
        \item Traditional posterior samples only allow you to plot contours out to 2-3$\sigma$.
        \item Nested sampling fully samples the tails, so in theory one could do $20\sigma$ contours.
        \item Requires further thought in alternatives to kernel density estimation.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Things every nested sampling user should know}
        \begin{itemize}
            \item ``Burn in'' can take a while, and results are not informative until then.
            \item Reducing the stopping criterion does not appreciably change run-time, but does reduce reliability.
            \item Run time is linear in the number of live points, so reduce this for exploratory runs $\sim\mathcal{O}(10)$, but increase to $\sim\mathcal{O}(1000)$ for production-ready runs.
            \item Most nested sampling algorithms are intensely parallelisable, and work best in pure MPI mode (no openMP).
        \end{itemize}
\end{frame}

%\begin{frame}
%    \frametitle{PolyChord 2.0}
%    \framesubtitle{Advances in nested sampling}
%\end{frame}

\begin{frame}
    \frametitle{Key software}
    \begin{description}
        \item[MultiNest] \url{github.com/farhanferoz/MultiNest}
        \item[PolyChord] \url{github.com/PolyChord/PolyChordLite}
        \item[DNest] \url{github.com/eggplantbren/DNest3}
        \item[dynesty] \url{github.com/joshspeagle/dynesty}
        \item[anesthetic] nested sampling visualisation \\ \url{github.com/williamjameshandley/anesthetic}
        \item[fgivenx] posterior plotting of functions \\ \url{github.com/williamjameshandley/fgivenx}
        \item[cosmology] Implemented as an alternative sampler in CosmoMC, MontePython, cosmosis, cobaya \& GAMBIT
    \end{description}
\end{frame}

\begin{frame}
    \frametitle{Summary}
    \begin{itemize}
        \item Nested sampling is a rich framework for performing the full pipeline of Bayesian inference
        \item Plenty of further work to do on the underlying theory
        \item Some understanding is required in order to operate \& get the most from nested sampling chains.
    \end{itemize}
\end{frame}

\include{include/further_reading}

\end{document}
```
# branch: remotes/origin/21cm_2020_workshop_cambridge
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Nested sampling}
\subtitle{An efficient and robust Bayesian inference tool for 21cm cosmology}
\date{21\textsuperscript{st} October 2020}

% Abstract
% --------
% Nested sampling: an efficient and robust Bayesian inference tool for 21cm
% cosmology
% 
% Nested sampling is an alternative Markov-chain Monte-Carlo technique for
% integrating and exploring probability distributions. With publicly available
% implementations such as MultiNest, PolyChord and dynesty, nested sampling has
% become widely adopted in astronomy as a powerful tool for computing Bayesian
% evidences and sampling challenging a-priori unknown parameter spaces.
% 
% In this talk I will give a user's guide to the theory of nested sampling in
% the context of 21cm Bayesian model comparison and parameter estimation, a
% survey of the current state of the art and the future of the field. This will
% aim to provide theoretical background for the Bayesian inference techniques
% and algorithms underpinning the science of several other talks which focus on
% applying nested sampling to 21cm cosmology.

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Bayesian inference in global 21cm cosmology}
    \begin{itemize}
        \item Data analysis problem
            \begin{align}
                \text{Data} &= \text{Signal} + \text{Foreground} + \text{Noise} \nonumber\\
                T_\text{data}(\nu) &= T_\text{21cm}(\nu;\theta_\text{21cm}) + T_\text{fg}(\nu;\theta_\text{fg}) + T_\text{noise}(\nu)
                \nonumber
            \end{align}
        \item We  can only statistically describe $T_\text{noise}$ 
            \begin{itemize}
                \item e.g. as an (un)correlated Gaussian random variable $P(T_\text{noise}) = \frac{1}{\sqrt{2\pi}\sigma} e^{-T_\text{noise}^2/2\sigma^2}$
            \end{itemize}
        \item Allows us to form a {\em likelihood} for the unknown parameters $\theta = (\theta_\text{21cm},\theta_\text{fg})$
            \begin{align}
                \mathcal{L}(\theta) &= P(\text{Data}|\theta) \propto e^{-\frac{1}{2}\chi(\theta)^2} \nonumber\\
                \chi^2(\theta) &= \sum_\nu\frac{1}{\sigma_\nu^2}{\left[T_\text{data}(\nu)-T_\text{21cm}(\nu;\theta_\text{21cm}) - T_\text{fg}(\nu;\theta_\text{fg})\right]}^2
                \nonumber
            \end{align}
        \item This misses the initial step of data compression (reduction, flagging, integration)
        \item In practice also include $\theta_\text{noise}$ as parameters as well.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The three pillars of Bayesian inference}
    \begin{itemize}
        \item Frequentist approaches maximise the likelihood $\mathcal{L}(\theta) = P(D|\theta)$
            \begin{itemize}
                \item anything using a ``least squares'' optimisation (e.g. \texttt{maxsmooth}, EDGES calibration)
            \end{itemize}
        \item Bayes theorem allows us to answer science questions using probabilities:
    \end{itemize}
    \begin{description}
        \item[Parameter estimation] Given a model, which range of parameters best describe the data?
            \[
                P(\theta|D,M) = \frac{P(D|\theta,M) P(\theta|M)}{P(D|M)}, \qquad
                \text{Posterior} = \frac{\text{Likelihood} \times \text{Prior}}{\text{Evidence}}
            \]
        \item[Model comparison] Which models do the data prefer?
            \[
                P(M|D) = \frac{P(D|M) P(M)}{P(D)}, \qquad
                \text{Model Probability} = \frac{\text{Evidence} \times \text{Model Prior}}{\text{Normalisation}}
            \]
        \item[Tension quantification] Are different (sub)sets of data consistent with one another? %\arxiv{1902.04029}\arxiv{1903.06682}
    \end{description}
\end{frame}

\begin{frame}
    \frametitle{Model comparison in 21cm cosmology}
    \begin{itemize}
        \item Model comparison: Given this data, what odds would a bookmaker put on a model?
        \item In general in 21cm cosmology model takes the form $T_\text{data} = T_\text{21cm} + T_\text{fg} + T_\text{noise}$ 
        \item Model comparison can be applied to any combination of these pieces
    \end{itemize}
    \begin{enumerate}
        \item Does the data contain a signal? \\(Does $T_\text{data} = T_\text{21cm} + T_\text{fg} + T_\text{noise}$ have a higher evidence than $T_\text{data} = T_\text{fg} + T_\text{noise}$)
        \item Which foreground model is better? (polynomial vs forward sky model)
        \item Which noise model is better? (correlated gaussian, uncorrelated gaussian, lorentzian)
        \item How many components should a foreground model have?
        \item How many components should a calibration model have?
    \end{enumerate}
\end{frame}


\begin{frame}
    \frametitle{Numerical tools}
    \cols{
        \begin{itemize}
            \item The key concept in numerical Bayesian inference is \emph{sampling} a distribution 
            \item i.e. drawing parameter vectors $\theta$ in proportion to the probability density $P(\theta)$.
            \item Compression of high-dimensional function.
            \item Posterior $P(\theta|D)$ encodes multidimensional generalisation of error bars
            \item Sampling is traditionally accomplished using random-walk MCMC tools like Gibbs sampling, Metropolis-Hastings, emcee or Hamiltonian Monte Carlo.
        \end{itemize}
    }{
        \includegraphics<1>[width=\textwidth]{figures/params.pdf}
        \includegraphics<2|handout:0>[width=\textwidth]{figures/reach.pdf}
        \includegraphics<3|handout:0>[width=\textwidth]{figures/params_full.pdf}
    }
    
\end{frame}

\begin{frame}
    \frametitle{Nested sampling in action}
    \cols[0.6]{
  \begin{itemize}
    \item Nested sampling is a completely different way of sampling. 
    \item Uses ensemble sampling to compress prior to posterior.
      \item Maintain a set $S$ of $n$ samples, which are sequentially updated:
  \begin{description}
      
    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\pi$). 
      
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new uniform sample with higher likelihood
  \end{description}
 \item Requires one to be able to sample uniformly within a region, subject to a {\em hard likelihood constraint}.
  \end{itemize}
    }{
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}
        \includegraphics<4          >[width=\textwidth,page=4]{figures/himmelblau}
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}
        \includegraphics<8|handout:0>[width=\textwidth,page=8]{figures/himmelblau}
        \includegraphics<9|handout:0>[width=\textwidth,page=14]{figures/himmelblau}
    }
\end{frame}


\begin{frame}
    \frametitle{Nested sampling in action}
    \cols[0.6]{
  \begin{itemize}
      \item At the end, one is left with a set of discarded points
      \item These may be weighted to form posterior samples
      \item They can also be used to calculate the evidence
      \item The evolving ensemble of live points allows algorithms to perform self-tuning and mode clustering
  \end{itemize}

    }{
        \includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}
        \includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}
    }

\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \cols[0.5]{%
        \texttt{MultiNest}
        \includegraphics[width=0.8\textwidth]{figures/multinest}
            \vfill
        \texttt{DNest}
        \includegraphics[width=\textwidth]{figures/dnest}
    }{%
        \texttt{PolyChord}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}
            \cols[0.5]{
                \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
                \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            }{
                \includegraphics[width=\textwidth]{figures/chains.png}
            }
            \vfill
    }
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling: Benefits and drawbacks}
    Relative to traditional numerical posterior samples (Metropolis Hastings, HMC, emcee), nested sampling:
    \begin{description}
        \item[$+$] Can calculate evidence (and therefore perform model comparison).
        \item[$+$] Can handle multi-modal distributions.
        \item[$+$] Requires little tuning for an a-priori unseen problem.
        \item[$+$] Highly parallelisable ($n_\mathrm{cores} \sim n_\mathrm{live} \gg 4$).
        \item[$-$] Slower than a well-tuned posterior sampler.
        \item[$-$] Run time is dependent on prior choice, and priors must be proper \\(some people view this as a feature rather than a bug).
    \end{description}
\end{frame}


\begin{frame}
    \frametitle{Nested Sampling: a user's guide}
    \begin{enumerate}
        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
            \begin{itemize}
                \item This means typically most of its time is spent on burn-in rather than posterior sampling
                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable
            \end{itemize}
        \item The number of live points $n_\mathrm{live}$ is a resolution parameter.
            \begin{itemize}
                \item Run time is linear in $n_\mathrm{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\mathrm{live}}}$.
                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
            \end{itemize}
        \item Most algorithms come with additional reliability parameter(s).
            \begin{itemize}
                \item e.g. \texttt{MultiNest}: $\text{eff}$, \texttt{PolyChord}: $n_\mathrm{repeats}$
                \item These are parameters which have no gain if set too conservatively, but increase the reliability
                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
            \end{itemize}
    \end{enumerate}
\end{frame}


\begin{frame}
    \frametitle{Key tools}
    \begin{description}
        \item[\texttt{anesthetic}] Nested sampling post processing \arxiv{1905.04768}\\
        \item[\texttt{insertion}] cross-checks using order statistics \arxiv{2006.03371}
            \hspace{5pt}\url{github.com/williamjameshandley/anesthetic}
        \item[\texttt{nestcheck}] cross-checks using unthreaded runs \arxiv{1804.06406}\\
            \hspace{5pt}\url{github.com/ejhigson/nestcheck}
        \item[\texttt{MultiNest}] Ellipsoidal rejection sampling \arxiv{0809.3437}\\
            \hspace{5pt}\url{github.com/farhanferoz/MultiNest}
        \item[\texttt{PolyChord}] Python/C++/Fortran state of the art \arxiv{1506.00171}\\
            \hspace{5pt}\url{github.com/PolyChord/PolyChordLite} 
        \item[\texttt{dynesty}] Python re-implementation of several codes \arxiv{1904.02180}\\
            \hspace{5pt}\url{github.com/joshspeagle/dynesty}
    \end{description}
\end{frame}

\end{document}
```
# branch: remotes/origin/tehran_2021
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley}
\usepackage{layouts}
\newcommand{\mean}[2][]{\left\langle#2\right\rangle_{#1}}
%\usepackage{calc}  


% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Bayesian methods for quantifying global parameter tensions between cosmological datasets}
%\subtitle{<+subtitle+>}
\date{24\textsuperscript{th} February 2021}

% Abstract
% --------
% The discrepancy between inferences and measurements of the Hubble constant
% arising from early and late-time datasets is hard to miss. In other contexts
% however, parameter tensions are more challenging to visualise and quantify.
% Topical examples include:
% 
% (a) tensions between cosmic microwave background and weak lensing data, since
% the tension arises in a non-trivial combination of parameters including
% $\sigma_8$ and $\Omega_m$
% 
% (b) tensions arising from or resolved by extensions to $\Lambda$CDM such as
% variable dark energy and/or spatial curvature which have non-linear
% degeneracies.
% 
% In these cases the parameter tensions may be hidden or over-emphasised when
% examining marginalised plots derived from a much higher-dimensional parameter
% space (such as those produced by getdist or corner). More advanced 'global'
% metrics are required to determine whether or not two experiments are in
% tension, or whether a proposed solution actually resolves the discrepancy.
% 
% This talk will highlight the recent statistical and computational advances in
% the theory of quantifying parameter tensions using a Bayesian and
% information-theoretic approach, with a focus on the Suspiciousness statistic.
% 
% These techniques will be demonstrated with applications to modern datasets
% such as Planck, ACT, SPT, DES, KiDS and SH0ES, and the current computational
% state-of-the-art toolkits will be indicated. The headline result is a
% technique and interpretation for computing the Suspiciousness statistic from
% MCMC chains alone, without the requirement of an explicit Bayesian evidence
% computation.
% 
% https://arxiv.org/abs/2007.08496
% https://arxiv.org/abs/1908.09139
% https://arxiv.org/abs/1903.06682
% https://arxiv.org/abs/1902.04029



\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Cosmological parameter tensions}
    \begin{figright}[0.51]{figures/H0}
        \begin{itemize}
            \item Measurements $H_0$ differ between early and late time observations \arxiv{1907.10625} 
            \item ``Tension'' means a disagreement between different datasets on the inferred value of model parameters.
            \item The presence of tension indicates an error in the model and/or at least one of the datasets.
            \item It is statistically incorrect to combine datasets when they are in tension.
        \end{itemize}
    \end{figright}
\end{frame}

\begin{frame}
    \frametitle{The importance of global measures of tension}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item In other situations the discrepancy doesn't exist in a single interpretable parameter
                \item For example: DES+\textit{Planck} \arxiv{1902.04029} 
                \item Are these two datasets in tension?
                \item Can we confidently combine them?
                \item There are a lot more parameters -- are we sure that we've chosen wisely?
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \includegraphics<1>{figures/DES_planck_1}
            \includegraphics<2>{figures/DES_planck_2}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The perils of manual marginal inspection}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item If you have enough parameters, then you might expect that tensions would naturally arise in some combinations by chance.
                \item For example, if you take ACT and \textit{Planck}, and construct a linear combination of parameters in maximum tension:
                    \begin{align*}
                        t=&-\Omega_b h^2 + 0.022 \Omega_c h^2 + 34\theta_{MC} -0.092 \tau\\ &+ 0.05 {\rm{ln}}(10^{10} A_s) + 0.067 n_s
                    \end{align*}
                \item In general you would expect such a parameter to be in $\sim \sqrt{d}-\sigma$ tension \arxiv{2007.08496}
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \includegraphics<1>[width=\textwidth]{figures/act_planck}
            \includegraphics<2>{figures/act_planck_t}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Bayesian language}
    \textbf{Notation}
    \begin{description}[leftmargin=!,labelwidth=200pt]
        \item [Datasets:] $A$ and $B$ (e.g. \textit{Planck} and DES)
        \item [Model:] $M$ (e.g. $\Lambda$CDM)
        \item [Parameters:] $\theta$ (e.g. $(\Omega_m,\sigma_8)$) 
        \item [Likelihoods:] $\mathcal{L}$: $P(A|\theta)$, $P(B|\theta)$ 
    \end{description}
    \textbf{Inference}
    \begin{description}[leftmargin=!,labelwidth=200pt]
        \item [Prior:] $\pi$: $P(\theta)$
        \item [Posteriors:]  $\mathcal{P}$: $P(\theta|B)$, $P(\theta|A)$, (evaluate samples).
        \item [Bayesian evidences:] $\mathcal{Z}=\mean[\pi]{\mathcal{L}}$, $P(A)$, $P(B)$ \hfill\arxiv{1506.00171}
        \item [Bayes theorem:] $\mathcal{L}\times\pi = \mathcal{P}\times\mathcal{Z}$
    \end{description}
    \textbf{Anatomy}
    \begin{description}[leftmargin=!,labelwidth=200pt]
        \item [Kullback--Leibler divergence:] $\mathcal{D}=\mean[\mathcal{P}]{\log\mathcal{P}/\pi}\qquad\sim \log \mathrm{Vol}(\pi)/\mathrm{Vol}(\mathcal{P})$
            \hfill\arxiv{1902.04029}
        \item [Model dimensionality:] $d = 2\times\mathrm{var}(\log\mathcal{L})$      \hfill \arxiv{1903.06682}
        \item [Occam's razor equation:]
        $ \log\mathcal{Z} = \langle\log \mathcal{L}\rangle_\mathcal{P} - \mathcal{D}  $
        \hfill\arxiv{2102.11511}
        \\\hfill (Released today by Hergt et al)
    \end{description}

\end{frame}
\begin{frame}
    \frametitle{Suspiciousness statistic \arxiv{1902.04029} \arxiv{2007.08496}}

    \begin{itemize}
        \item The natural Bayesian measure of tension is the Bayes ratio
            \begin{equation}
                \mathcal{R} = \frac{\mathcal{Z}_{AB}}{\mathcal{Z}_A\mathcal{Z}_B} = \frac{P(A,B)}{P(A)P(B)} = \frac{P(A|B)}{P(A)}= \frac{P(B|A)}{P(B)}
            \end{equation}
        \item $\mathcal{R}$ is prior dependent, one can artificially reduce tension by drawing arbitrarily wide priors.
        \item Can remove this prior dependency by dividing out the KL-dependent Occam factor to give a ``Suspiciousness'', computable from three MCMC chains:
            \begin{equation}
                \log S = \mean[\mathcal{P}_{AB}]{\log L_{AB}} - \mean[\mathcal{P}_{A}]{\log L_{A}}- \mean[\mathcal{P}_{B}]{\log L_{B}}
            \end{equation}
        \item Can be intepreted as the maximum Bayes ratio $\mathcal{R}$ allowed by reasonable priors.
        \item In the Gaussian case it is related to the usual Malhanobis distance
            \begin{equation}
                \log S = \frac{d}{2} - \frac{1}{2}(\mu_A-\mu_B)^T(\Sigma_A+\Sigma_B)^{-1}(\mu_A-\mu_B) 
            \end{equation}
            which can be used to calibrate it into a tension probability and ``$\sigma$'' quantification.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Curvature tension $\Omega_K$}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item $\Lambda$CDM assumes the universe is flat
                \item If you allow $\Omega_K\ne0$, \textit{Planck} (\texttt{plikTTTEEE}) has a moderate preference for closed universes (50:1)
                \item \textit{Planck}+CMB lensing +BAO strongly prefer a flat universe
                \item But, \textit{Planck} vs lensing is 2.5$\sigma$ in tension, and Planck vs BAO is 3$\sigma$.
                \item This is reduced if $\texttt{plik}\to\texttt{camspec}$
                    \begin{itemize}
                        \item Di Valentino et al~\arxiv{1911.02087}
                        \item Handley~\arxiv{1908.09139}
                        \item Efsthathiou \& Gratton~\arxiv{2002.06892}
                    \end{itemize}
                \item BAO and lensing summary statistics and compression strategy assume $\Lambda$CDM.
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \includegraphics<1>{figures/curvature_1}
            \includegraphics<2>{figures/curvature_2}
            \includegraphics<3>{figures/curvature_3}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Summary}
    \centerline{
        \begin{tabular}{llrl}
            Data & Model & Tension & Reference\\
            \hline
            DES vs Planck& $\Lambda$CDM & 2.1$\sigma$ & \arxiv{1902.04029}\\
            ACT vs Planck+SPT& $\Lambda$CDM & 2.8$\sigma$ & \arxiv{2007.08496}\\
            CMB lensing vs Planck& $K\Lambda$CDM & 2.5$\sigma$ & \arxiv{1908.09139}\\
            BAO vs Planck& $K\Lambda$CDM & 3$\sigma$ & \arxiv{1908.09139}\\
        \end{tabular}
    }

	Slides, figures and plotting code available at:
	\url{https://github.com/williamjameshandley/talks/tree/tehran\_2021}
\end{frame}

\end{document}
```
# branch: remotes/origin/tools_2021
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Review on Statistical Tools and Samplers}
\date{26\textsuperscript{th} November 2021}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{The name of the game}
    \begin{itemize}
        \item Many considerations in this talk applicable to Bayesian, Frequentist or ``Monte Carlo'' 
        \item I acknowledge my Bayesian bias!
        \item In general consider a function $\mathcal{P}(\theta)$ over some $d$-dimensional space
        \item This function can be ``forward calculated'': given any location $\theta$ in space, can compute $P$
        \item Analytic pen-and-paper results assumed unavailable/impossible
        \item We wish to:
            \begin{itemize}
                \item Explore the region(s) of high $\mathcal{P}$
                \item Generate representative samples, either in this region, or in the tails
                \item find the hypervolume under the curve $\int P(\theta) d\theta$
            \end{itemize}
        \item Examples include
            \begin{itemize}
                \item Generating samples from a Bayesian posterior distribution
                \item Generating datasets for computing test statistics
                \item Generating Monte Carlo events 
                \item Computing Bayesian evidences for model comparison
                \item Computing cross sections
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Notation}
    \begin{itemize}
        \item Space of parameters $\theta$
        \item Probability distribution/function $\mathcal{P}(\theta)$
        \item Region of valid parameters/support of function/prior/measure $\pi(\theta)$
            \begin{itemize}
                \item Note that for any numerical method even if not specified there is an implicit measure imposed by floating point arithmetic
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Why is this hard?}
    \begin{itemize}
        \item In general, the region of significantly non-zero $\mathcal{P}$ (the typical set) is very ``small''
        \item This problem gets exponentially worse the higher the dimensionality $d$ becomes

        \item Quantify this with the Kullback-Liebler divergence between distributions $\mathcal{P}(\theta)$ and $\pi(\theta)$
    \end{itemize}
    \begin{columns}[onlytextwidth]
        \column{0.6\textwidth}
        \vspace{-5pt}
        \[
        \hspace{-15pt}
        \mathcal{D} = \int \mathcal{P}(\theta) \log\frac{\mathcal{P}(\theta)}{\pi(\theta)} d\theta \sim \log \frac{V_\pi}{V_\mathcal{P}}, \quad \frac{V_\pi}{V_\mathcal{P}} \sim {\left(\frac{\ell_\mathcal{\pi}}{\ell_\mathcal{P}}\right)}^d \]
        \vspace{-10pt}
        \begin{itemize}
            \item Note that this is \alert{not} a distance
            \item One can verify $\sim$ as being $=$ for top hat distributions over regions $R_\mathcal{P}$  and $R_\mathcal{\pi}$ with volumes $V_\mathcal{P}$ and $V_\mathcal{\pi}$ respectively:
                \[\hspace{-20pt}
                    \mathcal{P}(\theta) = \left\{
                        \begin{array}{cl}
                            1/V_\mathcal{P} & \theta \in R_\mathcal{P}\\
                            0 & \text{otherwise}\\
                        \end{array}
                    \right.
                    \quad
                    \pi(\theta) = \left\{
                        \begin{array}{cl}
                            1/V_\pi & \theta \in R_\pi\\
                            0 & \text{otherwise}\\
                        \end{array}
                    \right.
                \]
            \item The integral smooths this over the distribution $\mathcal{P}$
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{How not to do it}
    \begin{itemize}
        \item The worst way to explore/integrate a probability distribution/generate samples is to randomly sample the space using $\pi(\theta)$~\arxiv{2012.09874}
        \item Gridding is also equivalently bad
        \item Whilst this works in low dimensions, if each parameter is confined within some fraction $f\sim\ell_\mathcal{P}/\ell_\pi$ of the space, then the volume fraction $\sim\mathcal{O}(f^d)$, or equivalently $\mathcal{D}\sim d\log f$ 
        \item Random sampling has an efficiency of $\boxed{\approx e^{-\mathcal{D}} \sim e^{-d\log f} = f^{-d}}$
        \item If you find that naive tail sampling is performant for e.g. importance sampling and unweighting, then your function likely has an unusual $\mathcal{D}$ scaling with $d$. 
        \item Turning this around, you can use the inefficiency of random sampling to estimate $\mathcal{D}$
        \item Paper being released soon:

            \hfill \emph{``Exploring phase space with Nested Sampling''} Handley, \textbf{Jan{\ss}en}, Schumann \& \textbf{Yallup}\\
            Also \textbf{Carragher} et al~\arxiv{2101.00428}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Why do sampling}
    \begin{itemize}
        \item Instead of randomly sampling the space, samples $\theta\sim\mathcal{P}$ drawn from the distribution exponentially concentrate around the significantly non-zero region of $\mathcal{P}$
        \item This represents an optimal compression of the space
        \item If you have generated a set of samples drawn from a distribution $S=\{\theta_i : i=1\cdots N,\theta_i\sim\mathcal{P}\}$, then one can compute integrals
            \[ \langle f(\theta) \rangle_\mathcal{P} = \int f(\theta) \mathcal{P}(\theta) d\theta \approx \sum_{\theta_i\in S} f(\theta_i) \]
        \item Typically this is done using weighted samples $S=\{(w_i, \theta_i) : i=1\cdots N,\theta_i\sim\mathcal{P}\}$
            \[ \langle f(\theta) \rangle_\mathcal{P} = \int f(\theta) \mathcal{P}(\theta) d\theta \approx \sum_{w_i, \theta_i\in S} w_i f(\theta_i) \]
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Metropolis Hastings} 
    \begin{itemize}
        \item Turn the $N$-dimensional problem into a one-dimensional one.
        \item Pick start point $\theta_0$.
        \item At step $i$:
            \begin{enumerate}
                \item Propose a new point $\theta_{i+1}$ a small step away from $\theta_{i}$
                \item If uphill $\mathcal{P}(\theta_{i+1}) > \mathcal{P}(\theta_i)$, make step\ldots
                \item \ldots otherwise make step with probability $\alpha = \mathcal{P}(\theta_{i+1}) / \mathcal{P}(\theta_i)$. 
            \end{enumerate}
        \item Requires a proposal distribution $\mathcal{Q}(\theta_{i+1}|\theta_i)$
        \item In general case where $\mathcal{Q}$ is not symmetric, need acceptance ratio:
            \begin{equation*}
                \alpha = \frac{\mathcal{P}(\theta_{i+1})\mathcal{Q}(\theta_{i}|\theta_{i+1})}{\mathcal{P}(\theta_{i})\mathcal{Q}(\theta_{i+1}|\theta_{i})}
            \end{equation*}
    \end{itemize}
    \begin{block}{TOOLS}
        Whilst many exist: $\texttt{PyMC3}$, $\texttt{cobaya}$, $\texttt{MontePython}$,\ldots, in practice the algo is so simple, and the proposal distribution so problem-specific, it's usually relatively efficient to write your own.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Metropolis Hastings} 
    \framesubtitle{Where can this go wrong?}
    \begin{itemize}
        \item Burn in
            \begin{itemize}
                \item It can take a while for the chain to equilibriate to the typical set
                \item It is hard to diagnose burn in, particularly in high dimensions
            \end{itemize}
        \item Multimodality
            \begin{itemize}
                \item If the function has multiple separated peaks, despite mathematical guarantees of convergence it will take a Hubble time to move between modes.
            \end{itemize}
        \item Correlated distributions
            \begin{itemize}
                \item In practice the peak(s) of the distribution have nontrivial structure (e.g. narrow ridges)
                \item Very hard to create a flexible enough proposal to accomodate all, and not strictly Markovian
            \end{itemize}
        \item Phase transitions
            \begin{itemize}
                \item A different kind of multi-modality, which can occur if the function is a ``slab and spike''
                \item Two regions -- one high-volume $V$ lower $\mathcal{P}$, the other low $V$ high $\mathcal{P}$. Difficult to transition
            \end{itemize}
        \item Poor parallelisation
            \begin{itemize}
                \item In practice it is not well parallelised, since majority of time is spent in burn-in
            \end{itemize}
    \end{itemize}
\end{frame}



\begin{frame}
    \frametitle{Hamiltonian Monte-Carlo} 
    \begin{itemize}
        \item Key idea: Treat $\log L(\Theta)$ as a potential energy
        \item Guide walker under ``force'': \[F(\Theta) =\nabla \log L(\Theta)\]
        \item Walker is naturally ``guided'' uphill
        \item Conserved quantities mean efficient acceptance ratios.
        \item Whilst the recieved wisdom is that this is ``tuning parameter free'', in practice the mass matrix has similar degrees of tuning unless the problem is naturally normalised (which physicists are generally quite good at doing anyway).
            \begin{block}{TOOLS}
                \item \texttt{stan} is a fully fledged, mature programming language with HMC as a default sampler.
                \item \texttt{TensorFlow} and \texttt{PyTorch} packages exist, although leader has not emerged.
            \end{block}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Ensemble sampling} 
    \begin{itemize}
        \item Instead of one walker, evolve a set of $n$ walkers.
        \item Can use information present in ensemble to guide proposals.
        \item Generally tuning parameter free
        \item Struggles with multimodal distributions
        \item Strive to be affine invariant
    \begin{block}{TOOLS}
        \begin{itemize}
            \item \texttt{emcee}: The MCMC Hammer~\arxiv{1202.3665}
            \item \texttt{zeus}: Ensemble slice sampling~\arxiv{2002.06212}
        \end{itemize}
    \end{block}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The fundamental issue with all of the above} 

    \begin{itemize}
        \item They can't integrate functions over the space
            \begin{align}
                Z
                &= P(D|M) 
                \nonumber\\
                &= \int P(D|\Theta,M)P(\Theta|M) d\Theta 
                \nonumber\\
                &= \left\langle L \right\rangle_\pi
                \nonumber
            \end{align}
        \item MCMC fundamentally explores the posterior, and cannot average over the prior.
        \item Simulated annealing gives one possibility for ``tricking'' MCMC into computing evidences.
            \begin{itemize}
                \item Inspired by thermodynamics.
                \item Suffers from similar issues to MCMC.
                \item Unclear how to choose correct annealing schedule
            \end{itemize}
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Nested sampling}
    \cols[0.6]{
  \begin{itemize}
    \item Nested sampling is a completely different way of sampling. 
    \item Uses ensemble sampling to compress prior to posterior.
      \item Maintain a set $S$ of $n$ samples, which are sequentially updated:
  \begin{description}
      
    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\pi$). 
      
    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new uniform sample with higher likelihood
  \end{description}
 \item Requires one to be able to sample uniformly within a region, subject to a {\em hard likelihood constraint}.
  \end{itemize}
    }{
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}
        \includegraphics<4          >[width=\textwidth,page=4]{figures/himmelblau}
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}
        \includegraphics<8|handout:0>[width=\textwidth,page=8]{figures/himmelblau}
        \includegraphics<9|handout:0>[width=\textwidth,page=14]{figures/himmelblau}
    }
\end{frame}


\begin{frame}
    \frametitle{Nested sampling}
    \cols[0.6]{
  \begin{itemize}
      \item At the end, one is left with a set of discarded points
      \item These may be weighted to form posterior samples
      \item They can also be used to calculate the normalising constant
          \begin{itemize}
              \item Critically, this is because nested sampling probabilistically estimates the volume of the parameter space
                  \[X_i \approx {\left(\frac{n}{n+1}\right)} X_{i-1} \quad\Rightarrow\quad
                  X_i \approx {\left(\frac{n}{n+1}\right)}^i \approx e^{-i/n} \]
              \item only statistical estimates, but we know the error bar
              \item Nested sampling thus estimates the density of states
              \item it is therefore a partition function calculator
          \end{itemize}
      \item The evolving ensemble of live points allows algorithms to perform self-tuning and mode clustering
  \end{itemize}

    }{
        \includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}
        \includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}
    }

\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \cols[0.5]{%
        \texttt{MultiNest}
        \includegraphics[width=0.8\textwidth]{figures/multinest}
            \vfill
        \texttt{DNest}
        \includegraphics[width=\textwidth]{figures/dnest}
    }{%
        \texttt{PolyChord}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}
            \cols[0.5]{
                \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
                \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            }{
                \includegraphics[width=\textwidth]{figures/chains.png}
            }
            \vfill
    }
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling: Benefits and drawbacks}
    Relative to traditional numerical posterior samples (Metropolis Hastings, HMC, emcee), nested sampling:
    \begin{description}
        \item[$+$] Can calculate evidence (and therefore perform model comparison).
        \item[$+$] Can handle multi-modal distributions.
        \item[$+$] Requires little tuning for an a-priori unseen problem.
        \item[$+$] Highly parallelisable ($n_\mathrm{cores} \sim n_\mathrm{live} \gg 4$).
        \item[$-$] Slower than a well-tuned posterior sampler.
        \item[$-$] Run time is dependent on prior choice, and priors must be proper \\(some people view this as a feature rather than a bug).
    \end{description}
\end{frame}


\begin{frame}
    \frametitle{Nested Sampling: a user's guide}
    \begin{enumerate}
        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
            \begin{itemize}
                \item This means typically most of its time is spent on burn-in rather than posterior sampling
                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable
            \end{itemize}
        \item The number of live points $n_\mathrm{live}$ is a resolution parameter.
            \begin{itemize}
                \item Run time is linear in $n_\mathrm{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\mathrm{live}}}$.
                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
            \end{itemize}
        \item Most algorithms come with additional reliability parameter(s).
            \begin{itemize}
                \item e.g. \texttt{MultiNest}: $\text{eff}$, \texttt{PolyChord}: $n_\mathrm{repeats}$
                \item These are parameters which have no gain if set too conservatively, but increase the reliability
                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
            \end{itemize}
    \end{enumerate}
\end{frame}


\begin{frame}
    \frametitle{Key tools for Nested Sampling}
    \begin{description}
        \item[\texttt{anesthetic}] Nested sampling post processing \arxiv{1905.04768}\\
        \item[\texttt{insertion}] cross-checks using order statistics \arxiv{2006.03371}
            \hspace{5pt}\url{github.com/williamjameshandley/anesthetic}
        \item[\texttt{nestcheck}] cross-checks using unthreaded runs \arxiv{1804.06406}\\
            \hspace{5pt}\url{github.com/ejhigson/nestcheck}
        \item[\texttt{MultiNest}] Ellipsoidal rejection sampling \arxiv{0809.3437}\\
            \hspace{5pt}\url{github.com/farhanferoz/MultiNest}
        \item[\texttt{PolyChord}] Python/C++/Fortran state of the art \arxiv{1506.00171}\\
            \hspace{5pt}\url{github.com/PolyChord/PolyChordLite} 
        \item[\texttt{dynesty}] Python re-implementation of several codes \arxiv{1904.02180}\\
            \hspace{5pt}\url{github.com/joshspeagle/dynesty}
    \end{description}
\end{frame}


%\begin{frame}
%    \frametitle{Likelihood free inference}
%    <+Content+>
%\end{frame}

\begin{frame}
    \frametitle{FAQs}
    
    \begin{itemize}
    \item What was that awesome website? \\
    \hfill Full credit to Chi-feng for this incredible online demonstration tool\\
    \hfill \href{https://chi-feng.github.io/mcmc-demo/}{chi-feng.github.io/mcmc-demo/}

    \item How do you make your plots look hand-drawn? \\
        \hfill \texttt{import matplotlib.pyplot as plt; plt.xkcd()}
    \end{itemize}
\end{frame}



\end{document}
```
# branch: remotes/origin/cavendish_2022
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Nested sampling: powering next-generation Bayesian inference tools}
\subtitle{for cosmology, particle physics and beyond}
\date{14\textsuperscript{th} December 2022}

% Weds 14^th December 2022 : Day 2 - candidates present a research seminar to
% the department – this will be in hybrid format and recorded in order that
% members of the selection panel and other members of the department who are
% unable to be present in person to view the lectures.  Your seminar
% presentation should be of no more than 30 minutes' duration, with a further
% 5-10 minutes allotted for questions and  discussion.  The audience will
% include colleagues from several Departments, therefore the first half of
% your presentation should be a broad introduction to your area of research
% and view to the future, and must be accessible to faculty across all
% research  disciplines. The second half of your presentation may be as
% technical  as you wish.

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{General setting}
    \begin{itemize}
        \item Integration is a fundamental concept in physics, statistics and data science:
    \end{itemize}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Partition functions}
            \vspace{-11pt}
            \[ Z(\beta) = \int e^{-\beta H(q,p)} dq dp \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Path integrals}
            \[ \Psi = \int e^{i S} \mathcal{D}x \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Bayesian marginals}
            \vspace{-11pt}
            \[ \mathcal{Z}(D) = \int \mathcal{L}(D|\theta) \pi(\theta) d\theta \]
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Need numerical tools if analytic solution unavailable.
            \item High-dimensional numerical integration is hard.
            \item Riemannian strategy estimates volumes geometrically:
                \[ \int f(x) d^nx \approx \sum_i f(x_i) \Delta V_i \sim \mathcal{O}(e^n) \]
            \item Curse of dimensionality $\Rightarrow$ exponential scaling.
            \item Nested sampling integrates \textbf{probabilistically}.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n$ random samples over the space.
            \item Delete outermost sample, and replace with a new random one at higher integrand value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n}\only<9->{\pm \frac{1}{n}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \sum_i f(x_i) \Delta V_i, \qquad V_i = V_0 e^{-\only<9->{(}i\only<9->{\pm\sqrt{i})}/n} \]
%            \item Nested sampling: completely different way to scan.
%            \item Ensemble sampling compresses entire space$\to$peak(s).
%            \item Sequentially update a set $S$ of $n$ samples:
%                \begin{itemize}
%                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from a measure $\pi$). 
%
%                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
%                \end{itemize}
%            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard constraint}:
%                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
%            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence}/integral of function \& \C[0]{posterior}/sample weights.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4          >[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<8-|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Nested sampling estimates the \textbf{density of states} and calculates partition functions
                \[Z(\beta) = \sum_i f(x_i)^\beta \Delta V_i\]
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune
                    \item exploration of multimodal functions
                    \item global and local optimisation
                \end{itemize}
            \item For this kind of numerical, generic, high-dimensional integration, it is the only game in town.
            %\item Interpreted as a Bayesian algorithm, it
            %    \begin{itemize}
            %        \item Computes the Bayesian evidence (model comparison)
            %        \item Produces (weighted) posterior samples (parameter estimation)
            %    \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''\\
   {\hfill --- John Skilling}
  \end{quote}
  \begin{itemize}
      \item A large fraction of the work in nested sampling to date has been in attempting to implement a hard-edged sampler in the nested sampling meta-algorithm.
      \item \url{https://projecteuclid.org/euclid.ba/1340370944}.
      \item There has also been much work beyond this (focus of this talk).
  \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \column{0.3\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{dynesty}~\arxiv{1904.02180}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Cosmology}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Battle-tested in Bayesian cosmology on
                \begin{itemize}
                    \item Parameter estimation: multimodal alternative to MCMC samplers.
                    \item Model comparison: using integration to compute the Bayesian evidence
                    \item Tension quantification: using deep tail sampling and suspiciousness computations.
                \end{itemize}
            \item Plays a critical role in major cosmology pipelines: Planck, DES, KiDS, BAO, SNe.
            \item The default $\Lambda$CDM cosmology is well-tuned to have Gaussian-like posteriors for CMB data. 
            \item Less true for alternative cosmologies/models and orthogonal datasets, so nested sampling crucial.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/pps_both}
        \includegraphics[width=0.49\textwidth]{figures/reach_fit-cropped.pdf}
        %\includegraphics[width=0.49\textwidth]{figures/curvature_3}
        \includegraphics[width=\textwidth]{figures/omegak_H0_2.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Astrophysics}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item In exoplanets~\arxiv{1806.00518}
                \begin{itemize}
                    \item Parameter estimation: determining properties of planets.
                    \item Model comparison: how many planets? Stellar modelling~\arxiv{2007.07278}.
                    \item exoplanet problems regularly have posterior phase transitions \arxiv{2102.03387}
                \end{itemize}
            \item In gravitational waves
                \begin{itemize}
                    \item Parameter estimation: Binary merger properties
                    \item Model comparison: Modified theories of gravity, selecting phenomenological parameterisations~\arxiv{1803.10210}
                    \item Likelihood reweighting: fast slow properties
                \end{itemize}
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/rv_full.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Particle physics}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{columns}
            \column{0.67\textwidth}
            \begin{itemize}
                \item Nested sampling for cross section computation/event generation
            \end{itemize}
            \column{0.3\textwidth}
            \[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2.\]
        \end{columns}
        \begin{itemize}
            \item Nested sampling can explore the phase space $\Omega$ and compute integral blind with comparable efficiency to HAAG/RAMBO~\arxiv{2205.02030}.
            \item Bayesian sparse reconstruction~\arxiv{1809.04598} applied to bump hunting allows evidence-based detection of signals in phenomenological backgrounds~\arxiv{2211.10391}.
            \item Now applying to lattice field theory, and lattice gravity Lagrangians.
            \item Particle statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL).
        \end{itemize}
        \column{0.17\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1-pdfjam-crop.pdf}
        \column{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/psi_predict-crop.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Machine learning}
    \begin{columns}
        \column{0.62\textwidth}
        \begin{itemize}
            \item Machine learning requires:
                \begin{itemize}
                    \item Training to find weights
                    \item Choice of architecture/topology/hyperparameters
                \end{itemize}
            \item Bayesian NNs treat training as a model fitting problem
            \item Compute posterior of weights (parameter estimation), rather than optimisation (gradient descent)
            \item Use evidence to determine best architecture (model comparison), correlates with out-of-sample performance! 
            \item Solving the full ``shallow learning'' problem without compromise \arxiv{2004.12211}\arxiv{2211.10391}. 
            \item Promising work ongoing to extend this to transfer learning and deep nets.
        \end{itemize}
        \column{0.38\textwidth}
        \includegraphics[width=\textwidth]{figures/nn_posterior-cropped.pdf}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{Applications of nested sampling}
%    \framesubtitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
%    \begin{columns}
%        \column{0.57\textwidth}
%        \begin{itemize}
%            \item Nested sampling for frequentist computation!?
%            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
%            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
%            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
%            \item Need insight/approximation to make efficient.
%            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
%            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
%        \end{itemize}
%        \column{0.43\textwidth}
%        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{and beyond\ldots}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/watkinson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mason-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/formanek-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mcaloone-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/stenczel-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/yallup-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/bex-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/claireburke-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/hobson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/lasenby-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mhandley-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/whandley-headshot.jpg}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        \includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/tdoa-cropped-1-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/tdoa-cropped-2-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/tdoa-cropped-3-crop.pdf}%
        \includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Beyond the meta-algorithm}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Dynamic nested sampling~\arxiv{1704.03459}
            \item Unwoven nested sampling~\arxiv{1703.09701}
            \item Accelerated nested sampling~\arxiv{2212.01760}
            \item Precision nested sampling~\arxiv{2006.03371}
            \item Multiobjective nested sampling
            \item Nested sampling with gradients?
            \item Reversible nested sampling?
            \item Transdimensional nested sampling?
            \item postprocessing: \texttt{anesthetic}~\arxiv{1905.04768}
            \item crosschecking: \texttt{nestcheck}~\arxiv{1804.06406}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/dynesty.pdf}
        \includegraphics[width=\textwidth]{figures/supernest.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology \arxiv{2210.07409}(NatAstro)}
    \begin{columns}
        \column{0.65\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
            \item Lead data analysis team (REACH first light in January)
            \item Nested sampling woven in from the ground up (calibrator, beam modelling, signal fitting, likelihood selection).
            \item All treated as parameterised model comparison problems.
        \includegraphics[height=0.3\textwidth]{figures/EDGES_antenna}
        \includegraphics[height=0.3\textwidth]{figures/SARAS}
        \includegraphics[height=0.3\textwidth]{figures/REACH_2.jpg}
        \end{itemize}
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT: combining particle physics \& cosmological data}
    \begin{columns}
        \column{0.52\textwidth}
        \begin{itemize}
            \item Multinational team of particle physicists, cosmologists and statisticians.
            \item Combine cosmological data, particle colliders, direct detection, \& neutrino detectors in a statistically principled manner~\arxiv{2205.13549}.
            \item Lead Cosmo/Dark Matter working group~\arxiv{2009.03286}.
            \item Nested sampling used for global fitting, and fine-tuning quantification~\arxiv{2101.00428}
        \end{itemize}
        \begin{center}
            \includegraphics[width=0.5\textwidth]{figures/gambit_logo.png}
        \end{center}
        \column{0.48\textwidth}
        \vspace{-40pt}
        \includegraphics[width=\textwidth]{figures/ALP_2.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Likelihood-free inference (\& nested sampling)}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item How do you do inference if you don't know the likelihood $P(D|\theta)$?
                \begin{itemize}
                    \item e.g.\ if you can simulate a disease outbreak, how can you infer a posterior on $R_0$, or select the most predictive model?
                \end{itemize}
            \item If you can forward simulate/model $\theta\to D$, then you have an implicit likelihood.
            \item LFI aims to (machine-)\emph{learn} the likelihood from carefully chosen training data $\{(\theta,D)\}$.
            \item Nested sampling has much to offer
                \begin{itemize}
                    \item truncation strategies
                    \item evidence driven compression
                    \item marginalised machine learning
                \end{itemize}
            \item In my view, LFI represents the future of inference -- in twenty years time this will be as well-used as MCMC techniques are today.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/noisy.pdf}
        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{CosmoTension}
    \framesubtitle{Resolving cosmological tensions with diverse data, novel theories and Bayesian machine learning}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item ERC grant $\Rightarrow$ UKRI Frontier, commencing 2023.
            \item Funds 3 PDRAs and 4 PhDs over 5 years.
            \item Research programme centered around combining novel theories of gravity, Boltzmann solvers~\arxiv{1906.01421}, reconstruction~\arxiv{1908.00906}, nested sampling \& likelihood free inference.
            \item Aims to disentangle cosmological tensions $H_0$, $\sigma_8$, $\Omega_K$ with next-generation data analysis techniques.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/denseoutput.pdf}
        \includegraphics[width=\textwidth]{figures/pps.pdf}
        \includegraphics[width=\textwidth]{figures/gantt.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{unimpeded: legacy suites for the next generation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item DiRAC 2020 RAC allocation of 30MCPUh
            \item Main goal: Planck Legacy Archive equivalent
            \item Parameter estimation $\to$ Model comparison
            \item MCMC $\to$ Nested sampling
            \item Planck $\to$ $\{\text{Planck}, \text{DESY1}, \text{BAO}, \ldots \}$
            \item Pairwise combinations
            \item Suite of tools for processing these 
                \begin{itemize}
                    \item \texttt{anesthetic} $2.0$
                    \item \texttt{unimpeded} $1.0$
                    \item \texttt{zenodo} archive
                    \item \texttt{margarine}
                \end{itemize}
            \item MCMC chains also available.
            \item Library of bijectors emulators for fast re-use
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{logos/dirac}
        \includegraphics[width=0.5\textwidth,page=21]{figures/himmelblau}%
        \includegraphics[width=0.5\textwidth,page=15]{figures/himmelblau}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Summary}
    \begin{itemize}
        \item Nested sampling is a unique, multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Performing Bayesian inference: parameter estimation, model comparison \& tension quantification.
            \end{itemize}
        \item It already forms the cornerstone of many data-intensive science analyses
        \item My research innovates at the frontier of this field, and applies these techniques to a wide variety of core scientific problems working with international teams.
    \end{itemize}
\end{frame}
\appendix
\begin{frame}
    \frametitle{How does Nested Sampling compare to other approaches?}
    \begin{columns}
        \column{0.7\textwidth}
        \begin{itemize}
            \item In all cases:
                \begin{itemize}
                    \item[$+$] NS can handle multimodal functions
                    \item[$+$] NS computes evidences, partition functions and integrals
                    \item[$+$] NS is self-tuning/black-box
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
    \end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \begin{itemize}
                \item Gradient descent
                    \begin{itemize}
                        \item[$-$] NS cannot use gradients
                        \item[$+$] NS does not require gradients
                    \end{itemize}
                \item Genetic algorithms
                    \begin{itemize}
                        \item[$+$] NS discarded points have statistical meaning
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Sampling}
            \begin{itemize}
                \item Metropolis-Hastings?
                    \begin{itemize}
                        \item[$-$] Nothing beats well-tuned customised MH
                        \item[$+$] NS is self tuning
                    \end{itemize}
                \item Hamiltonian Monte Carlo?
                    \begin{itemize}
                        \item[$-$] In millions of dimensions, HMC is king
                        \item[$+$] NS does not require gradients
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \begin{itemize}
                \item Thermodynamic integration
                    \begin{itemize}
                        \item[$+$] protective against phase trasitions
                        \item[$+$] No annealing schedule tuning 
                    \end{itemize}
                \item Sequential Monte Carlo
                    \begin{itemize}
                        \item[$-$] SMC experts classify NS as a kind of SMC
                        \item[$+$] NS is athermal
                    \end{itemize}
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Curvature tension: evidence for a closed universe(?)~\arxiv{1908.09139}}
    \begin{columns}
        \begin{column}{0.55\textwidth}
            \begin{itemize}
                \item If you allow $\Omega_K\ne0$, \textit{Planck} (\texttt{plikTTTEEE}) has a moderate preference for closed universes (50:1 betting odds on), $\Omega_K=-4.5\pm1.5\%$
                \item \textit{Planck}+lens+BAO strongly prefer $\Omega_K=0$.
                \item But, \textit{Planck} vs lensing is 2.5$\sigma$ in tension, and Planck vs BAO is 3$\sigma$.
                \item Reduced if $\texttt{plik}\to\texttt{camspec}$~\arxiv{2002.06892} 
                \item BAO and lensing summary assume $\Lambda$CDM.
                \item Doing this properly with BAO retains preference for closed universe (though closer to flat $\Omega_K =-0.4\pm0.2\%$)~\arxiv{2205.05892}.
                \item Present-day curvature has profound consequences for inflation~\arxiv{2205.07374}.
            \end{itemize}
        \end{column}
        \begin{column}{0.45\textwidth}
            \includegraphics<1|handout:0>[width=\textwidth]{figures/curvature_1}%
            \includegraphics<2|handout:0>[width=\textwidth]{figures/curvature_2}%
            \includegraphics<3          >[width=\textwidth]{figures/curvature_3}%
        \end{column}
    \end{columns}
\end{frame}

\end{document}
```
# branch: remotes/origin/corfu_2022
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley}
\usepackage{pythonhighlight}

\usepackage{listings}
 

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Next generation cosmological analysis with nested sampling}
\date{8\textsuperscript{th} September 2022}
\newcommand{\av}[2][]{\left\langle #2\right\rangle_{#1}}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Overview}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item DiRAC 2020 RAC allocation of 30MCPUh
            \item Main goal: Planck Legacy Archive equivalent
            \item Parameter estimation $\to$ Model comparison
            \item MCMC $\to$ Nested sampling
            \item Planck $\to$ $\{\text{Planck}, \text{DESY1}, \text{BAO}, \ldots \}$
            \item Pairwise combinations
            \item Suite of tools for processing these 
                \begin{itemize}
                    \item \texttt{anesthetic} $2.0$
                    \item \texttt{unimpeded} $1.0$
                    \item \texttt{zenodo} archive
                \end{itemize}
            \item MCMC chains also available.
            \item Work in progress, but beta testers requested (email \href{mailto:wh260@cam.ac.uk}{wh260@cam.ac.uk})
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{logos/dirac}
        \includegraphics[width=0.5\textwidth,page=21]{figures/himmelblau}%
        \includegraphics[width=0.5\textwidth,page=15]{figures/himmelblau}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The three pillars of Bayesian inference}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Parameter estimation}
            What do the data tell us about the parameters of a model?

            \textit{e.g. the size or age of a $\Lambda$CDM universe}
            \[ \hspace{-10pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}}, \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}, \] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}. \]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Model comparison}
            How much does the data support a particular model?

            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[6]{P(D)}}, \] \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[5]{\Pi_\mathcal{M}}}{\C[6]{\sum_m Z_m \Pi_m}}, \] \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[6]{\text{Normalisation}}}.\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Tension quantification}
            Do different datasets make consistent predictions from the same model? 

            \textit{e.g. CMB vs Type IA supernovae data}
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}, \] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Occam's Razor~\arxiv{2102.11511}}
    \begin{itemize}
        \item Bayesian inference quantifies Occam's Razor:
            \begin{itemize}
                \item \textit{``Entities are not to be multiplied without necessity''} \hfill --- William of Occam
                \item \textit{``Everything should be kept as simple as possible, but not simpler''} \hfill --- ``Albert Einstein''
            \end{itemize}
        %\item Consider the Evidence $\C[3]{\mathcal{Z}\equiv P(D|M)}$: 
        %    \begin{description}[Parameter estimation]
        %        \item [Parameter estimation] normalisation constant
        %        \item [Model comparison] critical update factor for \C[5]{model prior} to \C[4]{model posterior}
        %    \end{description}
        \item Properties of the evidence: rearrange Bayes' theorem for parameter estimation
            \[\C[0]{\mathcal{P}(\theta)} = \frac{\C[2]{\mathcal{L}(\theta)} \C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}}} \qquad\Rightarrow\qquad \C[3]{\log \mathcal{Z}} = \C[2]{\log\mathcal{L}(\theta)} - \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}} \]  
        \item Evidence is composed of a ``goodness of fit'' term  and ``Occam Penalty''
    \end{itemize}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item RHS true for all $\theta$. Take max likelihood value $\theta_*$:
                \[
                    \C[3]{\log \mathcal{Z}} = -\chi_\mathrm{min}^2 - \text{Mackay penalty}
                \]
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Be more Bayesian and take posterior average to get the ``Occam's razor equation''
                \[
                    \boxed{
                        \C[3]{\log \mathcal{Z}} = \av[{\C[0]{\mathcal{P}}}]{\C[2]{\log\mathcal{L}}} - \mathcal{D}_\mathrm{KL}
                    }
                \]
        \end{itemize}
    \end{columns}
    \vfill
    \begin{itemize}
        \item Natural regularisation which penalises models with too many parameters.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kullback Liebler divergence}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is is defined as:
                \[\mathcal{D}_\mathrm{KL} = \av[{\C[0]{\mathcal{P}}}]{\log\frac{\C[0]{\mathcal{P}}}{\C[2]{\pi}}} = \int \C[0]{\mathcal{P}(\theta)} \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}}d\theta.\]
            \item Whilst not a distance, $\mathcal{D}=0$ when $\C[0]{\mathcal{P}}=\C[1]{\pi}$.
            \item Occurs in the context of machine learning as an objective function for training functions.
            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
                \[ \mathcal{D}_\mathrm{KL} \approx \log \frac{\C[1]{V_\pi}}{\C[0]{V_\mathcal{P}}}.\]
                (this is exact for top-hat distributions).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why do sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The cornerstone of numerical Bayesian inference is working with \textbf{samples}.
            \item Generate a set of representative parameters drawn in proportion to the posterior $\theta\C[1]{\sim\mathcal{P}}$.
            \item The magic of marginalisation $\Rightarrow$ perform usual analysis on each sample in turn.
            \item The golden rule is \C[1]{stay in samples} until the last moment before computing summary statistics/triangle plots because \[\boxed{f(\:\av{X}\:)\ne \av{\:f(X)\:}}\]
            \item Generally need $\sim\mathcal{O}(12)$ independent samples to compute a value and error bar.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/samples.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The Planck legacy archive}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item \textit{Planck} collaboration science products
            \item distributed cosmology inference results as MCMC chains
            \item Across a grid of:
                \begin{itemize}
                    \item subsets/combinations of \textit{Planck} data
                        \begin{itemize}
                            \item TT, lowl, lowE, lensing
                        \end{itemize}
                    \item $\Lambda$CDM extensions 
                        \begin{itemize}
                            \item base, mnu, nrun, omegak, r
                        \end{itemize}
                \end{itemize}
            \item importance sampling across some other likelihoods (BAO, JLA,\ldots)
            \item Cannot compute evidences in high dimensions from MCMC chains
                \begin{itemize}
                    \item Only parameter estimation
                    \item no model comparison
                \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/pla}
        \includegraphics[width=\textwidth]{figures/pla1}
    \end{columns}
\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{block}{\textbf{MCMC}}
            \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
            \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
            \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
            \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
            \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
            \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
            \begin{itemize}
                \item<16> Single ``walker''
                \item<16> Explores posterior
                \item<16> Fast, if proposal matrix is tuned
                \item<16> Parameter estimation, suspiciousness calculation
                \item<16> Channel capacity optimised for generating posterior samples
            \end{itemize}
        \end{block}
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.5\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
            \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
            \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
            \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
            \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
            \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
            \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
            \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
            \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
            \begin{itemize}
                \item<16> Ensemble of ``live points''
                \item<16> Scans from prior to peak of likelihood
                \item<16> Slower, no tuning required
                \item<16> Parameter estimation, model comparison, tension quantification
                \item<16> Channel capacity optimised for computing partition function
            \end{itemize}
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The grid (so far)}
        \begin{itemize}
            \item Models: $[\Lambda\text{CDM}, \Omega_K, \nu, r, w, w(a)]$
            \item Data: [\texttt{plik}, \texttt{camspec}, \texttt{DESY1}, \texttt{bicep+keck}, \texttt{BAO(DR16)}, \texttt{pantheon} ]
            \item Pairwise combinations of datasets
            \item Breakdown of Planck \& BAO data
            \item Samplers: [Metropolis Hastings MCMC, Nested Sampling]
            \item These exhaust what is currently available by default in $\texttt{cobaya}$
            \item Wide priors to allow for importance readjustment as desired
            \item roughly halfway through computational allocation. 
            \item Feedback desirable as to what extensions to the grid would be of community interest (email \href{mailto:wh260@cam.ac.uk}{wh260@cam.ac.uk}).
            \item Further checking needed before first release by end of this year.
        \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{\texttt{unimpeded}}
    \framesubtitle{Universal Model comparison and Parameter Estimation Distributed over Every Dataset}
    \begin{columns}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Python tool for seamlessly downloading and cacheing chains
        \item Data stored on \texttt{zenodo} 
        \item hdf5 storage for fast \& reliable download \& storage
        \item Library of trained bijectors to be used as priors/emulators~\arxiv{2102.12478}/nuisance marginalised likelihoods~\arxiv{2207.11457}
        \item \texttt{anesthetic} compatible for processing of chains~\arxiv{1905.04768}
        \item $\alpha$-testers wanted! (email \href{mailto:wh260@cam.ac.uk}{wh260@cam.ac.uk}) 
        \item End goal -- community library which everyone contributes to so expensive runs reusable.
    \end{itemize}
        \column{0.5\textwidth}


\lstset{language=Python}
\lstset{frame=lines}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
from unimpeded import Unimpeded
store = Unimpeded(cache='data.hdf5')
samps = store('planck')
samps.H0.plot.kde_1d()
samps = store('planck', model='klcdm')
samps.H0.plot.kde_1d()
\end{lstlisting}
\includegraphics[width=\textwidth]{figures/unimpeded.pdf}

        
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The importance of global measures of tension}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item Hubble tension~\arxiv{1907.10625}
                    \begin{itemize}
                        \item \textit{Planck}: $H_0=67.4\pm0.5$
                        \item S$H_0$ES: $H_0=74.0\pm1.4$
                    \end{itemize}
                \item In other situations the discrepancy doesn't exist in a single interpretable parameter
                \item For example: DES+\textit{Planck} \arxiv{1902.04029} 
                \item Are these two datasets in tension?
                \item There are a lot more parameters -- are we sure that tensions aren't hiding? Are we sure we've chosen the best ones to reveal the tension?
                \item Should use ``Suspiciousness'' statistic $\mathcal{S}$, or Bayes ratio $\mathcal{R}$ to determine global tension.
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \includegraphics<1>{figures/DES_planck_1}
            \includegraphics<2>{figures/DES_planck_2}
        \end{column}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{The DES evidence ratio $R$}
    \begin{itemize}
        \item The Dark Energy Survey \arxiv{1708.01530} quantifies tension between two datasets $A$ and $B$ using the Bayes ratio:
            \[
                R = \frac{\mathcal{Z}_{AB}}{\mathcal{Z}_A \mathcal{Z}_B} = \frac{P(A\cap B)}{P(A)P(B)} = \frac{P(A|B)}{P(A)} = \frac{P(B|A)}{P(B)}
            \]
            where $\mathcal{Z}$ is the Bayesian evidence.
    \end{itemize}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Many attractive properties:
                \begin{itemize}
                    \item Symmetry
                    \item Parameterisation independence
                    \item Dimensional consistency
                    \item Use of well-defined Bayesian quantities
                \end{itemize}
            \item $R$ gives the relative change in our confidence in data $A$ in light of having seen $B$ (and vice-versa).
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item $R>1$ implies we have more confidence in $A$ having received $B$.
            \item Like evidences, it is prior-dependent \\ from $\mathcal{D}$ in $\log \mathcal{Z} = \av[\mathcal{P}]{\log\mathcal{L}}-\mathcal{D}$
            \item Increasing prior widths $\Rightarrow$ decreasing evidence. 
            \item Increasing prior widths $\Rightarrow$ increasing confidence.
        \end{itemize}
    \end{columns}
\end{frame}



\begin{frame}
    \frametitle{The DES evidence ratio $R$: Prior dependency}
    \centerline{\includegraphics[trim=0.6in 0.3in 0in 0in]{./plots/prior_dependency.pdf}}
    \vspace{10pt}
    \begin{itemize}
        \item What does it mean if increasing prior widths $\Rightarrow$ increasing confidence? 
        \item Wide priors mean {\em a-priori\/} the parameters could land anywhere.
        \item We should be proportionally more reassured when they land close to one another if the priors are wide
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{How do we deal with the prior dependency in $R$?}
    \begin{description}
        \item[Option 1] Take the Bayesian route, accept the prior dependency, and spend time trying to justify why a given set of priors are ``physical''.
        \item[Option 2] Try to find a principled way of removing this prior dependency
    \end{description}
    \begin{itemize}
        \item Decompose ratio using Occam's Razor equation $\log \mathcal{Z} = \av[\mathcal{P}]{\log\mathcal{L}} - \mathcal{D}$
        \[ 
            \begin{aligned}
                \log R &= \log\mathcal{Z}_{AB} - \log\mathcal{Z}_{A} - \log\mathcal{Z}_{A} \\
                &= \av[\mathcal{P}_{AB}]{\log\mathcal{L}_{AB}} - \av[\mathcal{P}_{A}]{\log\mathcal{L}_{A}}- \av[\mathcal{P}_{B}]{\log\mathcal{L}_{B}} - \mathcal{D}_{AB} + \mathcal{D}_{A} + \mathcal{D}_{B} \\
                &= \log \mathcal{S} + \log \mathcal{I}
            \end{aligned}
    \]
    where we have defined the suspiciousness $S$, which is prior independent, and the information $\mathcal{I}$, which depends on the parameter compression of the shared space
        \item Focussing on the prior-independent portion $\mathcal{S}$ gives $R$ for the  ``Narrowest reasonable priors'' which do not impinge on the posterior
        \item One of the critical observations is that one can only hide tension by widening priors. Narrowing them will only ever show tension if it is present.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Suspiciousness $S$}
    \begin{itemize}
        \item For a Gaussian set of posteriors:
            \[
                \log \mathcal{S} = \frac{d}{2}  -\frac{1}{2} (\mu_A-\mu_B){(\Sigma_{A}+\Sigma_{B})}^{-1}(\mu_A-\mu_B).
            \]
        \item The Malhanobis  term is suggestive, so we can use this to calibrate a ``sigma'' level of tension using a $\chi^2$ distribution for $\chi^2_d=d-2\log \mathcal{S}$, or a tension probability.
        \item $S$ is composed of evidences $\mathcal{Z}$ and KL divergences $\mathcal{D}$, which are Gaussian-independent concepts, so the only thing to determine is $d$, the ``number of shared parameters''.
        \item Can do this with Gaussian dimensionality $\frac{d}{2}=\text{var}_\mathcal{P}(\log \mathcal{L})$~\arxiv{1903.06682}
            \begin{align}
                \text{Planck vs BAO}:&      &p&=  42 \pm     4 \% \nonumber\\
                \text{Planck vs DESY1}:&      &p&=   3.2 \pm     1.0 \% \nonumber\\
                \text{Planck vs S$H_0$ES}:& &p&=   0.25 \pm     0.17 \% \nonumber
            \end{align}
        \item Under this metric, S$H_0$ES is unambiguously inconsistent, although not quite as brutal as $>4\sigma$. BAO is consistent, and DESY1 is inconsistent, but only just. This is pleasingly similar to ones intuition.
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Curvature tension?~\arxiv{1908.09139}}

    \begin{columns}

        \begin{column}{0.5\textwidth}

            \begin{itemize}
                \item If you allow $\Omega_K\ne0$, \textit{Planck} (\texttt{plikTTTEEE}) has a moderate preference for closed universes (50:1 betting odds on), $\Omega_K=-4.5\pm1.5\%$ \hfill \arxiv{1911.02087}
                \item \textit{Planck}+lens+BAO strongly prefer $\Omega_K=0$.
                \item But, \textit{Planck} vs lensing is 2.5$\sigma$ in tension, and Planck vs BAO is 3$\sigma$.
                \item Reduced if $\texttt{plik}\to\texttt{camspec}$ \hfill\arxiv{2002.06892} 
                \item BAO and lensing summary assume $\Lambda$CDM.
                \item Doing this properly with BAO retains preference for closed universe (though closer to flat $\Omega_K =-0.4\pm0.2\%$) \hfill\arxiv{2205.05892}
                \item Present-day curvature has profound consequences for inflation \hfill\arxiv{2205.07374}
            \end{itemize}

        \end{column}

        \begin{column}{0.5\textwidth}
            \includegraphics<1|handout:0>[width=\textwidth]{figures/curvature_1}%
            \includegraphics<2|handout:0>[width=\textwidth]{figures/curvature_2}%
            \includegraphics<3          >[width=\textwidth]{figures/curvature_3}%

        \end{column}

    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \begin{itemize}
        \item DiRAC RAC allocation for building a legacy grid of
            \begin{itemize}
                \item MCMC \& Nested sampling chains
                \item gridded over (pairwise) up-to-date datasets
                \item gridded over extensions to $\Lambda$CDM
                \item Bijectors \& emulators for fast re-use
                \item Importance sampling toolkit via \texttt{anesthetic} for (re)processing
                \item Long-term goal: community repository of chains to share model comparison compute resource
            \end{itemize}
        \item Looking for:
            \begin{itemize}
                \item $\alpha$-testers for \texttt{unimpeded}
                \item Suggestions for more datasets (and their incorporation into \texttt{cobaya})
            \end{itemize}
    \end{itemize}
\end{frame}

\end{document}
```
# branch: remotes/origin/KICC_2022
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley}
\usepackage{pythonhighlight}
\usepackage{tikz}
\usetikzlibrary[positioning]
\usetikzlibrary{calc}
\usetikzlibrary{fit}

\usepackage{listings}
 

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Next generation cosmological analysis with nested sampling}
\date{8\textsuperscript{th} September 2022}
\newcommand{\av}[2][]{\left\langle #2\right\rangle_{#1}}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Overview}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item DiRAC 2020 RAC allocation of 30MCPUh
            \item Main goal: Planck Legacy Archive equivalent
                \vspace{-10pt}
                \begin{itemize}
                    \item Parameter estimation $\to$ Model comparison
                    \item MCMC $\to$ Nested sampling
                    \item Planck $\to$ $\{\text{Planck}, \text{DESY1}, \text{BAO}, \ldots \}$
                    \item Pairwise combinations
                \end{itemize}
            \item Suite of tools for processing these 
                \begin{itemize}
                    \item \texttt{unimpeded} $1.0$
                    \item \texttt{margarine} $1.0$
                    \item \texttt{anesthetic} $2.0$
                    \item \texttt{zenodo} archive
                \end{itemize}
            \item MCMC chains also available.
            \item Work in progress, but $\alpha$-testers requested (email \href{mailto:wh260@cam.ac.uk}{wh260@cam.ac.uk})
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{logos/dirac}
        \includegraphics[width=0.5\textwidth,page=21]{figures/himmelblau}%
        \includegraphics[width=0.5\textwidth,page=15]{figures/himmelblau}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The three pillars of Bayesian inference}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Parameter estimation}
            What do the data tell us about the parameters of a model?

            \textit{e.g. the size or age of a $\Lambda$CDM universe}
            \[ \hspace{-10pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}}, \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}, \] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}. \]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Model comparison}
            How much does the data support a particular model?

            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}}, \] \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[5]{\Pi_\mathcal{M}}}{\C[7]{\sum_m Z_m \Pi_m}}, \] \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}.\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Tension quantification}
            Do different datasets make consistent predictions from the same model? 

            \textit{e.g. CMB vs Type IA supernovae data}
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}, \] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Occam's Razor~\arxiv{2102.11511}}
    \begin{itemize}
        \item Bayesian inference quantifies Occam's Razor:
            \begin{itemize}
                \item \textit{``Entities are not to be multiplied without necessity''} \hfill --- William of Occam
                \item \textit{``Everything should be kept as simple as possible, but not simpler''} \hfill --- ``Albert Einstein''
            \end{itemize}
        %\item Consider the Evidence $\C[3]{\mathcal{Z}\equiv P(D|M)}$: 
        %    \begin{description}[Parameter estimation]
        %        \item [Parameter estimation] normalisation constant
        %        \item [Model comparison] critical update factor for \C[5]{model prior} to \C[4]{model posterior}
        %    \end{description}
        \item Properties of the evidence: rearrange Bayes' theorem for parameter estimation
            \[\C[0]{\mathcal{P}(\theta)} = \frac{\C[2]{\mathcal{L}(\theta)} \C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}}} \qquad\Rightarrow\qquad \C[3]{\log \mathcal{Z}} = \C[2]{\log\mathcal{L}(\theta)} - \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}} \]  
        \item Evidence is composed of a ``goodness of fit'' term  and ``Occam Penalty''
    \end{itemize}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item RHS true for all $\theta$. Take max likelihood value $\theta_*$:
                \[
                    \C[3]{\log \mathcal{Z}} = -\chi_\mathrm{min}^2 - \text{Mackay penalty}
                \]
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Be more Bayesian and take posterior average to get the ``Occam's razor equation''
                \[
                    \boxed{
                        \C[3]{\log \mathcal{Z}} = \av[{\C[0]{\mathcal{P}}}]{\C[2]{\log\mathcal{L}}} - \mathcal{D}_\mathrm{KL}
                    }
                \]
        \end{itemize}
    \end{columns}
    \vfill
    \begin{itemize}
        \item Natural regularisation which penalises models with too many parameters.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kullback Liebler divergence}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is defined as:
                \[\mathcal{D}_\mathrm{KL} = \av[{\C[0]{\mathcal{P}}}]{\log\frac{\C[0]{\mathcal{P}}}{\C[1]{\pi}}} = \int \C[0]{\mathcal{P}(\theta)} \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}}d\theta.\]
            \item Whilst not a distance, $\mathcal{D}=0$ when $\C[0]{\mathcal{P}}=\C[1]{\pi}$.
            \item Occurs in the context of machine learning as an objective function for training functions.
            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
                \[ \mathcal{D}_\mathrm{KL} \approx \log \frac{\C[1]{V_\pi}}{\C[0]{V_\mathcal{P}}}.\]
                (this is exact for top-hat distributions).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why do sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The cornerstone of numerical Bayesian inference is working with \textbf{samples}.
            \item Generate a set of representative parameters drawn in proportion to the posterior $\theta\sim\C[0]{\mathcal{P}}$.
            \item The magic of marginalisation $\Rightarrow$ perform usual analysis on each sample in turn.
            \item The golden rule is \C[3]{stay in samples} until the last moment before computing summary statistics/triangle plots because \[\boxed{f(\:\av{X}\:)\ne \av{\:f(X)\:}}\]
            \item Generally need $\sim\mathcal{O}(12)$ independent samples to compute a value and error bar.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/samples.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The Planck legacy archive}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item \textit{Planck} collaboration science products
            \item Distributed cosmology inference results as MCMC chains
            \item Across a grid of:
                \begin{itemize}
                    \item subsets/combinations of \textit{Planck} data
                        \begin{itemize}
                            \item TT, lowl, lowE, lensing
                        \end{itemize}
                    \item $\Lambda$CDM extensions 
                        \begin{itemize}
                            \item base, mnu, nrun, omegak, r
                        \end{itemize}
                \end{itemize}
            \item importance sampling across some other likelihoods (BAO, JLA,\ldots)
            \item Cannot compute evidences in high dimensions from MCMC chains
                \begin{itemize}
                    \item Only parameter estimation
                    \item no model comparison
                \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/pla}
        \includegraphics[width=\textwidth]{figures/pla1}
    \end{columns}
\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{block}{\textbf{MCMC}}
            \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
            \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
            \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
            \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
            \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
            \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
            \begin{itemize}
                \item<16> Single ``walker''
                \item<16> Explores posterior
                \item<16> Fast, if proposal matrix is tuned
                \item<16> Parameter estimation, suspiciousness calculation
                \item<16> Channel capacity optimised for generating posterior samples
            \end{itemize}
        \end{block}
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.5\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
            \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
            \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
            \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
            \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
            \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
            \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
            \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
            \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
            \begin{itemize}
                \item<16> Ensemble of ``live points''
                \item<16> Scans from prior to peak of likelihood
                \item<16> Slower, no tuning required
                \item<16> Parameter estimation, model comparison, tension quantification
                \item<16> Channel capacity optimised for computing partition function
            \end{itemize}
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The grid (so far)}
        \begin{itemize}
            \item Models: $[\Lambda\text{CDM}, \Omega_K, \nu, r, w, w(a)]$
            \item Data: [\texttt{plik}, \texttt{camspec}, \texttt{DESY1}, \texttt{bicep+keck}, \texttt{BAO(DR16)}, \texttt{pantheon} ]
            \item Pairwise combinations of datasets
            \item Breakdown of Planck \& BAO data
            \item Samplers: [Metropolis Hastings MCMC, Nested Sampling]
            \item These exhaust what is currently available by default in $\texttt{cobaya}$
            \item Wide priors to allow for importance readjustment as desired
            \item roughly halfway through computational allocation. 
            \item Feedback desirable as to what extensions to the grid would be of community interest (email \href{mailto:wh260@cam.ac.uk}{wh260@cam.ac.uk}) (\texttt{Pantheon+}, S$H_0$ES, \texttt{NPIPE}, \texttt{DESY3},\ldots).
            \item Further checking needed before first release by end of this year.
        \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{\texttt{unimpeded}}
    \framesubtitle{Universal Model comparison and Parameter Estimation Distributed over Every Dataset}
    \begin{columns}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Python tool for seamlessly downloading, uploading and cacheing of chains
        \item Data stored on \texttt{zenodo} 
        \item hdf5 storage for fast \& reliable storage
        \item \texttt{anesthetic} compatible for processing of chains~\arxiv{1905.04768}
        \item $\alpha$-testers wanted! (email \href{mailto:wh260@cam.ac.uk}{wh260@cam.ac.uk}) 
        \item End goal -- community library which everyone contributes to so expensive inference products are reusable and reused.
    \end{itemize}
        \column{0.5\textwidth}


\lstset{language=Python}
\lstset{frame=lines}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
from unimpeded import Unimpeded
store = Unimpeded(cache='data.hdf5')
samps = store('planck')
samps.H0.plot.kde_1d()
samps = store('planck', model='klcdm')
samps.H0.plot.kde_1d()
\end{lstlisting}
\includegraphics[width=\textwidth]{figures/unimpeded.pdf}

        
    \end{columns}
\end{frame}

%\begin{frame}
%\begin{tikzpicture}[rednode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
%bluenode/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
%remember picture, overlay]
%
%\node<1->[bluenode](likelihoodA) at (1,2) {$ \mathcal{L}_A(\theta,\alpha_A)$};
%\node<1->[bluenode, right = 0 of likelihoodA.east](likelihoodB) {$ \mathcal{L}_B(\theta,\alpha_B)$};
%\node<2->[bluenode, below = of likelihoodB.west](likelihoodAB) {$ \mathcal{L}_A(\theta,\alpha_A) \mathcal{L}_B(\theta,\alpha_B)$};
%
%
%\draw<2->[->] (likelihoodA.south) -- (likelihoodA.south |- likelihoodAB.north);
%\draw<2->[->] (likelihoodB.south) -- (likelihoodB.south |- likelihoodAB.north);
%
%\node<3->[bluenode, right = 0 of likelihoodAB.east](prior) {$ \pi_{AB}(\theta,\alpha_A, \alpha_B)$};
%
%\node<4->[rednode, align=center, below = of likelihoodAB.east](nestedsampling)  {Nested Sampling with $\theta$, $\alpha_A$ and $\alpha_B$};
%
%\draw<4->[->] (likelihoodAB.south) -- (likelihoodAB.south |- nestedsampling.north);
%\draw<4->[->] (prior.south) -- (prior.south |- nestedsampling.north);
%
%
%\node<5->[bluenode, below = of nestedsampling.south](jointPosterior2) {$ \{\theta,\alpha_A,\alpha_B\}_{\mathcal{P}_{AB}}$};
%\draw<5->[->] (nestedsampling.south) -- (jointPosterior2.north);
%
%\node<6->[bluenode, below = of jointPosterior2.south](jointPosterior1) {$ \{\theta\}_{\mathcal{P}_{AB}}$};
%
%\draw<6->[->] (jointPosterior2.south) -- (jointPosterior1.north);
%
%\node<7->[bluenode,  right = of jointPosterior1.east](jointEvidence2) {$ \mathcal{Z}_{AB}$};
%\draw<7->[->] (nestedsampling.south-|jointEvidence2.north) -- (jointEvidence2.north);
%
%
%
%\end{tikzpicture}
%\end{frame}

%\begin{frame}
%\begin{tikzpicture}[squarednodeA/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
%squarednodeB/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
%squarednodeC/.style={rectangle, draw=green!60, fill=green!5, very thick, minimum size=5mm}]
%
%\node[squarednodeB](likelihood1) at (6, 1.5){$ \mathcal{L}_A(\theta,\alpha_A)$};
%\node[squarednodeB](prior1) at (8, 1.5){$ \pi_A(\theta,\alpha_A)$};
%
%\node[squarednodeC, text width=3cm, align=center](NestedMarg1) at (7, 0){Nested Sampling + \texttt{margarine}};
%
%\draw[->](likelihood1.south) -- (6, 0.5);
%\draw[->](prior1.south) -- (8, 0.5);
%
%\node[squarednodeB](marglike1) at (6, -1.5){$ \mathcal{L}_A(\theta)$};
%\node[squarednodeB](margprior1) at (9, -1.5){$ \pi(\theta)$};
%
%\draw[->](6, -0.5) -- (6, -1.2);
%\draw[->](8, -0.5) -- (9, -1.2);
%
%\node[squarednodeB](likelihood2) at (10, 1.5){$ \mathcal{L}_B(\theta,\alpha_B)$};
%\node[squarednodeB](prior2) at (12, 1.5){$ \pi_B(\theta,\alpha_B)$};
%
%\draw[->](likelihood2.south) -- (10, 0.5);
%\draw[->](prior2.south) -- (12, 0.5);
%
%\node[squarednodeC, text width=3cm, align=center](NestedMarg1) at (11, 0){Nested Sampling + \texttt{margarine}};
%
%\node[squarednodeB](marglike2) at (12, -1.5){$ \mathcal{L}_B(\theta)$};
%\draw[->](12, -0.5) -- (12, -1.2);
%\draw[->](10, -0.5) -- (9, -1.2);
%
%\node[squarednodeB](combinedlike) at (7, -3){$ \mathcal{L}_A(\theta) \mathcal{L}_B(\theta)$};
%
%\draw[->](marglike1.south) -- (combinedlike.north);
%\draw[->](marglike2.south) -- (combinedlike.north);
%
%\node[squarednodeA, text width=3cm, align=center](inference2) at (9, -4.5) {Nested Sampling with $\theta$};
%
%\draw[->](combinedlike.south) -- (8, -4);
%\draw[->](margprior1.south) -- (10, -4);
%
%\node[squarednodeB](jointEvidence) at (8, -6){$ \mathcal{Z}_{AB}$};
%\node[squarednodeB](jointPosterior) at (10, -6){$ \{\theta\}_{\mathcal{P}_{AB}}$};
%
%\draw[->](8, -5) -- (jointEvidence.north);
%\draw[->](10, -5) -- (jointPosterior.north);
%\end{tikzpicture}
%\end{frame}

\begin{frame}
    \frametitle{\texttt{margarine}: machine learning-enhanced Bayesian inference}
    %\begin{tikzpicture}[remember picture,overlay]
    %    \node (student) [xshift=-0.11\textwidth,yshift=-0.18\textwidth] at (current page.north east) {\includegraphics[width=0.2\textwidth]{figures/harry_bevins.jpg}};
    %    \node[below=0 of student.south,align=center]{Harry Bevins\\\arxiv{2207.11457}};
    %\end{tikzpicture}

    \begin{columns}
        \column{0.6\textwidth}
        \begin{columns}
            \column{0.3\textwidth}
            \includegraphics[width=\textwidth]{figures/harry_bevins.jpg}
            Harry Bevins \\ \arxiv{2205.12841} \\ \arxiv{2207.11457}
            \column{0.7\textwidth}
            \begin{itemize}
                \item Can use machine learning + grid to dramatically speed up inference
                \item Emulate the marginal posterior and prior with  masked autoregressive flows (\texttt{margarine})
                \item Use nested sampling evidences to compute nuisance marginalised likelihood $\mathcal{L}(\theta) = \mathcal{P}(\theta) \mathcal{Z}/\pi(\theta)$
            \end{itemize}
        \end{columns}
        \vspace{5pt}

        \begin{itemize}
            \item Library of pre-trained bijectors to be used as priors/emulators/nuisance marginalised likelihoods
            \item e.g. easy to apply a \textit{Planck}/DES/HERA/JWST prior or likelihood to your existing MCMC chains without needing to install the whole cosmology machinery.
        \end{itemize}

        \column{0.4\textwidth}
        \begin{tikzpicture}[
                rednode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
                bluenode/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
                greennode/.style={rectangle, draw=green!60, very thick, minimum size=5mm},
                node distance=0.5cm,
                remember picture, overlay
            ]
            \node<2->[bluenode, xshift=0.5\textwidth, yshift=-0.25\textwidth](likelihood) at (current page.north)  {$ \mathcal{L}(\theta,\alpha)$};
            \node<3->[bluenode, right = of likelihood.east](prior) {$ \pi(\theta,\alpha)$};

            \coordinate<3-> (likelihoodprior) at ($(likelihood.south)!0.5!(prior.south)$);

            \node<4->[rednode, below = of likelihoodprior](nestedsampling) {Nested Sampling};
            \draw<4->[->](likelihood.south) -- (likelihood|-nestedsampling.north);
            \draw<4->[->](prior.south) -- (prior|-nestedsampling.north);

            \node<5->[bluenode, below = of nestedsampling](posterior) {$ \{\theta,\alpha\}_\mathcal{P}$};
            \draw<5->[->](nestedsampling.south-|posterior) -- (posterior.north);
            \node<6->[bluenode, left = of posterior.west](evidence) {$ \mathcal{Z}$};
            \draw<6->[->](nestedsampling.south-|likelihood) -- (evidence.north);
            \node<7->[bluenode, right = of posterior.east](priorSamples) {$ \{\theta,\alpha\}_\pi$};
            \draw<7->[->](nestedsampling.south-|prior) -- (priorSamples.north);

            \coordinate<7-> (posteriorprior) at ($(posterior.south)!0.5!(priorSamples.south)$);

            \node<8->[rednode, below = of posteriorprior](margarine)  {\texttt{margarine}};

            \draw<8->[->](posterior.south) -- (margarine.north-|posterior.east);
            \draw<8->[->](priorSamples.south) -- (margarine.north-|priorSamples.west);

            \node<9->[bluenode, below = of posterior|-margarine.south](marginalPosterior) {$ \mathcal{P}_\text{emu}(\theta)$};


            \draw<9->[->](margarine.south-|marginalPosterior.east) -- (marginalPosterior.north);


            \node<10->[bluenode, below = of marginalPosterior.south-|margarine.south-|priorSamples](marginalPrior) {$ \pi_\text{emu}(\theta)$};
            \draw<10->[->](margarine.south-|priorSamples.west) -- (marginalPrior.north);


            \node<11->[bluenode, below = of marginalPosterior](marginalLikelihood) {$ \mathcal{L}_\text{emu}(\theta)$};


            \draw<11->[->](evidence.south) -- (marginalLikelihood.west);
            \draw<11->[->](marginalPosterior.south) -- (marginalLikelihood.north);
            \draw<11->[->](marginalPrior.west) -- (marginalLikelihood.east);

            \node<12->[greennode,behind path,fit=(nestedsampling) (marginalPosterior) (priorSamples) (evidence),] {};

        \end{tikzpicture}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Conclusions}
    \begin{itemize}
        \item DiRAC RAC allocation for building a legacy grid of
            \begin{itemize}
                \item MCMC \& Nested sampling chains
                \item gridded over (pairwise) up-to-date datasets
                \item gridded over extensions to $\Lambda$CDM
                \item Bijectors \& emulators for fast re-use
                \item Importance sampling toolkit via \texttt{anesthetic} for (re)processing
                \item Long-term goal: community repository of chains to share model comparison compute resource
            \end{itemize}
        \item Looking for:
            \begin{itemize}
                \item $\alpha$-testers for \texttt{unimpeded}
                \item Suggestions for more datasets (and their incorporation into \texttt{cobaya})
            \end{itemize}
    \end{itemize}
\end{frame}

\end{document}
```
# branch: remotes/origin/KICC_xmas_2022
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Theory, observation \& cosmological inference}
\subtitle{Introduction to KICC: 5 minutes on Will Handley's research}
\date{7\textsuperscript{th} December 2022}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Research overview}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Theory}
            \begin{itemize}
                \item Early universe cosmology
                \item Modified gravity
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Inference}
            \begin{itemize}
                \item Nested sampling
                \item Likelihood free inference
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Observation}
            \begin{itemize}
                \item REACH
                \item GAMBIT
            \end{itemize}
        \end{block}
    \end{columns}
    
    \begin{columns}[t]
        \column{0.28\textwidth}
        \begin{block}{\texttt{PolyChord}}
            \includegraphics[width=\textwidth]{figures/polychord1.pdf}
            \includegraphics[width=\textwidth]{figures/polychord2.pdf}
        \end{block}
        
        \column{0.35\textwidth}
        \begin{block}{\texttt{fgivenx}}
            \includegraphics[width=\textwidth]{figures/fgivenx.png}
        \end{block}

        \column{0.22\textwidth}
        \begin{block}{\texttt{anesthetic}}
            \includegraphics[width=\textwidth]{figures/anim_1-10.png}
        \end{block}
    \end{columns}



    \hfill Coming soon: \texttt{unimpeded}, \texttt{supernest}
\end{frame}

\begin{frame}
    \frametitle{Theory of the primordial and late-time universe}
    \begin{columns}
        \column{0.5\textwidth}
        \small
        \begin{tabular}{lp{5cm}}
            \raisebox{-.8\totalheight}{\includegraphics[width=50px]{images/metha_prathaban.jpg}}&
            \textbf{Metha Prathaban} (PhD1) \newline
            Palindromic \& two-sheeted universes -- boundary conditions \& Boltzmann solvers.
            \\

            \raisebox{-.7\totalheight}{\includegraphics[width=50px]{images/wei-ning_deng.jpg}}&
            \textbf{Wei-Ning Deng} (PhD1) \newline
            Primordial curvature \& comoving curvature perturbations $\mathcal{R}$.
            \\
            \raisebox{-.8\totalheight}{\includegraphics[width=50px]{images/sinah_legner.jpg}}&
            \textbf{Sinah Legner} (PhD1) 
            \raisebox{-.4\totalheight}{\includegraphics[height=20px]{images/will_barker.jpg}}
            \newline 
            Predictions \& perturbations from gravitational gauge-theories.
            \\

            \raisebox{-.8\totalheight}{\includegraphics[width=50px]{images/danielle_dineen.jpg}}&
            \textbf{Danielle Dineen} (MPhil) \newline
            Israel junction conditions and potential-independent predictions from inflation.
            \\
        \end{tabular}

        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/pps}
        \includegraphics[width=0.49\textwidth]{figures/phi4o3}
        \includegraphics[width=0.49\textwidth]{figures/oscode}
        \includegraphics[width=0.49\textwidth]{figures/R-RST-flat-BD.pdf}
        \includegraphics[width=0.49\textwidth]{figures/CMB.png}
    \end{columns}


\end{frame}

\begin{frame}
    \frametitle{Observation: REACH \& GAMBIT}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{tabular}{lp{5cm}}
            \raisebox{-.7\totalheight}{
                \includegraphics[width=50px]{images/ian_roque.jpg}%
                \begin{minipage}[b]{25px}
                    \includegraphics[width=25px]{images/nima_razavi_ghods.jpg} 
                \end{minipage}
            }&
            \textbf{Ian Roque} (PhD4) \newline
            Bayesian radiometer calibration for the REACH radio telescope.
            \\

            \raisebox{-.7\totalheight}{
                \includegraphics[width=50px]{images/thomas_gessey-jones.jpg}%
                \begin{minipage}[b]{25px}
                    \includegraphics[width=25px]{images/eloy_de_lera_acedo.jpg}\vspace{-1px} 
                    \includegraphics[width=25px]{images/anastasia_fialkov.jpg}%
                \end{minipage}
            }&
            \hspace{-4pt}\textbf{Thomas~Gessey-Jones}~(PhD3) \newline
            REACH 21cm universe theory: Pop III stars \& cosmic rays.
            \\

            \raisebox{-.7\totalheight}{
                \includegraphics[width=50px]{images/harry_bevins.jpg}%
                \begin{minipage}[b]{25px}
                    \includegraphics[width=25px]{images/eloy_de_lera_acedo.jpg}\vspace{-1px} 
                    \includegraphics[width=25px]{images/anastasia_fialkov.jpg}%
                \end{minipage}
            }&
            \textbf{Harry Bevins} (PhD4) \newline
            21cm data analysis and machine learning:
            \texttt{margarine}, \texttt{maxsmooth} \& \texttt{globalemu}.
            \\


            \raisebox{-.7\totalheight}{
                \includegraphics[width=50px]{images/sam_leeney.jpg}%
                \begin{minipage}[b]{25px}
                    \includegraphics[width=25px]{images/eloy_de_lera_acedo.jpg}%
                \end{minipage}
            }&
            \textbf{Sam Leeney} (MPhil) \newline
            Bayesian RFI excision for REACH and pulsars.
            \\
        \end{tabular}

        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/reach_system.pdf}
        \includegraphics[height=0.31\textwidth]{figures/saras.pdf}
        \includegraphics[height=0.31\textwidth]{figures/popIII.pdf}
        \includegraphics[height=0.22\textwidth]{figures/rfi3.pdf}
        \includegraphics[height=0.22\textwidth]{figures/rfi1.pdf}
        \includegraphics[height=0.22\textwidth]{figures/rfi2.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Inference: Nested sampling and Bayesian machine learning}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{tabular}{lp{5cm}}
            \raisebox{-.7\totalheight}{
                \includegraphics[width=50px]{images/adam_ormondroyd.jpg}%
                \begin{minipage}[b]{25px}
                    \includegraphics[width=25px]{images/mike_hobson.jpg}\vspace{-1px} 
                    \includegraphics[width=25px]{images/anthony_lasenby.jpg}
                \end{minipage}
            }&
            \textbf{Adam Ormondroyd} (PhD2) \newline
            Cosmic history reconstructions, clustering in nested sampling.
            \\

            \raisebox{-.7\totalheight}{
                \includegraphics[width=50px]{images/kilian_scheutwinkel.jpg}%
                \begin{minipage}[b]{25px}
                    \includegraphics[width=25px]{images/eloy_de_lera_acedo.jpg}%
                \end{minipage}
            }&
            \textbf{Kilian Scheutwinkel} (PhD2) \newline
            Likelihood-free inference and nested sampling
            \\

            \raisebox{-.5\totalheight}{
                \includegraphics[width=50px]{images/george_carter.jpg}%
                \begin{minipage}[b]{25px}
                    \includegraphics[width=25px]{images/mark_ashdown.jpg}\vspace{-1px}
                    \includegraphics[width=25px]{images/nima_razavi_ghods.jpg}%
                \end{minipage}
            }&
            \textbf{George Carter} (PhD3) \newline 
            Bayesian global sky modelling
            \\

            \raisebox{-.7\totalheight}{
                \includegraphics[width=50px]{images/allahyar_sahibzada.jpg}
            }&
            \textbf{Sahibzada Allahyar} (MPhil) \newline
            High-precision nested sampling, gravitational wave astrometry.
            \\
        \end{tabular}

        \column{0.4\textwidth}
        \includegraphics[height=0.9\textwidth]{figures/tmnre.pdf}
        \includegraphics[height=0.9\textwidth]{figures/bumphunt.pdf}
        \begin{tabular}{lp{3.7cm}}
            \raisebox{-.8\totalheight}{
                \includegraphics[width=50px]{images/david_yallup.jpg}%
            }&
            \hspace{-10pt}\textbf{David Yallup} (PostDoc) \newline
            Bayesian Neural Nets, sparse reconstruction, \& Gaussian Processes\\
        \end{tabular}
    \end{columns}

\end{frame}

\end{document}
```
# branch: remotes/origin/paris_lfi_2022
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley}
\newcommand{\av}[2][]{\left\langle #2\right\rangle_{#1}}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Nested Sampling and Likelihood Free Inference}
\date{21\textsuperscript{st} April 2022}
%Nested Sampling is an established numerical technique for optimising,
%sampling, integrating and scanning *a priori* unknown probability
%distributions. Whilst typically used in the context of traditional
%likelihood-driven Bayesian inference, it's capacity as a general sampler means
%that it is capable of exploring distributions on data [2105.13923] and joint
%spaces [1606.03757].  
%
%In this talk I will give a brief outline of the points
%of difference of nested sampling in comparison with other techniques, what it
%can uniquely offer in tackling the challenge of likelihood-free inference, and
%discuss ongoing work with collaborators in applying it in a variety of
%LFI-based approaches.

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical mathematical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Optimisation}
            \vspace{-5pt}
            \[\theta_\mathrm{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Sampling}
            \vspace{-5pt}
            \[\text{draw }\theta\sim f\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Integration}
            \vspace{-5pt}
            \[\int f(\theta) dV \]
            \vspace{-15pt}
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
            \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
            \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
            \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{MCMC sampling}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Markov chain based methods generate samples from posterior distribution by a stepping procedure
            \item This can get stuck in local peaks
            \item Cannot compute normalisation $\mathcal{Z}$ of Bayes theorem:
                \[ \C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)}\C[1]{P(\theta|M)}}{\C[3]{P(D|M)}}\]
                \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}}\times \C[1]{\pi}}{\C[3]{\mathcal{Z}}} \qquad \C[0]{\text{posterior}} = \frac{\C[2]{\text{likelihood}}\times \C[1]{\text{prior}}}{\C[3]{\text{evidence}}} \]
            \item We generally want the evidence $\C[3]{\mathcal{Z}=P(D|M)}$ for the second stage of inference: model comparison
                \[ P(M|D) = \frac{\C[3]{P(D|M)}P(M)}{P(D)} \qquad \text{Science}(M) = \frac{\C[3]{\mathcal{Z}_M} \Pi_M}{\sum_m \C[3]{\mathcal{Z}}_m \Pi_m} \]
            
        \end{itemize}
        \column{0.4\textwidth}

        \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
        \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
        \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
        \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
        \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
        \includegraphics<6>[width=\textwidth,page=21]{figures/himmelblau}%

    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Nested sampling: completely different way to sample
            \item Ensemble sampling to compress prior to posterior.
            \item Sequentially update a set $S$ of $n$ samples:
                \begin{description}

                    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\pi$). 

                    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new uniform sample with higher likelihood
                \end{description}
            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard likelihood constraint}:
                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}\]
            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence} \& \C[0]{posterior} weights.
        \end{itemize}
        \column{0.4\textwidth}

        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4          >[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<8|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%

    \end{columns}
\end{frame}

\begin{frame}
\frametitle{Mathematics of Nested Sampling}
\framesubtitle{A probabilistic Lebesgque integrator}
  \begin{columns}
      \column{0.5\textwidth}
  \begin{itemize}
    \item At each iteration, the likelihood contour will shrink in volume by  $\approx 1/n$.
    \item Nested sampling zooms in to the peak of the function $\mathcal{L}$ {\em exponentially}.
        \vspace{-5pt}
        \[
            \mathcal{Z} \approx \sum_i \Delta\mathcal{L}_i X_{i}, \quad
            X_{i+1} \approx \frac{n}{n+1}X_i, \quad X_{0} = 1 
        \]
        \vspace{-15pt}
    \item Although this is only approximate, we can quantify the error 
        \vspace{-10pt}
        \[
            P(X_i|X_{i-1}) = \frac{X_{i}^{n-1}}{nX_{i-1}^n}\times[0<X_i<X_{i-1}]
        \]
        \vspace{-15pt}
          \item Integral can be expressed in one of two ways
        \vspace{-10pt}
              \[
                  \mathcal{Z} \approx \sum_i \Delta\mathcal{L}_i X_{i} = \sum_i \mathcal{L}_i \Delta X_{i} 
      \]

  \end{itemize}
      \column{0.5\textwidth}
  \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/lesbesgue}%
  \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/lesbesgue}%
  \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/lesbesgue}%
  \includegraphics<4|handout:0>[width=\textwidth,page=4]{figures/lesbesgue}%
  \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/lesbesgue}%
  \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/lesbesgue}%
  \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/lesbesgue}%
  \includegraphics<8|handout:0>[width=\textwidth,page=8]{figures/lesbesgue}%
  \includegraphics<9|handout:0>[width=\textwidth,page=9]{figures/lesbesgue}%
  \includegraphics<10|handout:0>[width=\textwidth,page=10]{figures/lesbesgue}%
  \includegraphics<11|handout:0>[width=\textwidth,page=11]{figures/lesbesgue}%
  \includegraphics<12|handout:0>[width=\textwidth,page=12]{figures/lesbesgue}%
  \includegraphics<13|handout:0>[width=\textwidth,page=13]{figures/lesbesgue}%
  \includegraphics<14|handout:0>[width=\textwidth,page=14]{figures/lesbesgue}%
  \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/lesbesgue}%
  \includegraphics<16          >[width=\textwidth,page=16]{figures/lesbesgue}%
  \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Dead points: posteriors \& evidences}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded points
            \item These may be weighted to form weighted posterior samples using $w_i = \mathcal{L}_i \Delta X_i$
            \item They can also be used to calculate the normalisation $\mathcal{Z} = \sum \mathcal{L}_i \Delta X_i$, or more generally $\sum_i f(\mathcal{L}_i) \Delta X_i$.
                \begin{itemize}
                    \item Nested sampling probabilistically estimates the volume of the parameter space
                        \[X_i \approx {\left(\frac{n}{n+1}\right)} X_{i-1} \quad\Rightarrow\quad
                        X_i \approx {\left(\frac{n}{n+1}\right)}^i \approx e^{-i/n} \]
                    \item only statistical estimates, but we know the error bar
                    \item Nested sampling thus estimates the density of states
                    \item it is therefore a partition function calculator
                \end{itemize}
            \item The evolving ensemble of live points allows algorithms to perform self-tuning and mode clustering.
        \end{itemize}

        \column{0.4\textwidth}

        \includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        \includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%

    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}
        \column{0.5\textwidth}
        \texttt{MultiNest}
        \includegraphics[width=0.8\textwidth]{figures/multinest}
        \vfill
        \texttt{DNest}
        \includegraphics[width=\textwidth]{figures/dnest}
        \column{0.5\textwidth}
        \texttt{PolyChord}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \vfill
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{How does Nested Sampling compare to other approaches?}
    \begin{columns}
        \column{0.7\textwidth}
    \begin{itemize}
        \item In all cases:
            \begin{itemize}
                \item[$+$] NS can handle multimodal functions
                \item[$+$] NS computes evidences, partition functions and integrals
                \item[$+$] NS is self-tuning/black-box
            \end{itemize}
    \end{itemize}
        \column{0.3\textwidth}
        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Optimisation}
        \begin{itemize}
            \item Gradient descent
                \begin{itemize}
                    \item[$+$] NS does not require gradients
                \end{itemize}
            \item Genetic algorithms
                \begin{itemize}
                    \item[$+$] NS discarded points have statistical meaning
                \end{itemize}
        \end{itemize}
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Sampling}
        \begin{itemize}
            \item Metropolis-Hastings?
                \begin{itemize}
                    \item[$-$] Very little beats a well-tuned, customised MH
                    \item[$+$] NS is self tuning
                \end{itemize}
        \item Hamiltonian Monte Carlo?
        \begin{itemize}
            \item[$-$] In millions of dimensions, HMC is king
            \item[$+$] NS does not require gradients
        \end{itemize}
        \end{itemize}
            \end{block}
        \column{0.33\textwidth}
        \begin{block}{Integration}
            \begin{itemize}
                \item Thermodynamic integration
                    \begin{itemize}
                        \item[$+$] protective against phase trasitions
                        \item[$+$] No annealing schedule tuning 
                    \end{itemize}
                \item Sequential Monte Carlo
                    \begin{itemize}
                        \item[$-$] Some people (SMC experts) classify NS as a kind of SMC
                        \item[$+$] NS is athermal
                    \end{itemize}
            \end{itemize}
    \end{block}
    \end{columns}
\end{frame}



\begin{frame}
    \frametitle{Nested Sampling with Likelihood Free Inference}
    \begin{columns}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}

        \hfill Alsing \textit{et al.}~\arxiv{1903.00007}
    \begin{itemize}
        \item In density estimation likelihood free inference, the output is to learn one/all of:
            \begin{description}
                \item[Likelihood] $P(D|\theta)$
                \item[Posterior] $P(\theta|D)$ 
                \item[Joint] $P(D,\theta)$
            \end{description}
        \item In the first instance, nested sampling can be used to scan these learnt functions
    \end{itemize}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Data are compressed, so joint space $(D,\theta)$ is navigable by off-the-shelf codes. 
            \begin{itemize}
                \item Sanity checking the solution
                \item Computing evidences/Kullback Liebler divergences from likelihoods
            \end{itemize}
        \item Its self-tuning capacity and ability to handle multi-modal distributions can be very useful for diagnosing incompletely learnt functions
        \item Emulated likelihoods (e.g. normalising flows) are generally fast, so can deploy more likelihood hungry techniques like NS.
        \item As Pablo Lemos \& David Yallup will discuss, in principle can use it to train emulators by marginalisation rather than maximisation.
    \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling for Approximate Bayesian Computation/SBI}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Assume one has a generative model capable of turning parameters into mock data $D(\theta)$
            \item Given infinite computing power, ABC works by selecting $\{\theta : D(\theta)=D_\mathrm{observed}\}$
            \item These are samples from the posterior, without using a likelihood.
            \item In practice $D=D_\mathrm{obs}$ becomes $D\approx D_\mathrm{obs}$
            \item i.e. $|D-D_\mathrm{obs}|<\varepsilon$, or more generally $\boxed{\rho(D,D_\mathrm{obs})<\varepsilon}$, where $\rho$ is some suitably chosen objective function
            \item Main challenges are 
                \begin{enumerate}
                    \item Choice of $\rho$/summary stats
                    \item Choice of $\varepsilon$ schedule
                    \item Rejection sampling
                \end{enumerate}
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Nested sampling fits this well: In principle, can just change the usual hard likelihood constraints $\{\theta\sim\pi : \mathcal{L}(\theta)>\mathcal{L}_*\}$ to
                \[\{\theta~\sim\pi : \rho(D(\theta),D_\mathrm{obs})<\varepsilon\}\]
            (Brewer \& Foreman-Mackey~\arxiv{1606.03757})
            \item Ongoing work with Andrew Fowlie \& Sebastian Hoof
                \begin{itemize}
                    \item How to deal with nondeterminism 
                    \item How to interpret $\rho$ as a ``likelihood''
                    \item How to interpret the evidence $\mathcal{Z}$
                \end{itemize}
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling for truncated methods}

    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Will hear more on this tomorrow from Christoph
            \item Many Likelihood implicit approaches at the moment have some element of sampling direct from the prior
            \item Inefficient if number of parameters $>\mathcal{O}(\text{a few})$
            \item Can get round this by truncating to region:
                \[ \Gamma\{ \theta\in \mathrm{supp} p(\theta) \:|\: p(\theta|x_0)>\bar\varepsilon\} \]
            \item At the moment regions defined by nested boxes
            \item This seems ripe for replacement by NS
                \begin{itemize}
                    \item Has anybody tried this?
                    \item If not, why not?
                    \item Why not why not? \hfill(let's talk)
                \end{itemize}
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/tmnre}

        \hfill Cole \textit{et al.}~\arxiv{2111.08030}
    \end{columns}
    
\end{frame}


%\begin{frame}
%    \frametitle{<+Title+>}
%    <+Content+>
%\end{frame}


\begin{frame}
    \frametitle{Nested Sampling: a user's guide}
    \begin{enumerate}
        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
            \begin{itemize}
                \item This means typically most of its time is spent on burn-in rather than posterior sampling
                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable
            \end{itemize}
        \item The number of live points $n_\mathrm{live}$ is a resolution parameter.
            \begin{itemize}
                \item Run time is linear in $n_\mathrm{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\mathrm{live}}}$.
                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
            \end{itemize}
        \item Most algorithms come with additional reliability parameter(s).
            \begin{itemize}
                \item e.g. \texttt{MultiNest}: $\text{eff}$, \texttt{PolyChord}: $n_\mathrm{repeats}$
                \item These are parameters which have no gain if set too conservatively, but increase the reliability
                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
            \end{itemize}
    \end{enumerate}
\end{frame}


\appendix



\begin{frame}
    \frametitle{Occam's Razor~\arxiv{2102.11511}}
    \begin{itemize}
        \item Bayesian inference quantifies Occam's Razor:
            \begin{itemize}
                \item \textit{``Entities are not to be multiplied without necessity''} \hfill --- William of Occam
                \item \textit{``Everything should be kept as simple as possible, but not simpler''} \hfill --- ``Albert Einstein''
            \end{itemize}
        %\item Consider the Evidence $\C[3]{\mathcal{Z}\equiv P(D|M)}$: 
        %    \begin{description}[Parameter estimation]
        %        \item [Parameter estimation] normalisation constant
        %        \item [Model comparison] critical update factor for \C[5]{model prior} to \C[4]{model posterior}
        %    \end{description}
        \item Properties of the evidence: rearrange Bayes' theorem for parameter estimation
            \[\C[0]{\mathcal{P}(\theta)} = \frac{\C[2]{\mathcal{L}(\theta)} \C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}}} \qquad\Rightarrow\qquad \C[3]{\log \mathcal{Z}} = \C[2]{\log\mathcal{L}(\theta)} - \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}} \]  
        \item Evidence is composed of a ``goodness of fit'' term  and ``Occam Penalty''
    \end{itemize}
    \begin{columns}[t]
        \column{0.5\textwidth}
    \begin{itemize}
        \item RHS true for all $\theta$. Take max likelihood value $\theta_*$:
            \[
                \log \mathcal{Z} = -\chi_\mathrm{min}^2 - \text{Mackay penalty}
            \]
    \end{itemize}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Be more Bayesian and take posterior average to get the ``Occam's razor equation''
            \[
                \boxed{
                    \log \mathcal{Z} = \av[\mathcal{P}]{\log\mathcal{L}} - \mathcal{D}_\mathrm{KL}
            }
            \]
    \end{itemize}
    \end{columns}
    \vfill
    \begin{itemize}
        \item Natural regularisation which penalises models with too many parameters.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kullback Liebler divergence}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is is defined as:
                \[\mathcal{D}_\mathrm{KL} = \av[\mathcal{P}]{\log\frac{\mathcal{P}}{\pi}} = \int \mathcal{P}(\theta) \log \frac{\mathcal{P}(\theta)}{\pi(\theta)}d\theta.\]
            \item Whilst not a distance, $\mathcal{D}=0$ when $\mathcal{P}=\pi$.
            \item Occurs in the context of machine learning as an objective function for training functions.
            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
                \[ \mathcal{D}_\mathrm{KL} \approx \log \frac{V_\pi}{V_\mathrm{P}}.\]
                (this is exact for top-hat distributions).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Key tools for Nested Sampling}
    \begin{description}[\& \texttt{UltraNest}]
        \item[\texttt{anesthetic}] Nested sampling post processing \arxiv{1905.04768}\\
        \item[\texttt{insertion}] cross-checks using order statistics \arxiv{2006.03371}
            \hspace{5pt}\url{github.com/williamjameshandley/anesthetic}
        \item[\texttt{nestcheck}] cross-checks using unthreaded runs \arxiv{1804.06406}\\
            \hspace{5pt}\url{github.com/ejhigson/nestcheck}
        \item[\texttt{MultiNest}] Ellipsoidal rejection sampling \arxiv{0809.3437}\\
            \hspace{5pt}\url{github.com/farhanferoz/MultiNest}
        \item[\texttt{PolyChord}] Python/C++/Fortran state of the art \arxiv{1506.00171}\\
            \hspace{5pt}\url{github.com/PolyChord/PolyChordLite} 
        \item[\texttt{dynesty}] Python re-implementation of several codes \arxiv{1904.02180}\\
            \hspace{5pt}\url{github.com/joshspeagle/dynesty}
        \item[\& \texttt{UltraNest}] \hspace{5pt}\url{github.com/JohannesBuchner/UltraNest}
            \arxiv{2101.09604}
    \end{description}
\end{frame}


\end{document}
```
# branch: remotes/origin/paris_maxent_2022
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley}
\newcommand{\av}[2][]{\left\langle #2\right\rangle_{#1}}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Frontiers of Nested Sampling}
\date{20\textsuperscript{th} July 2022}
%Nested Sampling is an established numerical technique for optimising,
%sampling, integrating and scanning *a priori* unknown probability
%distributions. Whilst typically used in the context of traditional
%likelihood-driven Bayesian inference, it's capacity as a general sampler means
%that it is capable of exploring distributions on data [2105.13923] and joint
%spaces [1606.03757].  
%
%In this talk I will give a brief outline of the points
%of difference of nested sampling in comparison with other techniques, what it
%can uniquely offer in tackling the challenge of likelihood-free inference, and
%discuss ongoing work with collaborators in applying it in a variety of
%LFI-based approaches.

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Highlight: state-of-the-art Nature review}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Invented by John Skilling in 2004.
            \item Recent Nature review primer on nested sampling led by Andrew Fowlie and assembled by the community.
            \item Showcases the current set of tools, and applications from chemistry to cosmology.
            \item In this talk
                \begin{itemize}
                    \item Reminder on theory of nested sampling,
                    \item Updates to the meta algorithm since 2004,
                    \item Updates to the set of tools surrounding nested sampling,
                    \item Future research projects.
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/nature1}
        \includegraphics[width=\textwidth]{figures/nature2}
        \column{0.2\textwidth}
        \includegraphics[width=\textwidth]{figures/nature4}
        \includegraphics[width=\textwidth]{figures/nature5}
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical mathematical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Optimisation}
            \vspace{-5pt}
            \[\theta_\mathrm{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Sampling}
            \vspace{-5pt}
            \[\text{draw }\theta\sim f\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Integration}
            \vspace{-5pt}
            \[\int f(\theta) dV \]
            \vspace{-15pt}
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{MCMC sampling}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Markov chain based methods generate samples from posterior distribution by a stepping procedure.
            \item This can get stuck in local peaks.
            \item Cannot compute normalisation $\mathcal{Z}$ of Bayes theorem:
                \[ \C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)}\C[1]{P(\theta|M)}}{\C[3]{P(D|M)}},\]
                \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}}\times \C[1]{\pi}}{\C[3]{\mathcal{Z}}}, \qquad \C[0]{\text{posterior}} = \frac{\C[2]{\text{likelihood}}\times \C[1]{\text{prior}}}{\C[3]{\text{evidence}}}. \]
            \item We generally want the evidence $\C[3]{\mathcal{Z}=P(D|M)}$ for the second stage of inference: model comparison:
                \[ P(M|D) = \frac{\C[3]{P(D|M)}P(M)}{P(D)}, \qquad \text{Science}(M) = \frac{\C[3]{\mathcal{Z}_M} \Pi_M}{\sum_m \C[3]{\mathcal{Z}}_m \Pi_m}. \]

        \end{itemize}
        \column{0.4\textwidth}

        \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
        \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
        \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
        \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
        \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
        \includegraphics<6>[width=\textwidth,page=21]{figures/himmelblau}%

    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Nested sampling: completely different way to sample.
            \item Ensemble sampling to compress prior to posterior.
            \item Sequentially update a set $S$ of $n$ samples:
                \begin{itemize}
                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from the prior $\pi$). 

                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
                \end{itemize}
            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard likelihood constraint}:
                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence} \& \C[0]{posterior} weights.
        \end{itemize}
        \column{0.4\textwidth}

        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4          >[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<8|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%

    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Mathematics of Nested Sampling}
    \framesubtitle{A probabilistic Lebesgque integrator}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item At each iteration, the likelihood contour will shrink in volume by  $\approx 1/n$.
            \item Nested sampling zooms in to the peak of the function $\mathcal{L}$ {\em exponentially}.
                \vspace{-5pt}
                \[
                    \mathcal{Z} \approx \sum_i \Delta\mathcal{L}_i X_{i}, \quad
                    X_{i+1} \approx \frac{n}{n+1}X_i, \quad X_{0} = 1 .
                \]
                \vspace{-15pt}
            \item Although this is only approximate, we can quantify the error 
                \vspace{-10pt}
                \[
                    P(X_i|X_{i-1}) = \frac{X_{i}^{n-1}}{nX_{i-1}^n}\times[0<X_i<X_{i-1}].
                \]
                \vspace{-15pt}
            \item Integral can be discretised in several ways
                \vspace{-10pt}
                \[
                    \mathcal{Z} \approx \sum_i \Delta\mathcal{L}_i X_{i} = \sum_i \mathcal{L}_i \Delta X_{i} = \sum_i \tfrac{\mathcal{L}_i+\mathcal{L}_{i-1}}{2}{\small(X_{i-1}-X_i).}
                \]

        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/lesbesgue}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/lesbesgue}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/lesbesgue}%
        \includegraphics<4|handout:0>[width=\textwidth,page=4]{figures/lesbesgue}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/lesbesgue}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/lesbesgue}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/lesbesgue}%
        \includegraphics<8|handout:0>[width=\textwidth,page=8]{figures/lesbesgue}%
        \includegraphics<9|handout:0>[width=\textwidth,page=9]{figures/lesbesgue}%
        \includegraphics<10|handout:0>[width=\textwidth,page=10]{figures/lesbesgue}%
        \includegraphics<11|handout:0>[width=\textwidth,page=11]{figures/lesbesgue}%
        \includegraphics<12|handout:0>[width=\textwidth,page=12]{figures/lesbesgue}%
        \includegraphics<13|handout:0>[width=\textwidth,page=13]{figures/lesbesgue}%
        \includegraphics<14|handout:0>[width=\textwidth,page=14]{figures/lesbesgue}%
        \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/lesbesgue}%
        \includegraphics<16          >[width=\textwidth,page=16]{figures/lesbesgue}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Dead points: posteriors \& evidences}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded points.
            \item These may be weighted to form weighted posterior samples using $w_i = \mathcal{L}_i \Delta X_i$.
            \item They can also be used to calculate the normalisation $\mathcal{Z} = \sum \mathcal{L}_i \Delta X_i$, or more generally $\sum_i f(\mathcal{L}_i) \Delta X_i$.
                \begin{itemize}
                    \item Nested sampling probabilistically estimates the volume of the parameter space
                        \[X_i \approx {\left(\frac{n}{n+1}\right)} X_{i-1} \quad\Rightarrow\quad
                        X_i \approx {\left(\frac{n}{n+1}\right)}^i \approx e^{-i/n}, \]
                    \item Nested sampling thus estimates the density of states,
                    \item it is therefore a partition function calculator
                        $Z(\beta) = \sum_i \mathcal{L}_i^\beta \Delta X_i$.
                \end{itemize}
            \item The evolving ensemble of live points allows algorithms to perform self-tuning and mode clustering.
        \end{itemize}

        \column{0.4\textwidth}

        \includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        \includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%

    \end{columns}

\end{frame}

\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 
  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item A large fraction of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \item \url{https://projecteuclid.org/euclid.ba/1340370944}.
    \item There has also been much work beyond this (focus of this talk).
  \end{itemize}
 
\end{frame}


\begin{frame}
    \frametitle{Implementations of Nested Sampling}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{dynesty}~\arxiv{1904.02180}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \vfill
    \end{columns}
\end{frame}



\begin{frame}
    \frametitle{Types of nested sampler}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
                \item Constructs bounding region and draws many invalid points until one is found within $L_*$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.5\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{15pt}
    \begin{itemize}
        \item Nested samplers usually come with:
            \begin{itemize}
                \item \emph{resolution} parameter $n_\mathrm{live}$ (which improve results as $\sim\mathcal{O}(n_\mathrm{live}^{-1/2})$.
                    \item set of \emph{reliability} parameters~\arxiv{2101.04525}, which don't improve results if set arbitrarily high, but introduce systematic errors if set too low.
                    \item e.g. \texttt{Multinest} efficiency \texttt{eff} or \texttt{PolyChord} chain length $n_\mathrm{repeats}$.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Dynamic nested sampling}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{itemize}
            \item Small change to meta-algorithm: 
            \item Allow the number of live points to vary at run time \arxiv{1704.03459}.
        \end{itemize}
        \column{0.7\textwidth}
        \includegraphics[width=\textwidth]{figures/dynesty}
    \end{columns}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Separate creation and deletion:
                \begin{itemize}
                    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\pi$). 

                    \item[$S_{i+1}$:] 
                        \begin{itemize}
                            \item Delete the lowest likelihood sample in $S_{i}$ with criterion $D_i$,
                            \item Create  a new uniform sample with higher likelihood with criterion $C_i$.
                        \end{itemize}
                \end{itemize}
            \item Extremely straightforward to implement \\ (just let number of live points $n_i$ vary with $i$).
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Need to specify creation and deletion criteria.
            \item This can prove useful, e.g.
                \begin{itemize}
                    \item Killing off all remaining live points is equivalent to usual correction term,
                    \item Oversampling the prior by \texttt{nprior}.
                \end{itemize}
            \item However, DNS is exactly the right level of complexity to get the uninitiated excited, and does not result in dramatic speedups.
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling post-processing: Weaving and unweaving runs}
    \begin{columns}
        \column{0.7\textwidth}
        \begin{itemize}
            \item There is now a substantial literature on what you can do with a nested sampling run.
            \item John Skilling originally noted that two nested sampling runs can be ``merged'':
                \begin{itemize}
                    \item Take two nested sampling runs on likelihood $\mathcal{L}$ with $n$ live points and $m$ live points each,
                    \item Concatenate the dead points, and re-sort on likelihood,
                    \item The resulting set of points are dead points from an $n+m$-live point run.
                \end{itemize}
            \item To generalise to dynamic nested sampling one needs to record more information.
        \end{itemize}
        \column{0.3\textwidth}
        \includegraphics[width=0.8\textheight,angle=270]{figures/combine}
        \hfill
        \includegraphics[width=0.8\textheight,angle=270]{figures/combine_0}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Birth $\mathcal{L}_*$ and death $\mathcal{L}$ contours}
    \begin{itemize}
        \item A nested sampling run is usually recorded in a database containing parameter values $\theta$, (log)likelihood $\mathcal{L}$, and optionally some expensive derived parameters $\phi$.
        \item The full nested sampling run can be losselessly reconstructed with an extra column.
        \item Namely the set of \textbf{birth contours}, i.e. the $\mathcal{L}_*$ at which each point was born at.
        \item With the lossless compression of columns $\mathcal{L}$ and $\mathcal{L}_*$, one can:
            \begin{itemize}
                \item Compute the dynamic number of live points,
                \item Reconstruct the nested sampling run history,
                \item Decompose an $n$-live point run into $n$ single live point runs.
            \end{itemize}
        \item Single live point runs form the indivisible unit of nested sampling.
        \item This post processing suite is encapsulated in the continuously integrated python packages of \texttt{anesthetic}~\arxiv{1905.04768} and \texttt{nestcheck}~\arxiv{1804.06406}.
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Parameter cross-checks}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item ``Indivisible unit'' of nested sampling identified as the \emph{single live point run} (or \emph{thread}).
            \item When a nested sampling run is unwoven into its constituent threads, it can be recombined into smaller runs.
            \item Cross-checks such as bootstrap resampling can be applied to determine if these are consistent.
            \item This can be used to quantify the residual uncertainty in parameter estimation weights $w_i\approx L_i \Delta X_i$~\arxiv{1704.03459}.
                \begin{itemize}
                    \item in addition to Poisson uncertainty on $X_i$, there is also uncertainty associated with picking $\theta_i$ as representative of the entire $L_i$ contour~\arxiv{0801.3887}.
                \end{itemize}
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/nestcheck}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Insertion indices and order statistics}
    \begin{columns}
        \column{0.5\textwidth}
    \begin{itemize}
        \item At each iteration of nested sampling we generate a new live point and insert it into the list of live points sorted by loglikelihood.
        \item IF we have done things correctly, this should obey \emph{order statistics}.
        \item if it doesn't, our nested sampler is not drawing live points corectly;
        \item We demonstrate this with KS $p$-values~\arxiv{2006.03371}, to test reliability parameters.
        \item Should be extended to be more Bayesian.
        \item Needs extending to dynamic $n_\mathrm{live}$ case.
        \item This can be used at run-time to tune reliability parameters.
    \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/indices}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Frontier: Insertion indices}

\begin{itemize}
    \item For the purposes of estimating volume in a statistical way, we discard the likelihood information, focussing on the ordering of the contours. 
    \item Traditional nested sampling uses the fact that
\[
    P(X_j|X_{j-1}, n_\mathrm{live}) = \frac{n_j}{X_{j-1}}\left( \frac{X_j}{X_{j-1}} \right)^{n_j-1} [0<X_j<X_{j-1}].
    \]
\item marginalised out dependency on everything other than $X_{j-1}$ \&  compressed into a vector encoding the number of live points at each iteration $n_i$. 
\item  \textbf{Frontier}: is ``Skilling compression'' $(\mathcal{L},\mathcal{L}_*)\to n$ lossless or lossy for the purposes of volume estimation?
\item The results presented in~\arxiv{2006.03371} are suggestive that it is losing some useful information, as insertion indexes do provide further information in the context of a cross check (and are in fact a lossless compression of the birth and death contours). 
\item One possibility is that the Skilling compression is lossless in the context of perfect nested sampling, but if a run is biased then you may be able to use insertion indexes to partially correct a biased run. 
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Reversible nested sampling}
    \begin{itemize}
        \item One of the issues preventing nested sampling scaling to millions of dimensions is the need to compress from prior to posterior in all parameters.
            \begin{itemize}
                \item c.f. Skilling's argument that the Entropy/KL divergence is much greater than the width of the typical set/posterior bulk: $\mathcal{D}\sim d \gg \sqrt{d}$.
            \end{itemize}
        \item One could in principle reverse the direction of travel, and move outward from a peak.
        \item If one could guarantee that all peak looks gaussian close enough in (c.f. Laplace approximation), then one can estimate the final volume $X_N$, and reverse the usual argument.
        \item This could in principle be used to dramatically reduce the poisson error if one could estimate the volume in the final set of live points geometrically.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{AEONS: Approximating the end of nested sampling}
    \includegraphics[width=\textwidth]{figures/run_prodecure}
    \vspace{-20pt}
    \begin{itemize}
        \item A seasoned user knows when nested sampling is approaching the end of an HPC run.
        \item As $\Delta\log \mathcal{Z}$ approaches unity, and there are only ``a few'' nested sampling iterations remaining before posterior is crossed.
        \item \textbf{Frontier:} Can we quantify this using Gaussian approximations to make a rough progress bar (with uncertainty quantification) -- summer student working on this.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Importance nested sampling}
    \begin{columns}
    \column{0.5\textwidth}
    \begin{itemize}
        \item Importance sampling is the procedure:
            \begin{itemize}
                \item given a set of (weighted) samples $\{(w_i,\theta_i)\}$ drawn from posterior distribution $\mathcal{P}(\theta)$,
                \item If the likelihood $\mathcal{L}_0(\theta)=P(D_0|\theta)$ is updated with some additional data $D_1$, such that $\mathcal{L}_0 \to \mathcal{L}_0\times \mathcal{L}_1$~\arxiv{1902.04029},
                \item Can we re-weight $w_i$ without re-running a Markov Chain?
            \end{itemize}
        \item well-established for MCMC $w_i \to w_i \times L_1(\theta_i)$.
        \item For nested sampling, evidences are transformable $\mathcal{Z}_0\to \mathcal{Z}_0\times\langle L_1\rangle_{\mathcal{P}_0}$.
        \item Can we do better?
        \item \textbf{Frontier:} Is there a way to re-weight/thin a nested sampling run to recover the equivalent nested sampling run with a new likelihood?
    \end{itemize}
    \column{0.5\textwidth}
    \includegraphics[width=\textwidth]{figures/importance}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Transdimensional nested sampling}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item In some applications it is useful to consider parameter spaces where the number of active parameters vary, e.g.
                \begin{itemize}
                    \item Object detection~\arxiv{0809.3437},
                    \item Free-form reconstruction using \texttt{FlexKnots}~\arxiv{1908.00906}.
                \end{itemize}
            \item At the moment, reasonable performance can be achieved by letting $N$ be a parameter (up to $N_\mathrm{max}$) and then ignoring unused parameters~\arxiv{1506.09024}.
            \item Brendon Brewer~\arxiv{1411.3921} built some examples of RJMCMC diffusive nested sampling, but very problem specific.
            \item \textbf{Frontier:} Is there an ensemble-based methodology for transdimensional NS?
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1          >[width=\textwidth]{figures/recons1}
        \includegraphics<2|handout:0>[width=\textwidth]{figures/recons2}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Frontier: Multi-objective nested sampling}
    \begin{itemize}
        \item Nested sampling is useful as a pure optimiser, particularly for industrial applications where they lack likelihoods but have objective functions.
        \item It still provides a unique statistical interpretation for an arbitrary function $f(\theta)$ by providing a ``parameter compression'' $X_i\approx e^{i/n_\mathrm{live}} \leftrightarrow e^{\sum_j^i{1}/{n_j}}$ for each value of $f_i$.
        \item Also useful for providing a collection of solutions fairly distributed in parameter space once the desired optimum has been met.
        \item This collection of solutions is useful for rudimentary uncertainty quantification, but also for multi-objective optimisation.
        \item \textbf{Frontier}: Is there a more in-built multi-objective optimiser. Can nested sampling optimise two objective functions simultaneously up to a given compression?
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Likelihood values}
    \begin{columns}
        \column{0.5\textwidth}
    \begin{itemize}
        \item One of the virtues of nested sampling is that it only uses the ordering of likelihood values.
        \item \textbf{Frontier:} if one made a smoothness assumption, e.g. $\frac{d\log \mathcal{L}}{d\log X}\approx \mathrm{constant}$, is is possible to do better?
        \item This could dramatically reduce the poisson error, runtime and efficiency of NS.
    \end{itemize}
    \includegraphics[width=\textwidth]{figures/values1}
        \column{0.5\textwidth}
    \includegraphics[width=\textwidth]{figures/values}
    \includegraphics[width=\textwidth]{figures/values2}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Nested Sampling with Likelihood Free Inference}
    \begin{columns}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}

        \hfill Alsing \textit{et al.}~\arxiv{1903.00007}
    \begin{itemize}
        \item In density estimation likelihood free inference, the output is to learn one/all of:
            \begin{description}
                \item[Likelihood] $P(D|\theta)$,
                \item[Posterior] $P(\theta|D)$,
                \item[Joint] $P(D,\theta)$.
            \end{description}
        \item In the first instance, nested sampling can be used to scan these learnt functions.
    \end{itemize}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Data are compressed, so joint space $(D,\theta)$ is navigable by off-the-shelf codes. 
            \begin{itemize}
                \item Sanity checking the solution,
                \item Computing evidences/Kullback Liebler divergences from likelihoods.
            \end{itemize}
        \item Its self-tuning capacity and ability to handle multi-modal distributions can be very useful for diagnosing incompletely learnt functions.
        \item Emulated likelihoods (e.g. normalising flows) are generally fast, so can deploy more likelihood hungry techniques like NS.
        \item In principle can use it to train emulators by marginalisation rather than maximisation.
    \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling for Approximate Bayesian Computation/SBI}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Assume one has a generative model capable of turning parameters into mock data $D(\theta)$.
            \item Given infinite computing power, ABC works by selecting $\{\theta : D(\theta)=D_\mathrm{observed}\}$.
            \item These are samples from the posterior, without using a likelihood.
            \item In practice $D=D_\mathrm{obs}$ becomes $D\approx D_\mathrm{obs}$.
            \item i.e. $|D-D_\mathrm{obs}|<\varepsilon$, or more generally $\boxed{\rho(D,D_\mathrm{obs})<\varepsilon}$, where $\rho$ is some suitably chosen objective function.
            \item Main challenges are 
                \begin{enumerate}
                    \item Choice of $\rho$/summary stats,
                    \item Choice of $\varepsilon$ schedule,
                    \item Rejection sampling.
                \end{enumerate}
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Nested sampling fits this well: In principle, can just change the usual hard likelihood constraints $\{\theta\sim\pi : \mathcal{L}(\theta)>\mathcal{L}_*\}$ to
                \[\{\theta~\sim\pi : \rho(D(\theta),D_\mathrm{obs})<\varepsilon\}\]
            (Brewer \& Foreman-Mackey~\arxiv{1606.03757})
            \item Ongoing work with Andrew Fowlie \& Sebastian Hoof
                \begin{itemize}
                    \item How to deal with nondeterminism,
                    \item How to interpret $\rho$ as a ``likelihood'',
                    \item How to interpret the evidence $\mathcal{Z}$.
                \end{itemize}
        \end{itemize}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{Nested sampling for truncated methods}
%
%    \begin{columns}
%        \column{0.55\textwidth}
%        \begin{itemize}
%            \item Will hear more on this tomorrow from Christoph
%            \item Many Likelihood implicit approaches at the moment have some element of sampling direct from the prior
%            \item Inefficient if number of parameters $>\mathcal{O}(\text{a few})$
%            \item Can get round this by truncating to region:
%                \[ \Gamma\{ \theta\in \mathrm{supp} p(\theta) \:|\: p(\theta|x_0)>\bar\varepsilon\} \]
%            \item At the moment regions defined by nested boxes
%            \item \textbf{Frontier:} This seems ripe for replacement by NS -- PhD student currently working on this (Kilian Scheutwinkel)
%        \end{itemize}
%        \column{0.45\textwidth}
%        \includegraphics[width=\textwidth]{figures/tmnre}
%
%        \hfill Cole \textit{et al.}~\arxiv{2111.08030}
%    \end{columns}
%    
%\end{frame}



\begin{frame}
    \frametitle{Conclusions}
    \vspace{-8pt}
    \begin{itemize}
        \item Nested sampling is a truly unique tool -- the only general-purpose probabalistic integrator
    \end{itemize}
    \vspace{-18pt}
    \begin{columns}[t]
        \small
        \hspace{10pt}
        \column{0.33\textwidth}
        \begin{itemize}
            \itemsep0em
            \item Rejection and chain-based nested sampling,
            \item Dynamic nested sampling,
            \item Unweaving runs,
            \item Birth and death contours,
            \item Importance nested sampling,
        \end{itemize}
        \column{0.33\textwidth}
        \begin{itemize}
            \itemsep0em
            \item Cross checks,
            \item Insertion indices and order statistics,
            \item Reversible nested sampling,
            \item AEONS,
        \end{itemize}
        \column{0.33\textwidth}
        \begin{itemize}
            \itemsep0em
            \item Transdimensional nested sampling,
            \item Multiobjective nested sampling,
            \item Using likelihood values,
            \item Likelihood free inference.
        \end{itemize}
    \end{columns}
    \begin{block}{Watch out for:}
        \vspace{-5pt}
        \begin{description}
            \item[Johannes Buchner (next)] Analysing chain based samplers.
            \item[Nancy Paul] \texttt{nested\_fit} and charged ion spectroscopy.
            \item[Aleksandr Petrosyan (before coffee)] Accelerating nested sampling.
            \item[Livia Partay (after coffee)] Nested sampling in materials science.
            \item[Lune Maillard (after Livia)] Nested sampling and free energy surfaces.
            \item[Harry Bevins (Friday before lunch)] Nested sampling and normalising flows.
        \end{description}
    \end{block}
\end{frame}

\appendix


\begin{frame}
    \frametitle{How does Nested Sampling compare to other approaches?}
    \begin{columns}
        \column{0.7\textwidth}
        \begin{itemize}
            \item In all cases:
                \begin{itemize}
                    \item[$+$] NS can handle multimodal functions
                    \item[$+$] NS computes evidences, partition functions and integrals
                    \item[$+$] NS is self-tuning/black-box
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Optimisation}
            \begin{itemize}
                \item Gradient descent
                    \begin{itemize}
                        \item[$+$] NS does not require gradients
                    \end{itemize}
                \item Genetic algorithms
                    \begin{itemize}
                        \item[$+$] NS discarded points have statistical meaning
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Sampling}
            \begin{itemize}
                \item Metropolis-Hastings?
                    \begin{itemize}
                        \item[$-$] Very little beats a well-tuned, customised MH
                        \item[$+$] NS is self tuning
                    \end{itemize}
                \item Hamiltonian Monte Carlo?
                    \begin{itemize}
                        \item[$-$] In millions of dimensions, HMC is king
                        \item[$+$] NS does not require gradients
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Integration}
            \begin{itemize}
                \item Thermodynamic integration
                    \begin{itemize}
                        \item[$+$] protective against phase trasitions
                        \item[$+$] No annealing schedule tuning 
                    \end{itemize}
                \item Sequential Monte Carlo
                    \begin{itemize}
                        \item[$-$] Some people (SMC experts) classify NS as a kind of SMC
                        \item[$+$] NS is athermal
                    \end{itemize}
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling: a user's guide}
    \begin{enumerate}
        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
            \begin{itemize}
                \item This means typically most of its time is spent on burn-in rather than posterior sampling.
                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable.
            \end{itemize}
        \item The number of live points $n_\mathrm{live}$ is a resolution parameter.
            \begin{itemize}
                \item Run time is linear in $n_\mathrm{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\mathrm{live}}}$.
                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
            \end{itemize}
        \item Most algorithms come with additional reliability parameter(s).
            \begin{itemize}
                \item e.g. \texttt{MultiNest}: $\texttt{eff}$, \texttt{PolyChord}: $n_\mathrm{repeats}$.
                \item These are parameters which have no gain if set too conservatively, but increase the reliability.
                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
            \end{itemize}
    \end{enumerate}
\end{frame}




\end{document}
```
# branch: remotes/origin/reach_2022
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley}

\usepackage{tikz}
\newcommand{\av}[2][]{\left\langle #2\right\rangle_{#1}}
\newcommand{\PR}{\mathcal{P}_\mathcal{R}}
\newcommand{\Pknotj}[1]{\mathcal{P}_{#1}}
\newcommand{\Nknots}{N_\text{knots}}
\newcommand{\nlive}{n_\text{live}}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Statistical methods in cosmology}
%\subtitle{<+subtitle+>}
\date{12\textsuperscript{th} April 2022}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Lessons from the CMB}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item 21cm global signal detection is superficially similar to detecting the primordial CMB.
            \item Both are attempted to detect tiny $\sim 10^{-5}$ cosmological signals hidden beneath a large ``uninteresting'' foreground.
            \item Both measurements are frustrated by complicated contaminants.
            \item Both are amenable to a Bayesian analysis.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/9HXKAsNw5LtSRHFPWfeqiB.jpg}
        \includegraphics[width=\textwidth]{figures/Planck_satellite_cmb.jpg}

    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The three pillars of Bayesian inference}
    \begin{itemize}
        \item Parameter estimation: ``What do the data tell me about my model?'':
            \[ \C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}}, \qquad \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}, \qquad \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}. \]
        \item Model comparison: ``Which model best fits the data?'':
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[6]{P(D)}}, \qquad \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[5]{\Pi_\mathcal{M}}}{\C[6]{\sum_m Z_m \Pi_m}}, \qquad \C[4]{\text{Model Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Model Prior}}}{\C[6]{\text{Normalisation}}}.\]
        \item Tension quantification: ``Are datasets consistent within a given model?'' \arxiv{1902.04029}
        \[ \mathcal{R} = \frac{\mathcal{Z}_{AB}}{\mathcal{Z}_A\mathcal{Z}_\mathcal{B}}, \qquad \log\mathcal{S} = \av[\mathcal{P}_{AB}]{\log\mathcal{L}_{AB}}-\av[\mathcal{P}_{A}]{\log\mathcal{L}_{B}}-\av[\mathcal{P}_{B}]{\log\mathcal{L}_{B}}  \]
    \end{itemize}
\end{frame}

%\begin{frame}
%    \frametitle{Foregrounds and parametric models}
%    \begin{columns}[t]
%        \column{0.5\textwidth}
%        \begin{block}{CMB}
%            \begin{itemize}
%                \item 
%            \end{itemize}
%        \end{block}
%        \column{0.5\textwidth}
%        \begin{block}{21-cm global}
%            \begin{itemize}
%                \item 
%            \end{itemize}
%        \end{block}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{What is a model?}
    \begin{itemize}
        \item Model comparison in its purest form answers question such as:
            \begin{itemize}
                \item ``Is the universe $\Lambda$CDM?''
                \item ``Are neutrinos in a normal or inverted hierarchy?''
                \item ``Is there a detectable global signal in this data?''
            \end{itemize}
        \item However model $\mathcal{M}$ is likelihood $\C[2]{\mathcal{L}=P(D|\theta,M)}$ and priors $\C[1]{\pi=P(\theta|M)}$, $\C[5]{\Pi=P(M)}$
        \item Can use the evidence \C[3]{$\mathcal{Z}$} to decide on which out of a set of likelihoods best describe data (e.g. Gaussian, Cauchy, Poisson, radiometric).
        \item Can also use it for antenna selection~\arxiv{2106.10193}~\arxiv{2109.10098}.
        \item In principle can use it to decide between theoretically motivated priors (care needed)
        \item It can also be used for non-parametric reconstruction:
            \begin{itemize}
                \item ``How many polynomial terms best describe the data?''
                \item ``How complicated a sky model do I need?''
                \item ``Which is the best sky model?''
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
  \frametitle<1-5>{Primordial power spectrum $\PR(k)$ reconstruction~\arxiv{1908.00906}}
  \frametitle<6>{0 internal knots}
  \frametitle<7>{1 internal knot}
  \frametitle<8>{2 internal knots}
  \frametitle<9>{3 internal knots}
  \frametitle<10>{4 internal knots}
  \frametitle<11>{5 internal knots}
  \frametitle<12>{6 internal knots}
  \frametitle<13>{7 internal knots}
  \frametitle<14>{Bayes Factors}
  \frametitle<15>{Marginalised plot}
  \frametitle<16>{Kullback-Liebler divergences}
  %\framesubtitle{Primordial power spectrum $\PR(k)$ reconstruction}



  \begin{columns}
      \column{0.5\textwidth}
      \uncover<1->{
          \begin{itemize}
              \item Traditionally parameterise the primordial power spectrum with $(A_s,n_s)$
                  \[\mathcal{P}_\mathcal{R}(k) = A_s \left( \frac{k}{k_*} \right)^{n_s-1}\]
              \item To add more degrees of freedom, can add ``running'' parameters $n_\mathrm{run}$ (higher order polynomial in index)
              \item Alternative non-parametric technique introduces a more flexible phenomenological parameterisation: ``FlexKnots''
              \item Let the Bayesian evidence decide when you've introduced too many parameters
          \end{itemize}
      }
      \column{0.5\textwidth}

      \only<1-5>{
  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{7}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{4}
    % start coordinate
      \def\xstart{2}
      \def\ystart{5}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{3}
    % end coordinate
      \def\xend{5.5}
      \def\yend{5}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{3.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw<1-5> [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\PR(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2> (mn) -- (mx);
      \draw<2-5> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      \draw<2> (mx) node[below left]     {$(k_{2},\Pknotj{{2}})$};
      \onslide<2-5>{\movablevert{mn}};
      \onslide<2-5>{\movablevert{mx}};

      \draw<3> (mn) -- (start) -- (mx);
      \onslide<3-5>{\movablecross{start}};
      \draw<3-5> (start) node[above right] {$(k_2,\Pknotj{2})$};
      \draw<3> (mx) node[below left]     {$(k_{3},\Pknotj{{3}})$};
 
      \draw<4> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4-5>{\movablecross{mid}};
      \draw<4-5> (mid) node[below right] {$(k_3,\Pknotj{3})$};
      \draw<4> (mx) node[below left]     {$(k_{4},\Pknotj{{4}})$};

      \draw<5> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5>{\movablecross{end}};
      \draw<5> (end) node[above right] {$(k_4,\Pknotj{4})$};
      \draw<5> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};


      %\draw<2-> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-> (mn) node[below right]    {$(k_1,\Pknotj{1})$};
      %\draw<2-> (start) node[above right] {$(k_2,\Pknotj{2})$};
      %\draw<2-> (mid) node[below right]   {$(k_3,\Pknotj{3})$};
      %\draw<2-> (end) node[above right]   {$(k_4,\Pknotj{4})$};
      %\draw<2-> (mx) node[below left]     {$(k_{\Nknots},\Pknotj{{\Nknots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2->{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }
  }




    \includegraphics<6>[width=\textwidth]{figures/pps_both_1}
    \includegraphics<7>[width=\textwidth]{figures/pps_both_2}
    \includegraphics<8>[width=\textwidth]{figures/pps_both_3}
    \includegraphics<9>[width=\textwidth]{figures/pps_both_4}
    \includegraphics<10>[width=\textwidth]{figures/pps_both_5}
    \includegraphics<11>[width=\textwidth]{figures/pps_both_6}
    \includegraphics<12>[width=\textwidth]{figures/pps_both_7}
    \includegraphics<13>[width=\textwidth]{figures/pps_both_8}
    \includegraphics<14>[width=\textwidth]{figures/pps_evidence}
    \includegraphics<15>[width=\textwidth]{figures/pps_both}
    \includegraphics<16>[width=\textwidth]{figures/DKL.pdf}

  \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Occam's Razor~\arxiv{2102.11511}}
    \begin{itemize}
        \item Bayesian inference quantifies Occam's Razor:
            \begin{itemize}
                \item \textit{``Entities are not to be multiplied without necessity''} \hfill --- William of Occam
                \item \textit{``Everything should be kept as simple as possible, but not simpler''} \hfill --- ``Albert Einstein''
            \end{itemize}
        %\item Consider the Evidence $\C[3]{\mathcal{Z}\equiv P(D|M)}$: 
        %    \begin{description}[Parameter estimation]
        %        \item [Parameter estimation] normalisation constant
        %        \item [Model comparison] critical update factor for \C[5]{model prior} to \C[4]{model posterior}
        %    \end{description}
        \item Properties of the evidence: rearrange Bayes' theorem for parameter estimation
            \[\C[0]{\mathcal{P}(\theta)} = \frac{\C[2]{\mathcal{L}(\theta)} \C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}}} \qquad\Rightarrow\qquad \C[3]{\log \mathcal{Z}} = \C[2]{\log\mathcal{L}(\theta)} - \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}} \]  
        \item Evidence is composed of a ``goodness of fit'' term  and ``Occam Penalty''
    \end{itemize}
    \begin{columns}[t]
        \column{0.5\textwidth}
    \begin{itemize}
        \item RHS true for all $\theta$. Take max likelihood value $\theta_*$:
            \[
                \log \mathcal{Z} = -\chi_\mathrm{min}^2 - \text{Mackay penalty}
            \]
    \end{itemize}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Be more Bayesian and take posterior average to get the ``Occam's razor equation''
            \[
                \boxed{
                    \log \mathcal{Z} = \av[\mathcal{P}]{\log\mathcal{L}} - \mathcal{D}_\mathrm{KL}
            }
            \]
    \end{itemize}
    \end{columns}
    \vfill
    \begin{itemize}
        \item Natural regularisation which penalises models with too many parameters.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kullback Liebler divergence}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is is defined as:
                \[\mathcal{D}_\mathrm{KL} = \av[\mathcal{P}]{\log\frac{\mathcal{P}}{\pi}} = \int \mathcal{P}(\theta) \log \frac{\mathcal{P}(\theta)}{\pi(\theta)}d\theta.\]
            \item Whilst not a distance, $\mathcal{D}=0$ when $\mathcal{P}=\pi$.
            \item Occurs in the context of machine learning as an objective function for training functions.
            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
                \[ \mathcal{D}_\mathrm{KL} \approx \log \frac{V_\pi}{V_\mathrm{P}}.\]
                (this is exact for top-hat distributions).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why do sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The cornerstone of numerical Bayesian inference is working with \C[3]{samples}.
            \item Generate a set of representative parameters drawn in proportion to the posterior $\theta\sim\mathcal{P}$.
            \item The magic of marginalisation $\Rightarrow$ perform usual analysis on each sample in turn.
            \item The golden rule is \C[1]{stay in samples} until the last moment before computing summary statistics/triangle plots because \[\boxed{f(\:\av{X}\:)\ne \av{\:f(X)\:}}\]
            \item Generally need $\sim\mathcal{O}(12)$ independent samples to compute a value and error bar.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/samples.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{How to generate samples}
    \begin{itemize}
        \item MCMC!
        \item \href{https://chi-feng.github.io/mcmc-demo/}{chi-feng.github.io/mcmc-demo/}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling: Benefits and drawbacks}
    Relative to traditional numerical posterior samples (Metropolis Hastings, HMC, emcee), nested sampling:
    \begin{description}
        \item[$+$] Can calculate evidence (and therefore perform model comparison).
        \item[$+$] Can calculate KL divergence.
        \item[$+$] Can handle multi-modal distributions.
        \item[$+$] Requires little tuning for an a-priori unseen problem.
        \item[$+$] Highly parallelisable ($n_\mathrm{cores} \sim n_\mathrm{live} \gg 4$).
        \item[$+$] Does not require gradients
        \item[$-$] Slower than a well-tuned posterior sampler.
        \item[$-$] Run time is dependent on prior choice, and priors must be proper \\(some people view this as a feature rather than a bug).
    \end{description}
\end{frame}

\begin{frame}
    \frametitle{The importance of global measures of tension}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item Hubble tension~\arxiv{1907.10625}
                    \begin{itemize}
                        \item \textit{Planck}: $H_0=67.4\pm0.5$
                        \item S$H_0$ES: $H_0=74.0\pm1.4$
                    \end{itemize}
                \item In other situations the discrepancy doesn't exist in a single interpretable parameter
                \item For example: DES+\textit{Planck} \arxiv{1902.04029} 
                \item Are these two datasets in tension?
                \item There are a lot more parameters -- are we sure that tensions aren't hiding? Are we sure we've chosen the best ones to reveal the tension?
                \item Should use ``Suspiciousness'' statistic $\mathcal{S}$, or Bayes ratio $\mathcal{R}$ to determine global tension.
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \includegraphics<1>{figures/DES_planck_1}
            \includegraphics<2>{figures/DES_planck_2}
        \end{column}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Future extensions for REACH}
    \begin{itemize}
        \item Tension quantification for cross validation
            \begin{itemize}
                \item Between experiments
                \item Between REACH antennae
                \item Between different subsets of the REACH timestream
            \end{itemize}
        \item Model marginalisation rather than comparison
        \item FlexKnot reconstructions
        \item Likelihood selection
        \item Occam factors on evidence plots.
        \item Integration of calibration and cosmology pipelines
    \end{itemize}
\end{frame}


%\begin{frame}
%    \frametitle{Nested sampling}
%    \cols[0.6]{
%  \begin{itemize}
%    \item Nested sampling is a completely different way of sampling. 
%    \item Uses ensemble sampling to compress prior to posterior.
%      \item Maintain a set $S$ of $n$ samples, which are sequentially updated:
%  \begin{description}
%      
%    \item[$S_0$:] Generate $n$ samples uniformly over the space (from the prior $\pi$). 
%      
%    \item[$S_{n+1}$:] Delete the lowest likelihood sample in $S_{n}$, and replace it with a new uniform sample with higher likelihood
%  \end{description}
% \item Requires one to be able to sample uniformly within a region, subject to a {\em hard likelihood constraint}.
%  \end{itemize}
%    }{
%        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}
%        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}
%        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}
%        \includegraphics<4          >[width=\textwidth,page=4]{figures/himmelblau}
%        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}
%        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}
%        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}
%        \includegraphics<8|handout:0>[width=\textwidth,page=8]{figures/himmelblau}
%        \includegraphics<9|handout:0>[width=\textwidth,page=14]{figures/himmelblau}
%    }
%\end{frame}
%
%
%\begin{frame}
%    \frametitle{Nested sampling}
%    \cols[0.6]{
%  \begin{itemize}
%      \item At the end, one is left with a set of discarded points
%      \item These may be weighted to form posterior samples
%      \item They can also be used to calculate the normalising constant
%          \begin{itemize}
%              \item Critically, this is because nested sampling probabilistically estimates the volume of the parameter space
%                  \[X_i \approx {\left(\frac{n}{n+1}\right)} X_{i-1} \quad\Rightarrow\quad
%                  X_i \approx {\left(\frac{n}{n+1}\right)}^i \approx e^{-i/n} \]
%              \item only statistical estimates, but we know the error bar
%              \item Nested sampling thus estimates the density of states
%              \item it is therefore a partition function calculator
%          \end{itemize}
%      \item The evolving ensemble of live points allows algorithms to perform self-tuning and mode clustering
%  \end{itemize}
%
%    }{
%        \includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}
%        \includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}
%    }
%
%\end{frame}
%
%\begin{frame}
%    \frametitle{Implementations of Nested Sampling}
%    %\begin{columns}
%    %    \begin{column}{0.33}
%    %        \includegraphics[width=\textwidth]{figures/multinest}
%    %    \end{column} 
%    %\end{columns}
%    \cols[0.5]{%
%        \texttt{MultiNest}
%        \includegraphics[width=0.8\textwidth]{figures/multinest}
%            \vfill
%        \texttt{DNest}
%        \includegraphics[width=\textwidth]{figures/dnest}
%    }{%
%        \texttt{PolyChord}
%        \includegraphics[width=\textwidth]{figures/polychord}
%        \vfill
%        \texttt{NeuralNest}
%            \cols[0.5]{
%                \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
%                \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
%            }{
%                \includegraphics[width=\textwidth]{figures/chains.png}
%            }
%            \vfill
%    }
%\end{frame}
%
%\begin{frame}
%    \frametitle{Nested Sampling: a user's guide}
%    \begin{enumerate}
%        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
%            \begin{itemize}
%                \item This means typically most of its time is spent on burn-in rather than posterior sampling
%                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable
%            \end{itemize}
%        \item The number of live points $n_\mathrm{live}$ is a resolution parameter.
%            \begin{itemize}
%                \item Run time is linear in $n_\mathrm{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\mathrm{live}}}$.
%                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
%            \end{itemize}
%        \item Most algorithms come with additional reliability parameter(s).
%            \begin{itemize}
%                \item e.g. \texttt{MultiNest}: $\text{eff}$, \texttt{PolyChord}: $n_\mathrm{repeats}$
%                \item These are parameters which have no gain if set too conservatively, but increase the reliability
%                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
%            \end{itemize}
%    \end{enumerate}
%\end{frame}
%
%
%\begin{frame}
%    \frametitle{Key tools for Nested Sampling}
%    \begin{description}
%        \item[\texttt{anesthetic}] Nested sampling post processing \arxiv{1905.04768}\\
%        \item[\texttt{insertion}] cross-checks using order statistics \arxiv{2006.03371}
%            \hspace{5pt}\url{github.com/williamjameshandley/anesthetic}
%        \item[\texttt{nestcheck}] cross-checks using unthreaded runs \arxiv{1804.06406}\\
%            \hspace{5pt}\url{github.com/ejhigson/nestcheck}
%        \item[\texttt{MultiNest}] Ellipsoidal rejection sampling \arxiv{0809.3437}\\
%            \hspace{5pt}\url{github.com/farhanferoz/MultiNest}
%        \item[\texttt{PolyChord}] Python/C++/Fortran state of the art \arxiv{1506.00171}\\
%            \hspace{5pt}\url{github.com/PolyChord/PolyChordLite} 
%        \item[\texttt{dynesty}] Python re-implementation of several codes \arxiv{1904.02180}\\
%            \hspace{5pt}\url{github.com/joshspeagle/dynesty}
%    \end{description}
%\end{frame}


%\begin{frame}
%    \frametitle{Likelihood free inference}
%    <+Content+>
%\end{frame}

\begin{frame}
    \frametitle{FAQs}
    
    \begin{itemize}
    \item What was that awesome website? \\
    \hfill Full credit to Chi-feng for this incredible online demonstration tool\\
    \hfill \href{https://chi-feng.github.io/mcmc-demo/}{chi-feng.github.io/mcmc-demo/}

    \item How do you make your plots look hand-drawn? \\
        \vspace{5pt}
        \hfill\parbox{0.5\textwidth}{
            \texttt{import matplotlib.pyplot as plt}
            \texttt{ plt.xkcd()}
        }
    \end{itemize}
\end{frame}



\end{document}
```
# branch: remotes/origin/triumf_2022
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley}
\usepackage{tikz}
\newcommand{\av}[2][]{\left\langle #2\right\rangle_{#1}}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Nested Sampling: a multi-purpose numerical tool \\for particle physics and cosmology}
\date{29\textsuperscript{th} July 2022}
%Nested Sampling is an established numerical technique for optimising,
%sampling, integrating and scanning *a priori* unknown probability
%distributions. Whilst typically used in the context of traditional
%likelihood-driven Bayesian inference, it's capacity as a general sampler means
%that it is capable of exploring distributions on data [2105.13923] and joint
%spaces [1606.03757].  
%
%In this talk I will give a brief outline of the points
%of difference of nested sampling in comparison with other techniques, what it
%can uniquely offer in tackling the challenge of likelihood-free inference, and
%discuss ongoing work with collaborators in applying it in a variety of
%LFI-based approaches.

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Highlight: state-of-the-art Nature review [\href{https://www.nature.com/articles/s43586-022-00121-x}{NatRev}]}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Invented by John Skilling in 2004.
            \item Recent Nature review primer on nested sampling led by Andrew Fowlie and assembled by the community.
            \item Showcases the current set of tools, and applications from chemistry to cosmology.
            \item In this talk
                \begin{itemize}
                    \item User guide to nested sampling
                    \item Particle physics applications
                    \item Cosmology applications
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/nature1}
        \includegraphics[width=\textwidth]{figures/nature2}
        \column{0.2\textwidth}
        \includegraphics[width=\textwidth]{figures/nature4}
        \includegraphics[width=\textwidth]{figures/nature5}
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Optimisation}
            \vspace{-5pt}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Sampling}
            \vspace{-5pt}
            \[\text{draw }\theta\sim f\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Integration}
            \vspace{-5pt}
            \[\int f(\theta) dV \]
            \vspace{-15pt}
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{MCMC sampling}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Markov chain based methods generate samples from distribution by a stepping procedure.
            \item This can get stuck in local peaks.
            \item Cannot compute normalisation $\mathcal{Z}$ of Bayes theorem:
                \[ \C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)}\C[1]{P(\theta|M)}}{\C[3]{P(D|M)}},\]
                \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}}\times \C[1]{\pi}}{\C[3]{\mathcal{Z}}}, \qquad \C[0]{\text{posterior}} = \frac{\C[2]{\text{likelihood}}\times \C[1]{\text{prior}}}{\C[3]{\text{evidence}}}. \]
            \item We generally want the evidence $\C[3]{\mathcal{Z}=P(D|M)}$ for the second stage of inference: model comparison:
                \[ P(M|D) = \frac{\C[3]{P(D|M)}P(M)}{P(D)}, \qquad \text{Science}(M) = \frac{\C[3]{\mathcal{Z}_M} \Pi_M}{\sum_m \C[3]{\mathcal{Z}}_m \Pi_m}. \]

        \end{itemize}
        \column{0.4\textwidth}

        \includegraphics<1|handout:0>[width=\textwidth,page=16]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=17]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=18]{figures/himmelblau}%
        \includegraphics<4|handout:0>[width=\textwidth,page=19]{figures/himmelblau}%
        \includegraphics<5          >[width=\textwidth,page=20]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=21]{figures/himmelblau}%

    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Nested sampling: completely different way to scan.
            \item Ensemble sampling compresses entire space$\to$peak(s).
            \item Sequentially update a set $S$ of $n$ samples:
                \begin{itemize}
                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from a measure $\pi$). 

                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
                \end{itemize}
            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard constraint}:
                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence}/integral of function \& \C[0]{posterior}/sample weights.
        \end{itemize}
        \column{0.4\textwidth}

        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4          >[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<8|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%

    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Integration in high dimensions}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Numerical integration $\int f(x) dV$ in high dimensions is hard.
            \item \texttt{scipy.integrate(...)} is unusable in more than four dimensions.
            \item This is due to the curse of dimensionality: need to sum $\sim N^d$ units to compute $\approx \sum_i f(x_i) \Delta V_i$.
            \item Additionally, estimating volumes with geometry becomes exponentially hard as $d$ increases.
            \item \textit{Aside: \textbf{Riemannian integration} (blue) is taught as standard. An orthogonal approach (red) [usually theoretical] is \textbf{Lesbesgue integration}.}
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/integration}
        \includegraphics[width=\textwidth]{figures/2560px-Riemannvslebesgue.svg.png}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{(Lesbesgue) Integrating with nested sampling}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item At each iteration, the likelihood contour will shrink in volume $X$ by  $\approx 1/n$.
            \item Nested sampling zooms in to the peak of the function $\mathcal{L}$ {\em exponentially}.
                \vspace{-5pt}
                \[
                    \mathcal{Z} \approx \sum_i \Delta\mathcal{L}_i X_{i}, \quad
                    X_{i+1} \approx \frac{n}{n+1}X_i, \quad X_{0} = 1 .
                \]
                \vspace{-15pt}
            \item Although this is only approximate, we can quantify the error 
                \vspace{-10pt}
                \[
                    P(X_i|X_{i-1}) = \frac{X_{i}^{n-1}}{nX_{i-1}^n}\times[0<X_i<X_{i-1}].
                \]
                \vspace{-15pt}
            \item Integral can be discretised in several ways
                \vspace{-10pt}
                \[
                    \mathcal{Z} \approx \sum_i \Delta\mathcal{L}_i X_{i} = \sum_i \mathcal{L}_i \Delta X_{i} = \sum_i \tfrac{\mathcal{L}_i+\mathcal{L}_{i-1}}{2}{\small(X_{i-1}-X_i).}
                \]

        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/lesbesgue}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/lesbesgue}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/lesbesgue}%
        \includegraphics<4|handout:0>[width=\textwidth,page=4]{figures/lesbesgue}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/lesbesgue}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/lesbesgue}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/lesbesgue}%
        \includegraphics<8|handout:0>[width=\textwidth,page=8]{figures/lesbesgue}%
        \includegraphics<9|handout:0>[width=\textwidth,page=9]{figures/lesbesgue}%
        \includegraphics<10|handout:0>[width=\textwidth,page=10]{figures/lesbesgue}%
        \includegraphics<11|handout:0>[width=\textwidth,page=11]{figures/lesbesgue}%
        \includegraphics<12|handout:0>[width=\textwidth,page=12]{figures/lesbesgue}%
        \includegraphics<13|handout:0>[width=\textwidth,page=13]{figures/lesbesgue}%
        \includegraphics<14|handout:0>[width=\textwidth,page=14]{figures/lesbesgue}%
        \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/lesbesgue}%
        \includegraphics<16          >[width=\textwidth,page=16]{figures/lesbesgue}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Dead points: posteriors \& evidences}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded points.
            \item These may be weighted to form weighted posterior samples using $w_i = \mathcal{L}_i \Delta X_i$.
            \item They can also be used to calculate the integral $\mathcal{Z} = \sum \mathcal{L}_i \Delta X_i$, or more generally $\sum_i f(\mathcal{L}_i) \Delta X_i$.
                \begin{itemize}
                    \item Nested sampling probabilistically estimates the volume of the parameter space
                        \[X_i \approx {\left(\frac{n}{n+1}\right)} X_{i-1} \quad\Rightarrow\quad
                        X_i \approx {\left(\frac{n}{n+1}\right)}^i \approx e^{-i/n}, \]
                    \item Nested sampling thus estimates the density of states,
                    \item it is therefore a partition function calculator
                        $Z(\beta) = \sum_i \mathcal{L}_i^\beta \Delta X_i$.
                \end{itemize}
            \item The evolving ensemble of live points allows algorithms to perform self-tuning and mode clustering.
        \end{itemize}

        \column{0.4\textwidth}

        \includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        \includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%

    \end{columns}

\end{frame}

\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 
  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item A large fraction of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \item \url{https://projecteuclid.org/euclid.ba/1340370944}.
    \item There has also been much work beyond this (focus of this talk).
  \end{itemize}
 
\end{frame}


\begin{frame}
    \frametitle{Implementations of Nested Sampling}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{dynesty}~\arxiv{1904.02180}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \vfill
    \end{columns}
\end{frame}



\begin{frame}
    \frametitle{Types of nested sampler}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
                \item Constructs bounding region and draws many invalid points until one is found within $L_*$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.5\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{15pt}
    \begin{itemize}
        \item Nested samplers usually come with:
            \begin{itemize}
                \item \emph{resolution} parameter $n_\text{live}$ (which improve results as $\sim\mathcal{O}(n_\text{live}^{-1/2})$.
                    \item set of \emph{reliability} parameters~\arxiv{2101.04525}, which don't improve results if set arbitrarily high, but introduce systematic errors if set too low.
                    \item e.g. \texttt{Multinest} efficiency \texttt{eff} or \texttt{PolyChord} chain length $n_\text{repeats}$.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Exploration of phase space~\arxiv{2106.02056}}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Nested sampling for cross section computation/event generation.
            \item Numerically compute collisional cross section 
                \vspace{-5pt}\[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2,\]
                $\Omega$ phase space of kinematic configurations $\Phi$, each with matrix element $\mathcal{M}(\Phi)$.
            \item Current state of the art e.g. HAAG (improvement on RAMBO) requires knowledge of $\mathcal{M}(\Phi)$.
            \item Nested sampling can explore the phase space and compute integral blind with comparable efficiency.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1.pdf}
        \includegraphics[width=\textwidth]{figures/phase_space.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
    \begin{columns}
        \column{0.52\textwidth}
        \begin{itemize}
            \item Nested sampling for frequentist computation!?
            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
            \item Need insight/approximation to make efficient.
            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
        \end{itemize}
        \column{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
    \end{columns}
    
\end{frame}

\begin{frame}
    \frametitle{Quantification of fine tuning~\arxiv{2101.00428}~\arxiv{2205.13549}}
    \vspace{-10pt}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Example: Cosmological constraints on decaying axion-like particles~\arxiv{2205.13549}.
            \item Subset of parameters $\xi,m_a,\tau,g_{a\gamma}$: ALP fraction, mass, lifetime and photon coupling.
                {(\small Also vary cosmology, $\tau_n$ and nuisance params)}
            \item Data: CMB, BBN, FIRAS, SMM, BAO.
            \item Standard profile likelihood fit shows ruled out regions and best-fit point.
            \item<2-> Nested sampling scan:
                \begin{itemize}
                    \item Quantifies amount of parameter space ruled out with Kullback-Liebler divergence $\mathcal{D}_\mathrm{KL}$.
                    \item Identifies best fit region as statistically irrelevant from information theory/Bayesian.
                    \item No evidence for decaying ALPs. Fit the data equally well: but more constrained parameters create Occam penalty.
                \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/ALP_1.pdf}
        \includegraphics<2          >[width=\textwidth]{figures/ALP_2.pdf}
        \includegraphics<3|handout:0>[width=\textwidth]{figures/ALP_3.pdf}
    \end{columns}
    
\end{frame}


\begin{frame}
    \frametitle{What is a model?}
    \begin{itemize}
        \item Model comparison in its purest form answers question such as:
            \begin{itemize}
                \item ``Is the universe $\Lambda$CDM?''
                \item ``Are neutrinos in a normal or inverted hierarchy?''
                \item ``Is there a detectable global signal in this data?''
            \end{itemize}
        \item However model $\mathcal{M}$ is likelihood $\C[2]{\mathcal{L}=P(D|\theta,M)}$ and priors $\C[1]{\pi=P(\theta|M)}$, $\C[5]{\Pi=P(M)}$.
        \item Can use the evidence \C[3]{$\mathcal{Z}$} to decide on which out of a set of likelihoods best describe data (e.g. Gaussian, Cauchy, Poisson, radiometric).
        \item Can also use it for antenna selection~\arxiv{2106.10193}~\arxiv{2109.10098}.
        \item In principle can use it to decide between theoretically motivated priors (care needed).
        \item It can also be used for non-parametric reconstruction:
            \begin{itemize}
                \item ``How many polynomial terms best describe the data?''
                \item ``How complicated a sky model do I need?''
                \item ``Which is the best sky model?''
            \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Model comparison and parameter estimation~\arxiv{1908.09139}}

    \begin{columns}

        \begin{column}{0.5\textwidth}

            \begin{itemize}
                \item If you allow $\Omega_K\ne0$, \textit{Planck} (\texttt{plikTTTEEE}) has a moderate preference for closed universes (50:1 betting odds on), $\Omega_K=-4.5\pm1.5\%$
                \item \textit{Planck}+lens+BAO strongly prefer $\Omega_K=0$.
                \item But, \textit{Planck} vs lensing is 2.5$\sigma$ in tension, and Planck vs BAO is 3$\sigma$.
                \item Reduced if $\texttt{plik}\to\texttt{camspec}$~\arxiv{2002.06892} 
                \item BAO and lensing summary assume $\Lambda$CDM.
                \item Doing this properly with BAO retains preference for closed universe (though closer to flat $\Omega_K =-0.4\pm0.2\%$)~\arxiv{2205.05892}.
                \item Present-day curvature has profound consequences for inflation~\arxiv{2205.07374}.
            \end{itemize}

        \end{column}

        \begin{column}{0.5\textwidth}
            \includegraphics<1|handout:0>[width=\textwidth]{figures/curvature_1}%
            \includegraphics<2|handout:0>[width=\textwidth]{figures/curvature_2}%
            \includegraphics<3          >[width=\textwidth]{figures/curvature_3}%

        \end{column}

    \end{columns}

\end{frame}


\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}


\begin{frame}
    \frametitle<1-5>{Primordial power spectrum $\mathcal{P}_\mathcal{R}(k)$ reconstruction~\arxiv{1908.00906}}
  \frametitle<6|handout:0>{0 internal knots}
  \frametitle<7|handout:0>{1 internal knot}
  \frametitle<8|handout:0>{2 internal knots}
  \frametitle<9|handout:0>{3 internal knots}
  \frametitle<10|handout:0>{4 internal knots}
  \frametitle<11|handout:0>{5 internal knots}
  \frametitle<12|handout:0>{6 internal knots}
  \frametitle<13|handout:0>{7 internal knots}
  \frametitle<14|handout:0>{Bayes Factors}
  \frametitle<15|handout:0>{Marginalised plot}
  \frametitle<16|handout:0>{Kullback-Liebler divergences}
  %\framesubtitle{Primordial power spectrum $\mathcal{P}_\mathcal{R}(k)$ reconstruction}


  \begin{columns}
      \column{0.5\textwidth}
      \uncover<1->{
          \begin{itemize}
              \item Traditionally parameterise the primordial power spectrum with $(A_s,n_s)$
                  \[\mathcal{P}_\mathcal{R}(k) = A_s \left( \frac{k}{k_*} \right)^{n_s-1}\]
              \item To add more degrees of freedom, can add ``running'' parameters $n_\text{run}$ (higher order polynomial in index)
              \item Alternative non-parametric technique introduces a more flexible phenomenological parameterisation: ``FlexKnots''
              \item Let the Bayesian evidence decide when you've introduced too many parameters
          \end{itemize}
      }
      \column{0.5\textwidth}

      \only<1-5|handout:0>{
  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{7}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{4}
    % start coordinate
      \def\xstart{2}
      \def\ystart{5}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{3}
    % end coordinate
      \def\xend{5.5}
      \def\yend{5}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{3.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw<1-5|handout:0> [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\mathcal{P}_\mathcal{R}(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1|handout:0> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1|handout:0> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2|handout:0> (mn) -- (mx);
      \draw<2-5|handout:0> (mn) node[below right]    {$(k_1,\mathcal{P}_{1})$};
      \draw<2|handout:0> (mx) node[below left]     {$(k_{2},\mathcal{P}_{{2}})$};
      \onslide<2-5|handout:0>{\movablevert{mn}};
      \onslide<2-5|handout:0>{\movablevert{mx}};

      \draw<3|handout:0> (mn) -- (start) -- (mx);
      \onslide<3-5|handout:0>{\movablecross{start}};
      \draw<3-5|handout:0> (start) node[above right] {$(k_2,\mathcal{P}_{2})$};
      \draw<3|handout:0> (mx) node[below left]     {$(k_{3},\mathcal{P}_{{3}})$};
 
      \draw<4|handout:0> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4-5|handout:0>{\movablecross{mid}};
      \draw<4-5|handout:0> (mid) node[below right] {$(k_3,\mathcal{P}_{3})$};
      \draw<4|handout:0> (mx) node[below left]     {$(k_{4},\mathcal{P}_{{4}})$};

      \draw<5|handout:0> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5|handout:0>{\movablecross{end}};
      \draw<5|handout:0> (end) node[above right] {$(k_4,\mathcal{P}_{4})$};
      \draw<5|handout:0> (mx) node[below left]     {$(k_{N_\text{knots}},\mathcal{P}_{N_\text{knots}})$};


      %\draw<2-|handout:0> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-|handout:0> (mn) node[below right]    {$(k_1,\mathcal{P}_{1})$};
      %\draw<2-|handout:0> (start) node[above right] {$(k_2,\mathcal{P}_{2})$};
      %\draw<2-|handout:0> (mid) node[below right]   {$(k_3,\mathcal{P}_{3})$};
      %\draw<2-|handout:0> (end) node[above right]   {$(k_4,\mathcal{P}_{4})$};
      %\draw<2-|handout:0> (mx) node[below left]     {$(k_{N_\text{knots}},\mathcal{P}_{N_\text{knots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2-|handout:0>{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }
  }




    \includegraphics<6|handout:0>[width=\textwidth]{figures/pps_both_1}
    \includegraphics<7|handout:0>[width=\textwidth]{figures/pps_both_2}
    \includegraphics<8|handout:0>[width=\textwidth]{figures/pps_both_3}
    \includegraphics<9|handout:0>[width=\textwidth]{figures/pps_both_4}
    \includegraphics<10|handout:0>[width=\textwidth]{figures/pps_both_5}
    \includegraphics<11|handout:0>[width=\textwidth]{figures/pps_both_6}
    \includegraphics<12|handout:0>[width=\textwidth]{figures/pps_both_7}
    \includegraphics<13|handout:0>[width=\textwidth]{figures/pps_both_8}
    \includegraphics<14|handout:0>[width=\textwidth]{figures/pps_evidence}
    \includegraphics<15          >[width=\textwidth]{figures/pps_both}
    \includegraphics<16|handout:0>[width=\textwidth]{figures/DKL.pdf}

  \end{columns}
\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology [\href{https://www.nature.com/articles/s41550-022-01709-9}{NatAstro}]}
    \begin{columns}
        \column{0.62\textwidth}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Gives a specific absorption trough, which if detected allows constraints on the physics of the dark ages decade(s) before SKA.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
        \includegraphics[height=0.32\textwidth]{figures/EDGES_antenna}
        \includegraphics[height=0.32\textwidth]{figures/SARAS}
        \includegraphics[height=0.32\textwidth]{figures/REACH_2.jpg}
        \end{itemize}

        \column{0.38\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}


    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology [\href{https://www.nature.com/articles/s41550-022-01709-9}{NatAstro}]}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item EDGES~[\href{https://www.nature.com/articles/nature25792}{Nat}] claimed a controversial 2019 detection.
            \item SARAS3~[\href{https://www.nature.com/articles/s41550-022-01610-5}{NatAstro}]  would have detected this by 2021.
            \item REACH~[\href{https://www.nature.com/articles/s41550-022-01709-9}{NatAstro}] aims to settle the debate.
                \begin{itemize}
                    \item Broader band,
                    \item Honesty about systematic modelling,
                    \item State of the art inference.
                \end{itemize}
            \item Create parameterised models of sky, beam and signal, breaking degeneracy with a time-dependent likelihood to measure all three simultaneously.
            \item Use model comparison based reconstruction to determine complexity of parameterisation.
            \item Use model comparison to select likelihoods.
            \item A collaboration powered by nested sampling.
        \end{itemize}

        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/EDGES}
        \includegraphics[width=0.8\textwidth]{figures/REACH_1}

    \end{columns}

\end{frame}

%\begin{frame}
%    \frametitle{Nested Sampling with Likelihood Free Inference}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}
%
%        \hfill Alsing \textit{et al.}~\arxiv{1903.00007}
%    \begin{itemize}
%        \item In density estimation likelihood free inference, the output is to learn one/all of:
%            \begin{description}
%                \item[Likelihood] $P(D|\theta)$,
%                \item[Posterior] $P(\theta|D)$,
%                \item[Joint] $P(D,\theta)$.
%            \end{description}
%        \item In the first instance, nested sampling can be used to scan these learnt functions.
%    \end{itemize}
%        \column{0.5\textwidth}
%    \begin{itemize}
%        \item Data are compressed, so joint space $(D,\theta)$ is navigable by off-the-shelf codes. 
%            \begin{itemize}
%                \item Sanity checking the solution,
%                \item Computing evidences/Kullback Liebler divergences from likelihoods.
%            \end{itemize}
%        \item Its self-tuning capacity and ability to handle multi-modal distributions can be very useful for diagnosing incompletely learnt functions.
%        \item Emulated likelihoods (e.g. normalising flows) are generally fast, so can deploy more likelihood hungry techniques like NS.
%        \item In principle can use it to train emulators by marginalisation rather than maximisation.
%    \end{itemize}
%    \end{columns}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Nested Sampling for Approximate Bayesian Computation/SBI}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item Assume one has a generative model capable of turning parameters into mock data $D(\theta)$.
%            \item Given infinite computing power, ABC works by selecting $\{\theta : D(\theta)=D_\text{observed}\}$.
%            \item These are samples from the posterior, without using a likelihood.
%            \item In practice $D=D_\text{obs}$ becomes $D\approx D_\text{obs}$.
%            \item i.e. $|D-D_\text{obs}|<\varepsilon$, or more generally $\boxed{\rho(D,D_\text{obs})<\varepsilon}$, where $\rho$ is some suitably chosen objective function.
%            \item Main challenges are 
%                \begin{enumerate}
%                    \item Choice of $\rho$/summary stats,
%                    \item Choice of $\varepsilon$ schedule,
%                    \item Rejection sampling.
%                \end{enumerate}
%        \end{itemize}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item Nested sampling fits this well: In principle, can just change the usual hard likelihood constraints $\{\theta\sim\pi : \mathcal{L}(\theta)>\mathcal{L}_*\}$ to
%                \[\{\theta~\sim\pi : \rho(D(\theta),D_\text{obs})<\varepsilon\}\]
%            (Brewer \& Foreman-Mackey~\arxiv{1606.03757})
%            \item Ongoing work with Andrew Fowlie \& Sebastian Hoof
%                \begin{itemize}
%                    \item How to deal with nondeterminism,
%                    \item How to interpret $\rho$ as a ``likelihood'',
%                    \item How to interpret the evidence $\mathcal{Z}$.
%                \end{itemize}
%        \end{itemize}
%    \end{columns}
%\end{frame}

%\begin{frame}
%    \frametitle{Nested sampling for truncated methods}
%
%    \begin{columns}
%        \column{0.55\textwidth}
%        \begin{itemize}
%            \item Will hear more on this tomorrow from Christoph
%            \item Many Likelihood implicit approaches at the moment have some element of sampling direct from the prior
%            \item Inefficient if number of parameters $>\mathcal{O}(\text{a few})$
%            \item Can get round this by truncating to region:
%                \[ \Gamma\{ \theta\in \text{supp} p(\theta) \:|\: p(\theta|x_0)>\bar\varepsilon\} \]
%            \item At the moment regions defined by nested boxes
%            \item \textbf{Frontier:} This seems ripe for replacement by NS -- PhD student currently working on this (Kilian Scheutwinkel)
%        \end{itemize}
%        \column{0.45\textwidth}
%        \includegraphics[width=\textwidth]{figures/tmnre}
%
%        \hfill Cole \textit{et al.}~\arxiv{2111.08030}
%    \end{columns}
%    
%\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \begin{columns}
        \column{0.65\textwidth}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Performing Bayesian inference and model comparison.
            \end{itemize}
        \item It is applied widely across cosmology and particle physics.
        \item It can be applied to both Bayesian and Frequentist inference.
    \end{itemize}
        \column{0.35\textwidth}
    \includegraphics[height=0.6\textwidth]{figures/pps_both}%
    \includegraphics[height=0.6\textwidth]{figures/REACH_2}%
    \end{columns}
    \includegraphics[height=0.2\textwidth]{figures/integration}%
    \includegraphics[height=0.2\textwidth]{figures/curvature_3}%
    \includegraphics[height=0.2\textwidth]{figures/ALP_3}%
\end{frame}


\appendix
\begin{frame}
    \frametitle{How does Nested Sampling compare to other approaches?}
    \begin{columns}
        \column{0.7\textwidth}
        \begin{itemize}
            \item In all cases:
                \begin{itemize}
                    \item[$+$] NS can handle multimodal functions
                    \item[$+$] NS computes evidences, partition functions and integrals
                    \item[$+$] NS is self-tuning/black-box
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Optimisation}
            \begin{itemize}
                \item Gradient descent
                    \begin{itemize}
                        \item[$+$] NS does not require gradients
                    \end{itemize}
                \item Genetic algorithms
                    \begin{itemize}
                        \item[$+$] NS discarded points have statistical meaning
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Sampling}
            \begin{itemize}
                \item Metropolis-Hastings?
                    \begin{itemize}
                        \item[$-$] Very little beats a well-tuned, customised MH
                        \item[$+$] NS is self tuning
                    \end{itemize}
                \item Hamiltonian Monte Carlo?
                    \begin{itemize}
                        \item[$-$] In millions of dimensions, HMC is king
                        \item[$+$] NS does not require gradients
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Integration}
            \begin{itemize}
                \item Thermodynamic integration
                    \begin{itemize}
                        \item[$+$] protective against phase trasitions
                        \item[$+$] No annealing schedule tuning 
                    \end{itemize}
                \item Sequential Monte Carlo
                    \begin{itemize}
                        \item[$-$] Some people (SMC experts) classify NS as a kind of SMC
                        \item[$+$] NS is athermal
                    \end{itemize}
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling: a user's guide}
    \begin{enumerate}
        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
            \begin{itemize}
                \item This means typically most of its time is spent on burn-in rather than posterior sampling.
                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable.
            \end{itemize}
        \item The number of live points $n_\text{live}$ is a resolution parameter.
            \begin{itemize}
                \item Run time is linear in $n_\text{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\text{live}}}$.
                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
            \end{itemize}
        \item Most algorithms come with additional reliability parameter(s).
            \begin{itemize}
                \item e.g. \texttt{MultiNest}: $\texttt{eff}$, \texttt{PolyChord}: $n_\text{repeats}$.
                \item These are parameters which have no gain if set too conservatively, but increase the reliability.
                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
            \end{itemize}
    \end{enumerate}
\end{frame}




\begin{frame}
    \frametitle{Occam's Razor~\arxiv{2102.11511}}
    \begin{itemize}
        \item Bayesian inference quantifies Occam's Razor:
            \begin{itemize}
                \item \textit{``Entities are not to be multiplied without necessity''} \hfill --- William of Occam
                \item \textit{``Everything should be kept as simple as possible, but not simpler''} \hfill --- ``Albert Einstein''
            \end{itemize}
        %\item Consider the Evidence $\C[3]{\mathcal{Z}\equiv P(D|M)}$: 
        %    \begin{description}[Parameter estimation]
        %        \item [Parameter estimation] normalisation constant
        %        \item [Model comparison] critical update factor for \C[5]{model prior} to \C[4]{model posterior}
        %    \end{description}
        \item Properties of the evidence: rearrange Bayes' theorem for parameter estimation
            \[\C[0]{\mathcal{P}(\theta)} = \frac{\C[2]{\mathcal{L}(\theta)} \C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}}} \qquad\Rightarrow\qquad \C[3]{\log \mathcal{Z}} = \C[2]{\log\mathcal{L}(\theta)} - \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}}.\]  
        \item Evidence is composed of a ``goodness of fit'' term  and ``Occam Penalty''.
    \end{itemize}
    \begin{columns}[t]
        \column{0.5\textwidth}
    \begin{itemize}
        \item RHS true for all $\theta$. Take max likelihood value $\theta_*$:
            \[
                \log \mathcal{Z} = -\chi_\text{min}^2 - \text{Mackay penalty.}
            \]
    \end{itemize}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Be more Bayesian and take posterior average to get the ``Occam's razor equation''
            \[
                \boxed{
                    \log \mathcal{Z} = \av[\mathcal{P}]{\log\mathcal{L}} - \mathcal{D}_\text{KL}.
            }
            \]
    \end{itemize}
    \end{columns}
    \vfill
    \begin{itemize}
        \item Natural regularisation which penalises models with too many parameters.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kullback Liebler divergence}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is is defined as:
                \[\mathcal{D}_\text{KL} = \av[\mathcal{P}]{\log\frac{\mathcal{P}}{\pi}} = \int \mathcal{P}(\theta) \log \frac{\mathcal{P}(\theta)}{\pi(\theta)}d\theta.\]
            \item Whilst not a distance, $\mathcal{D}=0$ when $\mathcal{P}=\pi$.
            \item Occurs in the context of machine learning as an objective function for training functions.
            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
                \[ \mathcal{D}_\text{KL} \approx \log \frac{V_\pi}{V_\text{P}}.\]
                (this is exact for top-hat distributions).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\end{document}
```
# branch: remotes/origin/amsterdam_lfi_2023
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\newcommand{\av}[2][]{\left\langle #2\right\rangle_{#1}}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{High dimensional nested sampling}
\date{26\textsuperscript{th} January 2023}
%Nested Sampling is an established numerical technique for optimising,
%sampling, integrating and scanning *a priori* unknown probability
%distributions. Whilst typically used in the context of traditional
%likelihood-driven Bayesian inference, it's capacity as a general sampler means
%that it is capable of exploring distributions on data [2105.13923] and joint
%spaces [1606.03757].  
%
%In this talk I will give a brief outline of the points
%of difference of nested sampling in comparison with other techniques, what it
%can uniquely offer in tackling the challenge of likelihood-free inference, and
%discuss ongoing work with collaborators in applying it in a variety of
%LFI-based approaches.

\begin{document}


\begin{frame}
    \titlepage
\end{frame}

\section{What is nested sampling?}
\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The three pillars of Bayesian inference}
    Where NS has been used historically\vspace{-10pt}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Parameter estimation}
            What do the data tell us about the parameters of a model?

            \textit{e.g. the size or age of a $\Lambda$CDM universe}
            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Model comparison}
            How much does the data support a particular model?

            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}} \vspace{-7pt}\]
            \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[5]{\Pi_\mathcal{M}}}{\C[7]{\sum_m Z_m \Pi_m}} \]
            \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Tension quantification}
            Do different datasets make consistent predictions from the same model? 
            \textit{e.g. CMB vs Type IA supernovae data}
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
        \end{block}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Why do sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item The cornerstone of numerical Bayesian inference is working with \textbf{samples}.
            \item Generate a set of representative parameters drawn in proportion a distributionto the posterior $\theta\sim\C[0]{\mathcal{P}}$.
            \item The magic of marginalisation $\Rightarrow$ perform usual analysis on each sample in turn.
            \item The golden rule is \C[3]{stay in samples} until the last moment before computing summary statistics/triangle plots because \[\boxed{f(\:\av{X}\:)\ne \av{\:f(X)\:}}\]
            \item Generally need $\sim\mathcal{O}(12)$ independent samples to compute a value and error bar.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/samples.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.48\textwidth}
        \begin{block}{\textbf{MCMC}}
            \only<16>{
                \begin{itemize}
                    \item Single ``walker''
                    \item Explores posterior
                    \item Fast, if proposal matrix is tuned
                    \item Parameter estimation, suspiciousness calculation
                    \item Channel capacity optimised for generating posterior samples
                \end{itemize}
            }
        \end{block}
            \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
            \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
            \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
            \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
            \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
            \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.48\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \only<16>{
                \begin{itemize}
                    \item Ensemble of ``live points''
                    \item Scans from prior to peak of likelihood
                    \item Slower, no tuning required
                    \item Parameter estimation, model comparison, tension quantification
                    \item Channel capacity optimised for computing partition function
                \end{itemize}
            }
        \end{block}
            \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
            \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
            \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
            \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
            \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
            \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
            \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
            \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
            \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} 
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Sequentially update a set $S$ of $n$ samples:
                \begin{itemize}
                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from the prior $\pi$). 

                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
                \end{itemize}
            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard likelihood constraint}:
                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence} \& \C[0]{posterior} weights.
            \item The evolving ensemble of live points allows algorithms to perform self-tuning and mode clustering.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth,page=4]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Probabalistic volume estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Key idea in NS: estimating volumes probabilistically
        \[
            \frac{\C[1]{V_\mathrm{after}}}{\C[0]{V_\mathrm{before}}} 
            \approx \frac{\C[1]{n_\mathrm{in}}}{\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}}
        \]
            \item This is the \textbf{only} way to calculate volume in high dimensions $d>3$.
                \begin{itemize}
                    \item Geometry is exponentially inefficient
                \end{itemize}
            \item If you want to innovate at the frontier of SBI+NS, this is a USP.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/compression_1}%
        \includegraphics<2->[width=\textwidth]{figures/compression_2}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Dead points: posteriors \& evidences}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded points.
            \item These may be weighted to form weighted posterior samples using $w_i = \mathcal{L}_i \Delta X_i$.
            \item They can also be used to calculate the normalisation $\mathcal{Z} = \sum \mathcal{L}_i \Delta X_i$, or more generally $\sum_i f(\mathcal{L}_i) \Delta X_i$.
                \begin{itemize}
                    \item Nested sampling probabilistically estimates the volume of the parameter space
                        \[X_i \approx {\left(\frac{n}{n+1}\right)} X_{i-1} \quad\Rightarrow\quad
                        X_i \approx {\left(\frac{n}{n+1}\right)}^i \approx e^{-i/n}, \]
                    \item Nested sampling thus estimates the density of states,
                    \item it is therefore a partition function calculator
                        $Z(\beta) = \sum_i \mathcal{L}_i^\beta \Delta X_i$.
                \end{itemize}
        \end{itemize}

        \column{0.4\textwidth}

        \includegraphics[width=\textwidth,page=15]{figures/himmelblau}%

    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Time complexity of nested sampling}
    \includegraphics[width=\textwidth]{figures/run_prodecure}
    \vspace{-20pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-20pt}
    \begin{itemize}
        \item $x$-axis: log-compression of live points
        \item Area $\propto$ posterior mass
        \item Shows Bayesian balance of likelihood vs prior
        \item Run proceeds right to left
        \item Run finishes after bump (typical set)
    \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Time complexity\vspace{-5pt}
                \[ \boxed{T = n_\mathrm{live} \times T_\mathcal{L} \times T_\mathrm{sampler} \times \mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)}\vspace{-5pt}  \]
            \item Error complexity $\boxed{\sigma \propto \sqrt{\mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)/n_\mathrm{live}}}$
        \end{itemize}
    \end{columns}

\end{frame}

\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 
  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item A large fraction of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
        \begin{itemize}
            \item \url{https://projecteuclid.org/euclid.ba/1340370944}.
        \end{itemize}
    \item There has also been much work beyond this 
        \begin{itemize}
            \item see ``Frontiers of nested sampling'' \url{https://willhandley.co.uk/talks}.
        \end{itemize}
  \end{itemize}
 
\end{frame}


\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \column{0.3\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{dynesty}~\arxiv{1904.02180}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Types of nested sampler}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
\item Constructs bounding region and draws many invalid points until $\mathcal{L}(\theta)>\mathcal{L}_*$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{5pt}
    \begin{itemize}
        \item Nested samplers usually come with:
            \begin{itemize}
                \item \emph{resolution} parameter $n_\mathrm{live}$ (which improve results as $\sim\mathcal{O}(n_\mathrm{live}^{-1/2})$.
                    \item set of \emph{reliability} parameters~\arxiv{2101.04525}, which don't improve results if set arbitrarily high, but introduce systematic errors if set too low.
                    \item e.g. \texttt{Multinest} efficiency \texttt{eff} or \texttt{PolyChord} chain length $n_\mathrm{repeats}$.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling as an optimiser}
    \begin{itemize}
        \item Nested sampling can be used in ``pure optimisation mode'' for a function $f(\theta)$ by:
        \item Turning off the stopping criterion (which only makes sense for likelihoods $f=\mathcal{L}$)
            \begin{itemize}
                \item stopping instead after a fixed number of iterations.
            \end{itemize}
        \item Pros:
            \begin{itemize}
                \item The iteration number $i$ is interpretable as ``log-volume compressed'' $X = e^{-i/n_\mathrm{live}}$
                \item It is excellent at exploring multimodal functions (local optima)
                \item The live points allow an element of multi-objective optimisation, e.g. given a maximum cost, what configurations of nuclear reactor are available?
            \end{itemize}
        \item Cons:
            \begin{itemize}
                \item It is not very fast as an optimiser!
                \item this can be fixed by running a gradient descent/simplex from the final set of live points to ``polish'' the solution (PolyChord has this as an option with \texttt{settings.optimise=True})
            \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{How does Nested Sampling compare to other approaches?}
    \begin{columns}
        \column{0.7\textwidth}
    \begin{itemize}
        \item In all cases:
            \begin{itemize}
                \item[$+$] NS can handle multimodal functions
                \item[$+$] NS computes evidences, partition functions and integrals
                \item[$+$] NS is self-tuning/black-box
            \end{itemize}
    \end{itemize}
        \column{0.3\textwidth}
        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
    \end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
        \begin{itemize}
            \item Gradient descent
                \begin{itemize}
                    \item[$+$] NS does not require gradients
                \end{itemize}
            \item Genetic algorithms
                \begin{itemize}
                    \item[$+$] NS discarded points have statistical meaning
                \end{itemize}
        \end{itemize}
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Sampling}
        \begin{itemize}
            \item Metropolis-Hastings?
                \begin{itemize}
                    \item[$-$] Very little beats a well-tuned, customised MH
                    \item[$+$] NS is self tuning
                \end{itemize}
        \item Hamiltonian Monte Carlo?
        \begin{itemize}
            \item[$-$] In millions of dimensions, HMC is king
            \item[$+$] NS does not require gradients
        \end{itemize}
        \end{itemize}
            \end{block}
        \column{0.33\textwidth}
        \begin{block}{Integration}
            \begin{itemize}
                \item Thermodynamic integration
                    \begin{itemize}
                        \item[$+$] protective against phase trasitions
                        \item[$+$] No annealing schedule tuning 
                    \end{itemize}
                \item Sequential Monte Carlo
                    \begin{itemize}
                        \item[$-$] Some people (SMC experts) classify NS as a kind of SMC
                        \item[$+$] NS is athermal
                    \end{itemize}
            \end{itemize}
    \end{block}
    \end{columns}
\end{frame}



\begin{frame}
    \frametitle{Nested Sampling with Likelihood Free Inference}
    \begin{columns}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}

        \hfill Alsing \textit{et al.}~\arxiv{1903.00007}
    \begin{itemize}
        \item In density estimation likelihood free inference, the output is to learn one/all of:
            \begin{description}
                \item[Likelihood] $P(D|\theta)$
                \item[Posterior] $P(\theta|D)$ 
                \item[Joint] $P(D,\theta)$
            \end{description}
    \end{itemize}
        \column{0.5\textwidth}
    \begin{itemize}
        \item In the first instance, nested sampling can be used to scan these learnt functions.
        \item Data are compressed, so joint space $(D,\theta)$ is navigable by off-the-shelf codes. 
            \begin{itemize}
                \item Sanity checking the solution.
                \item Computing evidences/Kullback Liebler divergences from likelihoods.
            \end{itemize}
        \item Its self-tuning capacity and ability to handle multi-modal distributions can be very useful for diagnosing incompletely learnt functions.
        \item Emulated likelihoods (e.g. normalising flows) are generally fast, so can deploy more likelihood hungry techniques like NS.
    \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling for Approximate Bayesian Computation/SBI}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Assume one has a generative model capable of turning parameters into mock data $D(\theta)$
            \item Given infinite computing power, ABC works by selecting $\{\theta : D(\theta)=D_\mathrm{observed}\}$
            \item These are samples from the posterior, without using a likelihood.
            \item In practice $D=D_\mathrm{obs}$ becomes $D\approx D_\mathrm{obs}$
            \item i.e. $|D-D_\mathrm{obs}|<\varepsilon$, or more generally $\boxed{\rho(D,D_\mathrm{obs})<\varepsilon}$, where $\rho$ is some suitably chosen objective function
            \item Main challenges are 
                \begin{enumerate}
                    \item Choice of $\rho$/summary stats
                    \item Choice of $\varepsilon$ schedule
                    \item Rejection sampling
                \end{enumerate}
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Nested sampling fits this well: In principle, can just change the usual hard likelihood constraints $\{\theta\sim\pi : \mathcal{L}(\theta)>\mathcal{L}_*\}$ to
                \[\{\theta~\sim\pi : \rho(D(\theta),D_\mathrm{obs})<\varepsilon\}\]
            (Brewer \& Foreman-Mackey~\arxiv{1606.03757})
            \item Ongoing work with Andrew Fowlie \& Sebastian Hoof
                \begin{itemize}
                    \item How to deal with nondeterminism 
                    \item How to interpret $\rho$ as a ``likelihood''
                    \item How to interpret the evidence $\mathcal{Z}$
                \end{itemize}
        \end{itemize}
    \end{columns}
\end{frame}
%\begin{frame}
%    \frametitle{<+Title+>}
%    <+Content+>
%\end{frame}


\begin{frame}
    \frametitle{Nested Sampling: a user's guide}
    \begin{enumerate}
        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
            \begin{itemize}
                \item This means typically most of its time is spent on burn-in rather than posterior sampling
                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable
            \end{itemize}
        \item The number of live points $n_\mathrm{live}$ is a resolution parameter.
            \begin{itemize}
                \item Run time is linear in $n_\mathrm{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\mathrm{live}}}$.
                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
                \item Extreme MPI parallelisation means walltime can be made constant.
            \end{itemize}
        \item Most algorithms come with additional reliability parameter(s).
            \begin{itemize}
                \item e.g. \texttt{MultiNest}: $\text{eff}$, \texttt{PolyChord}: $n_\mathrm{repeats}$
                \item These are parameters which have no gain if set too conservatively, but increase the reliability
                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{A note on dynesty}
    \begin{itemize}
        \item \texttt{dynesty} is a pure Python re-implementation of many existing codes 
            \begin{itemize}
                \item (which also now implement dynamic nested sampling)
            \end{itemize}
        \item This means that it just works\textsuperscript{TM} (particularly on OSX).
        \item However:
    \end{itemize}
    \begin{enumerate}
        \item Python overheads can be significant (particularly for fast likelihoods)
        \item It is not very well parallelised in MPI (one of the key advantages of NS)
        \item If you are using the techniques it has reimplemented (\texttt{MultiNest}, \texttt{PolyChord}, \texttt{UltraNest}), you should cite these as well as \texttt{dynesty}!
    \end{enumerate}
    \begin{itemize}
        \item If you are finding \texttt{dynesty} is slow, it may be worth switching!
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Key tools for Nested Sampling}
    \begin{description}[\& \texttt{UltraNest}]
        \item[\texttt{anesthetic}] Nested sampling post processing \arxiv{1905.04768}\\
        \item[\texttt{insertion}] cross-checks using order statistics \arxiv{2006.03371}
            \hspace{5pt}\url{github.com/williamjameshandley/anesthetic}
        \item[\texttt{nestcheck}] cross-checks using unthreaded runs \arxiv{1804.06406}\\
            \hspace{5pt}\url{github.com/ejhigson/nestcheck}
        \item[\texttt{MultiNest}] Ellipsoidal rejection sampling \arxiv{0809.3437}\\
            \hspace{5pt}\url{github.com/farhanferoz/MultiNest}
        \item[\texttt{PolyChord}] Python/C++/Fortran state of the art \arxiv{1506.00171}\\
            \hspace{5pt}\url{github.com/PolyChord/PolyChordLite} 
        \item[\texttt{dynesty}] Python re-implementation of several codes \arxiv{1904.02180}\\
            \hspace{5pt}\url{github.com/joshspeagle/dynesty}
        \item[\& \texttt{UltraNest}] \hspace{5pt}\url{github.com/JohannesBuchner/UltraNest}
            \arxiv{2101.09604}
        \item[\texttt{SuperNest}] Accelerated nested sampling with prior repartitioning \arxiv{2212.01760}
    \end{description}
\end{frame}


\appendix



\begin{frame}
    \frametitle{Occam's Razor~\arxiv{2102.11511}}
    \begin{itemize}
        \item Bayesian inference quantifies Occam's Razor:
            \begin{itemize}
                \item \textit{``Entities are not to be multiplied without necessity''} \hfill --- William of Occam
                \item \textit{``Everything should be kept as simple as possible, but not simpler''} \hfill --- ``Albert Einstein''
            \end{itemize}
        %\item Consider the Evidence $\C[3]{\mathcal{Z}\equiv P(D|M)}$: 
        %    \begin{description}[Parameter estimation]
        %        \item [Parameter estimation] normalisation constant
        %        \item [Model comparison] critical update factor for \C[5]{model prior} to \C[4]{model posterior}
        %    \end{description}
        \item Properties of the evidence: rearrange Bayes' theorem for parameter estimation
            \[\C[0]{\mathcal{P}(\theta)} = \frac{\C[2]{\mathcal{L}(\theta)} \C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}}} \qquad\Rightarrow\qquad \C[3]{\log \mathcal{Z}} = \C[2]{\log\mathcal{L}(\theta)} - \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}} \]  
        \item Evidence is composed of a ``goodness of fit'' term  and ``Occam Penalty''
    \end{itemize}
    \begin{columns}[t]
        \column{0.5\textwidth}
    \begin{itemize}
        \item RHS true for all $\theta$. Take max likelihood value $\theta_*$:
            \[
                \log \mathcal{Z} = -\chi_\mathrm{min}^2 - \text{Mackay penalty}
            \]
    \end{itemize}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Be more Bayesian and take posterior average to get the ``Occam's razor equation''
            \[
                \boxed{
                    \log \mathcal{Z} = \av[\mathcal{P}]{\log\mathcal{L}} - \mathcal{D}_\mathrm{KL}
            }
            \]
    \end{itemize}
    \end{columns}
    \vfill
    \begin{itemize}
        \item Natural regularisation which penalises models with too many parameters.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kullback Liebler divergence}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is is defined as:
                \[\mathcal{D}_\mathrm{KL} = \av[\mathcal{P}]{\log\frac{\mathcal{P}}{\pi}} = \int \mathcal{P}(\theta) \log \frac{\mathcal{P}(\theta)}{\pi(\theta)}d\theta.\]
            \item Whilst not a distance, $\mathcal{D}=0$ when $\mathcal{P}=\pi$.
            \item Occurs in the context of machine learning as an objective function for training functions.
            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
                \[ \mathcal{D}_\mathrm{KL} \approx \log \frac{V_\pi}{V_\mathrm{P}}.\]
                (this is exact for top-hat distributions).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\end{document}
```
# branch: remotes/origin/kcl_2023
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usepackage{pythonhighlight}
\usepackage{tikz}
\usetikzlibrary{calc}
\newcommand{\av}[2][]{\left\langle #2\right\rangle_{#1}}
\newcommand{\student}[3]{
    \tikz[overlay,remember picture]{
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {\includegraphics[width=0.09\textheight]{figures/students/#1}};
        \node[anchor=north east] (B) at (A.north west) {#2};
        \node[anchor=north east] (B) at (B.south east) {\tiny #3};
    }
    \vspace{-0.1\textheight}
}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{ Nested sampling: {\large powering next-generation inference and machine learning tools}}
\subtitle{for cosmology, particle physics and beyond}
\date{18\textsuperscript{th} August 2023}

\begin{document}



\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Highlight: state-of-the-art Nature review [\href{https://www.nature.com/articles/s43586-022-00121-x}{NatRev}]}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Invented by John Skilling in 2004.
            \item Recent Nature review primer on nested sampling led by Andrew Fowlie and assembled by the community.
            \item Showcases the current set of tools, and applications from chemistry to cosmology.
            \item Recent 1.5 day conference in Munich: ``Frontiers of Nested Sampling''
            \item Planned week-long NSCON 2024
            \item In this talk:
                \begin{itemize}
                    \item User guide to nested sampling
                    \item Particle physics applications
                    \item Cosmology applications
                    \item Machine learning applications
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/nature1}
        \includegraphics[width=\textwidth]{figures/nature2}
        \column{0.2\textwidth}
        \includegraphics[width=\textwidth]{figures/nature4}
        \includegraphics[width=\textwidth]{figures/nature5}
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Where is Nested Sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item For many purposes, in your Neural Net you should group Nested Sampling with (MCMC) techniques such as:
                \begin{itemize}
                    \item Metropolis-Hastings (PyMC, MontePython)
                    \item Hamiltonian Monte Carlo (Stan, blackjax)
                    \item Ensemble sampling (emcee, zeus). 
                    \item Variational Inference (Pyro)
                    \item Sequential Monte Carlo 
                    \item Thermodynamic integration
                    \item Genetic algorithms
                \end{itemize}
            \item You may have heard of it branded form:
                \begin{itemize}
                    \item MultiNest
                    \item PolyChord
                    \item dynesty
                    \item ultranest
                \end{itemize}
            \end{itemize}
        \column{0.5\textwidth}
        \begin{columns}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/emcee}
        \includegraphics[width=\textwidth]{figures/metropolis-hastings}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/zeus}
        \end{columns}
        \includegraphics[width=\textwidth]{figures/hmc_explained}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Integration in Physics}
    \begin{itemize}
        \item Integration is a fundamental concept in physics, statistics and data science:
    \end{itemize}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Partition functions}
            \vspace{-11pt}
            \[ Z(\beta) = \int e^{-\beta H(q,p)} dq dp \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Path integrals}
            \[ \Psi = \int e^{i S} \mathcal{D}x \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Bayesian marginals}
            \vspace{-11pt}
            \[ \mathcal{Z}(D) = \int \mathcal{L}(D|\theta) \pi(\theta) d\theta \]
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Need numerical tools if analytic solution unavailable.
            \item High-dimensional numerical integration is hard.
            \item Riemannian strategy estimates volumes geometrically:
                \[ \int f(x) d^nx \approx \sum_i f(x_i) \Delta V_i \sim \mathcal{O}(e^n) \]
            \item Curse of dimensionality $\Rightarrow$ exponential scaling.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Probabalistic volume estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Key idea in NS: estimating volumes probabilistically
        \[
            \frac{\C[1]{V_\mathrm{after}}}{\C[0]{V_\mathrm{before}}} 
            \approx \frac{\C[1]{n_\mathrm{in}}}{\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}}
        \]
            \item This is the \textbf{only} way to calculate volume in high dimensions $d>3$.
                \begin{itemize}
                    \item Geometry is exponentially inefficient.
                \end{itemize}
            \item This estimation process does not depend on geometry, topology or dimensionality
            \item This really is the unique selling point of nested sampling.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/compression_1}%
        \includegraphics<2->[width=\textwidth]{figures/compression_2}%
    \end{columns}
\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.48\textwidth}
        \begin{block}{\textbf{MCMC}}
            \only<16>{
                \begin{itemize}
                    \item Single ``walker''
                    \item Explores posterior
                    \item Fast, if proposal matrix is tuned
                    \item Parameter estimation, suspiciousness calculation
                    \item Channel capacity optimised for generating posterior samples
                \end{itemize}
            }
        \end{block}
            \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
            \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
            \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
            \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
            \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
            \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.48\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \only<16>{
                \begin{itemize}
                    \item Ensemble of ``live points''
                    \item Scans from prior to peak of likelihood
                    \item Slower, no tuning required
                    \item Parameter estimation, model comparison, tension quantification
                    \item Channel capacity optimised for computing partition function
                \end{itemize}
            }
        \end{block}
            \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
            \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
            \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
            \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
            \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
            \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
            \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
            \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
            \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} 
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n$ random samples over the space.
            \item Delete outermost sample, and replace with a new random one at higher integrand value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n}\only<5->{\pm \frac{1}{n}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \sum_i f(x_i) \Delta V_i, \qquad V_i = V_0 e^{-\only<5->{(}i\only<5->{\pm\sqrt{i})}/n} \]
%            \item Nested sampling: completely different way to scan.
%            \item Ensemble sampling compresses entire space$\to$peak(s).
%            \item Sequentially update a set $S$ of $n$ samples:
%                \begin{itemize}
%                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from a measure $\pi$). 
%
%                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
%                \end{itemize}
%            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard constraint}:
%                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
%            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence}/integral of function \& \C[0]{posterior}/sample weights.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4-         >[width=\textwidth,page=4]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: Lebesgue integration}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Nested sampling estimates the \textbf{density of states} and calculates partition functions
                \[Z(\beta) = \sum_i f(x_i)^\beta \Delta V_i\]
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune
                    \item exploration of multimodal functions
                    \item global and local optimisation
                \end{itemize}
            \item For this kind of numerical, generic, high-dimensional integration, it is the only game in town.
            %\item Interpreted as a Bayesian algorithm, it
            %    \begin{itemize}
            %        \item Computes the Bayesian evidence (model comparison)
            %        \item Produces (weighted) posterior samples (parameter estimation)
            %    \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 
  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item A large fraction of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \item \url{https://projecteuclid.org/euclid.ba/1340370944}.
    \item There has also been much work beyond this (see 'frontiers of nested sampling talk' 
        \begin{itemize}
            \item See ``Frontiers of nested sampling'': \href{https://www.willhandley.co.uk/talks}{willhandley.co.uk/talks}
        \end{itemize}
  \end{itemize}
 
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Types of nested sampler}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
\item Constructs bounding region and draws many invalid points until $\mathcal{L}(\theta)>\mathcal{L}_*$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{5pt}
    \begin{itemize}
        \item Nested samplers usually come with:
            \begin{itemize}
                \item \emph{resolution} parameter $n_\mathrm{live}$ (which improve results as $\sim\mathcal{O}(n_\mathrm{live}^{-1/2})$.
                    \item set of \emph{reliability} parameters~\arxiv{2101.04525}, which don't improve results if set arbitrarily high, but introduce systematic errors if set too low.
                    \item e.g. \texttt{Multinest} efficiency \texttt{eff} or \texttt{PolyChord} chain length $n_\mathrm{repeats}$.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Applications: The three pillars of Bayesian inference}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Parameter estimation}
            What do the data tell us about the parameters of a model?

            \textit{e.g. the size or age of a $\Lambda$CDM universe}
            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Model comparison}
            How much does the data support a particular model?

            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}} \vspace{-7pt}\]
            \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[5]{\Pi_\mathcal{M}}}{\C[7]{\sum_m Z_m \Pi_m}} \]
            \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Tension quantification}
            Do different datasets make consistent predictions from the same model? 
            \textit{e.g. CMB vs Type IA supernovae data}
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Cosmology}
    \student{adam_ormondroyd}{Adam Ormondroyd}{PhD}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Battle-tested in Bayesian cosmology on
                \begin{itemize}
                    \item Parameter estimation: multimodal alternative to MCMC samplers.
                    \item Model comparison: using integration to compute the Bayesian evidence
                    \item Tension quantification: using deep tail sampling and suspiciousness computations.
                \end{itemize}
            \item Plays a critical role in major cosmology pipelines: Planck, DES, KiDS, BAO, SNe.
            \item The default $\Lambda$CDM cosmology is well-tuned to have Gaussian-like posteriors for CMB data. 
            \item Less true for alternative cosmologies/models and orthogonal datasets, so nested sampling crucial.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/pps_both}
        \includegraphics[width=0.49\textwidth]{figures/reach_fit-cropped.pdf}
        %\includegraphics[width=0.49\textwidth]{figures/curvature_3}
        \includegraphics[width=\textwidth]{figures/omegak_H0_2.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Astrophysics}
\student{metha_prathaban}{Metha Prathaban}{PhD}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item In exoplanets~\arxiv{1806.00518}
                \begin{itemize}
                    \item Parameter estimation: determining properties of planets.
                    \item Model comparison: how many planets? Stellar modelling~\arxiv{2007.07278}.
                    \item exoplanet problems regularly have posterior phase transitions \arxiv{2102.03387}
                \end{itemize}
            \item In gravitational waves
                \begin{itemize}
                    \item Parameter estimation: Binary merger properties
                    \item Model comparison: Modified theories of gravity, selecting phenomenological parameterisations~\arxiv{1803.10210}
                    \item Likelihood reweighting: fast slow properties
                \end{itemize}
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/rv_full.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Particle physics}
    \student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{columns}
            \column{0.67\textwidth}
            \begin{itemize}
                \item Nested sampling for cross section computation/event generation
            \end{itemize}
            \column{0.3\textwidth}
            \[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2.\]
        \end{columns}
        \begin{itemize}
            \item Nested sampling can explore the phase space $\Omega$ and compute integral blind with comparable efficiency to HAAG/RAMBO~\arxiv{2205.02030}.
            \item Bayesian sparse reconstruction~\arxiv{1809.04598} applied to bump hunting allows evidence-based detection of signals in phenomenological backgrounds~\arxiv{2211.10391}.
            \item Fine tuning quantification
            \item Fast estimation of small $p$-values \arxiv{2106.02056}(PRL), just make switch:\\ $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.

        \end{itemize}
        \column{0.17\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1-pdfjam-crop.pdf}
        \column{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/psi_predict-crop.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Lattice field theory}
\student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Consider standard field theory Lagrangian:
                \[ Z(\beta) = \int D\phi e^{-\beta S(\phi)}, \quad S(\phi) = \int dx^\mu \mathcal{L}(\phi) \]
            \item Discretize onto spacetime grid.
            \item Compute partition function
            \item NS unique traits:
                \begin{itemize}
                    \item Get full partition function for free
                    \item allows for critical tuning
                    \item avoids critical slowing down
                \end{itemize}
            \item Applications in lattice gravity, QCD, condensed matter physics
            \item Publication imminent (next week)
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/potential_shape}
        \includegraphics[width=0.49\textwidth]{figures/2d_phase}
        \includegraphics[width=\textwidth]{figures/lattice_field_theory.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Machine learning}
\student{kamran_javid}{Kamran Javid}{PDRA}
    \begin{columns}
        \column{0.62\textwidth}
        \begin{itemize}
            \item Machine learning requires:
                \begin{itemize}
                    \item Training to find weights
                    \item Choice of architecture/topology/hyperparameters
                \end{itemize}
            \item Bayesian NNs treat training as a model fitting problem
            \item Compute posterior of weights (parameter estimation), rather than optimisation (gradient descent)
            \item Use evidence to determine best architecture (model comparison), correlates with out-of-sample performance! 
            \item Solving the full ``shallow learning'' problem without compromise \arxiv{2004.12211}\arxiv{2211.10391}. 
                \begin{itemize}
                    \item Promising work ongoing to extend this to transfer learning and deep nets.
                \end{itemize}
            \item More generally, dead points are optimally spaced  for training traditional ML approaches.
        \end{itemize}
        \column{0.38\textwidth}
        \includegraphics[width=\textwidth]{figures/nn_posterior-cropped.pdf}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{Applications of nested sampling}
%    \framesubtitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
%    \begin{columns}
%        \column{0.57\textwidth}
%        \begin{itemize}
%            \item Nested sampling for frequentist computation!?
%            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
%            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
%            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
%            \item Need insight/approximation to make efficient.
%            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
%            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
%        \end{itemize}
%        \column{0.43\textwidth}
%        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{And beyond\ldots}
    \only<1-4>{
        \student{watkinson-headshot}{Catherine Watkinson}{Senior Data Scientist}
    }
    \only<5->{
        \student{mcaloone-headshot}{Thomas Mcaloone}{PhD $\to$ Data Scientist}
    }
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/watkinson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mason-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/formanek-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mcaloone-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/stenczel-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/yallup-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/bex-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/claireburke-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/hobson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/lasenby-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mhandley-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/whandley-headshot.jpg}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        \includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/tdoa-cropped-1-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/tdoa-cropped-2-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/tdoa-cropped-3-crop.pdf}%
        \includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology {\small \arxiv{2210.07409}(NatAstro)}}
    \student{ian_roque}{Ian Roque}{PhD}
    \vspace{10pt}
    \begin{columns}
        \column{0.65\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
            \item Lead data analysis team (REACH first light in January)
            \item Nested sampling woven in from the ground up (calibrator, beam modelling, signal fitting, likelihood selection).
            \item All treated as parameterised model comparison problems.
        \includegraphics[height=0.3\textwidth]{figures/EDGES_antenna}
        \includegraphics[height=0.3\textwidth]{figures/SARAS}
        \includegraphics[height=0.3\textwidth]{figures/REACH_2.jpg}
        \end{itemize}
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT: combining particle physics \& cosmological data}
    \begin{columns}
        \column{0.52\textwidth}
        \begin{itemize}
            \item Multinational team of particle physicists, cosmologists and statisticians.
            \item Combine cosmological data, particle colliders, direct detection, \& neutrino detectors in a statistically principled manner~\arxiv{2205.13549}.
            \item Lead Cosmo/Dark Matter working group~\arxiv{2009.03286}.
            \item Nested sampling used for global fitting, and fine-tuning quantification~\arxiv{2101.00428}
        \end{itemize}
        \begin{center}
            \includegraphics[width=0.5\textwidth]{figures/gambit_logo.png}
        \end{center}
        \column{0.48\textwidth}
        \vspace{-40pt}
        \includegraphics[width=\textwidth]{figures/ALP_2.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Likelihood-free inference (aka SBI)}
\student{kilian_scheutwinkel}{Kilian Scheutwinkel}{PhD}

    \vspace{10pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item How do you do inference if you don't know the likelihood $P(D|\theta)$?
                \begin{itemize}
                    \item e.g.\ if you can simulate a disease outbreak, how can you infer a posterior on $R_0$, or select the most predictive model?
                \end{itemize}
            \item If you can forward simulate/model $\theta\to D$, then you have an implicit likelihood.
            \item LFI aims to (machine-)\emph{learn} the likelihood from carefully chosen training data $\{(\theta,D)\}$.
            \item Nested sampling has much to offer
                \begin{itemize}
                    \item truncation strategies (\texttt{PolySwyft})
                    \item evidence driven compression
                    \item marginalised machine learning
                \end{itemize}
            \item In my view, LFI represents the future of inference -- in twenty years time this will be as well-used as MCMC techniques are today.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/noisy.pdf}
        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{unimpeded: PLA for the next generation}
\student{harry_bevins}{Harry Bevins}{PhD$\to$JRF}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item DiRAC 2020 RAC allocation of 30MCPUh
            \item Main goal: Planck Legacy Archive equivalent
            \item Parameter estimation $\to$ Model comparison
            \item MCMC $\to$ Nested sampling
            \item Planck $\to$ $\{\text{Planck}, \text{DESY1}, \text{BAO}, \ldots \}$
            \item Pairwise combinations
            \item Suite of tools for processing these 
                \begin{itemize}
                    \item \texttt{anesthetic} $2.0$
                    \item \texttt{unimpeded} $1.0$
                    \item \texttt{zenodo} archive
                    \item \texttt{margarine}
                \end{itemize}
            \item MCMC chains also available.
            \item Library of bijectors emulators for fast re-use
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{logos/dirac}
        \includegraphics[width=0.5\textwidth,page=21]{figures/himmelblau}%
        \includegraphics[width=0.5\textwidth,page=15]{figures/himmelblau}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{CosmoTension}
    \framesubtitle{Resolving cosmological tensions with diverse data, novel theories and Bayesian machine learning}
\student{will_barker}{Will Barker}{PhD$\to$JRF}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item ERC grant $\Rightarrow$ UKRI Frontier, commencing 2023.
            \item Funds 3 PDRAs and 4 PhDs over 5 years.
            \item Research programme centered around combining novel theories of gravity, Boltzmann solvers~\arxiv{1906.01421}, reconstruction~\arxiv{1908.00906}, nested sampling \& likelihood free inference.
            \item Aims to disentangle cosmological tensions $H_0$, $\sigma_8$, $\Omega_K$ with next-generation data analysis techniques.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/denseoutput.pdf}
        \includegraphics[width=\textwidth]{figures/pps.pdf}
        \includegraphics[width=\textwidth]{figures/gantt.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\href{https://www.github.com/handley-lab}{github.com/handley-lab}}
    \tikz[overlay,remember picture]
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
            \includegraphics[width=0.1\textheight]{figures/students/adam_ormondroyd.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/cole_meldorf.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/danielle_dineen.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/david_yallup.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/dily_ong.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/harry_bevins.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/ian_roque.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/kilian_scheutwinkel.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/metha_prathaban.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/sinah_legner.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/thomas_gessey-jones.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/wei-ning_deng.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/will_barker.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/zixiao_hu.jpg}%
    };
    \vspace{-0.1\textheight}
    \begin{columns}
        \column{0.65\textwidth}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Performing Bayesian inference and model comparison.
            \end{itemize}
        \item It is applied widely across cosmology, particle physics \& machine learning.
        \item It's unique traits as the only numerical Lebesgue integrator mean with compute it will continue to grow in importance.
    \end{itemize}
        \column{0.35\textwidth}
    \includegraphics[height=0.6\textwidth]{figures/pps_both}%
    \includegraphics[height=0.6\textwidth]{figures/REACH_2}%
    \end{columns}
    \includegraphics[height=0.2\textwidth]{figures/integration}%
    \includegraphics[height=0.2\textwidth]{figures/curvature_3}%
    \includegraphics[height=0.2\textwidth]{figures/ALP_3}%
\end{frame}


\appendix
\begin{frame}
    \frametitle{How does Nested Sampling compare to other approaches?}
    \begin{columns}
        \column{0.7\textwidth}
        \begin{itemize}
            \item In all cases:
                \begin{itemize}
                    \item[$+$] NS can handle multimodal functions
                    \item[$+$] NS computes evidences, partition functions and integrals
                    \item[$+$] NS is self-tuning/black-box
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
    \end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \begin{itemize}
                \item Gradient descent
                    \begin{itemize}
                        \item[$-$] NS cannot use gradients
                        \item[$+$] NS does not require gradients
                    \end{itemize}
                \item Genetic algorithms
                    \begin{itemize}
                        \item[$+$] NS discarded points have statistical meaning
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Sampling}
            \begin{itemize}
                \item Metropolis-Hastings?
                    \begin{itemize}
                        \item[$-$] Nothing beats well-tuned customised MH
                        \item[$+$] NS is self tuning
                    \end{itemize}
                \item Hamiltonian Monte Carlo?
                    \begin{itemize}
                        \item[$-$] In millions of dimensions, HMC is king
                        \item[$+$] NS does not require gradients
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \begin{itemize}
                \item Thermodynamic integration
                    \begin{itemize}
                        \item[$+$] protective against phase trasitions
                        \item[$+$] No annealing schedule tuning 
                    \end{itemize}
                \item Sequential Monte Carlo
                    \begin{itemize}
                        \item[$-$] SMC experts classify NS as a kind of SMC
                        \item[$+$] NS is athermal
                    \end{itemize}
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling: a user's guide}
    \begin{enumerate}
        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
            \begin{itemize}
                \item This means typically most of its time is spent on burn-in rather than posterior sampling.
                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable.
            \end{itemize}
        \item The number of live points $n_\text{live}$ is a resolution parameter.
            \begin{itemize}
                \item Run time is linear in $n_\text{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\text{live}}}$.
                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
            \end{itemize}
        \item Most algorithms come with additional reliability parameter(s).
            \begin{itemize}
                \item e.g. \texttt{MultiNest}: $\texttt{eff}$, \texttt{PolyChord}: $n_\text{repeats}$.
                \item These are parameters which have no gain if set too conservatively, but increase the reliability.
                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Time complexity of nested sampling}
    \includegraphics[width=\textwidth]{figures/run_prodecure}
    \vspace{-20pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-20pt}
    \begin{itemize}
        \item $x$-axis: log-compression of live points
        \item Area $\propto$ posterior mass
        \item Shows Bayesian balance of likelihood vs prior
        \item Run proceeds right to left
        \item Run finishes after bump (typical set)
    \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Time complexity\vspace{-5pt}
                \[ \boxed{T = n_\mathrm{live} \times T_\mathcal{L} \times T_\mathrm{sampler} \times \mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)}\vspace{-5pt}  \]
            \item Error complexity $\boxed{\sigma \propto \sqrt{\mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)/n_\mathrm{live}}}$
        \end{itemize}
    \end{columns}

\end{frame}




\begin{frame}
    \frametitle{Occam's Razor~\arxiv{2102.11511}}
    \begin{itemize}
        \item Bayesian inference quantifies Occam's Razor:
            \begin{itemize}
                \item \textit{``Entities are not to be multiplied without necessity''} \hfill --- William of Occam
                \item \textit{``Everything should be kept as simple as possible, but not simpler''} \hfill --- ``Albert Einstein''
            \end{itemize}
        %\item Consider the Evidence $\C[3]{\mathcal{Z}\equiv P(D|M)}$: 
        %    \begin{description}[Parameter estimation]
        %        \item [Parameter estimation] normalisation constant
        %        \item [Model comparison] critical update factor for \C[5]{model prior} to \C[4]{model posterior}
        %    \end{description}
        \item Properties of the evidence: rearrange Bayes' theorem for parameter estimation
            \[\C[0]{\mathcal{P}(\theta)} = \frac{\C[2]{\mathcal{L}(\theta)} \C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}}} \qquad\Rightarrow\qquad \C[3]{\log \mathcal{Z}} = \C[2]{\log\mathcal{L}(\theta)} - \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}}.\]  
        \item Evidence is composed of a ``goodness of fit'' term  and ``Occam Penalty''.
    \end{itemize}
    \begin{columns}[t]
        \column{0.5\textwidth}
    \begin{itemize}
        \item RHS true for all $\theta$. Take max likelihood value $\theta_*$:
            \[
                \log \mathcal{Z} = -\chi_\text{min}^2 - \text{Mackay penalty.}
            \]
    \end{itemize}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Be more Bayesian and take posterior average to get the ``Occam's razor equation''
            \[
                \boxed{
                    \log \mathcal{Z} = \av[\mathcal{P}]{\log\mathcal{L}} - \mathcal{D}_\text{KL}.
            }
            \]
    \end{itemize}
    \end{columns}
    \vfill
    \begin{itemize}
        \item Natural regularisation which penalises models with too many parameters.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kullback Liebler divergence}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is is defined as:
                \[\mathcal{D}_\text{KL} = \av[\mathcal{P}]{\log\frac{\mathcal{P}}{\pi}} = \int \mathcal{P}(\theta) \log \frac{\mathcal{P}(\theta)}{\pi(\theta)}d\theta.\]
            \item Whilst not a distance, $\mathcal{D}=0$ when $\mathcal{P}=\pi$.
            \item Occurs in the context of machine learning as an objective function for training functions.
            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
                \[ \mathcal{D}_\text{KL} \approx \log \frac{V_\pi}{V_\text{P}}.\]
                (this is exact for top-hat distributions).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Nested sampling for frequentist computation!?
            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
            \item Need insight/approximation to make efficient.
            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
    \end{columns}
    
\end{frame}

\begin{frame}
    \frametitle{Exploration of phase space~\arxiv{2106.02056}}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Nested sampling for cross section computation/event generation.
            \item Numerically compute collisional cross section 
                \vspace{-5pt}\[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2,\]
                $\Omega$ phase space of kinematic configurations $\Phi$, each with matrix element $\mathcal{M}(\Phi)$.
            \item Current state of the art e.g. HAAG (improvement on RAMBO) requires knowledge of $\mathcal{M}(\Phi)$.
            \item Nested sampling can explore the phase space and compute integral blind with comparable efficiency.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1.pdf}
        \includegraphics[width=\textwidth]{figures/phase_space.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Quantification of fine tuning~\arxiv{2101.00428}~\arxiv{2205.13549}}
    \vspace{-10pt}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Example: Cosmological constraints on decaying axion-like particles~\arxiv{2205.13549}.
            \item Subset of parameters $\xi,m_a,\tau,g_{a\gamma}$: ALP fraction, mass, lifetime and photon coupling.
                {(\small Also vary cosmology, $\tau_n$ and nuisance params)}
            \item Data: CMB, BBN, FIRAS, SMM, BAO.
            \item Standard profile likelihood fit shows ruled out regions and best-fit point.
            \item<2-> Nested sampling scan:
                \begin{itemize}
                    \item Quantifies amount of parameter space ruled out with Kullback-Liebler divergence $\mathcal{D}_\mathrm{KL}$.
                    \item Identifies best fit region as statistically irrelevant from information theory/Bayesian.
                    \item No evidence for decaying ALPs. Fit the data equally well: but more constrained parameters create Occam penalty.
                \end{itemize}
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/ALP_1.pdf}
        \includegraphics<2          >[width=\textwidth]{figures/ALP_2.pdf}
        \includegraphics<3|handout:0>[width=\textwidth]{figures/ALP_3.pdf}
    \end{columns}
    
\end{frame}

\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}


\begin{frame}
    \frametitle<1-5>{Primordial power spectrum $\mathcal{P}_\mathcal{R}(k)$ reconstruction~\arxiv{1908.00906}}
  \frametitle<6|handout:0>{0 internal knots}
  \frametitle<7|handout:0>{1 internal knot}
  \frametitle<8|handout:0>{2 internal knots}
  \frametitle<9|handout:0>{3 internal knots}
  \frametitle<10|handout:0>{4 internal knots}
  \frametitle<11|handout:0>{5 internal knots}
  \frametitle<12|handout:0>{6 internal knots}
  \frametitle<13|handout:0>{7 internal knots}
  \frametitle<14|handout:0>{Bayes Factors}
  \frametitle<15|handout:0>{Marginalised plot}
  \frametitle<16|handout:0>{Kullback-Liebler divergences}
  %\framesubtitle{Primordial power spectrum $\mathcal{P}_\mathcal{R}(k)$ reconstruction}


  \begin{columns}
      \column{0.55\textwidth}
      \uncover<1->{
          \begin{itemize}
              \item Traditionally parameterise the primordial power spectrum with $(A_s,n_s)$
                  \[\mathcal{P}_\mathcal{R}(k) = A_s \left( \frac{k}{k_*} \right)^{n_s-1}\]
              \item To add more degrees of freedom, can add ``running'' parameters $n_\text{run}$ (higher order polynomial in index)
              \item Alternative non-parametric technique introduces a more flexible phenomenological parameterisation: ``FlexKnots''
              \item Let the Bayesian evidence decide when you've introduced too many parameters
          \end{itemize}
      }
      \column{0.45\textwidth}

      \only<1-5|handout:0>{
  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{7}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{4}
    % start coordinate
      \def\xstart{2}
      \def\ystart{5}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{3}
    % end coordinate
      \def\xend{5.5}
      \def\yend{5}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{3.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw<1-5|handout:0> [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\mathcal{P}_\mathcal{R}(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1|handout:0> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1|handout:0> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2|handout:0> (mn) -- (mx);
      \draw<2-5|handout:0> (mn) node[below right]    {$(k_1,\mathcal{P}_{1})$};
      \draw<2|handout:0> (mx) node[below left]     {$(k_{2},\mathcal{P}_{{2}})$};
      \onslide<2-5|handout:0>{\movablevert{mn}};
      \onslide<2-5|handout:0>{\movablevert{mx}};

      \draw<3|handout:0> (mn) -- (start) -- (mx);
      \onslide<3-5|handout:0>{\movablecross{start}};
      \draw<3-5|handout:0> (start) node[above right] {$(k_2,\mathcal{P}_{2})$};
      \draw<3|handout:0> (mx) node[below left]     {$(k_{3},\mathcal{P}_{{3}})$};
 
      \draw<4|handout:0> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4-5|handout:0>{\movablecross{mid}};
      \draw<4-5|handout:0> (mid) node[below right] {$(k_3,\mathcal{P}_{3})$};
      \draw<4|handout:0> (mx) node[below left]     {$(k_{4},\mathcal{P}_{{4}})$};

      \draw<5|handout:0> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5|handout:0>{\movablecross{end}};
      \draw<5|handout:0> (end) node[above right] {$(k_4,\mathcal{P}_{4})$};
      \draw<5|handout:0> (mx) node[below left]     {$(k_{N_\text{knots}},\mathcal{P}_{N_\text{knots}})$};


      %\draw<2-|handout:0> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-|handout:0> (mn) node[below right]    {$(k_1,\mathcal{P}_{1})$};
      %\draw<2-|handout:0> (start) node[above right] {$(k_2,\mathcal{P}_{2})$};
      %\draw<2-|handout:0> (mid) node[below right]   {$(k_3,\mathcal{P}_{3})$};
      %\draw<2-|handout:0> (end) node[above right]   {$(k_4,\mathcal{P}_{4})$};
      %\draw<2-|handout:0> (mx) node[below left]     {$(k_{N_\text{knots}},\mathcal{P}_{N_\text{knots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2-|handout:0>{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }
  }




    \includegraphics<6|handout:0>[width=\textwidth]{figures/pps_both_1}
    \includegraphics<7|handout:0>[width=\textwidth]{figures/pps_both_2}
    \includegraphics<8|handout:0>[width=\textwidth]{figures/pps_both_3}
    \includegraphics<9|handout:0>[width=\textwidth]{figures/pps_both_4}
    \includegraphics<10|handout:0>[width=\textwidth]{figures/pps_both_5}
    \includegraphics<11|handout:0>[width=\textwidth]{figures/pps_both_6}
    \includegraphics<12|handout:0>[width=\textwidth]{figures/pps_both_7}
    \includegraphics<13|handout:0>[width=\textwidth]{figures/pps_both_8}
    \includegraphics<14|handout:0>[width=\textwidth]{figures/pps_evidence}
    \includegraphics<15          >[width=\textwidth]{figures/pps_both}
    \includegraphics<16|handout:0>[width=\textwidth]{figures/DKL.pdf}

  \end{columns}
\end{frame}


\end{document}
```
# branch: remotes/origin/lcdm_2023
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{fit}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Cosmological inference tools}
\subtitle{Marginal statistics and fully Bayesian forecasts}
\date{12\textsuperscript{th} October 2023}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Marginal inference}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Many cosmological likelihoods come with nuisance parameters that have limited relevance for onward inference.
            \item Notation: $\mathcal{L} = P(D|\theta,\alpha,M)$
                \begin{itemize}
                    \item[$\mathcal{L}$] Likelihood \hfill (e.g. \texttt{plik}),
                    \item[$D$] Data \hfill (e.g. CMB),
                    \item[$\theta$] Cosmological parameters \hfill (e.g. $\Omega_m$, $H_0$\ldots),
                    \item[$\alpha$] Nuisance parameters \hfill (e.g. $A_\text{planck}$\ldots),
                    \item[$M$] Model \hfill (e.g. $\Lambda$CDM).
                \end{itemize}
            \item Some marginal statistics (e.g. marginal means, posteriors\ldots) are easy to compute.
            \item More machinery is needed for e.g. nuisance marginalised likelihoods and marginal KL divergences $\mathcal{D}_\text{KL}$.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/planck_2018_plik.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nuisance marginalised likelihoods: Theory \arxiv{2207.11457}}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item Bayes theorem
                \begin{align}
                    \C[2]{\mathcal{L}}(\theta,\alpha) 
                    \times 
                    \C[1]{\pi}(\theta,\alpha) &= 
                    \C[0]{\mathcal{P}}(\theta,\alpha)
                    \times
                    \C[3]{\mathcal{Z}}\\
                    \C[2]{\text{Likelihood}}
                    \times
                    \C[1]{\text{Prior}}
                    &=
                    \C[0]{\text{Posterior}}
                    \times
                    \C[3]{\text{Evidence}}
                    \nonumber
                \end{align}
                \small{$\alpha$: nuisance parameters, $\theta$: cosmo parameters.}
            \item Marginal Bayes theorem
                \begin{equation}
                    \C[2]{\mathcal{L}}(\theta) 
                    \times 
                    \C[1]{\pi}(\theta) = 
                    \C[0]{\mathcal{P}}(\theta)
                    \times
                    \C[3]{\mathcal{Z}}
                \end{equation}
            \item Non-trivially gives \textbf{nuisance-free likelihood}
                \begin{equation}
                    \boxed{
                        \C[2]{\mathcal{L}}(\theta) 
                        = 
                        \frac{
                            \C[0]{\mathcal{P}}(\theta)
                            \C[3]{\mathcal{Z}}
                        }{
                            \C[1]{\pi}(\theta)
                        }
                    }
                    =
                    \frac{
                        \int \C[2]{\mathcal{L}}(\theta,\alpha) \C[1]{\pi}(\theta,\alpha) d{\alpha}
                    }
                    {
                        \int \C[1]{\pi}(\theta,\alpha) d{\alpha}
                    }
                \end{equation}
        \end{itemize}
        \column{0.5\textwidth}
        \textbf{Key properties}
        \begin{itemize}
            \item Given datasets $A$ and $B$, each with own nuisance parameters $\alpha_A$ and $\alpha_B$:
            \item If you use $\mathcal{L}_A(\theta)$, you get the same (marginal) posterior and evidence if you had run with nuisance parameters $\alpha_A$ (ditto $B$).
            \item If you run inference on $\mathcal{L}_A(\theta)\times\mathcal{L}_B(\theta)$, you get the same (marginal) posterior and evidence if you had run with all nuisance parameters $\alpha_A$, $\alpha_B$ on.
            \item[] \textit{(weak marginal consistency requirements on joint $\pi(\theta,\alpha_A,\alpha_B)$ and marginal priors)}
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nuisance marginalised likelihoods: Practice}
    \student{harry_bevins}{Harry Bevins}{PhD$\to$JRF}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{columns}
            \column{0.3\textwidth}
            \[
                \boxed{
                    \C[2]{\mathcal{L}}(\theta) 
                    = 
                    \frac{
                        \C[0]{\mathcal{P}}(\theta)
                        \C[3]{\mathcal{Z}}
                    }{
                        \C[1]{\pi}(\theta)
                    }
                }
            \]
            \column{0.7\textwidth}
            \begin{itemize}
                \item To compute the nuisance marginalised likelihood, need:
                    \begin{enumerate}
                        \item Bayesian evidence $\C[3]{\mathcal{Z}}$
                        \item Marginal prior and posterior densities
                    \end{enumerate}
            \end{itemize}
        \end{columns}
        \begin{enumerate}
            \item Use nested sampling to compute evidence $\C[3]{\mathcal{Z}}$ and marginal samples $\{\theta,\alpha\}_\mathcal{P}$ and $\{\theta,\alpha\}_\pi$.
            \item Use normalising flows to compute density estimators $\C[0]{\mathcal{P}(\theta)}$, $\C[1]{\pi(\theta)}$ from marginal samples.
        \end{enumerate}
        \begin{itemize}
            %\item Combination termed \texttt{margarine} \arxiv{2205.12841}
            \item Emulators usually much faster than original likelihoods
            \item Library of pre-trained bijectors to be used as priors/emulators/nuisance marginalised likelihoods
            \item e.g. easy to apply a \textit{Planck}/DES/HERA/JWST prior or likelihood to your existing MCMC chains without needing to install the whole cosmology machinery.
        \end{itemize}

        \column{0.4\textwidth}
        \begin{tikzpicture}[
                rednode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
                bluenode/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
                greennode/.style={rectangle, draw=green!60, very thick, minimum size=5mm},
                node distance=0.5cm,
                remember picture, overlay
            ]
            \node<1->[bluenode, xshift=0.5\textwidth, yshift=-0.25\textwidth](likelihood) at (current page.north)  {$ \mathcal{L}(\theta,\alpha)$};
            \node<1->[bluenode, right = of likelihood.east](prior) {$ \pi(\theta,\alpha)$};

            \coordinate<1-> (likelihoodprior) at ($(likelihood.south)!0.5!(prior.south)$);

            \node<2->[rednode, below = of likelihoodprior](nestedsampling) {Nested Sampling};
            \draw<2->[->](likelihood.south) -- (likelihood|-nestedsampling.north);
            \draw<2->[->](prior.south) -- (prior|-nestedsampling.north);

            \node<3->[bluenode, below = of nestedsampling](posterior) {$ \{\theta,\alpha\}_\mathcal{P}$};
            \draw<3->[->](nestedsampling.south-|posterior) -- (posterior.north);
            \node<4->[bluenode, left = of posterior.west](evidence) {$ \mathcal{Z}$};
            \draw<4->[->](nestedsampling.south-|likelihood) -- (evidence.north);
            \node<5->[bluenode, right = of posterior.east](priorSamples) {$ \{\theta,\alpha\}_\pi$};
            \draw<5->[->](nestedsampling.south-|prior) -- (priorSamples.north);

            \coordinate<5-> (posteriorprior) at ($(posterior.south)!0.5!(priorSamples.south)$);

            \node<6->[rednode, below = of posteriorprior](margarine)  {Density Estimation};

            \draw<6->[->](posterior.south) -- (margarine.north-|posterior.east);
            \draw<6->[->](priorSamples.south) -- (margarine.north-|priorSamples.west);

            \node<7->[bluenode, below = of posterior|-margarine.south](marginalPosterior) {$ \mathcal{P}(\theta)$};


            \draw<7->[->](margarine.south-|marginalPosterior.east) -- (marginalPosterior.north);


            \node<8->[bluenode, below = of marginalPosterior.south-|margarine.south-|priorSamples](marginalPrior) {$ \pi(\theta)$};
            \draw<8->[->](margarine.south-|priorSamples.west) -- (marginalPrior.north);


            \node<9->[bluenode, below = of marginalPosterior](marginalLikelihood) {$ \mathcal{L}(\theta)$};


            \draw<9->[->](evidence.south) -- (marginalLikelihood.west);
            \draw<9->[->](marginalPosterior.south) -- (marginalLikelihood.north);
            \draw<9->[->](marginalPrior.west) -- (marginalLikelihood.east);

            \node<10->[greennode,behind path,fit=(nestedsampling) (marginalPosterior) (priorSamples) (evidence),] {};

        \end{tikzpicture}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Combination}
    \begin{columns}
        \column{0.5\textwidth}
%\begin{tikzpicture}[squarednodeA/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
%squarednodeB/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
%squarednodeC/.style={rectangle, draw=green!60, fill=green!5, very thick, minimum size=5mm}]
%
%\node[squarednodeA, text width=3cm, align=center](inference3) at (17, -1.5) {Nested Sampling with $\theta$, $\alpha_A$ and $\alpha_B$};
%
%\node[squarednodeB](fulllikelihood1) at (15, 1.5){$ \mathcal{L}_A(\theta,\alpha_A)$};
%\node[squarednodeB](fulllikelihood2) at (16.85, 1.5){$ \mathcal{L}_B(\theta,\alpha_B)$};
%\node[squarednodeB](fulljointlikelihood) at (16, 0){$ \mathcal{L}_A(\theta,\alpha_A) \mathcal{L}_B(\theta,\alpha_B)$};
%\node[squarednodeB](fullprior) at (19, 1.5){$ \pi_{AB}(\theta,\alpha_A, \alpha_B)$};
%
%\draw[->](fulllikelihood1.south) -- (15.5, 0.3);
%\draw[->](fulllikelihood2.south) -- (16.5, 0.3);
%\draw[->](fullprior.south) -- (inference3.north);
%\draw[->](fulljointlikelihood.south) -- (inference3.north);
%
%\node[squarednodeB](jointEvidence2) at (15, -3){$ \mathcal{Z}_{AB}$};
%\node[squarednodeB](jointPosterior2) at (17, -3){$ \{\theta\}_{\mathcal{P}_{AB}}$};
%
%\draw[<-](jointEvidence2.north) -- (16, -2);
%\draw[<-](jointPosterior2.north) -- (inference3.south);
%
%\node[squarednodeB](jointPosteriorNuisance) at (19, -3){$ \{\alpha_A, \alpha_B\}_{\mathcal{P}_{AB}}$};
%\draw[->](18, -2) -- (jointPosteriorNuisance.north);
%\draw[blue,thick](16.2, -3.5) -- (20.1, -3.5) -- (20.1, -2.5) -- (16.2, -2.5) -- (16.2, -3.5);
%
%\end{tikzpicture}

        \includegraphics[width=0.7\textwidth]{figures/full_margarine.pdf}

        \column{0.5\textwidth}
%\begin{tikzpicture}[squarednodeA/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
%squarednodeB/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
%squarednodeC/.style={rectangle, draw=green!60, fill=green!5, very thick, minimum size=5mm}]
%
%\node[squarednodeB](likelihood1) at (6, 1.5){$ \mathcal{L}_A(\theta,\alpha_A)$};
%\node[squarednodeB](prior1) at (8, 1.5){$ \pi_A(\theta,\alpha_A)$};
%
%\node[squarednodeC, text width=3cm, align=center](NestedMarg1) at (7, 0){Nested Sampling + \textsc{Margarine}};
%
%\draw[->](likelihood1.south) -- (6, 0.5);
%\draw[->](prior1.south) -- (8, 0.5);
%
%\node[squarednodeB](marglike1) at (6, -1.5){$ \mathcal{L}_A(\theta)$};
%\node[squarednodeB](margprior1) at (9, -1.5){$ \pi(\theta)$};
%
%\draw[->](6, -0.5) -- (6, -1.2);
%\draw[->](8, -0.5) -- (9, -1.2);
%
%\node[squarednodeB](likelihood2) at (10, 1.5){$ \mathcal{L}_B(\theta,\alpha_B)$};
%\node[squarednodeB](prior2) at (12, 1.5){$ \pi_B(\theta,\alpha_B)$};
%
%\draw[->](likelihood2.south) -- (10, 0.5);
%\draw[->](prior2.south) -- (12, 0.5);
%
%\node[squarednodeC, text width=3cm, align=center](NestedMarg1) at (11, 0){Nested Sampling + \textsc{Margarine}};
%
%\node[squarednodeB](marglike2) at (12, -1.5){$ \mathcal{L}_B(\theta)$};
%\draw[->](12, -0.5) -- (12, -1.2);
%\draw[->](10, -0.5) -- (9, -1.2);
%
%\node[squarednodeB](combinedlike) at (7, -3){$ \mathcal{L}_A(\theta) \mathcal{L}_B(\theta)$};
%
%\draw[->](marglike1.south) -- (combinedlike.north);
%\draw[->](marglike2.south) -- (combinedlike.north);
%
%\node[squarednodeA, text width=3cm, align=center](inference2) at (9, -4.5) {Nested Sampling with $\theta$};
%
%\draw[->](combinedlike.south) -- (8, -4);
%\draw[->](margprior1.south) -- (10, -4);
%
%\node[squarednodeB](jointEvidence) at (8, -6){$ \mathcal{Z}_{AB}$};
%\node[squarednodeB](jointPosterior) at (10, -6){$ \{\theta\}_{\mathcal{P}_{AB}}$};
%
%\draw[->](8, -5) -- (jointEvidence.north);
%\draw[->](10, -5) -- (jointPosterior.north);
%\end{tikzpicture}
        \includegraphics[width=0.9\textwidth]{figures/quick_margarine.pdf}
        
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{History of margarine}
    \student{dily_ong}{Dily Ong}{PhD}
    \begin{itemize}
        \item Papamakarios et al \arxiv{1912.02762} (normalising flows)
        \item Alsing et al \arxiv{1903.00007} (Delfi)
        \item Nested sampling with any prior you like (Alsing \& Handley) \arxiv{2102.12478}
        \item \texttt{margarine} (theory) Bevins et al \arxiv{2207.11457}
        \item \texttt{margarine} (practice) Bevins et al \arxiv{2205.12841}
        \item Next step: \texttt{unimpeded}
            \begin{itemize}
                \item pip-installable download system for DiRAC chains
            \end{itemize}
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Cosmological forecasting}
    \framesubtitle{Have you ever done a Fisher forecast, and then felt Bayesian guilt?}
    \vspace{-20pt}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item Cosmologists are interested in forecasting what a Bayesian analysis of future data might produce.
            \item Useful for:
                \begin{itemize}
                    \item white papers/grants,
                    \item optimising existing instruments/strategies,
                    \item picking theory/observation to explore next.
                \end{itemize}
            \item To do this properly:
                \begin{enumerate}
                    \item start from current knowledge $\pi(\theta)$, derived from current data
                    \item Pick potential dataset $D$ that might be collected from $P(D)\: (=\mathcal{Z})$
                    \item Derive posterior $P(\theta|D)$
                    \item Summarise science (e.g. constraint on $\theta$, ability to perform model comparison)
                \end{enumerate}
        \end{itemize}

        \column{0.5\textwidth}
        \begin{itemize}
            \item This procedure should be marginalised over:
                \begin{enumerate}
                    \item All possible parameters $\theta$ (consistent with prior knowledge)
                    \item All possible data $D$
                \end{enumerate}
            \item i.e. marginalised over the joint $P(\theta,D)=P(D|\theta)P(\theta)$.
            \item Historically this has proven very challenging.
            \item Most analyses assume a fiducial cosmology $\theta_*$, and/or a Gaussian likelihood/posterior (c.f. Fisher forecasting).
            \item This runs the risk of biasing forecasts by baking in a given theory/data realisation.
        \end{itemize}
        
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Simulation-based inference \& model comparison}
    \includegraphics<1>[page=1]{figures/sbi.pdf}%
    \includegraphics<2>[page=2]{figures/sbi.pdf}%
    \includegraphics<3>[page=3]{figures/sbi.pdf}%
\end{frame}

\begin{frame}
    \frametitle{Evidence networks~\arxiv{2305.11241}}
    \begin{columns}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Procedure proposed by Jeffreys \& Wandelt:
            \begin{enumerate}
                \item Generate labelled data from model $A$ and model $B$.
                \item Train a probabilistic classifier to distinguish between the two.
                \item Use neural ratio trick to extract Bayes Factor $B = P(D|A)/P(D|B)$.
            \end{enumerate}
        \item Fully marginalises out parameters
        \item Only works in the data space
        \item Model comparison without nested sampling!
    \end{itemize}
        
        \column{0.5\textwidth}
\includegraphics{figures/sbi-0.pdf}
\includegraphics{figures/sbi-1.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Fully Bayesian Forecasting~\arxiv{2309.06942}}
    \student{thomas_gessey-jones}{Thomas Gessey-Jones}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Simulation based inference gives us the language to marginalise over parameters $\theta$ and possible future data $D$.
            \item Evidence networks give us the ability to do this at scale for forecasting.
            \item Demonstrated in 21cm global experiments, marginalising over:
                \begin{itemize}
                    \item theoretical uncertainty
                    \item foreground uncertainty
                    \item systematic uncertainty
                \end{itemize}
            \item Able to say ``at 67mK radiometer noise'', have a 50\% chance of 5$\sigma$ Bayes factor detection.
            \item Can use to optimise instrument design
            \item Re-usable package: \texttt{prescience}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fbf.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{A word of caution on evidence networks}
    \includegraphics<1>[page=4]{figures/sbi.pdf}%
    \includegraphics<2>[page=5]{figures/sbi.pdf}%
\end{frame}

\begin{frame}
    \frametitle{A word of caution on evidence networks}
    \begin{itemize}
        \item Does not give evidence/partition function (which can be useful), only Bayes factor.
        \item Only valid if the true data lies within domain of extrapolation of neural network 
            \begin{itemize}
                \item True for forecasting.
                \item False for real data.
            \end{itemize}
        \item By throwing away parameter $\theta$ fitting, model cannot respond to mis-specified data.
        \item This criticism applies to any method claiming ``amortization''
            \begin{itemize}
                \item Amortized methods claim to train a system which works for all possible observed $D$.
                \item As Bayesians we should be suspicious, since the only truth we know is the observed data.
            \end{itemize}
        \item Traditional SBI interchange:
            \begin{description}
                \item[audience] What if your simulator is missing (X,Y,Z,\ldots)?
                \item[speaker] The exact same thing affects likelihood-based analysis
            \end{description}
        \item Here, the audience is implicitly making a query about the danger of working in data space, whilst the speaker's comment only applies to parameter space $\theta$.
        \item We should therefore focus on SBI approaches which have tunable parameter spaces (i.e. interpretable posteriors).
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\href{https://www.github.com/handley-lab}{github.com/handley-lab}}
    \tikz[overlay,remember picture]
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
            \includegraphics[width=0.1\textheight]{figures/students/adam_ormondroyd.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/cole_meldorf.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/david_yallup.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/dily_ong.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/harry_bevins.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/ian_roque.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/george_carter.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/kilian_scheutwinkel.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/metha_prathaban.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/namu_kroupa.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/sinah_legner.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/thomas_gessey-jones.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/wei-ning_deng.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/will_barker.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/zixiao_hu.jpg}%
    };
    \begin{itemize}
        \item \texttt{margarine} as a tool for marginal cosmological inference
        \item \texttt{unimpeded} as a tool for distributing fast reusable inference products
        \item \texttt{prescience} as a tool for fully Bayesian forecasting
    \end{itemize}
\end{frame}
 
%\appendix
%\begin{frame}
%    \frametitle{A word on emulators: the dead measure}
%    \includegraphics[width=\textwidth]{figures/dead_measure}
%    \begin{columns}
%        \column{0.69\textwidth}
%        \begin{itemize}
%            \item At the end, one is left with a set of discarded ``dead'' points.
%            \item Dead points have a unique scale-invariant distribution $\propto\: \tfrac{dV}{V}$.
%            \item Uniform over original region, exponentially concentrating on region of interest (until termination volume).
%            \item Good for training emulators (HERA~\arxiv{2108.07282}).
%        \end{itemize}
%        \column{0.3\textwidth}
%        \begin{block}{Applications}
%        \begin{itemize}
%            \item training emulators.
%            \item gridding simulations
%            \item beta flows
%            \item ``dead measure'' 
%        \end{itemize}
%        \end{block}
%    \end{columns}
%\end{frame}
 
\end{document}
```
# branch: remotes/origin/manchester_2023
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{ Nested sampling: {\large powering next-generation inference and machine learning tools}}
\subtitle{for astrophysics, cosmology, particle physics and beyond}
\date{10\textsuperscript{th} October 2023}

\begin{document}



\begin{frame}
    \titlepage
\end{frame}

%\begin{frame}
%    \frametitle{Highlight: state-of-the-art Nature review [\href{https://www.nature.com/articles/s43586-022-00121-x}{NatRev}]}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item Invented by John Skilling in 2004.
%            \item Recent Nature review primer on nested sampling led by Andrew Fowlie and assembled by the community.
%            \item Showcases the current set of tools, and applications from chemistry to cosmology.
%            \item Recent 1.5 day conference in Munich: ``Frontiers of Nested Sampling''
%            \item Planned week-long NSCON 2024
%            \item In this talk:
%                \begin{itemize}
%                    \item User guide to nested sampling
%                    \item Particle physics applications
%                    \item Cosmology applications
%                    \item Machine learning applications
%                \end{itemize}
%        \end{itemize}
%        \column{0.3\textwidth}
%        \includegraphics[width=\textwidth]{figures/nature1}
%        \includegraphics[width=\textwidth]{figures/nature2}
%        \column{0.2\textwidth}
%        \includegraphics[width=\textwidth]{figures/nature4}
%        \includegraphics[width=\textwidth]{figures/nature5}
%    \end{columns}
%
%\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Where is Nested Sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item For many purposes, in your Neural Net you should group Nested Sampling with (MCMC) techniques such as:
                \begin{itemize}
                    \item Metropolis-Hastings (PyMC, MontePython)
                    \item Hamiltonian Monte Carlo (Stan, blackjax)
                    \item Ensemble sampling (emcee, zeus). 
                    \item Variational Inference (Pyro)
                    \item Sequential Monte Carlo 
                    \item Thermodynamic integration
                    \item Genetic algorithms
                \end{itemize}
            \item You may have heard of it branded form:
                \begin{itemize}
                    \item MultiNest
                    \item PolyChord
                    \item dynesty
                    \item ultranest
                \end{itemize}
            \end{itemize}
        \column{0.5\textwidth}
        \begin{columns}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/emcee}
        \includegraphics[width=\textwidth]{figures/metropolis-hastings}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/zeus}
        \end{columns}
        \includegraphics[width=\textwidth]{figures/hmc_explained}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Integration in Physics}
    \begin{itemize}
        \item Integration is a fundamental concept in physics, statistics and data science:
    \end{itemize}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Partition functions}
            \vspace{-11pt}
            \[ Z(\beta) = \int e^{-\beta H(q,p)} dq dp \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Path integrals}
            \[ \Psi = \int e^{i S} \mathcal{D}x \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Bayesian marginals}
            \vspace{-11pt}
            \[ \mathcal{Z}(D) = \int \mathcal{L}(D|\theta) \pi(\theta) d\theta \]
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Need numerical tools if analytic solution unavailable.
            \item High-dimensional numerical integration is hard.
            \item Riemannian strategy estimates volumes geometrically:
                \[ \int f(x) d^nx \approx \sum_i f(x_i) \Delta V_i \sim \mathcal{O}(e^n) \]
            \item Curse of dimensionality $\Rightarrow$ exponential scaling.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Probabalistic volume estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Key idea in NS: estimating volumes probabilistically
        \[
            \frac{\C[1]{V_\mathrm{after}}}{\C[0]{V_\mathrm{before}}} 
            \approx \frac{\C[1]{n_\mathrm{in}}}{\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}}
        \]
            \item This is the \textbf{only} way to calculate volume in high dimensions $d>3$.
                \begin{itemize}
                    \item Geometry is exponentially inefficient.
                \end{itemize}
            \item This estimation process does not depend on geometry, topology or dimensionality
            \item Basis of all Monte-Carlo integration
            \item Nested Sampling uniquely uses a nested framework to couple together MC integrals in a robust, scalable manner.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/compression_1}%
        \includegraphics<2->[width=\textwidth]{figures/compression_2}%
    \end{columns}
\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.48\textwidth}
        \begin{block}{\textbf{MCMC}}
            \only<16>{
                \begin{itemize}
                    \item Single ``walker''
                    \item Explores posterior
                    \item Fast, if proposal matrix is tuned
                    \item Parameter estimation, suspiciousness calculation
                    \item Channel capacity optimised for generating posterior samples
                \end{itemize}
            }
        \end{block}
            \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
            \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
            \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
            \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
            \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
            \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.48\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \only<16>{
                \begin{itemize}
                    \item Ensemble of ``live points''
                    \item Scans from prior to peak of likelihood
                    \item Slower, no tuning required
                    \item Parameter estimation, model comparison, tension quantification
                    \item Channel capacity optimised for computing partition function
                \end{itemize}
            }
        \end{block}
            \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
            \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
            \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
            \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
            \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
            \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
            \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
            \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
            \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} 
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: live points}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n$ random samples over the space.
            \item Delete outermost sample, and replace with a new random one at higher integrand value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n}\only<5->{\pm \frac{1}{n}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \int f(x) dV \approx \sum_i f(x_i) \Delta V_i, \quad V_i = V_0 e^{-\only<5->{(}i\only<5->{\pm\sqrt{i})}/n} \]
%            \item Nested sampling: completely different way to scan.
%            \item Ensemble sampling compresses entire space$\to$peak(s).
%            \item Sequentially update a set $S$ of $n$ samples:
%                \begin{itemize}
%                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from a measure $\pi$). 
%
%                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
%                \end{itemize}
%            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard constraint}:
%                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
%            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence}/integral of function \& \C[0]{posterior}/sample weights.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4-         >[width=\textwidth,page=4]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: dead points}
    \includegraphics[width=\textwidth]{figures/dead_measure}
    \begin{columns}
        \column{0.69\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Dead points have a unique scale-invariant distribution $\propto\: \tfrac{dV}{V}$.
            \item Uniform over original region, exponentially concentrating on region of interest (until termination volume).
            \item Good for training emulators (HERA~\arxiv{2108.07282}).
        \end{itemize}
        \column{0.3\textwidth}
        \begin{block}{Applications}
        \begin{itemize}
            \item training emulators.
            \item gridding simulations
            \item beta flows
            \item ``dead measure'' 
        \end{itemize}
        \end{block}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{The nested sampling meta-algorithm: Lebesgue integration}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Full dead-point coverage of tails enables integration.
            \item Can be weighted to form posterior samples, prior samples, or anything in between.
            \item Nested sampling estimates the \textbf{density of states} and calculates partition functions
                \[Z(\beta) = \sum_i f(x_i)^\beta \Delta V_i.\]
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune
                    \item exploration of multimodal functions
                    \item global and local optimisation
                \end{itemize}
            %\item Interpreted as a Bayesian algorithm, it
            %    \begin{itemize}
            %        \item Computes the Bayesian evidence (model comparison)
            %        \item Produces (weighted) posterior samples (parameter estimation)
            %    \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 
  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item A large fraction of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \item \url{https://projecteuclid.org/euclid.ba/1340370944}.
    \item There has also been much work beyond this (see 'Frontiers of nested sampling' talk)
        \begin{itemize}
            \item \href{https://www.willhandley.co.uk/talks}{willhandley.co.uk/talks}
        \end{itemize}
  \end{itemize}
 
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Types of nested sampler}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
\item Constructs bounding region and draws many invalid points until $\mathcal{L}(\theta)>\mathcal{L}_*$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{5pt}
    \begin{itemize}
        \item Nested samplers usually come with:
            \begin{itemize}
                \item \emph{resolution} parameter $n_\mathrm{live}$ (which improve results as $\sim\mathcal{O}(n_\mathrm{live}^{-1/2})$.
                    \item set of \emph{reliability} parameters~\arxiv{2101.04525}, which don't improve results if set arbitrarily high, but introduce systematic errors if set too low.
                    \item e.g. \texttt{Multinest} efficiency \texttt{eff} or \texttt{PolyChord} chain length $n_\mathrm{repeats}$.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Applications: The three pillars of Bayesian inference}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Parameter estimation}
            What do the data tell us about the parameters of a model?

            \textit{e.g. the size or age of a $\Lambda$CDM universe}
            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Model comparison}
            How much does the data support a particular model?

            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}} \vspace{-7pt}\]
            \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[5]{\Pi_\mathcal{M}}}{\C[7]{\sum_m Z_m \Pi_m}} \]
            \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Tension quantification}
            Do different datasets make consistent predictions from the same model? 
            \textit{e.g. CMB vs Type IA supernovae data}
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Cosmology}
    \student{adam_ormondroyd}{Adam Ormondroyd}{PhD}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Battle-tested in Bayesian cosmology on
                \begin{itemize}
                    \item Parameter estimation: multimodal alternative to MCMC samplers.
                    \item Model comparison: using integration to compute the Bayesian evidence
                    \item Tension quantification: using deep tail sampling and suspiciousness computations.
                \end{itemize}
            \item Plays a critical role in major cosmology pipelines: Planck, DES, KiDS, BAO, SNe.
            \item The default $\Lambda$CDM cosmology is well-tuned to have Gaussian-like posteriors for CMB data. 
            \item Less true for alternative cosmologies/models and orthogonal datasets, so nested sampling crucial.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/pps_both}
        \includegraphics[width=0.49\textwidth]{figures/reach_fit-cropped.pdf}
        %\includegraphics[width=0.49\textwidth]{figures/curvature_3}
        \includegraphics[width=\textwidth]{figures/omegak_H0_2.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Astrophysics}
\student{metha_prathaban}{Metha Prathaban}{PhD}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item In exoplanets~\arxiv{1806.00518}
                \begin{itemize}
                    \item Parameter estimation: determining properties of planets.
                    \item Model comparison: how many planets? Stellar modelling~\arxiv{2007.07278}.
                    \item exoplanet problems regularly have posterior phase transitions \arxiv{2102.03387}
                \end{itemize}
            \item In gravitational waves
                \begin{itemize}
                    \item Parameter estimation: Binary merger properties
                    \item Model comparison: Modified theories of gravity, selecting phenomenological parameterisations~\arxiv{1803.10210}
                    \item Likelihood reweighting: fast slow properties
                \end{itemize}
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/rv_full.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Particle physics}
    \student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{columns}
            \column{0.67\textwidth}
            \begin{itemize}
                \item Nested sampling for cross section computation/event generation
            \end{itemize}
            \column{0.3\textwidth}
            \[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2.\]
        \end{columns}
        \begin{itemize}
            \item Nested sampling can explore the phase space $\Omega$ and compute integral blind with comparable efficiency to HAAG/RAMBO~\arxiv{2205.02030}.
            \item Bayesian sparse reconstruction~\arxiv{1809.04598} applied to bump hunting allows evidence-based detection of signals in phenomenological backgrounds~\arxiv{2211.10391}.
            \item Fine tuning quantification
            \item Fast estimation of small $p$-values \arxiv{2106.02056}(PRL), just make switch:\\ $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.

        \end{itemize}
        \column{0.17\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1-pdfjam-crop.pdf}
        \column{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/psi_predict-crop.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Lattice field theory}
\student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Consider standard field theory Lagrangian:
                \[ Z(\beta) = \int D\phi e^{-\beta S(\phi)}, \quad S(\phi) = \int dx^\mu \mathcal{L}(\phi) \]
            \item Discretize onto spacetime grid.
            \item Compute partition function
            \item NS unique traits:
                \begin{itemize}
                    \item Get full partition function for free
                    \item allows for critical tuning
                    \item avoids critical slowing down
                \end{itemize}
            \item Applications in lattice gravity, QCD, condensed matter physics
            \item Publication imminent (next week)
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/potential_shape}
        \includegraphics[width=0.49\textwidth]{figures/2d_phase}
        \includegraphics[width=\textwidth]{figures/lattice_field_theory.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Machine learning}
\student{kamran_javid}{Kamran Javid}{PDRA}
    \begin{columns}
        \column{0.62\textwidth}
        \begin{itemize}
            \item Machine learning requires:
                \begin{itemize}
                    \item Training to find weights
                    \item Choice of architecture/topology/hyperparameters
                \end{itemize}
            \item Bayesian NNs treat training as a model fitting problem
            \item Compute posterior of weights (parameter estimation), rather than optimisation (gradient descent)
            \item Use evidence to determine best architecture (model comparison), correlates with out-of-sample performance! 
            \item Solving the full ``shallow learning'' problem without compromise \arxiv{2004.12211}\arxiv{2211.10391}. 
                \begin{itemize}
                    \item Promising work ongoing to extend this to transfer learning and deep nets.
                \end{itemize}
            \item More generally, dead points are optimally spaced  for training traditional ML approaches e.g. \arxiv{2309.05697}
        \end{itemize}
        \column{0.38\textwidth}
        \includegraphics[width=\textwidth]{figures/nn_posterior-cropped.pdf}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{Applications of nested sampling}
%    \framesubtitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
%    \begin{columns}
%        \column{0.57\textwidth}
%        \begin{itemize}
%            \item Nested sampling for frequentist computation!?
%            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
%            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
%            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
%            \item Need insight/approximation to make efficient.
%            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
%            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
%        \end{itemize}
%        \column{0.43\textwidth}
%        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{And beyond\ldots}
    \only<1-4>{
        \student{watkinson-headshot}{Catherine Watkinson}{Senior Data Scientist}
    }
    \only<5->{
        \student{mcaloone-headshot}{Thomas Mcaloone}{PhD $\to$ Data Scientist}
    }
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/watkinson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mason-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/formanek-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mcaloone-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/stenczel-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/yallup-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/bex-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/claireburke-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/hobson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/lasenby-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mhandley-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/whandley-headshot.jpg}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        \includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/tdoa-cropped-1-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/tdoa-cropped-2-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/tdoa-cropped-3-crop.pdf}%
        \includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology {\small \arxiv{2210.07409}(NatAstro)}}
    \student{ian_roque}{Ian Roque}{PhD}
    \vspace{10pt}
    \begin{columns}
        \column{0.65\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
            \item Lead data analysis team (REACH first light in January)
            \item Nested sampling woven in from the ground up (calibrator, beam modelling, signal fitting, likelihood selection).
            \item All treated as parameterised model comparison problems.
        \includegraphics[height=0.3\textwidth]{figures/EDGES_antenna}
        \includegraphics[height=0.3\textwidth]{figures/SARAS}
        \includegraphics[height=0.3\textwidth]{figures/REACH_2.jpg}
        \end{itemize}
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT: combining particle physics \& cosmological data}
    \begin{columns}
        \column{0.52\textwidth}
        \begin{itemize}
            \item Multinational team of particle physicists, cosmologists and statisticians.
            \item Combine cosmological data, particle colliders, direct detection, \& neutrino detectors in a statistically principled manner~\arxiv{2205.13549}.
            \item Lead Cosmo/Dark Matter working group~\arxiv{2009.03286}.
            \item Nested sampling used for global fitting, and fine-tuning quantification~\arxiv{2101.00428}
        \end{itemize}
        \begin{center}
            \includegraphics[width=0.5\textwidth]{figures/gambit_logo.png}
        \end{center}
        \column{0.48\textwidth}
        \vspace{-40pt}
        \includegraphics[width=\textwidth]{figures/ALP_2.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Likelihood-free inference (aka SBI)}
\student{kilian_scheutwinkel}{Kilian Scheutwinkel}{PhD}

    \vspace{10pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item How do you do inference if you don't know the likelihood $P(D|\theta)$?
                \begin{itemize}
                    \item e.g.\ if you can simulate a disease outbreak, how can you infer a posterior on $R_0$, or select the most predictive model?
                \end{itemize}
            \item If you can forward simulate/model $\theta\to D$, then you have an implicit likelihood.
            \item LFI aims to (machine-)\emph{learn} the likelihood from forward simulations $\{(\theta,D)\}$.
            \item Nested sampling has much to offer
                \begin{itemize}
                    \item truncation strategies (\texttt{PolySwyft})
                    \item evidence driven compression
                    \item marginalised machine learning
                \end{itemize}
            \item In my view, LFI represents the future of inference -- in twenty years time this will be as well-used as MCMC techniques are today.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/noisy.pdf}
        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{unimpeded: PLA for the next generation}
\student{harry_bevins}{Harry Bevins}{PhD$\to$JRF}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item DiRAC 2020 RAC allocation of 30MCPUh
            \item Main goal: Planck Legacy Archive equivalent
            \item Parameter estimation $\to$ Model comparison
            \item MCMC $\to$ Nested sampling
            \item Planck $\to$ $\{\text{Planck}, \text{DESY1}, \text{BAO}, \ldots \}$
            \item Pairwise combinations
            \item Suite of tools for processing these 
                \begin{itemize}
                    \item \texttt{anesthetic} $2.0$
                    \item \texttt{unimpeded} $1.0$
                    \item \texttt{zenodo} archive
                    \item \texttt{margarine}
                \end{itemize}
            \item MCMC chains also available.
            \item Library of bijectors emulators for fast re-use
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{logos/dirac}
        \includegraphics[width=0.5\textwidth,page=21]{figures/himmelblau}%
        \includegraphics[width=0.5\textwidth,page=15]{figures/himmelblau}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{CosmoTension}
    \framesubtitle{Resolving cosmological tensions with diverse data, novel theories and Bayesian machine learning}
\student{will_barker}{Will Barker}{PhD$\to$JRF}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item ERC grant $\Rightarrow$ UKRI Frontier, commencing 2023.
            \item Funds 3 PDRAs and 4 PhDs over 5 years.
            \item Research programme centered around combining novel theories of gravity, Boltzmann solvers~\arxiv{1906.01421}, reconstruction~\arxiv{1908.00906}, nested sampling \& likelihood free inference.
            \item Aims to disentangle cosmological tensions $H_0$, $\sigma_8$, $\Omega_K$ with next-generation data analysis techniques.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/denseoutput.pdf}
        \includegraphics[width=\textwidth]{figures/pps.pdf}
        \includegraphics[width=\textwidth]{figures/gantt.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Fully Bayesian Forecasting~\arxiv{2309.06942}}
    \student{thomas_gessey-jones}{Thomas Gessey-Jones}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Experimental design necessitates forecasting the constraints that future data might give.
            \item Have you ever done a Fisher forecast, and then felt Bayesian guilt?
            \item Simulation based inference gives us the language to marginalise over parameters $\theta$ and possible future data $D$.
            \item Evidence networks~\arxiv{2305.11241} (Jeffreys \& Wandelt) give us the ability to do this at scale in the case of forecasting.
            \item Can answer questions such as ``Given current knowledge/theoretical uncertainty $\pi(\theta)$, how probable is a detection?''
            \item Re-usable package: \texttt{prescience}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fbf.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\href{https://www.github.com/handley-lab}{github.com/handley-lab}}
    \tikz[overlay,remember picture]
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
            \includegraphics[width=0.1\textheight]{figures/students/adam_ormondroyd.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/cole_meldorf.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/david_yallup.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/dily_ong.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/harry_bevins.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/ian_roque.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/george_carter.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/kilian_scheutwinkel.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/metha_prathaban.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/namu_kroupa.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/sinah_legner.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/thomas_gessey-jones.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/wei-ning_deng.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/will_barker.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/zixiao_hu.jpg}%
    };
    \vspace{-0.1\textheight}
    \begin{columns}
        \column{0.65\textwidth}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Performing Bayesian inference and model comparison.
            \end{itemize}
        \item It is applied widely across cosmology, particle physics \& machine learning.
        \item It's unique traits as the only numerical Lebesgue integrator mean with compute it will continue to grow in importance.
    \end{itemize}
        \column{0.35\textwidth}
    \includegraphics[height=0.6\textwidth]{figures/pps_both}%
    \includegraphics[height=0.6\textwidth]{figures/REACH_2}%
    \end{columns}
    \includegraphics[height=0.2\textwidth]{figures/integration}%
    \includegraphics[height=0.2\textwidth]{figures/curvature_3}%
    \includegraphics[height=0.2\textwidth]{figures/ALP_3}%
\end{frame}


\appendix
\begin{frame}
    \frametitle{How does Nested Sampling compare to other approaches?}
    \begin{columns}
        \column{0.7\textwidth}
        \begin{itemize}
            \item In all cases:
                \begin{itemize}
                    \item[$+$] NS can handle multimodal functions
                    \item[$+$] NS computes evidences, partition functions and integrals
                    \item[$+$] NS is self-tuning/black-box
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
    \end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \begin{itemize}
                \item Gradient descent
                    \begin{itemize}
                        \item[$-$] NS cannot use gradients
                        \item[$+$] NS does not require gradients
                    \end{itemize}
                \item Genetic algorithms
                    \begin{itemize}
                        \item[$+$] NS discarded points have statistical meaning
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Sampling}
            \begin{itemize}
                \item Metropolis-Hastings?
                    \begin{itemize}
                        \item[$-$] Nothing beats well-tuned customised MH
                        \item[$+$] NS is self tuning
                    \end{itemize}
                \item Hamiltonian Monte Carlo?
                    \begin{itemize}
                        \item[$-$] In millions of dimensions, HMC is king
                        \item[$+$] NS does not require gradients
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \begin{itemize}
                \item Thermodynamic integration
                    \begin{itemize}
                        \item[$+$] protective against phase trasitions
                        \item[$+$] No annealing schedule tuning 
                    \end{itemize}
                \item Sequential Monte Carlo
                    \begin{itemize}
                        \item[$-$] SMC experts classify NS as a kind of SMC
                        \item[$+$] NS is athermal
                    \end{itemize}
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling: a user's guide}
    \begin{enumerate}
        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
            \begin{itemize}
                \item This means typically most of its time is spent on burn-in rather than posterior sampling.
                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable.
            \end{itemize}
        \item The number of live points $n_\text{live}$ is a resolution parameter.
            \begin{itemize}
                \item Run time is linear in $n_\text{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\text{live}}}$.
                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
            \end{itemize}
        \item Most algorithms come with additional reliability parameter(s).
            \begin{itemize}
                \item e.g. \texttt{MultiNest}: $\texttt{eff}$, \texttt{PolyChord}: $n_\text{repeats}$.
                \item These are parameters which have no gain if set too conservatively, but increase the reliability.
                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Time complexity of nested sampling}
    \includegraphics[width=\textwidth]{figures/run_prodecure}
    \vspace{-20pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-20pt}
    \begin{itemize}
        \item $x$-axis: log-compression of live points
        \item Area $\propto$ posterior mass
        \item Shows Bayesian balance of likelihood vs prior
        \item Run proceeds right to left
        \item Run finishes after bump (typical set)
    \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Time complexity\vspace{-5pt}
                \[ \boxed{T = n_\mathrm{live} \times T_\mathcal{L} \times T_\mathrm{sampler} \times \mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)}\vspace{-5pt}  \]
            \item Error complexity $\boxed{\sigma \propto \sqrt{\mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)/n_\mathrm{live}}}$
        \end{itemize}
    \end{columns}

\end{frame}




\begin{frame}
    \frametitle{Occam's Razor~\arxiv{2102.11511}}
    \begin{itemize}
        \item Bayesian inference quantifies Occam's Razor:
            \begin{itemize}
                \item \textit{``Entities are not to be multiplied without necessity''} \hfill --- William of Occam
                \item \textit{``Everything should be kept as simple as possible, but not simpler''} \hfill --- ``Albert Einstein''
            \end{itemize}
        %\item Consider the Evidence $\C[3]{\mathcal{Z}\equiv P(D|M)}$: 
        %    \begin{description}[Parameter estimation]
        %        \item [Parameter estimation] normalisation constant
        %        \item [Model comparison] critical update factor for \C[5]{model prior} to \C[4]{model posterior}
        %    \end{description}
        \item Properties of the evidence: rearrange Bayes' theorem for parameter estimation
            \[\C[0]{\mathcal{P}(\theta)} = \frac{\C[2]{\mathcal{L}(\theta)} \C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}}} \qquad\Rightarrow\qquad \C[3]{\log \mathcal{Z}} = \C[2]{\log\mathcal{L}(\theta)} - \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}}.\]  
        \item Evidence is composed of a ``goodness of fit'' term  and ``Occam Penalty''.
    \end{itemize}
    \begin{columns}[t]
        \column{0.5\textwidth}
    \begin{itemize}
        \item RHS true for all $\theta$. Take max likelihood value $\theta_*$:
            \[
                \log \mathcal{Z} = -\chi_\text{min}^2 - \text{Mackay penalty.}
            \]
    \end{itemize}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Be more Bayesian and take posterior average to get the ``Occam's razor equation''
            \[
                \boxed{
                    \log \mathcal{Z} = \av[\mathcal{P}]{\log\mathcal{L}} - \mathcal{D}_\text{KL}.
            }
            \]
    \end{itemize}
    \end{columns}
    \vfill
    \begin{itemize}
        \item Natural regularisation which penalises models with too many parameters.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kullback Liebler divergence}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is is defined as:
                \[\mathcal{D}_\text{KL} = \av[\mathcal{P}]{\log\frac{\mathcal{P}}{\pi}} = \int \mathcal{P}(\theta) \log \frac{\mathcal{P}(\theta)}{\pi(\theta)}d\theta.\]
            \item Whilst not a distance, $\mathcal{D}=0$ when $\mathcal{P}=\pi$.
            \item Occurs in the context of machine learning as an objective function for training functions.
            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
                \[ \mathcal{D}_\text{KL} \approx \log \frac{V_\pi}{V_\text{P}}.\]
                (this is exact for top-hat distributions).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Nested sampling for frequentist computation!?
            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
            \item Need insight/approximation to make efficient.
            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
    \end{columns}
    
\end{frame}

\begin{frame}
    \frametitle{Exploration of phase space~\arxiv{2106.02056}}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Nested sampling for cross section computation/event generation.
            \item Numerically compute collisional cross section 
                \vspace{-5pt}\[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2,\]
                $\Omega$ phase space of kinematic configurations $\Phi$, each with matrix element $\mathcal{M}(\Phi)$.
            \item Current state of the art e.g. HAAG (improvement on RAMBO) requires knowledge of $\mathcal{M}(\Phi)$.
            \item Nested sampling can explore the phase space and compute integral blind with comparable efficiency.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1.pdf}
        \includegraphics[width=\textwidth]{figures/phase_space.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Quantification of fine tuning~\arxiv{2101.00428}~\arxiv{2205.13549}}
    \vspace{-10pt}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Example: Cosmological constraints on decaying axion-like particles~\arxiv{2205.13549}.
            \item Subset of parameters $\xi,m_a,\tau,g_{a\gamma}$: ALP fraction, mass, lifetime and photon coupling.
                {(\small Also vary cosmology, $\tau_n$ and nuisance params)}
            \item Data: CMB, BBN, FIRAS, SMM, BAO.
            \item Standard profile likelihood fit shows ruled out regions and best-fit point.
            \item<2-> Nested sampling scan:
                \begin{itemize}
                    \item Quantifies amount of parameter space ruled out with Kullback-Liebler divergence $\mathcal{D}_\mathrm{KL}$.
                    \item Identifies best fit region as statistically irrelevant from information theory/Bayesian.
                    \item No evidence for decaying ALPs. Fit the data equally well: but more constrained parameters create Occam penalty.
                \end{itemize}
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/ALP_1.pdf}
        \includegraphics<2          >[width=\textwidth]{figures/ALP_2.pdf}
        \includegraphics<3|handout:0>[width=\textwidth]{figures/ALP_3.pdf}
    \end{columns}
    
\end{frame}

\end{document}
```
# branch: remotes/origin/maxent_2023
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{<+Title+>}
\subtitle{<+subtitle+>}
\date{<+Date+>}

\begin{document}


\begin{frame}
    \frametitle{The scaling frontier of nested sampling}
    %\includegraphics[width=\textwidth]{figures/run_prodecure}
    \begin{columns}[t]
        \column{0.47\textwidth}
        \begin{block}{How fast in nested sampling?}
            \[ \boxed{T = T_\mathcal{L} \times n_\mathrm{live} \times f_\mathrm{sampler} \times \mathcal{D}_\mathrm{KL}} \]
        \end{block}
        \column{0.43\textwidth}
        \begin{block}{How accurate is nested sampling?}
            \[ \boxed{\sigma \approx \sqrt{\mathcal{D}_\mathrm{KL}/n_\mathrm{live}}} \]
        \end{block}
    \end{columns}
    \vspace{10pt}
    \begin{columns}
        \column{0.5\textwidth}
        in $d$ dimensional parameter space:
        \begin{description}
            \item[$T_\mathcal{L}$:] likelihood eval time \hfill$\sim\mathcal{O}(d)$
            \item[$n_\mathrm{live}$:] number of live points\hfill$\sim\mathcal{O}(d)$
            \item[$f_\mathrm{sampler}$:] efficiency of point generation \\ region$\sim\mathcal{O}(e^{d/d_0})$ or path$\sim\mathcal{O}(d)$
            \item[$\mathcal{D}_\mathrm{KL}$:] KL between prior and posterior $\approx\log{V_\pi}/{V_\mathcal{P}}$ \hfill$\sim\mathcal{O}(d)$
        \end{description}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Most attention on algorithmically improving $f_\mathrm{sampler}$, but only a fraction of the story!
            \item In my summary, will highlight where others fit into this picture
            \item $\mathcal{D}_\mathrm{KL}$ appears twice, so improvements here are quadratically important.
            \item Gradients give you $d$ more information.
        \end{itemize}
    \end{columns}

\end{frame}

\end{document}
```
# branch: remotes/origin/miapbp_2023
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usepackage{pythonhighlight}
\usepackage{tikz}
\newcommand{\av}[2][]{\left\langle #2\right\rangle_{#1}}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Gradients and Nested Sampling}
\subtitle{The present state of the art}
\date{7\textsuperscript{th} July 2023}

\begin{document}


\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Highlight: state-of-the-art Nature review primer \arxiv{2205.15570} }
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Invented by John Skilling in 2004.
            \item Recent Nature review primer on nested sampling led by Andrew Fowlie and assembled by the community.
            \item Showcases the current set of tools, and applications from chemistry to cosmology.
            \item Buchner technical review \arxiv{2101.09675}
            \item In this talk
                \begin{itemize}
                    \item What is nested sampling?
                    \item How can it use gradients?
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/nature1}
        \includegraphics[width=\textwidth]{figures/nature2}
        \column{0.2\textwidth}
        \includegraphics[width=\textwidth]{figures/nature4}
        \includegraphics[width=\textwidth]{figures/nature5}
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Where is Nested Sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item For many purposes, in your Neural Net you should group Nested Sampling with (MCMC) techniques such as:
                \begin{itemize}
                    \item Metropolis-Hastings (PyMC, MontePython)
                    \item Hamiltonian Monte Carlo (Stan, blackjax)
                    \item Ensemble sampling (emcee, zeus). 
                    \item Variational Inference (Pyro)
                    \item Sequential Monte Carlo 
                    \item Thermodynamic integration
                    \item Genetic algorithms
                \end{itemize}
            \end{itemize}
        \column{0.5\textwidth}
        \begin{columns}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/emcee}
        \includegraphics[width=\textwidth]{figures/metropolis-hastings}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/zeus}
        \end{columns}
        \includegraphics[width=\textwidth]{figures/hmc_explained}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Integration in Physics}
    \begin{itemize}
        \item Integration is a fundamental concept in physics, statistics and data science:
    \end{itemize}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Partition functions}
            \vspace{-11pt}
            \[ Z(\beta) = \int e^{-\beta H(q,p)} dq dp \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Path integrals}
            \[ \Psi = \int e^{i S} \mathcal{D}x \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Bayesian marginals}
            \vspace{-11pt}
            \[ \mathcal{Z}(D) = \int \mathcal{L}(D|\theta) \pi(\theta) d\theta \]
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Need numerical tools if analytic solution unavailable.
            \item High-dimensional numerical integration is hard.
            \item Riemannian strategy estimates volumes geometrically:
                \[ \int f(x) d^nx \approx \sum_i f(x_i) \Delta V_i \sim \mathcal{O}(e^n) \]
            \item Curse of dimensionality $\Rightarrow$ exponential scaling.
            \item Nested sampling integrates \textbf{probabilistically}.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{Integration in high dimensions}
%    \begin{columns}
%        \column{0.6\textwidth}
%        \begin{itemize}
%            \item Nested sampling can compute the \C[3]{Bayesian evidence} $\C[3]{\mathcal{Z}} = \int\C[2]{\mathcal{L}(\theta)}\C[1]{\pi(\theta)}d\theta$
%            \item Numerical integration $\int f(x) dV$ in high dimensions is hard.
%            \item \texttt{scipy.integrate(...)} is unusable in more than four dimensions.
%            \item This is due to the curse of dimensionality: need to sum $\sim N^d$ units to compute $\approx \sum_i f(x_i) \Delta V_i$.
%            \item Additionally, estimating volumes with geometry becomes exponentially hard as $d$ increases.
%            \item \textit{Aside: \textbf{Riemannian integration} (blue) is taught as standard. An orthogonal approach (red) [usually theoretical] is \textbf{Lesbesgue integration}.}
%        \end{itemize}
%        \column{0.4\textwidth}
%        \includegraphics[width=\textwidth]{figures/integration}
%        \includegraphics[width=\textwidth]{figures/2560px-Riemannvslebesgue.svg.png}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Probabalistic volume estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Key idea in NS: estimating volumes probabilistically
                \[
                    \only<-2>{
                    \frac{\C[1]{V_\mathrm{after}}}{\C[0]{V_\mathrm{before}}} 
                    \approx \frac{\C[1]{n_\mathrm{in}}}{\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}}
                }
                    \only<3>{
                    \frac{\C[1]{V_\mathrm{after}}}{\C[0]{V_\mathrm{before}}} 
                    \approx \frac{\C[1]{n_\mathrm{in}}+1}{\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}+2}
                }
                \only<4>{\hspace{-15pt}
                    \frac{\C[1]{V_\mathrm{after}}}{\C[0]{V_\mathrm{before}}} 
\sim \frac{\C[1]{n_\mathrm{in}}+1}{\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}+2} \pm \sqrt{\tfrac{(\C[1]{n_\mathrm{in}}+1)(\C[0]{n_\mathrm{out}}+1)}{(\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}+2)^2(\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}+3)}}
                }
                \]
            \item This is the \textbf{only} way to calculate volume in high dimensions $d>3$.
            \item This process does not depend on geometry, topology or dimensionality
            \item The errors however are not small.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/compression_1}%
        \includegraphics<2->[width=\textwidth]{figures/compression_2}%
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{The three pillars of Bayesian inference}
%    \begin{columns}[t]
%        \column{0.33\textwidth}
%        \begin{block}{Parameter estimation}
%            What do the data tell us about the parameters of a model?
%
%            \textit{e.g. the size or age of a $\Lambda$CDM universe}
%            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
%            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
%            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
%        \end{block}
%        \column{0.3\textwidth}
%        \begin{block}{Model comparison}
%            How much does the data support a particular model?
%
%            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
%            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}} \vspace{-7pt}\]
%            \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[5]{\Pi_\mathcal{M}}}{\C[7]{\sum_m Z_m \Pi_m}} \]
%            \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}\]
%        \end{block}
%        \column{0.33\textwidth}
%        \begin{block}{Tension quantification}
%            Do different datasets make consistent predictions from the same model? 
%            \textit{e.g. CMB vs Type IA supernovae data}
%            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
%            \[
%                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
%                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
%                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
%                \end{aligned}
%            \]
%        \end{block}
%    \end{columns}
%\end{frame}


%\begin{frame} \frametitle{Why do sampling?}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \vspace{-10pt}
%        \begin{itemize}
%            \item The cornerstone of numerical Bayesian inference is working with \textbf{samples}.
%            \item Generate a set of representative parameters drawn in proportion a distributionto the posterior $\theta\sim\C[0]{\mathcal{P}}$.
%            \item The magic of marginalisation $\Rightarrow$ perform usual analysis on each sample in turn.
%            \item The golden rule is \C[3]{stay in samples} until the last moment before computing summary statistics/triangle plots because \[\boxed{f(\:\av{X}\:)\ne \av{\:f(X)\:}}\]
%            \item Generally need $\sim\mathcal{O}(12)$ independent samples to compute a value and error bar.
%        \end{itemize}
%        \column{0.5\textwidth}
%        \includegraphics[width=\textwidth]{figures/samples.pdf}
%    \end{columns}
%\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.48\textwidth}
        \begin{block}{\textbf{MCMC}}
            \only<16>{
                \begin{itemize}
                    \item Single ``walker''
                    \item Explores posterior
                    \item Fast, if proposal matrix is tuned
                    \item Parameter estimation, suspiciousness calculation
                    \item Channel capacity optimised for generating posterior samples
                \end{itemize}
            }
        \end{block}
        \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
        \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
        \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
        \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
        \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
        \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.48\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \only<16>{
                \begin{itemize}
                    \item Ensemble of ``live points''
                    \item Scans from prior to peak of likelihood
                    \item Slower, no tuning required
                    \item Parameter estimation, model comparison, tension quantification
                    \item Channel capacity optimised for computing partition function
                \end{itemize}
            }
        \end{block}
        \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
        \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} 
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Sequentially update a set $S$ of $n$ samples:
                \begin{itemize}
                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from the prior $\pi$). 

                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
                \end{itemize}
            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard likelihood constraint}:
                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence} \& \C[0]{posterior} weights.
            \item The evolving ensemble of live points allows algorithms to perform self-tuning and mode clustering.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth,page=4]{figures/himmelblau}%
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{(Lesbesgue) Integrating with nested sampling}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item At each iteration, the likelihood contour shrinks in volume $X$ by  $\approx 1/n$.
            \item Nested sampling zooms in to the peak of the function $\mathcal{L}$ {\em exponentially}.
                \vspace{-5pt}
                \[
                    \mathcal{Z} \approx \sum_i \Delta\mathcal{L}_i X_{i}, \quad
                    X_{i+1} \approx \frac{n}{n+1}X_i, \quad X_{0} = 1 .
                \]
                \vspace{-15pt}
            \item At the end, one is left with a set of discarded points.
            \item These may be weighted to form weighted posterior samples using $w_i = \mathcal{L}_i \Delta X_i$.
            \item Estimates the density of states, and is therefore a partition function calculator
                    $Z(\beta) = \sum_i \mathcal{L}_i^\beta \Delta X_i$.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/lesbesgue}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/lesbesgue}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/lesbesgue}%
        \includegraphics<4|handout:0>[width=\textwidth,page=4]{figures/lesbesgue}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/lesbesgue}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/lesbesgue}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/lesbesgue}%
        \includegraphics<8|handout:0>[width=\textwidth,page=8]{figures/lesbesgue}%
        \includegraphics<9|handout:0>[width=\textwidth,page=9]{figures/lesbesgue}%
        \includegraphics<10|handout:0>[width=\textwidth,page=10]{figures/lesbesgue}%
        \includegraphics<11|handout:0>[width=\textwidth,page=11]{figures/lesbesgue}%
        \includegraphics<12|handout:0>[width=\textwidth,page=12]{figures/lesbesgue}%
        \includegraphics<13|handout:0>[width=\textwidth,page=13]{figures/lesbesgue}%
        \includegraphics<14|handout:0>[width=\textwidth,page=14]{figures/lesbesgue}%
        \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/lesbesgue}%
        \includegraphics<16          >[width=\textwidth,page=16]{figures/lesbesgue}%
        \includegraphics<17|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Sampling from a hard likelihood constraint} 

    \begin{quote}
        ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''

        {\hfill --- John Skilling}
    \end{quote}

    \begin{itemize}

        \item A large fraction of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
        \item \url{https://projecteuclid.org/euclid.ba/1340370944}.
        \item There has also been much work beyond this (see ``Frontiers of nested sampling'' talk from last year: \href{https://www.willhandley.co.uk/talks}{willhandley.co.uk/talks})
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Types of nested sampler}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
                \item Constructs bounding region and draws many invalid points until $\mathcal{L}(\theta)>\mathcal{L}_*$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{5pt}
    \begin{itemize}
        \item Nested samplers usually come with:
            \begin{itemize}
                \item \emph{resolution} parameter $n_\mathrm{live}$ (which improve results as $\sim\mathcal{O}(n_\mathrm{live}^{-1/2})$.
                    \item set of \emph{reliability} parameters~\arxiv{2101.04525}, which don't improve results if set arbitrarily high, but introduce systematic errors if set too low.
                    \item e.g. \texttt{Multinest} efficiency \texttt{eff} or \texttt{PolyChord} chain length $n_\mathrm{repeats}$.
                \end{itemize}
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Applications of nested sampling}
        \framesubtitle{Cosmology}
        \begin{columns}
            \column{0.55\textwidth}
            \begin{itemize}
                \item Battle-tested in Bayesian cosmology on
                    \begin{itemize}
                        \item Parameter estimation: multimodal alternative to MCMC samplers.
                        \item Model comparison: using integration to compute the Bayesian evidence
                        \item Tension quantification: using deep tail sampling and suspiciousness computations.
                    \end{itemize}
                \item Plays a critical role in major cosmology pipelines: Planck, DES, KiDS, BAO, SNe.
                \item The default $\Lambda$CDM cosmology is well-tuned to have Gaussian-like posteriors for CMB data. 
                \item Less true for alternative cosmologies/models and orthogonal datasets, so nested sampling crucial.
            \end{itemize}
            \column{0.45\textwidth}
            \includegraphics[width=0.49\textwidth]{figures/pps_both}
            \includegraphics[width=0.49\textwidth]{figures/reach_fit-cropped.pdf}
        %\includegraphics[width=0.49\textwidth]{figures/curvature_3}
            \includegraphics[width=\textwidth]{figures/omegak_H0_2.pdf}
        \end{columns}
    \end{frame}

    \begin{frame}
        \frametitle{Applications of nested sampling}
        \framesubtitle{Particle physics}
        \begin{columns}
            \column{0.56\textwidth}
            \begin{columns}
                \column{0.67\textwidth}
                \begin{itemize}
                    \item Nested sampling for cross section computation/event generation
                \end{itemize}
                \column{0.3\textwidth}
                \[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2.\]
            \end{columns}
            \begin{itemize}
                \item Nested sampling can explore the phase space $\Omega$ and compute integral blind with comparable efficiency to HAAG/RAMBO~\arxiv{2205.02030}.
                \item Bayesian sparse reconstruction~\arxiv{1809.04598} applied to bump hunting allows evidence-based detection of signals in phenomenological backgrounds~\arxiv{2211.10391}.
                \item Now applying to lattice field theory, and lattice gravity Lagrangians.
                \item Fine tuning quantification
            \end{itemize}
            \column{0.17\textwidth}
            \includegraphics[width=\textwidth]{figures/phase_space_1-pdfjam-crop.pdf}
            \column{0.27\textwidth}
            \includegraphics[width=\textwidth]{figures/psi_predict-crop.pdf}
        \end{columns}
    \end{frame}

    \begin{frame}
        \frametitle{Applications of nested sampling}
        \framesubtitle{Machine learning}
        \begin{columns}
            \column{0.62\textwidth}
            \begin{itemize}
                \item Machine learning requires:
                    \begin{itemize}
                        \item Training to find weights
                        \item Choice of architecture/topology/hyperparameters
                    \end{itemize}
                \item Bayesian NNs treat training as a model fitting problem
                \item Compute posterior of weights (parameter estimation), rather than optimisation (gradient descent)
                \item Use evidence to determine best architecture (model comparison), correlates with out-of-sample performance! 
                \item Solving the full ``shallow learning'' problem without compromise \arxiv{2004.12211}\arxiv{2211.10391}. 
                \item Promising work ongoing to extend this to transfer learning and deep nets.
            \end{itemize}
            \column{0.38\textwidth}
            \includegraphics[width=\textwidth]{figures/nn_posterior-cropped.pdf}
        \end{columns}
    \end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{and beyond\ldots}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/watkinson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mason-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/formanek-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mcaloone-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/stenczel-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/yallup-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/bex-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/claireburke-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/hobson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/lasenby-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mhandley-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/whandley-headshot.jpg}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        %\includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/tdoa-cropped-1-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/tdoa-cropped-2-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/tdoa-cropped-3-crop.pdf}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        %\includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{How does Nested Sampling compare to other approaches?}
    \begin{columns}
        \column{0.7\textwidth}
        \begin{itemize}
            \item In all cases:
                \begin{itemize}
                    \item[$+$] NS can handle multimodal functions
                    \item[$+$] NS computes evidences, partition functions and integrals
                    \item[$+$] NS is self-tuning/black-box
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
    \end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \begin{itemize}
                \item Gradient descent
                    \begin{itemize}
                        \item[$-$] NS cannot use gradients
                        \item[$+$] NS does not require gradients
                    \end{itemize}
                \item Genetic algorithms
                    \begin{itemize}
                        \item[$+$] NS discarded points have statistical meaning
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Sampling}
            \begin{itemize}
                \item Metropolis-Hastings?
                    \begin{itemize}
                        \item[$-$] Nothing beats well-tuned customised MH
                        \item[$+$] NS is self tuning
                    \end{itemize}
                \item Hamiltonian Monte Carlo?
                    \begin{itemize}
                        \item[$-$] In millions of dimensions, HMC is king
                        \item[$+$] NS does not require gradients
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \begin{itemize}
                \item Thermodynamic integration
                    \begin{itemize}
                        \item[$+$] protective against phase trasitions
                        \item[$+$] No annealing schedule tuning 
                    \end{itemize}
                \item Sequential Monte Carlo
                    \begin{itemize}
                        \item[$-$] SMC experts classify NS as a kind of SMC
                        \item[$+$] NS is athermal
                    \end{itemize}
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Advantages and disadvantages of nested sampling}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{exampleblock}{Advantages}
            \begin{itemize}
                \item Doesn't need gradients
                \item Ensemble sampler
                \item Multimodal exploration
                \item Very Parallelisable
            \end{itemize}
        \end{exampleblock}
        \column{0.3\textwidth}
        \begin{alertblock}{Disadvantages}
            \begin{itemize}
                \item Doesn't use gradients
                \item Slow (but steady)
                \item Struggles with stochastic likelihoods (nondeterminism)
                \item Limited to $\sim\mathcal{O}(10^3)$ dimensions
            \end{itemize}
        \end{alertblock}
        \column{0.3\textwidth}
        \begin{block}{Unique elements}
            \begin{itemize}
                \item Ensemble sampler
                \item Estimates volumes, entropies and evidences
                \item Athermal evolution
                \item Order statistics
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{10pt}
    The main goal of using gradients is to improve dimensionality scaling/reliability
\end{frame}

\begin{frame}
    \frametitle{Gradients in nested sampling}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The challenge: Sample uniformly within likelihood contour:
                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
                making use of  $\nabla_\theta \log \mathcal{L}$
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:3>[width=\textwidth,page=1]{figures/himmelblau_gradient}%
        \includegraphics<2|handout:3>[width=\textwidth,page=2]{figures/himmelblau_gradient}%
        \includegraphics<3|handout:3>[width=\textwidth,page=3]{figures/himmelblau_gradient}%
        %\includegraphics<4|handout:3>[width=\textwidth,page=4]{figures/himmelblau_gradient}%
        %\includegraphics<5|handout:3>[width=\textwidth,page=5]{figures/himmelblau_gradient}%
        %\includegraphics<6|handout:3>[width=\textwidth,page=6]{figures/himmelblau_gradient}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why doesn't HMC work?}
    \textbf{``Tabletop'' distributions are difficult to sample!}
    \begin{itemize}
        \item Nested sampling requires you to sample from the truncated \emph{prior}, not the likelihood
        \item Other than at the boundaries, it is not obvious how to use a likelihood gradient to navigate the prior.
        \item In addition, since nested sampling begins in the tails, proceeding through the typical set and onto the peak, the normalisation of the gradient is typically wildly misnormalised
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Constrained Hamiltonian Monte Carlo~\arxiv{1005.0157}}
    \framesubtitle{aka: Gailean nested sampling~\arxiv{1312.5638}; Reflective slice sampling~\oldarxiv{physics/0009028}}

    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The primary way to sample from ``Tabletop'' distributions is with reflection:
                \begin{itemize}
                    \item Define start $x_0$, velocity $v$,
                    \item Update $x_{i+1} = x_i + v \Delta t$
                    \item When you reach the edge, reflect using $\hat{n}$:
                        $ v \to v - 2 (v\cdot \hat{n}) \hat{n}$
                    \item $n$ can be taken to be $\nabla \log P$
                \end{itemize}
            \item In practice since don't know the exact boundary, care needs to be taken to generate unbiased samples 
                \begin{itemize}
                    \item e.g. by reflecting whenever one is outside, not just once to get us back in.
                \end{itemize}
            \item Radford Neal~\oldarxiv{physics/0009028} section 7 is best reference for this.
        \end{itemize}

        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/reflect}
        \vspace{1em}

        \includegraphics[width=\textwidth]{figures/bounce}
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Historical attempts}
    \begin{description}
        \item[Betancourt] Hamiltonian constrained nested sampling~\arxiv{1005.0157}.
        \item[Feroz] Galilean Nested Sampling~\arxiv{1312.5638}.
        \item[Speagle] Incorporated into \texttt{dynesty}~\arxiv{1904.02180}.
        \item[Habeck] Habeck Demonic nested sampling -- uses thermodynamic analogy to soften the hard boundary with a Maxwell daemon~\doi{10.1063/1.4905971}.
        \item[Baldock] Total Enthalpy HMC, incoporating momenta in a more HMC like way, but specialised to materials science~\arxiv{1710.11085}.
        \item[Cai] ProxNest for high-dimensional convex imaging problems~\arxiv{2106.03646}.
    \end{description}
\end{frame}

\begin{frame}
    \frametitle{Things we have tried/are trying}
    \begin{description}[Stephen Thorpe]
        \item[Pablo Lemos] Updated existing HMC/Galilean nested sampling to use differentiable programming (jax/torch) -- code release imminent
        \item[Boris Deletic] Masters project with C++ implementation for lattice field theory calculations
        \item[Stephen Thorpe] Curved slice sampling.
        \item[Sam Leeney] Variety of options under exploration:
            \begin{itemize}
                \item Tuning the HMC mass matrix with iteration number
                \item Posterior repartitioning~\arxiv{1908.04655} to ``borrow'' some of the likelihood
                \item Sampling with $X$ rather than $\mathcal{L}$.
            \end{itemize}
    \end{description}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling with gradients}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Current techniques don't make use of:
            \item We have ``cloud'' of gradients at every point
            \item We have an estimate of the prior volume $X$
            \item $\nabla \log X \:\propto\: \nabla \log L$
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:3>[width=\textwidth,page=1]{figures/himmelblau_gradient}%
        \includegraphics<2|handout:3>[width=\textwidth,page=2]{figures/himmelblau_gradient}%
        \includegraphics<3|handout:3>[width=\textwidth,page=3]{figures/himmelblau_gradient}%
        \includegraphics<4|handout:3>[width=\textwidth,page=4]{figures/himmelblau_gradient}%
        \includegraphics<5|handout:3>[width=\textwidth,page=5]{figures/himmelblau_gradient}%
        \includegraphics<6|handout:3>[width=\textwidth,page=6]{figures/himmelblau_gradient}%
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{Applications of nested sampling}
%    \framesubtitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
%    \begin{columns}
%        \column{0.57\textwidth}
%        \begin{itemize}
%            \item Nested sampling for frequentist computation!?
%            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
%            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
%            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
%            \item Need insight/approximation to make efficient.
%            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
%            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
%        \end{itemize}
%        \column{0.43\textwidth}
%        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
%    \end{columns}
%\end{frame}

%\begin{frame}
%    \frametitle{Nested Sampling with Likelihood Free Inference}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}
%
%        \hfill Alsing \textit{et al.}~\arxiv{1903.00007}
%    \begin{itemize}
%        \item In density estimation likelihood free inference, the output is to learn one/all of:
%            \begin{description}
%                \item[Likelihood] $P(D|\theta)$,
%                \item[Posterior] $P(\theta|D)$,
%                \item[Joint] $P(D,\theta)$.
%            \end{description}
%        \item In the first instance, nested sampling can be used to scan these learnt functions.
%    \end{itemize}
%        \column{0.5\textwidth}
%    \begin{itemize}
%        \item Data are compressed, so joint space $(D,\theta)$ is navigable by off-the-shelf codes. 
%            \begin{itemize}
%                \item Sanity checking the solution,
%                \item Computing evidences/Kullback Liebler divergences from likelihoods.
%            \end{itemize}
%        \item Its self-tuning capacity and ability to handle multi-modal distributions can be very useful for diagnosing incompletely learnt functions.
%        \item Emulated likelihoods (e.g. normalising flows) are generally fast, so can deploy more likelihood hungry techniques like NS.
%        \item In principle can use it to train emulators by marginalisation rather than maximisation.
%    \end{itemize}
%    \end{columns}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Nested Sampling for Approximate Bayesian Computation/SBI}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item Assume one has a generative model capable of turning parameters into mock data $D(\theta)$.
%            \item Given infinite computing power, ABC works by selecting $\{\theta : D(\theta)=D_\text{observed}\}$.
%            \item These are samples from the posterior, without using a likelihood.
%            \item In practice $D=D_\text{obs}$ becomes $D\approx D_\text{obs}$.
%            \item i.e. $|D-D_\text{obs}|<\varepsilon$, or more generally $\boxed{\rho(D,D_\text{obs})<\varepsilon}$, where $\rho$ is some suitably chosen objective function.
%            \item Main challenges are 
%                \begin{enumerate}
%                    \item Choice of $\rho$/summary stats,
%                    \item Choice of $\varepsilon$ schedule,
%                    \item Rejection sampling.
%                \end{enumerate}
%        \end{itemize}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item Nested sampling fits this well: In principle, can just change the usual hard likelihood constraints $\{\theta\sim\pi : \mathcal{L}(\theta)>\mathcal{L}_*\}$ to
%                \[\{\theta~\sim\pi : \rho(D(\theta),D_\text{obs})<\varepsilon\}\]
%            (Brewer \& Foreman-Mackey~\arxiv{1606.03757})
%            \item Ongoing work with Andrew Fowlie \& Sebastian Hoof
%                \begin{itemize}
%                    \item How to deal with nondeterminism,
%                    \item How to interpret $\rho$ as a ``likelihood'',
%                    \item How to interpret the evidence $\mathcal{Z}$.
%                \end{itemize}
%        \end{itemize}
%    \end{columns}
%\end{frame}

%\begin{frame}
%    \frametitle{Nested sampling for truncated methods}
%
%    \begin{columns}
%        \column{0.55\textwidth}
%        \begin{itemize}
%            \item Will hear more on this tomorrow from Christoph
%            \item Many Likelihood implicit approaches at the moment have some element of sampling direct from the prior
%            \item Inefficient if number of parameters $>\mathcal{O}(\text{a few})$
%            \item Can get round this by truncating to region:
%                \[ \Gamma\{ \theta\in \text{supp} p(\theta) \:|\: p(\theta|x_0)>\bar\varepsilon\} \]
%            \item At the moment regions defined by nested boxes
%            \item \textbf{Frontier:} This seems ripe for replacement by NS -- PhD student currently working on this (Kilian Scheutwinkel)
%        \end{itemize}
%        \column{0.45\textwidth}
%        \includegraphics[width=\textwidth]{figures/tmnre}
%
%        \hfill Cole \textit{et al.}~\arxiv{2111.08030}
%    \end{columns}
%    
%\end{frame}


\begin{frame}
    \frametitle{Conclusions}
    \begin{itemize}
        \item Nested sampling is a robust, multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Performing Bayesian inference and model comparison.
            \end{itemize}
        \item It is applied widely across a variety of fields
        \item It can't currently use gradients very effectively.
        \item If it could, probababilistic programming means we can reasonably expect most codes to be able to provide them
        \item One of my main aims this week is to find ideas from other fields which NS could use.
    \end{itemize}
\end{frame}


%\begin{frame}
%    \frametitle{How does Nested Sampling compare to other approaches?}
%    \begin{columns}
%        \column{0.7\textwidth}
%        \begin{itemize}
%            \item In all cases:
%                \begin{itemize}
%                    \item[$+$] NS can handle multimodal functions
%                    \item[$+$] NS computes evidences, partition functions and integrals
%                    \item[$+$] NS is self-tuning/black-box
%                \end{itemize}
%        \end{itemize}
%        \column{0.3\textwidth}
%        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
%    \end{columns}
%    \begin{columns}[t]
%        \column{0.3\textwidth}
%        \begin{block}{Optimisation}
%            \begin{itemize}
%                \item Gradient descent
%                    \begin{itemize}
%                        \item[$-$] NS cannot use gradients
%                        \item[$+$] NS does not require gradients
%                    \end{itemize}
%                \item Genetic algorithms
%                    \begin{itemize}
%                        \item[$+$] NS discarded points have statistical meaning
%                    \end{itemize}
%            \end{itemize}
%        \end{block}
%        \column{0.3\textwidth}
%        \begin{block}{Sampling}
%            \begin{itemize}
%                \item Metropolis-Hastings?
%                    \begin{itemize}
%                        \item[$-$] Nothing beats well-tuned customised MH
%                        \item[$+$] NS is self tuning
%                    \end{itemize}
%                \item Hamiltonian Monte Carlo?
%                    \begin{itemize}
%                        \item[$-$] In millions of dimensions, HMC is king
%                        \item[$+$] NS does not require gradients
%                    \end{itemize}
%            \end{itemize}
%        \end{block}
%        \column{0.3\textwidth}
%        \begin{block}{Integration}
%            \begin{itemize}
%                \item Thermodynamic integration
%                    \begin{itemize}
%                        \item[$+$] protective against phase trasitions
%                        \item[$+$] No annealing schedule tuning 
%                    \end{itemize}
%                \item Sequential Monte Carlo
%                    \begin{itemize}
%                        \item[$-$] SMC experts classify NS as a kind of SMC
%                        \item[$+$] NS is athermal
%                    \end{itemize}
%            \end{itemize}
%        \end{block}
%    \end{columns}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Nested Sampling: a user's guide}
%    \begin{enumerate}
%        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
%            \begin{itemize}
%                \item This means typically most of its time is spent on burn-in rather than posterior sampling.
%                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable.
%            \end{itemize}
%        \item The number of live points $n_\text{live}$ is a resolution parameter.
%            \begin{itemize}
%                \item Run time is linear in $n_\text{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\text{live}}}$.
%                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
%            \end{itemize}
%        \item Most algorithms come with additional reliability parameter(s).
%            \begin{itemize}
%                \item e.g. \texttt{MultiNest}: $\texttt{eff}$, \texttt{PolyChord}: $n_\text{repeats}$.
%                \item These are parameters which have no gain if set too conservatively, but increase the reliability.
%                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
%            \end{itemize}
%    \end{enumerate}
%\end{frame}
%
%
%
%
%\begin{frame}
%    \frametitle{Occam's Razor~\arxiv{2102.11511}}
%    \begin{itemize}
%        \item Bayesian inference quantifies Occam's Razor:
%            \begin{itemize}
%                \item \textit{``Entities are not to be multiplied without necessity''} \hfill --- William of Occam
%                \item \textit{``Everything should be kept as simple as possible, but not simpler''} \hfill --- ``Albert Einstein''
%            \end{itemize}
%        %\item Consider the Evidence $\C[3]{\mathcal{Z}\equiv P(D|M)}$: 
%        %    \begin{description}[Parameter estimation]
%        %        \item [Parameter estimation] normalisation constant
%        %        \item [Model comparison] critical update factor for \C[5]{model prior} to \C[4]{model posterior}
%        %    \end{description}
%        \item Properties of the evidence: rearrange Bayes' theorem for parameter estimation
%            \[\C[0]{\mathcal{P}(\theta)} = \frac{\C[2]{\mathcal{L}(\theta)} \C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}}} \qquad\Rightarrow\qquad \C[3]{\log \mathcal{Z}} = \C[2]{\log\mathcal{L}(\theta)} - \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}}.\]  
%        \item Evidence is composed of a ``goodness of fit'' term  and ``Occam Penalty''.
%    \end{itemize}
%    \begin{columns}[t]
%        \column{0.5\textwidth}
%    \begin{itemize}
%        \item RHS true for all $\theta$. Take max likelihood value $\theta_*$:
%            \[
%                \log \mathcal{Z} = -\chi_\text{min}^2 - \text{Mackay penalty.}
%            \]
%    \end{itemize}
%        \column{0.5\textwidth}
%    \begin{itemize}
%        \item Be more Bayesian and take posterior average to get the ``Occam's razor equation''
%            \[
%                \boxed{
%                    \log \mathcal{Z} = \av[\mathcal{P}]{\log\mathcal{L}} - \mathcal{D}_\text{KL}.
%            }
%            \]
%    \end{itemize}
%    \end{columns}
%    \vfill
%    \begin{itemize}
%        \item Natural regularisation which penalises models with too many parameters.
%    \end{itemize}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Kullback Liebler divergence}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is is defined as:
%                \[\mathcal{D}_\text{KL} = \av[\mathcal{P}]{\log\frac{\mathcal{P}}{\pi}} = \int \mathcal{P}(\theta) \log \frac{\mathcal{P}(\theta)}{\pi(\theta)}d\theta.\]
%            \item Whilst not a distance, $\mathcal{D}=0$ when $\mathcal{P}=\pi$.
%            \item Occurs in the context of machine learning as an objective function for training functions.
%            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
%                \[ \mathcal{D}_\text{KL} \approx \log \frac{V_\pi}{V_\text{P}}.\]
%                (this is exact for top-hat distributions).
%        \end{itemize}
%        \column{0.5\textwidth}
%        \includegraphics{figures/volumes.pdf}
%    \end{columns}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
%    \begin{columns}
%        \column{0.55\textwidth}
%        \begin{itemize}
%            \item Nested sampling for frequentist computation!?
%            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
%            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
%            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
%            \item Need insight/approximation to make efficient.
%            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
%            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
%        \end{itemize}
%        \column{0.45\textwidth}
%        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
%    \end{columns}
%    
%\end{frame}
%
%\begin{frame}
%    \frametitle{Exploration of phase space~\arxiv{2106.02056}}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item Nested sampling for cross section computation/event generation.
%            \item Numerically compute collisional cross section 
%                \vspace{-5pt}\[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2,\]
%                $\Omega$ phase space of kinematic configurations $\Phi$, each with matrix element $\mathcal{M}(\Phi)$.
%            \item Current state of the art e.g. HAAG (improvement on RAMBO) requires knowledge of $\mathcal{M}(\Phi)$.
%            \item Nested sampling can explore the phase space and compute integral blind with comparable efficiency.
%        \end{itemize}
%        \column{0.5\textwidth}
%        \includegraphics[width=\textwidth]{figures/phase_space_1.pdf}
%        \includegraphics[width=\textwidth]{figures/phase_space.pdf}
%    \end{columns}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Quantification of fine tuning~\arxiv{2101.00428}~\arxiv{2205.13549}}
%    \vspace{-10pt}
%    \begin{columns}
%        \column{0.55\textwidth}
%        \begin{itemize}
%            \item Example: Cosmological constraints on decaying axion-like particles~\arxiv{2205.13549}.
%            \item Subset of parameters $\xi,m_a,\tau,g_{a\gamma}$: ALP fraction, mass, lifetime and photon coupling.
%                {(\small Also vary cosmology, $\tau_n$ and nuisance params)}
%            \item Data: CMB, BBN, FIRAS, SMM, BAO.
%            \item Standard profile likelihood fit shows ruled out regions and best-fit point.
%            \item<2-> Nested sampling scan:
%                \begin{itemize}
%                    \item Quantifies amount of parameter space ruled out with Kullback-Liebler divergence $\mathcal{D}_\mathrm{KL}$.
%                    \item Identifies best fit region as statistically irrelevant from information theory/Bayesian.
%                    \item No evidence for decaying ALPs. Fit the data equally well: but more constrained parameters create Occam penalty.
%                \end{itemize}
%        \end{itemize}
%        \column{0.45\textwidth}
%        \includegraphics<1|handout:0>[width=\textwidth]{figures/ALP_1.pdf}
%        \includegraphics<2          >[width=\textwidth]{figures/ALP_2.pdf}
%        \includegraphics<3|handout:0>[width=\textwidth]{figures/ALP_3.pdf}
%    \end{columns}
%    
%\end{frame}



\appendix
\begin{frame}
    \frametitle{Time complexity of nested sampling}
    \includegraphics[width=\textwidth]{figures/run_prodecure}
    \vspace{-20pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-20pt}
        \begin{itemize}
            \item $x$-axis: log-compression of live points
            \item Area $\propto$ posterior mass
            \item Shows Bayesian balance of likelihood vs prior
            \item Run proceeds right to left
            \item Run finishes after bump (typical set)
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Time complexity\vspace{-5pt}
                \[ \boxed{T = n_\mathrm{live} \times T_\mathcal{L} \times T_\mathrm{sampler} \times \mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)}\vspace{-5pt}  \]
            \item Error complexity $\boxed{\sigma \propto \sqrt{\mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)/n_\mathrm{live}}}$
        \end{itemize}
    \end{columns}

\end{frame}

\end{document}
```
# branch: remotes/origin/reach_2023
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{REACH: Nested sampling tools}
%\subtitle{<+subtitle+>}
\date{26\textsuperscript{th} September 2023}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{What do we use nested sampling for?}
        Given a (scalar) function $f$ with a vector of parameters $x$, nested sampling can be used for:
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[x_\text{max} = \max_x{f(x)}\]
            \hfill \emph{Experimental design}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad x\sim f\]
            \hfill \emph{Parameter estimation}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(x) dV \]
            \hfill \emph{Model comparison}
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame} \frametitle{The three pillars of Bayesian inference}
    \vspace{-20pt}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Parameter estimation}
            What do the data tell us about the parameters of a model?

            \textit{e.g. the central frequency $\nu_0$ of Gaussian global signal}
            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Model comparison}
            How much does the data support a particular model?

            \textit{e.g. $\Lambda$CDM vs cosmic strings}
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}} \vspace{-7pt}\]
            \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[5]{\Pi_\mathcal{M}}}{\C[7]{\sum_m Z_m \Pi_m}} \]
            \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Tension quantification}
            Do different datasets make consistent predictions from the same model? 
            \textit{e.g. SARAS vs EDGES}
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
        \end{block}
    \end{columns}
    \begin{itemize}
        \item Note, REACH has been a powerful ambassador for ``model'' involving theory, systematics and inference parameters~\arxiv{2204.04491}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Integration in Physics}
    \begin{itemize}
        \item Integration is a fundamental concept in physics, statistics and data science:
    \end{itemize}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Partition functions}
            \vspace{-11pt}
            \[ Z(\beta) = \int e^{-\beta H(q,p)} dq dp \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Path integrals}
            \[ \Psi = \int e^{i S} \mathcal{D}x \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Bayesian marginals}
            \vspace{-11pt}
            \[ \mathcal{Z}(D) = \int \mathcal{L}(D|\theta) \pi(\theta) d\theta \]
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Need numerical tools if analytic solution unavailable.
            \item High-dimensional numerical integration is hard.
            \item Riemannian strategy estimates volumes geometrically:
                \[ \int f(x) d^nx \approx \sum_i f(x_i) \Delta V_i \sim \mathcal{O}(e^n) \]
            \item Curse of dimensionality $\Rightarrow$ exponential scaling.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{The nested sampling meta-algorithm: live points}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n$ random samples over the space.
            \item Delete outermost sample, and replace with a new random one at higher integrand value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n}\only<9->{\pm \frac{1}{n}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \int f(x) dV \approx \sum_i f(x_i) \Delta V_i, \quad V_i = V_0 e^{-\only<9->{(}i\only<9->{\pm\sqrt{i})}/n} \]
%            \item Nested sampling: completely different way to scan.
%            \item Ensemble sampling compresses entire space$\to$peak(s).
%            \item Sequentially update a set $S$ of $n$ samples:
%                \begin{itemize}
%                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from a measure $\pi$). 
%
%                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
%                \end{itemize}
%            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard constraint}:
%                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
%            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence}/integral of function \& \C[0]{posterior}/sample weights.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4|handout:0>[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<8-         >[width=\textwidth,page=8]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: dead points}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Can be weighted to form posterior samples, prior samples, or anything in between.
            \item Nested sampling estimates the \textbf{density of states} and calculates partition functions
                \[Z(\beta) = \sum_i f(x_i)^\beta \Delta V_i.\]
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune
                    \item exploration of multimodal functions
                    \item global and local optimisation
                \end{itemize}
            %\item Interpreted as a Bayesian algorithm, it
            %    \begin{itemize}
            %        \item Computes the Bayesian evidence (model comparison)
            %        \item Produces (weighted) posterior samples (parameter estimation)
            %    \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The dead measure}
    \includegraphics[width=\textwidth]{figures/dead_measure}
    \begin{columns}
        \column{0.69\textwidth}
        \begin{itemize}
            \item Dead points have a unique scale-invariant distribution $\propto\: \tfrac{dV}{V}$.
            \item Uniform over original region, exponentially concentrating on region of interest (until termination volume).
            \item Full coverage of tails enables integration.
            \item Good for training emulators (HERA~\arxiv{2108.07282}).
        \end{itemize}
        \column{0.3\textwidth}
        \begin{block}{Applications}
        \begin{itemize}
            \item training emulators.
            \item gridding simulations
            \item beta flows\ldots
            \item good name for a band
        \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{\texttt{anesthetic}: the nested sampling toolkit}
    \student{adam_ormondroyd}{Adam Ormondroyd {\tiny \& Lukas Hergt}}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Post-processing nested sampling calculations requires care (e.g.\ weighted samples)
            \item \texttt{anesthetic} does this all for you
            \item pip installable (\texttt{pip install anesthetic})
            \item docs: \href{https://anesthetic.readthedocs.io/en/latest/}{\texttt{anesthetic.readthedocs.io}}
            \item nested sampling analysis (merging, sampling, weighting)
            \item dynamic replaying of nested sampling
            \item plotting
            \item built on \texttt{pandas} \& \texttt{matplotlib}\\ (extending to weighted dataframes)
        \end{itemize}
        \column{0.5\textwidth}
        \centerline{\includegraphics[width=\textwidth]{figures/anesthetic_docs}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Replaying nested sampling}
    \includegraphics[width=\textwidth]{figures/dead_measure_live}
    \begin{columns}
        \column{0.65\textwidth}
        \begin{itemize}
            \item Can extract live points at iteration $i$ from dead points
            \item Can extract posterior at any temperature
            \item Try out GUI for dynamically replaying run:
                \begin{itemize}
                    \item \texttt{\$\ \ \  anesthetic <file root> \#bash}
                    \item \texttt{>>> samples.gui()\ \ \ \ \ \ \ \ \ \ \#python}
                \end{itemize}
        \end{itemize}
        \column{0.34\textwidth}
        \begin{block}{Applications}
        \begin{itemize}
            \item diagnosing run failures
            \item training emulators
            \item multiobjective optimisation
        \end{itemize}
        \end{block}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{\texttt{anesthetic} vs \texttt{getdist} vs \texttt{corner.py}}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item \texttt{anesthetic}
                \begin{itemize}
                    \item Defaults optimised for nested sampling
                    \item Colour choice better for priors
                    \item Not much smoothing
                \end{itemize}
            \item \texttt{getdist}
                \begin{itemize}
                    \item State-of-the-art smoothing \& edge correction
                    \item Often over-smoothed
                \end{itemize}
            \item \texttt{corner.py}
                \begin{itemize}
                    \item Good for MCMC histograms
                    \item Can't do overlapping plots
                    \item Gotcha: Slightly different ``2$\sigma$'' convention
                \end{itemize}
        \end{itemize}
        
        \column{0.5\textwidth}
        \centerline{
            \includegraphics<1>[width=0.85\textwidth]{figures/posterior_plotter}%
        }
        \includegraphics<2>[width=\textwidth]{figures/anesthetic_kde}%
        \includegraphics<3>[width=\textwidth]{figures/anesthetic_hist}%
        \includegraphics<4>[width=\textwidth]{figures/anesthetic_hist_levels}%
        \includegraphics<5>[width=\textwidth]{figures/getdist}%
        \includegraphics<6>[width=\textwidth]{figures/corner}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Likelihood-free inference (aka SBI)}
    \student{kilian_scheutwinkel}{Kilian Scheutwinkel}{PhD}
    \vspace{10pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item How do you do inference if you don't know the likelihood $P(D|\theta)$?
            \item If you can forward simulate/model $\theta\to D$, then you have an implicit likelihood.
            \item LFI aims to (machine-)\emph{learn} the likelihood from forward simulations $\{(\theta,D)\}$.
            \item Current REACH-related work
                \begin{itemize}
                    \item Anchal \texttt{swyft}
                    \item Kilian {\footnotesize \texttt{PolySwyft}:~likelihood-free~nested~sampling}
                \end{itemize}
            \item In my view this is the frontier of REACH data analysis
            \item We have a rudimentary version of this with work on ``likelihood selection''~\arxiv{2204.04491}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/noisy.pdf}
        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Fully Bayesian Forecasting~\arxiv{2309.06942}}
    \student{thomas_gessey-jones}{Thomas Gessey-Jones}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Experimental design necessitates forecasting the constraints that future data might give.
            \item Have you ever done a Fisher forecast, and then felt Bayesian guilt?
            \item Simulation based inference gives us the language to marginalise over parameters $\theta$ and possible future data $D$.
            \item Evidence networks~\arxiv{2305.11241} (Jeffreys \& Wandelt) give us the ability to do this at scale in the case of forecasting.
            \item Can answer questions such as ``Given current knowledge/theoretical uncertainty $\pi(\theta)$, how probable is a detection?''
            \item Re-usable package: \texttt{prescience}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fbf.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Future work/discussion points}
    \begin{itemize}
        \item \{multiobjective,\}$ $\{antenna,\}\{optimisation\} (John)
        \item Fully Bayesian Forecasts (Thomas)
        \item Simulation based inference (Kilian \& Anchal)
        \item margarine (Harry)
        \item High-dimensional gradient-based nested sampling (Sam)
    \end{itemize}
\end{frame}
% What could we do better in antenna design in future?
% multiobjective optimisation
% evidence networks

\end{document}
```
# branch: remotes/origin/stockholm_2023
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{pythonhighlight}
\usepackage{title_page}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{fit}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{\texttt{unimpeded} (\& other cosmological inference tools)}
%\subtitle{Marginal statistics and fully Bayesian forecasts}
\date{19\textsuperscript{th} October 2023}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}
\begin{frame}
    \frametitle{Highlighted tools for this audience}
    \begin{enumerate}
        \item \texttt{anesthetic} \arxiv{1905.04768}
            \begin{itemize}
                \item how to get the most out of your nested sampling runs 
\item \texttt{\href{https://github.com/handley-lab/anesthetic}{github.com/handley-lab/anesthetic}}
            \end{itemize}
        \item \texttt{margarine} \arxiv{2207.11457}
            \begin{itemize}
                \item easy-to-use (neural) density estimation for marginal statistics
                \item \texttt{\href{https://github.com/htjb/margarine}{github.com/htjb/margarine}} 
            \end{itemize}
        \item \texttt{prescience} \arxiv{2309.06942}
            \begin{itemize}
                \item Fully Bayesian forecasts (no more need for Fisher)
                \item \texttt{\href{https://github.com/ThomasGesseyJones/FullyBayesianForecastsExample}{github.com/ThomasGesseyJones/FullyBayesianForecastsExample}} 
            \end{itemize}
        \item \texttt{unimpeded}
            \begin{itemize}
                \item up/downloading tool for transfering inference products (beyond chains)
                \item \texttt{\href{https://github.com/handley-lab/unimpeded}{github.com/handley-lab/unimpeded}} 
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Marginal inference}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Many cosmological likelihoods come with nuisance parameters that have limited relevance for onward inference.
            \item Notation: $\mathcal{L} = P(D|\theta,\alpha,M)$
                \begin{itemize}
                    \item[$\mathcal{L}$] Likelihood \hfill (e.g. \texttt{plik}),
                    \item[$D$] Data \hfill (e.g. CMB),
                    \item[$\theta$] Cosmological parameters \hfill (e.g. $\Omega_m$, $H_0$\ldots),
                    \item[$\alpha$] Nuisance parameters \hfill (e.g. $A_\text{planck}$\ldots),
                    \item[$M$] Model \hfill (e.g. $\Lambda$CDM).
                \end{itemize}
            \item Some marginal statistics (e.g. marginal means, posteriors\ldots) are easy to compute.
            \item More machinery is needed for e.g. nuisance marginalised likelihoods and marginal KL divergences $\mathcal{D}_\text{KL}$.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/planck_2018_plik.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Relevant examples for this audience}
    \begin{enumerate}
        \item CMB
            \begin{itemize}
                \item Galatic foreground parameters: $A_i$, $n_i$,\ldots
                \item Calibration: $c$, $y$,\ldots
            \end{itemize}
        \item Gravitational wave events
            \begin{itemize}
                \item Nuisance event parameters: $M_1$, $M_2$, $\theta$, $\phi$, $r$, $z$, \ldots
            \end{itemize}
        \item (time-delay) supernovae
            \begin{itemize}
                \item Nuisance lens parameters: $\Phi_\text{lens}$
                \item Supernova systematics
            \end{itemize}
    \end{enumerate}
    \begin{itemize}
        \item Many modern cosmological problems present themselves in hierarchical form
        \item These (often well-studied) objects have a set of event-specific parameters $\alpha$ alongside  parameters of global interest $\theta$ (e.g. $H_0$).
        \item Ideally we would be able to have a ``nuisance marginalised'' likelihood for combining hierarchically.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Nuisance marginalised likelihoods: Theory \arxiv{2207.11457}}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item Bayes theorem
                \begin{align}
                    \C[2]{\mathcal{L}}(\theta,\alpha) 
                    \times 
                    \C[1]{\pi}(\theta,\alpha) &= 
                    \C[0]{\mathcal{P}}(\theta,\alpha)
                    \times
                    \C[3]{\mathcal{Z}}\\
                    \C[2]{\text{Likelihood}}
                    \times
                    \C[1]{\text{Prior}}
                    &=
                    \C[0]{\text{Posterior}}
                    \times
                    \C[3]{\text{Evidence}}
                    \nonumber
                \end{align}
                \small{$\alpha$: nuisance parameters, $\theta$: cosmo parameters.}
            \item Marginal Bayes theorem
                \begin{equation}
                    \C[2]{\mathcal{L}}(\theta) 
                    \times 
                    \C[1]{\pi}(\theta) = 
                    \C[0]{\mathcal{P}}(\theta)
                    \times
                    \C[3]{\mathcal{Z}}
                \end{equation}
            \item Non-trivially gives \textbf{nuisance-free likelihood}
                \begin{equation}
                    \boxed{
                        \C[2]{\mathcal{L}}(\theta) 
                        = 
                        \frac{
                            \C[0]{\mathcal{P}}(\theta)
                            \C[3]{\mathcal{Z}}
                        }{
                            \C[1]{\pi}(\theta)
                        }
                    }
                    =
                    \frac{
                        \int \C[2]{\mathcal{L}}(\theta,\alpha) \C[1]{\pi}(\theta,\alpha) d{\alpha}
                    }
                    {
                        \int \C[1]{\pi}(\theta,\alpha) d{\alpha}
                    }
                \end{equation}
        \end{itemize}
        \column{0.5\textwidth}
        \textbf{Key properties}
        \begin{itemize}
            \item Given datasets $A$ and $B$, each with own nuisance parameters $\alpha_A$ and $\alpha_B$:
            \item If you use $\mathcal{L}_A(\theta)$, you get the same (marginal) posterior and evidence if you had run with nuisance parameters $\alpha_A$ (ditto $B$).
            \item If you run inference on $\mathcal{L}_A(\theta)\times\mathcal{L}_B(\theta)$, you get the same (marginal) posterior and evidence if you had run with all nuisance parameters $\alpha_A$, $\alpha_B$ on.
            \item[] \textit{(weak marginal consistency requirements on joint $\pi(\theta,\alpha_A,\alpha_B)$ and marginal priors)}
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nuisance marginalised likelihoods: Practice}
    \student{harry_bevins}{Harry Bevins}{PhD$\to$JRF}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{columns}
            \column{0.3\textwidth}
            \[
                \boxed{
                    \C[2]{\mathcal{L}}(\theta) 
                    = 
                    \frac{
                        \C[0]{\mathcal{P}}(\theta)
                        \C[3]{\mathcal{Z}}
                    }{
                        \C[1]{\pi}(\theta)
                    }
                }
            \]
            \column{0.7\textwidth}
            \begin{itemize}
                \item To compute the nuisance marginalised likelihood, need:
                    \begin{enumerate}
                        \item Bayesian evidence $\C[3]{\mathcal{Z}}$
                        \item Marginal prior and posterior densities
                    \end{enumerate}
            \end{itemize}
        \end{columns}
        \begin{enumerate}
            \item Use nested sampling to compute evidence $\C[3]{\mathcal{Z}}$ and marginal samples $\{\theta,\alpha\}_\mathcal{P}$ and $\{\theta,\alpha\}_\pi$.
            \item Use normalising flows to compute density estimators $\C[0]{\mathcal{P}(\theta)}$, $\C[1]{\pi(\theta)}$ from marginal samples.
        \end{enumerate}
        \begin{itemize}
            %\item Combination termed \texttt{margarine} \arxiv{2205.12841}
            \item Emulators usually much faster than original likelihoods
            \item Library of pre-trained bijectors to be used as priors/emulators/nuisance marginalised likelihoods
            \item e.g. easy to apply a \textit{Planck}/DES/HERA/JWST prior or likelihood to your existing MCMC chains without needing to install the whole cosmology machinery.
        \end{itemize}

        \column{0.4\textwidth}
        \begin{tikzpicture}[
                rednode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
                bluenode/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
                greennode/.style={rectangle, draw=green!60, very thick, minimum size=5mm},
                node distance=0.5cm,
                remember picture, overlay
            ]
            \node<1->[bluenode, xshift=0.5\textwidth, yshift=-0.25\textwidth](likelihood) at (current page.north)  {$ \mathcal{L}(\theta,\alpha)$};
            \node<1->[bluenode, right = of likelihood.east](prior) {$ \pi(\theta,\alpha)$};

            \coordinate<1-> (likelihoodprior) at ($(likelihood.south)!0.5!(prior.south)$);

            \node<2->[rednode, below = of likelihoodprior](nestedsampling) {Nested Sampling};
            \draw<2->[->](likelihood.south) -- (likelihood|-nestedsampling.north);
            \draw<2->[->](prior.south) -- (prior|-nestedsampling.north);

            \node<3->[bluenode, below = of nestedsampling](posterior) {$ \{\theta,\alpha\}_\mathcal{P}$};
            \draw<3->[->](nestedsampling.south-|posterior) -- (posterior.north);
            \node<4->[bluenode, left = of posterior.west](evidence) {$ \mathcal{Z}$};
            \draw<4->[->](nestedsampling.south-|likelihood) -- (evidence.north);
            \node<5->[bluenode, right = of posterior.east](priorSamples) {$ \{\theta,\alpha\}_\pi$};
            \draw<5->[->](nestedsampling.south-|prior) -- (priorSamples.north);

            \coordinate<5-> (posteriorprior) at ($(posterior.south)!0.5!(priorSamples.south)$);

            \node<6->[rednode, below = of posteriorprior](margarine)  {Density Estimation};

            \draw<6->[->](posterior.south) -- (margarine.north-|posterior.east);
            \draw<6->[->](priorSamples.south) -- (margarine.north-|priorSamples.west);

            \node<7->[bluenode, below = of posterior|-margarine.south](marginalPosterior) {$ \mathcal{P}(\theta)$};


            \draw<7->[->](margarine.south-|marginalPosterior.east) -- (marginalPosterior.north);


            \node<8->[bluenode, below = of marginalPosterior.south-|margarine.south-|priorSamples](marginalPrior) {$ \pi(\theta)$};
            \draw<8->[->](margarine.south-|priorSamples.west) -- (marginalPrior.north);


            \node<9->[bluenode, below = of marginalPosterior](marginalLikelihood) {$ \mathcal{L}(\theta)$};


            \draw<9->[->](evidence.south) -- (marginalLikelihood.west);
            \draw<9->[->](marginalPosterior.south) -- (marginalLikelihood.north);
            \draw<9->[->](marginalPrior.west) -- (marginalLikelihood.east);

            \node<10->[greennode,behind path,fit=(nestedsampling) (marginalPosterior) (priorSamples) (evidence),] {};

        \end{tikzpicture}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Combination}
    \begin{columns}
        \column{0.5\textwidth}
%\begin{tikzpicture}[squarednodeA/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
%squarednodeB/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
%squarednodeC/.style={rectangle, draw=green!60, fill=green!5, very thick, minimum size=5mm}]
%
%\node[squarednodeA, text width=3cm, align=center](inference3) at (17, -1.5) {Nested Sampling with $\theta$, $\alpha_A$ and $\alpha_B$};
%
%\node[squarednodeB](fulllikelihood1) at (15, 1.5){$ \mathcal{L}_A(\theta,\alpha_A)$};
%\node[squarednodeB](fulllikelihood2) at (16.85, 1.5){$ \mathcal{L}_B(\theta,\alpha_B)$};
%\node[squarednodeB](fulljointlikelihood) at (16, 0){$ \mathcal{L}_A(\theta,\alpha_A) \mathcal{L}_B(\theta,\alpha_B)$};
%\node[squarednodeB](fullprior) at (19, 1.5){$ \pi_{AB}(\theta,\alpha_A, \alpha_B)$};
%
%\draw[->](fulllikelihood1.south) -- (15.5, 0.3);
%\draw[->](fulllikelihood2.south) -- (16.5, 0.3);
%\draw[->](fullprior.south) -- (inference3.north);
%\draw[->](fulljointlikelihood.south) -- (inference3.north);
%
%\node[squarednodeB](jointEvidence2) at (15, -3){$ \mathcal{Z}_{AB}$};
%\node[squarednodeB](jointPosterior2) at (17, -3){$ \{\theta\}_{\mathcal{P}_{AB}}$};
%
%\draw[<-](jointEvidence2.north) -- (16, -2);
%\draw[<-](jointPosterior2.north) -- (inference3.south);
%
%\node[squarednodeB](jointPosteriorNuisance) at (19, -3){$ \{\alpha_A, \alpha_B\}_{\mathcal{P}_{AB}}$};
%\draw[->](18, -2) -- (jointPosteriorNuisance.north);
%\draw[blue,thick](16.2, -3.5) -- (20.1, -3.5) -- (20.1, -2.5) -- (16.2, -2.5) -- (16.2, -3.5);
%
%\end{tikzpicture}

        \includegraphics[width=0.7\textwidth]{figures/full_margarine.pdf}

        \column{0.5\textwidth}
%\begin{tikzpicture}[squarednodeA/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
%squarednodeB/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
%squarednodeC/.style={rectangle, draw=green!60, fill=green!5, very thick, minimum size=5mm}]
%
%\node[squarednodeB](likelihood1) at (6, 1.5){$ \mathcal{L}_A(\theta,\alpha_A)$};
%\node[squarednodeB](prior1) at (8, 1.5){$ \pi_A(\theta,\alpha_A)$};
%
%\node[squarednodeC, text width=3cm, align=center](NestedMarg1) at (7, 0){Nested Sampling + \textsc{Margarine}};
%
%\draw[->](likelihood1.south) -- (6, 0.5);
%\draw[->](prior1.south) -- (8, 0.5);
%
%\node[squarednodeB](marglike1) at (6, -1.5){$ \mathcal{L}_A(\theta)$};
%\node[squarednodeB](margprior1) at (9, -1.5){$ \pi(\theta)$};
%
%\draw[->](6, -0.5) -- (6, -1.2);
%\draw[->](8, -0.5) -- (9, -1.2);
%
%\node[squarednodeB](likelihood2) at (10, 1.5){$ \mathcal{L}_B(\theta,\alpha_B)$};
%\node[squarednodeB](prior2) at (12, 1.5){$ \pi_B(\theta,\alpha_B)$};
%
%\draw[->](likelihood2.south) -- (10, 0.5);
%\draw[->](prior2.south) -- (12, 0.5);
%
%\node[squarednodeC, text width=3cm, align=center](NestedMarg1) at (11, 0){Nested Sampling + \textsc{Margarine}};
%
%\node[squarednodeB](marglike2) at (12, -1.5){$ \mathcal{L}_B(\theta)$};
%\draw[->](12, -0.5) -- (12, -1.2);
%\draw[->](10, -0.5) -- (9, -1.2);
%
%\node[squarednodeB](combinedlike) at (7, -3){$ \mathcal{L}_A(\theta) \mathcal{L}_B(\theta)$};
%
%\draw[->](marglike1.south) -- (combinedlike.north);
%\draw[->](marglike2.south) -- (combinedlike.north);
%
%\node[squarednodeA, text width=3cm, align=center](inference2) at (9, -4.5) {Nested Sampling with $\theta$};
%
%\draw[->](combinedlike.south) -- (8, -4);
%\draw[->](margprior1.south) -- (10, -4);
%
%\node[squarednodeB](jointEvidence) at (8, -6){$ \mathcal{Z}_{AB}$};
%\node[squarednodeB](jointPosterior) at (10, -6){$ \{\theta\}_{\mathcal{P}_{AB}}$};
%
%\draw[->](8, -5) -- (jointEvidence.north);
%\draw[->](10, -5) -- (jointPosterior.north);
%\end{tikzpicture}
        \includegraphics[width=0.9\textwidth]{figures/quick_margarine.pdf}
        
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{History of margarine}
    \begin{itemize}
        \item Papamakarios et al \arxiv{1912.02762} (normalising flows)
        \item Alsing et al \arxiv{1903.00007} (Delfi)
        \item Nested sampling with any prior you like (Alsing \& Handley) \arxiv{2102.12478}
        \item \texttt{margarine} (theory) Bevins et al \arxiv{2207.11457}
        \item \texttt{margarine} (practice) Bevins et al \arxiv{2205.12841}
    \end{itemize}

\end{frame}

\begin{frame}[fragile]
    \frametitle{\texttt{unimpeded}}
    \framesubtitle{Universal Model comparison and Parameter Estimation Distributed over Every Dataset}
    \student{dily_ong}{Dily Ong}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Python tool for seamlessly downloading, uploading and cacheing of chains
        \item Data stored on \texttt{zenodo} 
        \item hdf5 storage for fast \& reliable storage
        \item \texttt{anesthetic} compatible for processing of chains~\arxiv{1905.04768}
        \item $\alpha$-testers wanted! (email \href{mailto:wh260@cam.ac.uk}{wh260@cam.ac.uk}) 
        \item End goal -- community library which everyone contributes to so expensive inference products are reusable and reused.
    \end{itemize}
        \column{0.5\textwidth}


\lstset{language=Python}
\lstset{frame=lines}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
from unimpeded import Unimpeded
store = Unimpeded(cache='data.hdf5')
samps = store('planck')
samps.H0.plot.kde_1d()
samps = store('planck', model='klcdm')
samps.H0.plot.kde_1d()
\end{lstlisting}
\includegraphics[width=\textwidth]{figures/unimpeded.pdf}

        
    \end{columns}
\end{frame}
 

\begin{frame}
    \frametitle{Inference Legacy Archive}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item DiRAC 2020 RAC allocation of 30MCPUh
            \item Main goal: Planck Legacy Archive equivalent
            \item Parameter estimation $\to$ Model comparison
            \item MCMC $\to$ Nested sampling
            \item Planck $\to$ $\{\text{Planck}, \text{DESY1}, \text{BAO}, \ldots \}$
            \item Pairwise combinations
            \item Suite of tools for processing these 
                \begin{itemize}
                    \item \texttt{anesthetic} $2.0$
                    \item \texttt{unimpeded} $1.0$
                    \item \texttt{zenodo} archive
                    \item \texttt{margarine}
                \end{itemize}
            \item MCMC chains also available.
            \item Library of bijectors emulators for fast re-use
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{logos/dirac}
        \includegraphics[width=0.5\textwidth,page=21]{figures/himmelblau}%
        \includegraphics[width=0.5\textwidth,page=15]{figures/himmelblau}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Cosmological forecasting}
    \student{thomas_gessey-jones}{Thomas Gessey-Jones}{PhD}
    \framesubtitle{Have you ever done a Fisher forecast, and then felt Bayesian guilt?}
    \vspace{-20pt}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item Cosmologists are interested in forecasting what a Bayesian analysis of future data might produce.
            \item Useful for:
                \begin{itemize}
                    \item white papers/grants,
                    \item optimising existing instruments/strategies,
                    \item picking theory/observation to explore next.
                \end{itemize}
            \item To do this properly:
                \begin{enumerate}
                    \item start from current knowledge $\pi(\theta)$, derived from current data
                    \item Pick potential dataset $D$ that might be collected from $P(D)\: (=\mathcal{Z})$
                    \item Derive posterior $P(\theta|D)$
                    \item Summarise science (e.g. constraint on $\theta$, ability to perform model comparison)
                \end{enumerate}
        \end{itemize}

        \column{0.5\textwidth}
        \begin{itemize}
            \item This procedure should be marginalised over:
                \begin{enumerate}
                    \item All possible parameters $\theta$ (consistent with prior knowledge)
                    \item All possible data $D$
                \end{enumerate}
            \item i.e. marginalised over the joint $P(\theta,D)=P(D|\theta)P(\theta)$.
            \item Historically this has proven very challenging.
            \item Most analyses assume a fiducial cosmology $\theta_*$, and/or a Gaussian likelihood/posterior (c.f. Fisher forecasting).
            \item This runs the risk of biasing forecasts by baking in a given theory/data realisation.
        \end{itemize}
        
    \end{columns}

\end{frame}


\begin{frame}
    \frametitle{Fully Bayesian Forecasting~\arxiv{2309.06942}}
    \student{thomas_gessey-jones}{Thomas Gessey-Jones}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Simulation based inference gives us the language to marginalise over parameters $\theta$ and possible future data $D$.
            \item Evidence networks~\arxiv{2305.11241} give us the ability to do this at scale for forecasting.
            \item Demonstrated in 21cm global experiments, marginalising over:
                \begin{itemize}
                    \item theoretical uncertainty
                    \item foreground uncertainty
                    \item systematic uncertainty
                \end{itemize}
            \item Able to say ``at 67mK radiometer noise'', have a 50\% chance of 5$\sigma$ Bayes factor detection.
            \item Can use to optimise instrument design
            \item Re-usable package: \texttt{prescience}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fbf.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\href{https://www.github.com/handley-lab}{github.com/handley-lab}}
    \tikz[overlay,remember picture]
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
            \includegraphics[width=0.1\textheight]{figures/students/adam_ormondroyd.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/cole_meldorf.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/david_yallup.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/dily_ong.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/harry_bevins.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/ian_roque.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/george_carter.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/kilian_scheutwinkel.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/metha_prathaban.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/namu_kroupa.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/sinah_legner.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/thomas_gessey-jones.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/wei-ning_deng.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/will_barker.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/zixiao_hu.jpg}%
    };
    \begin{enumerate}
        \item \texttt{anesthetic} \arxiv{1905.04768}
            \begin{itemize}
                \item how to get the most out of your nested sampling runs 
\item \texttt{\href{https://github.com/handley-lab/anesthetic}{github.com/handley-lab/anesthetic}}
            \end{itemize}
        \item \texttt{margarine} \arxiv{2207.11457}
            \begin{itemize}
                \item easy-to-use (neural) density estimation for marginal statistics
                \item \texttt{\href{https://github.com/htjb/margarine}{github.com/htjb/margarine}} 
            \end{itemize}
        \item \texttt{prescience} \arxiv{2309.06942}
            \begin{itemize}
                \item Fully Bayesian forecasts (no more need for Fisher)
                \item \texttt{\href{https://github.com/ThomasGesseyJones/FullyBayesianForecastsExample}{github.com/ThomasGesseyJones/FullyBayesianForecastsExample}} 
            \end{itemize}
        \item \texttt{unimpeded}
            \begin{itemize}
                \item up/downloading tool for transfering inference products (beyond chains)
                \item \texttt{\href{https://github.com/handley-lab/unimpeded}{github.com/handley-lab/unimpeded}} 
            \end{itemize}
    \end{enumerate}
\end{frame}
 
%\appendix
%\begin{frame}
%    \frametitle{A word on emulators: the dead measure}
%    \includegraphics[width=\textwidth]{figures/dead_measure}
%    \begin{columns}
%        \column{0.69\textwidth}
%        \begin{itemize}
%            \item At the end, one is left with a set of discarded ``dead'' points.
%            \item Dead points have a unique scale-invariant distribution $\propto\: \tfrac{dV}{V}$.
%            \item Uniform over original region, exponentially concentrating on region of interest (until termination volume).
%            \item Good for training emulators (HERA~\arxiv{2108.07282}).
%        \end{itemize}
%        \column{0.3\textwidth}
%        \begin{block}{Applications}
%        \begin{itemize}
%            \item training emulators.
%            \item gridding simulations
%            \item beta flows
%            \item ``dead measure'' 
%        \end{itemize}
%        \end{block}
%    \end{columns}
%\end{frame}
\end{document}
```
# branch: remotes/origin/ucl_2023
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usepackage{pythonhighlight}
\usepackage{tikz}
\usetikzlibrary{calc}
\newcommand{\av}[2][]{\left\langle #2\right\rangle_{#1}}
\newcommand{\student}[3]{
    \tikz[overlay,remember picture]{
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {\includegraphics[width=0.09\textheight]{figures/students/#1}};
        \node[anchor=north east] (B) at (A.north west) {#2};
        \node[anchor=north east] (B) at (B.south east) {\tiny #3};
    }
    \vspace{-0.1\textheight}
}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{ Nested sampling: {\large powering next-generation inference and machine learning tools}}
\subtitle{for astrophysics, cosmology, particle physics and beyond}
\date{18\textsuperscript{th} September 2023}

\begin{document}



\begin{frame}
    \titlepage
\end{frame}

%\begin{frame}
%    \frametitle{Highlight: state-of-the-art Nature review [\href{https://www.nature.com/articles/s43586-022-00121-x}{NatRev}]}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item Invented by John Skilling in 2004.
%            \item Recent Nature review primer on nested sampling led by Andrew Fowlie and assembled by the community.
%            \item Showcases the current set of tools, and applications from chemistry to cosmology.
%            \item Recent 1.5 day conference in Munich: ``Frontiers of Nested Sampling''
%            \item Planned week-long NSCON 2024
%            \item In this talk:
%                \begin{itemize}
%                    \item User guide to nested sampling
%                    \item Particle physics applications
%                    \item Cosmology applications
%                    \item Machine learning applications
%                \end{itemize}
%        \end{itemize}
%        \column{0.3\textwidth}
%        \includegraphics[width=\textwidth]{figures/nature1}
%        \includegraphics[width=\textwidth]{figures/nature2}
%        \column{0.2\textwidth}
%        \includegraphics[width=\textwidth]{figures/nature4}
%        \includegraphics[width=\textwidth]{figures/nature5}
%    \end{columns}
%
%\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Where is Nested Sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item For many purposes, in your Neural Net you should group Nested Sampling with (MCMC) techniques such as:
                \begin{itemize}
                    \item Metropolis-Hastings (PyMC, MontePython)
                    \item Hamiltonian Monte Carlo (Stan, blackjax)
                    \item Ensemble sampling (emcee, zeus). 
                    \item Variational Inference (Pyro)
                    \item Sequential Monte Carlo 
                    \item Thermodynamic integration
                    \item Genetic algorithms
                \end{itemize}
            \item You may have heard of it branded form:
                \begin{itemize}
                    \item MultiNest
                    \item PolyChord
                    \item dynesty
                    \item ultranest
                \end{itemize}
            \end{itemize}
        \column{0.5\textwidth}
        \begin{columns}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/emcee}
        \includegraphics[width=\textwidth]{figures/metropolis-hastings}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/zeus}
        \end{columns}
        \includegraphics[width=\textwidth]{figures/hmc_explained}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Integration in Physics}
    \begin{itemize}
        \item Integration is a fundamental concept in physics, statistics and data science:
    \end{itemize}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Partition functions}
            \vspace{-11pt}
            \[ Z(\beta) = \int e^{-\beta H(q,p)} dq dp \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Path integrals}
            \[ \Psi = \int e^{i S} \mathcal{D}x \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Bayesian marginals}
            \vspace{-11pt}
            \[ \mathcal{Z}(D) = \int \mathcal{L}(D|\theta) \pi(\theta) d\theta \]
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Need numerical tools if analytic solution unavailable.
            \item High-dimensional numerical integration is hard.
            \item Riemannian strategy estimates volumes geometrically:
                \[ \int f(x) d^nx \approx \sum_i f(x_i) \Delta V_i \sim \mathcal{O}(e^n) \]
            \item Curse of dimensionality $\Rightarrow$ exponential scaling.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Probabalistic volume estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Key idea in NS: estimating volumes probabilistically
        \[
            \frac{\C[1]{V_\mathrm{after}}}{\C[0]{V_\mathrm{before}}} 
            \approx \frac{\C[1]{n_\mathrm{in}}}{\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}}
        \]
            \item This is the \textbf{only} way to calculate volume in high dimensions $d>3$.
                \begin{itemize}
                    \item Geometry is exponentially inefficient.
                \end{itemize}
            \item This estimation process does not depend on geometry, topology or dimensionality
            \item Basis of all Monte-Carlo integration
            \item Nested Sampling uniquely uses a nested framework to couple together MC integrals in a robust, scalable manner.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/compression_1}%
        \includegraphics<2->[width=\textwidth]{figures/compression_2}%
    \end{columns}
\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.48\textwidth}
        \begin{block}{\textbf{MCMC}}
            \only<16>{
                \begin{itemize}
                    \item Single ``walker''
                    \item Explores posterior
                    \item Fast, if proposal matrix is tuned
                    \item Parameter estimation, suspiciousness calculation
                    \item Channel capacity optimised for generating posterior samples
                \end{itemize}
            }
        \end{block}
            \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
            \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
            \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
            \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
            \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
            \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.48\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \only<16>{
                \begin{itemize}
                    \item Ensemble of ``live points''
                    \item Scans from prior to peak of likelihood
                    \item Slower, no tuning required
                    \item Parameter estimation, model comparison, tension quantification
                    \item Channel capacity optimised for computing partition function
                \end{itemize}
            }
        \end{block}
            \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
            \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
            \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
            \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
            \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
            \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
            \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
            \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
            \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} 
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n$ random samples over the space.
            \item Delete outermost sample, and replace with a new random one at higher integrand value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n}\only<5->{\pm \frac{1}{n}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \int f(x) dV \approx \sum_i f(x_i) \Delta V_i, \quad V_i = V_0 e^{-\only<5->{(}i\only<5->{\pm\sqrt{i})}/n} \]
%            \item Nested sampling: completely different way to scan.
%            \item Ensemble sampling compresses entire space$\to$peak(s).
%            \item Sequentially update a set $S$ of $n$ samples:
%                \begin{itemize}
%                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from a measure $\pi$). 
%
%                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
%                \end{itemize}
%            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard constraint}:
%                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
%            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence}/integral of function \& \C[0]{posterior}/sample weights.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4-         >[width=\textwidth,page=4]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: Lebesgue integration}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Can be weighted to form posterior samples, prior samples, or anything in between.
            \item Nested sampling estimates the \textbf{density of states} and calculates partition functions
                \[Z(\beta) = \sum_i f(x_i)^\beta \Delta V_i.\]
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune
                    \item exploration of multimodal functions
                    \item global and local optimisation
                \end{itemize}
            %\item Interpreted as a Bayesian algorithm, it
            %    \begin{itemize}
            %        \item Computes the Bayesian evidence (model comparison)
            %        \item Produces (weighted) posterior samples (parameter estimation)
            %    \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 
  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item A large fraction of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \item \url{https://projecteuclid.org/euclid.ba/1340370944}.
    \item There has also been much work beyond this (see 'Frontiers of nested sampling' talk)
        \begin{itemize}
            \item \href{https://www.willhandley.co.uk/talks}{willhandley.co.uk/talks}
        \end{itemize}
  \end{itemize}
 
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Types of nested sampler}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
\item Constructs bounding region and draws many invalid points until $\mathcal{L}(\theta)>\mathcal{L}_*$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{5pt}
    \begin{itemize}
        \item Nested samplers usually come with:
            \begin{itemize}
                \item \emph{resolution} parameter $n_\mathrm{live}$ (which improve results as $\sim\mathcal{O}(n_\mathrm{live}^{-1/2})$.
                    \item set of \emph{reliability} parameters~\arxiv{2101.04525}, which don't improve results if set arbitrarily high, but introduce systematic errors if set too low.
                    \item e.g. \texttt{Multinest} efficiency \texttt{eff} or \texttt{PolyChord} chain length $n_\mathrm{repeats}$.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Applications: The three pillars of Bayesian inference}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Parameter estimation}
            What do the data tell us about the parameters of a model?

            \textit{e.g. the size or age of a $\Lambda$CDM universe}
            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Model comparison}
            How much does the data support a particular model?

            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}} \vspace{-7pt}\]
            \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[5]{\Pi_\mathcal{M}}}{\C[7]{\sum_m Z_m \Pi_m}} \]
            \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Tension quantification}
            Do different datasets make consistent predictions from the same model? 
            \textit{e.g. CMB vs Type IA supernovae data}
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Cosmology}
    \student{adam_ormondroyd}{Adam Ormondroyd}{PhD}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Battle-tested in Bayesian cosmology on
                \begin{itemize}
                    \item Parameter estimation: multimodal alternative to MCMC samplers.
                    \item Model comparison: using integration to compute the Bayesian evidence
                    \item Tension quantification: using deep tail sampling and suspiciousness computations.
                \end{itemize}
            \item Plays a critical role in major cosmology pipelines: Planck, DES, KiDS, BAO, SNe.
            \item The default $\Lambda$CDM cosmology is well-tuned to have Gaussian-like posteriors for CMB data. 
            \item Less true for alternative cosmologies/models and orthogonal datasets, so nested sampling crucial.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/pps_both}
        \includegraphics[width=0.49\textwidth]{figures/reach_fit-cropped.pdf}
        %\includegraphics[width=0.49\textwidth]{figures/curvature_3}
        \includegraphics[width=\textwidth]{figures/omegak_H0_2.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Astrophysics}
\student{metha_prathaban}{Metha Prathaban}{PhD}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item In exoplanets~\arxiv{1806.00518}
                \begin{itemize}
                    \item Parameter estimation: determining properties of planets.
                    \item Model comparison: how many planets? Stellar modelling~\arxiv{2007.07278}.
                    \item exoplanet problems regularly have posterior phase transitions \arxiv{2102.03387}
                \end{itemize}
            \item In gravitational waves
                \begin{itemize}
                    \item Parameter estimation: Binary merger properties
                    \item Model comparison: Modified theories of gravity, selecting phenomenological parameterisations~\arxiv{1803.10210}
                    \item Likelihood reweighting: fast slow properties
                \end{itemize}
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/rv_full.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Particle physics}
    \student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{columns}
            \column{0.67\textwidth}
            \begin{itemize}
                \item Nested sampling for cross section computation/event generation
            \end{itemize}
            \column{0.3\textwidth}
            \[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2.\]
        \end{columns}
        \begin{itemize}
            \item Nested sampling can explore the phase space $\Omega$ and compute integral blind with comparable efficiency to HAAG/RAMBO~\arxiv{2205.02030}.
            \item Bayesian sparse reconstruction~\arxiv{1809.04598} applied to bump hunting allows evidence-based detection of signals in phenomenological backgrounds~\arxiv{2211.10391}.
            \item Fine tuning quantification
            \item Fast estimation of small $p$-values \arxiv{2106.02056}(PRL), just make switch:\\ $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.

        \end{itemize}
        \column{0.17\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1-pdfjam-crop.pdf}
        \column{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/psi_predict-crop.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Lattice field theory}
\student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Consider standard field theory Lagrangian:
                \[ Z(\beta) = \int D\phi e^{-\beta S(\phi)}, \quad S(\phi) = \int dx^\mu \mathcal{L}(\phi) \]
            \item Discretize onto spacetime grid.
            \item Compute partition function
            \item NS unique traits:
                \begin{itemize}
                    \item Get full partition function for free
                    \item allows for critical tuning
                    \item avoids critical slowing down
                \end{itemize}
            \item Applications in lattice gravity, QCD, condensed matter physics
            \item Publication imminent (next week)
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/potential_shape}
        \includegraphics[width=0.49\textwidth]{figures/2d_phase}
        \includegraphics[width=\textwidth]{figures/lattice_field_theory.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Machine learning}
\student{kamran_javid}{Kamran Javid}{PDRA}
    \begin{columns}
        \column{0.62\textwidth}
        \begin{itemize}
            \item Machine learning requires:
                \begin{itemize}
                    \item Training to find weights
                    \item Choice of architecture/topology/hyperparameters
                \end{itemize}
            \item Bayesian NNs treat training as a model fitting problem
            \item Compute posterior of weights (parameter estimation), rather than optimisation (gradient descent)
            \item Use evidence to determine best architecture (model comparison), correlates with out-of-sample performance! 
            \item Solving the full ``shallow learning'' problem without compromise \arxiv{2004.12211}\arxiv{2211.10391}. 
                \begin{itemize}
                    \item Promising work ongoing to extend this to transfer learning and deep nets.
                \end{itemize}
            \item More generally, dead points are optimally spaced  for training traditional ML approaches e.g. \arxiv{2309.05697}
        \end{itemize}
        \column{0.38\textwidth}
        \includegraphics[width=\textwidth]{figures/nn_posterior-cropped.pdf}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{Applications of nested sampling}
%    \framesubtitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
%    \begin{columns}
%        \column{0.57\textwidth}
%        \begin{itemize}
%            \item Nested sampling for frequentist computation!?
%            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
%            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
%            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
%            \item Need insight/approximation to make efficient.
%            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
%            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
%        \end{itemize}
%        \column{0.43\textwidth}
%        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{And beyond\ldots}
    \only<1-4>{
        \student{watkinson-headshot}{Catherine Watkinson}{Senior Data Scientist}
    }
    \only<5->{
        \student{mcaloone-headshot}{Thomas Mcaloone}{PhD $\to$ Data Scientist}
    }
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/watkinson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mason-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/formanek-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mcaloone-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/stenczel-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/yallup-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/bex-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/claireburke-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/hobson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/lasenby-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mhandley-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/whandley-headshot.jpg}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        \includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/tdoa-cropped-1-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/tdoa-cropped-2-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/tdoa-cropped-3-crop.pdf}%
        \includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology {\small \arxiv{2210.07409}(NatAstro)}}
    \student{ian_roque}{Ian Roque}{PhD}
    \vspace{10pt}
    \begin{columns}
        \column{0.65\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
            \item Lead data analysis team (REACH first light in January)
            \item Nested sampling woven in from the ground up (calibrator, beam modelling, signal fitting, likelihood selection).
            \item All treated as parameterised model comparison problems.
        \includegraphics[height=0.3\textwidth]{figures/EDGES_antenna}
        \includegraphics[height=0.3\textwidth]{figures/SARAS}
        \includegraphics[height=0.3\textwidth]{figures/REACH_2.jpg}
        \end{itemize}
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT: combining particle physics \& cosmological data}
    \begin{columns}
        \column{0.52\textwidth}
        \begin{itemize}
            \item Multinational team of particle physicists, cosmologists and statisticians.
            \item Combine cosmological data, particle colliders, direct detection, \& neutrino detectors in a statistically principled manner~\arxiv{2205.13549}.
            \item Lead Cosmo/Dark Matter working group~\arxiv{2009.03286}.
            \item Nested sampling used for global fitting, and fine-tuning quantification~\arxiv{2101.00428}
        \end{itemize}
        \begin{center}
            \includegraphics[width=0.5\textwidth]{figures/gambit_logo.png}
        \end{center}
        \column{0.48\textwidth}
        \vspace{-40pt}
        \includegraphics[width=\textwidth]{figures/ALP_2.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Likelihood-free inference (aka SBI)}
\student{kilian_scheutwinkel}{Kilian Scheutwinkel}{PhD}

    \vspace{10pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item How do you do inference if you don't know the likelihood $P(D|\theta)$?
                \begin{itemize}
                    \item e.g.\ if you can simulate a disease outbreak, how can you infer a posterior on $R_0$, or select the most predictive model?
                \end{itemize}
            \item If you can forward simulate/model $\theta\to D$, then you have an implicit likelihood.
            \item LFI aims to (machine-)\emph{learn} the likelihood from forward simulations $\{(\theta,D)\}$.
            \item Nested sampling has much to offer
                \begin{itemize}
                    \item truncation strategies (\texttt{PolySwyft})
                    \item evidence driven compression
                    \item marginalised machine learning
                \end{itemize}
            \item In my view, LFI represents the future of inference -- in twenty years time this will be as well-used as MCMC techniques are today.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/noisy.pdf}
        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{unimpeded: PLA for the next generation}
\student{harry_bevins}{Harry Bevins}{PhD$\to$JRF}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item DiRAC 2020 RAC allocation of 30MCPUh
            \item Main goal: Planck Legacy Archive equivalent
            \item Parameter estimation $\to$ Model comparison
            \item MCMC $\to$ Nested sampling
            \item Planck $\to$ $\{\text{Planck}, \text{DESY1}, \text{BAO}, \ldots \}$
            \item Pairwise combinations
            \item Suite of tools for processing these 
                \begin{itemize}
                    \item \texttt{anesthetic} $2.0$
                    \item \texttt{unimpeded} $1.0$
                    \item \texttt{zenodo} archive
                    \item \texttt{margarine}
                \end{itemize}
            \item MCMC chains also available.
            \item Library of bijectors emulators for fast re-use
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{logos/dirac}
        \includegraphics[width=0.5\textwidth,page=21]{figures/himmelblau}%
        \includegraphics[width=0.5\textwidth,page=15]{figures/himmelblau}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{CosmoTension}
    \framesubtitle{Resolving cosmological tensions with diverse data, novel theories and Bayesian machine learning}
\student{will_barker}{Will Barker}{PhD$\to$JRF}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item ERC grant $\Rightarrow$ UKRI Frontier, commencing 2023.
            \item Funds 3 PDRAs and 4 PhDs over 5 years.
            \item Research programme centered around combining novel theories of gravity, Boltzmann solvers~\arxiv{1906.01421}, reconstruction~\arxiv{1908.00906}, nested sampling \& likelihood free inference.
            \item Aims to disentangle cosmological tensions $H_0$, $\sigma_8$, $\Omega_K$ with next-generation data analysis techniques.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/denseoutput.pdf}
        \includegraphics[width=\textwidth]{figures/pps.pdf}
        \includegraphics[width=\textwidth]{figures/gantt.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Fully Bayesian Forecasting~\arxiv{2309.06942}}
    \student{thomas_gessey-jones}{Thomas Gessey-Jones}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Experimental design necessitates forecasting the constraints that future data might give.
            \item Have you ever done a Fisher forecast, and then felt Bayesian guilt?
            \item Simulation based inference gives us the language to marginalise over parameters $\theta$ and possible future data $D$.
            \item Evidence networks~\arxiv{2305.11241} (Jeffreys \& Wandelt) give us the ability to do this at scale in the case of forecasting.
            \item Can answer questions such as ``Given current knowledge/theoretical uncertainty $\pi(\theta)$, how probable is a detection?''
            \item Re-usable package: \texttt{prescience}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fbf.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\href{https://www.github.com/handley-lab}{github.com/handley-lab}}
    \tikz[overlay,remember picture]
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
            \includegraphics[width=0.1\textheight]{figures/students/adam_ormondroyd.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/cole_meldorf.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/danielle_dineen.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/david_yallup.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/dily_ong.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/harry_bevins.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/ian_roque.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/kilian_scheutwinkel.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/metha_prathaban.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/sinah_legner.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/thomas_gessey-jones.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/wei-ning_deng.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/will_barker.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/zixiao_hu.jpg}%
    };
    \vspace{-0.1\textheight}
    \begin{columns}
        \column{0.65\textwidth}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Performing Bayesian inference and model comparison.
            \end{itemize}
        \item It is applied widely across cosmology, particle physics \& machine learning.
        \item It's unique traits as the only numerical Lebesgue integrator mean with compute it will continue to grow in importance.
    \end{itemize}
        \column{0.35\textwidth}
    \includegraphics[height=0.6\textwidth]{figures/pps_both}%
    \includegraphics[height=0.6\textwidth]{figures/REACH_2}%
    \end{columns}
    \includegraphics[height=0.2\textwidth]{figures/integration}%
    \includegraphics[height=0.2\textwidth]{figures/curvature_3}%
    \includegraphics[height=0.2\textwidth]{figures/ALP_3}%
\end{frame}


\appendix
\begin{frame}
    \frametitle{How does Nested Sampling compare to other approaches?}
    \begin{columns}
        \column{0.7\textwidth}
        \begin{itemize}
            \item In all cases:
                \begin{itemize}
                    \item[$+$] NS can handle multimodal functions
                    \item[$+$] NS computes evidences, partition functions and integrals
                    \item[$+$] NS is self-tuning/black-box
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
    \end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \begin{itemize}
                \item Gradient descent
                    \begin{itemize}
                        \item[$-$] NS cannot use gradients
                        \item[$+$] NS does not require gradients
                    \end{itemize}
                \item Genetic algorithms
                    \begin{itemize}
                        \item[$+$] NS discarded points have statistical meaning
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Sampling}
            \begin{itemize}
                \item Metropolis-Hastings?
                    \begin{itemize}
                        \item[$-$] Nothing beats well-tuned customised MH
                        \item[$+$] NS is self tuning
                    \end{itemize}
                \item Hamiltonian Monte Carlo?
                    \begin{itemize}
                        \item[$-$] In millions of dimensions, HMC is king
                        \item[$+$] NS does not require gradients
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \begin{itemize}
                \item Thermodynamic integration
                    \begin{itemize}
                        \item[$+$] protective against phase trasitions
                        \item[$+$] No annealing schedule tuning 
                    \end{itemize}
                \item Sequential Monte Carlo
                    \begin{itemize}
                        \item[$-$] SMC experts classify NS as a kind of SMC
                        \item[$+$] NS is athermal
                    \end{itemize}
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling: a user's guide}
    \begin{enumerate}
        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
            \begin{itemize}
                \item This means typically most of its time is spent on burn-in rather than posterior sampling.
                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable.
            \end{itemize}
        \item The number of live points $n_\text{live}$ is a resolution parameter.
            \begin{itemize}
                \item Run time is linear in $n_\text{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\text{live}}}$.
                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
            \end{itemize}
        \item Most algorithms come with additional reliability parameter(s).
            \begin{itemize}
                \item e.g. \texttt{MultiNest}: $\texttt{eff}$, \texttt{PolyChord}: $n_\text{repeats}$.
                \item These are parameters which have no gain if set too conservatively, but increase the reliability.
                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Time complexity of nested sampling}
    \includegraphics[width=\textwidth]{figures/run_prodecure}
    \vspace{-20pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-20pt}
    \begin{itemize}
        \item $x$-axis: log-compression of live points
        \item Area $\propto$ posterior mass
        \item Shows Bayesian balance of likelihood vs prior
        \item Run proceeds right to left
        \item Run finishes after bump (typical set)
    \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Time complexity\vspace{-5pt}
                \[ \boxed{T = n_\mathrm{live} \times T_\mathcal{L} \times T_\mathrm{sampler} \times \mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)}\vspace{-5pt}  \]
            \item Error complexity $\boxed{\sigma \propto \sqrt{\mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)/n_\mathrm{live}}}$
        \end{itemize}
    \end{columns}

\end{frame}




\begin{frame}
    \frametitle{Occam's Razor~\arxiv{2102.11511}}
    \begin{itemize}
        \item Bayesian inference quantifies Occam's Razor:
            \begin{itemize}
                \item \textit{``Entities are not to be multiplied without necessity''} \hfill --- William of Occam
                \item \textit{``Everything should be kept as simple as possible, but not simpler''} \hfill --- ``Albert Einstein''
            \end{itemize}
        %\item Consider the Evidence $\C[3]{\mathcal{Z}\equiv P(D|M)}$: 
        %    \begin{description}[Parameter estimation]
        %        \item [Parameter estimation] normalisation constant
        %        \item [Model comparison] critical update factor for \C[5]{model prior} to \C[4]{model posterior}
        %    \end{description}
        \item Properties of the evidence: rearrange Bayes' theorem for parameter estimation
            \[\C[0]{\mathcal{P}(\theta)} = \frac{\C[2]{\mathcal{L}(\theta)} \C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}}} \qquad\Rightarrow\qquad \C[3]{\log \mathcal{Z}} = \C[2]{\log\mathcal{L}(\theta)} - \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}}.\]  
        \item Evidence is composed of a ``goodness of fit'' term  and ``Occam Penalty''.
    \end{itemize}
    \begin{columns}[t]
        \column{0.5\textwidth}
    \begin{itemize}
        \item RHS true for all $\theta$. Take max likelihood value $\theta_*$:
            \[
                \log \mathcal{Z} = -\chi_\text{min}^2 - \text{Mackay penalty.}
            \]
    \end{itemize}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Be more Bayesian and take posterior average to get the ``Occam's razor equation''
            \[
                \boxed{
                    \log \mathcal{Z} = \av[\mathcal{P}]{\log\mathcal{L}} - \mathcal{D}_\text{KL}.
            }
            \]
    \end{itemize}
    \end{columns}
    \vfill
    \begin{itemize}
        \item Natural regularisation which penalises models with too many parameters.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kullback Liebler divergence}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is is defined as:
                \[\mathcal{D}_\text{KL} = \av[\mathcal{P}]{\log\frac{\mathcal{P}}{\pi}} = \int \mathcal{P}(\theta) \log \frac{\mathcal{P}(\theta)}{\pi(\theta)}d\theta.\]
            \item Whilst not a distance, $\mathcal{D}=0$ when $\mathcal{P}=\pi$.
            \item Occurs in the context of machine learning as an objective function for training functions.
            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
                \[ \mathcal{D}_\text{KL} \approx \log \frac{V_\pi}{V_\text{P}}.\]
                (this is exact for top-hat distributions).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Nested sampling for frequentist computation!?
            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
            \item Need insight/approximation to make efficient.
            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
    \end{columns}
    
\end{frame}

\begin{frame}
    \frametitle{Exploration of phase space~\arxiv{2106.02056}}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Nested sampling for cross section computation/event generation.
            \item Numerically compute collisional cross section 
                \vspace{-5pt}\[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2,\]
                $\Omega$ phase space of kinematic configurations $\Phi$, each with matrix element $\mathcal{M}(\Phi)$.
            \item Current state of the art e.g. HAAG (improvement on RAMBO) requires knowledge of $\mathcal{M}(\Phi)$.
            \item Nested sampling can explore the phase space and compute integral blind with comparable efficiency.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1.pdf}
        \includegraphics[width=\textwidth]{figures/phase_space.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Quantification of fine tuning~\arxiv{2101.00428}~\arxiv{2205.13549}}
    \vspace{-10pt}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Example: Cosmological constraints on decaying axion-like particles~\arxiv{2205.13549}.
            \item Subset of parameters $\xi,m_a,\tau,g_{a\gamma}$: ALP fraction, mass, lifetime and photon coupling.
                {(\small Also vary cosmology, $\tau_n$ and nuisance params)}
            \item Data: CMB, BBN, FIRAS, SMM, BAO.
            \item Standard profile likelihood fit shows ruled out regions and best-fit point.
            \item<2-> Nested sampling scan:
                \begin{itemize}
                    \item Quantifies amount of parameter space ruled out with Kullback-Liebler divergence $\mathcal{D}_\mathrm{KL}$.
                    \item Identifies best fit region as statistically irrelevant from information theory/Bayesian.
                    \item No evidence for decaying ALPs. Fit the data equally well: but more constrained parameters create Occam penalty.
                \end{itemize}
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/ALP_1.pdf}
        \includegraphics<2          >[width=\textwidth]{figures/ALP_2.pdf}
        \includegraphics<3|handout:0>[width=\textwidth]{figures/ALP_3.pdf}
    \end{columns}
    
\end{frame}

\end{document}
```
# branch: remotes/origin/unam_2023
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{ Nested sampling: {\large powering next-generation inference and machine learning tools}}
\subtitle{for astrophysics, cosmology, particle physics and beyond}
\date{4\textsuperscript{th} December 2023}

\begin{document}



\begin{frame}
    \titlepage
\end{frame}

%\begin{frame}
%    \frametitle{Highlight: state-of-the-art Nature review [\href{https://www.nature.com/articles/s43586-022-00121-x}{NatRev}]}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item Invented by John Skilling in 2004.
%            \item Recent Nature review primer on nested sampling led by Andrew Fowlie and assembled by the community.
%            \item Showcases the current set of tools, and applications from chemistry to cosmology.
%            \item Recent 1.5 day conference in Munich: ``Frontiers of Nested Sampling''
%            \item Planned week-long NSCON 2024
%            \item In this talk:
%                \begin{itemize}
%                    \item User guide to nested sampling
%                    \item Particle physics applications
%                    \item Cosmology applications
%                    \item Machine learning applications
%                \end{itemize}
%        \end{itemize}
%        \column{0.3\textwidth}
%        \includegraphics[width=\textwidth]{figures/nature1}
%        \includegraphics[width=\textwidth]{figures/nature2}
%        \column{0.2\textwidth}
%        \includegraphics[width=\textwidth]{figures/nature4}
%        \includegraphics[width=\textwidth]{figures/nature5}
%    \end{columns}
%
%\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Where is Nested Sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item For many purposes, in your Neural Net you should group Nested Sampling with (MCMC) techniques such as:
                \begin{itemize}
                    \item Metropolis-Hastings (PyMC, MontePython)
                    \item Hamiltonian Monte Carlo (Stan, blackjax)
                    \item Ensemble sampling (emcee, zeus). 
                    \item Variational Inference (Pyro)
                    \item Sequential Monte Carlo 
                    \item Thermodynamic integration
                    \item Genetic algorithms
                \end{itemize}
            \item You may have heard of it branded form:
                \begin{itemize}
                    \item MultiNest
                    \item PolyChord
                    \item dynesty
                    \item ultranest
                \end{itemize}
            \end{itemize}
        \column{0.5\textwidth}
        \begin{columns}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/emcee}
        \includegraphics[width=\textwidth]{figures/metropolis-hastings}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/zeus}
        \end{columns}
        \includegraphics[width=\textwidth]{figures/hmc_explained}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Integration in Physics}
    \begin{itemize}
        \item Integration is a fundamental concept in physics, statistics and data science:
    \end{itemize}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Partition functions}
            \vspace{-11pt}
            \[ Z(\beta) = \int e^{-\beta H(q,p)} dq dp \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Path integrals}
            \[ \Psi = \int e^{i S} \mathcal{D}x \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Bayesian marginals}
            \vspace{-11pt}
            \[ \mathcal{Z}(D) = \int \mathcal{L}(D|\theta) \pi(\theta) d\theta \]
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Need numerical tools if analytic solution unavailable.
            \item High-dimensional numerical integration is hard.
            \item Riemannian strategy estimates volumes geometrically:
                \[ \int f(x) d^nx \approx \sum_i f(x_i) \Delta V_i \sim \mathcal{O}(e^n) \]
            \item Curse of dimensionality $\Rightarrow$ exponential scaling.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Probabalistic volume estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Key idea in NS: estimating volumes probabilistically
        \[
            \frac{\C[1]{V_\mathrm{after}}}{\C[0]{V_\mathrm{before}}} 
            \approx \frac{\C[1]{n_\mathrm{in}}}{\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}}
        \]
            \item This is the \textbf{only} way to calculate volume in high dimensions $d>3$.
                \begin{itemize}
                    \item Geometry is exponentially inefficient.
                \end{itemize}
            \item This estimation process does not depend on geometry, topology or dimensionality
            \item Basis of all Monte-Carlo integration
            \item Nested Sampling uniquely uses a nested framework to couple together MC integrals in a robust, scalable manner.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/compression_1}%
        \includegraphics<2->[width=\textwidth]{figures/compression_2}%
    \end{columns}
\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.48\textwidth}
        \begin{block}{\textbf{MCMC}}
            \only<16>{
                \begin{itemize}
                    \item Single ``walker''
                    \item Explores posterior
                    \item Fast, if proposal matrix is tuned
                    \item Parameter estimation, suspiciousness calculation
                    \item Channel capacity optimised for generating posterior samples
                \end{itemize}
            }
        \end{block}
            \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
            \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
            \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
            \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
            \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
            \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.48\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \only<16>{
                \begin{itemize}
                    \item Ensemble of ``live points''
                    \item Scans from prior to peak of likelihood
                    \item Slower, no tuning required
                    \item Parameter estimation, model comparison, tension quantification
                    \item Channel capacity optimised for computing partition function
                \end{itemize}
            }
        \end{block}
            \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
            \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
            \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
            \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
            \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
            \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
            \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
            \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
            \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} 
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: live points}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n$ random samples over the space.
            \item Delete outermost sample, and replace with a new random one at higher integrand value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n}\only<5->{\pm \frac{1}{n}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \int f(x) dV \approx \sum_i f(x_i) \Delta V_i, \quad V_i = V_0 e^{-\only<5->{(}i\only<5->{\pm\sqrt{i})}/n} \]
%            \item Nested sampling: completely different way to scan.
%            \item Ensemble sampling compresses entire space$\to$peak(s).
%            \item Sequentially update a set $S$ of $n$ samples:
%                \begin{itemize}
%                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from a measure $\pi$). 
%
%                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
%                \end{itemize}
%            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard constraint}:
%                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
%            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence}/integral of function \& \C[0]{posterior}/sample weights.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4-         >[width=\textwidth,page=4]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: dead points}
    \includegraphics[width=\textwidth]{figures/dead_measure}
    \begin{columns}
        \column{0.69\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Dead points have a unique scale-invariant distribution $\propto\: \tfrac{dV}{V}$.
            \item Uniform over original region, exponentially concentrating on region of interest (until termination volume).
            \item Good for training emulators (HERA~\arxiv{2108.07282}).
        \end{itemize}
        \column{0.3\textwidth}
        \begin{block}{Applications}
        \begin{itemize}
            \item training emulators.
            \item gridding simulations
            \item beta flows
            \item ``dead measure'' 
        \end{itemize}
        \end{block}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{The nested sampling meta-algorithm: Lebesgue integration}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Full dead-point coverage of tails enables integration.
            \item Can be weighted to form posterior samples, prior samples, or anything in between.
            \item Nested sampling estimates the \textbf{density of states} and calculates partition functions
                \[Z(\beta) = \sum_i f(x_i)^\beta \Delta V_i.\]
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune
                    \item exploration of multimodal functions
                    \item global and local optimisation
                \end{itemize}
            %\item Interpreted as a Bayesian algorithm, it
            %    \begin{itemize}
            %        \item Computes the Bayesian evidence (model comparison)
            %        \item Produces (weighted) posterior samples (parameter estimation)
            %    \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 
  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item A large fraction of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \item \url{https://projecteuclid.org/euclid.ba/1340370944}.
    \item There has also been much work beyond this (see 'Frontiers of nested sampling' talk)
        \begin{itemize}
            \item \href{https://www.willhandley.co.uk/talks}{willhandley.co.uk/talks}
        \end{itemize}
  \end{itemize}
 
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Types of nested sampler}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
\item Constructs bounding region and draws many invalid points until $\mathcal{L}(\theta)>\mathcal{L}_*$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{5pt}
    \begin{itemize}
        \item Nested samplers usually come with:
            \begin{itemize}
                \item \emph{resolution} parameter $n_\mathrm{live}$ (which improve results as $\sim\mathcal{O}(n_\mathrm{live}^{-1/2})$.
                    \item set of \emph{reliability} parameters~\arxiv{2101.04525}, which don't improve results if set arbitrarily high, but introduce systematic errors if set too low.
                    \item e.g. \texttt{Multinest} efficiency \texttt{eff} or \texttt{PolyChord} chain length $n_\mathrm{repeats}$.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Applications: The three pillars of Bayesian inference}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Parameter estimation}
            What do the data tell us about the parameters of a model?

            \textit{e.g. the size or age of a $\Lambda$CDM universe}
            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Model comparison}
            How much does the data support a particular model?

            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}} \vspace{-7pt}\]
            \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[5]{\Pi_\mathcal{M}}}{\C[7]{\sum_m Z_m \Pi_m}} \]
            \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Tension quantification}
            Do different datasets make consistent predictions from the same model? 
            \textit{e.g. CMB vs Type IA supernovae data}
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Cosmology}
    \student{adam_ormondroyd}{Adam Ormondroyd}{PhD}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Battle-tested in Bayesian cosmology on
                \begin{itemize}
                    \item Parameter estimation: multimodal alternative to MCMC samplers.
                    \item Model comparison: using integration to compute the Bayesian evidence
                    \item Tension quantification: using deep tail sampling and suspiciousness computations.
                \end{itemize}
            \item Plays a critical role in major cosmology pipelines: Planck, DES, KiDS, BAO, SNe.
            \item The default $\Lambda$CDM cosmology is well-tuned to have Gaussian-like posteriors for CMB data. 
            \item Less true for alternative cosmologies/models and orthogonal datasets, so nested sampling crucial.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/pps_both}
        \includegraphics[width=0.49\textwidth]{figures/reach_fit-cropped.pdf}
        %\includegraphics[width=0.49\textwidth]{figures/curvature_3}
        \includegraphics[width=\textwidth]{figures/omegak_H0_2.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Astrophysics}
\student{metha_prathaban}{Metha Prathaban}{PhD}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item In exoplanets~\arxiv{1806.00518}
                \begin{itemize}
                    \item Parameter estimation: determining properties of planets.
                    \item Model comparison: how many planets? Stellar modelling~\arxiv{2007.07278}.
                    \item exoplanet problems regularly have posterior phase transitions \arxiv{2102.03387}
                \end{itemize}
            \item In gravitational waves
                \begin{itemize}
                    \item Parameter estimation: Binary merger properties
                    \item Model comparison: Modified theories of gravity, selecting phenomenological parameterisations~\arxiv{1803.10210}
                    \item Likelihood reweighting: fast slow properties
                \end{itemize}
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/rv_full.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Particle physics}
    \student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{columns}
            \column{0.67\textwidth}
            \begin{itemize}
                \item Nested sampling for cross section computation/event generation
            \end{itemize}
            \column{0.3\textwidth}
            \[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2.\]
        \end{columns}
        \begin{itemize}
            \item Nested sampling can explore the phase space $\Omega$ and compute integral blind with comparable efficiency to HAAG/RAMBO~\arxiv{2205.02030}.
            \item Bayesian sparse reconstruction~\arxiv{1809.04598} applied to bump hunting allows evidence-based detection of signals in phenomenological backgrounds~\arxiv{2211.10391}.
            \item Fine tuning quantification
            \item Fast estimation of small $p$-values \arxiv{2106.02056}(PRL), just make switch:\\ $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.

        \end{itemize}
        \column{0.17\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1-pdfjam-crop.pdf}
        \column{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/psi_predict-crop.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Lattice field theory}
\student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Consider standard field theory Lagrangian:
                \[ Z(\beta) = \int D\phi e^{-\beta S(\phi)}, \quad S(\phi) = \int dx^\mu \mathcal{L}(\phi) \]
            \item Discretize onto spacetime grid.
            \item Compute partition function
            \item NS unique traits:
                \begin{itemize}
                    \item Get full partition function for free
                    \item allows for critical tuning
                    \item avoids critical slowing down
                \end{itemize}
            \item Applications in lattice gravity, QCD, condensed matter physics
            \item Publication imminent (next week)
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/potential_shape}
        \includegraphics[width=0.49\textwidth]{figures/2d_phase}
        \includegraphics[width=\textwidth]{figures/lattice_field_theory.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Machine learning}
\student{kamran_javid}{Kamran Javid}{PDRA}
    \begin{columns}
        \column{0.62\textwidth}
        \begin{itemize}
            \item Machine learning requires:
                \begin{itemize}
                    \item Training to find weights
                    \item Choice of architecture/topology/hyperparameters
                \end{itemize}
            \item Bayesian NNs treat training as a model fitting problem
            \item Compute posterior of weights (parameter estimation), rather than optimisation (gradient descent)
            \item Use evidence to determine best architecture (model comparison), correlates with out-of-sample performance! 
            \item Solving the full ``shallow learning'' problem without compromise \arxiv{2004.12211}\arxiv{2211.10391}. 
                \begin{itemize}
                    \item Promising work ongoing to extend this to transfer learning and deep nets.
                \end{itemize}
            \item More generally, dead points are optimally spaced  for training traditional ML approaches e.g. \arxiv{2309.05697}
        \end{itemize}
        \column{0.38\textwidth}
        \includegraphics[width=\textwidth]{figures/nn_posterior-cropped.pdf}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{Applications of nested sampling}
%    \framesubtitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
%    \begin{columns}
%        \column{0.57\textwidth}
%        \begin{itemize}
%            \item Nested sampling for frequentist computation!?
%            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
%            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
%            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
%            \item Need insight/approximation to make efficient.
%            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
%            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
%        \end{itemize}
%        \column{0.43\textwidth}
%        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{And beyond\ldots}
    \only<1-4>{
        \student{watkinson-headshot}{Catherine Watkinson}{Senior Data Scientist}
    }
    \only<5->{
        \student{mcaloone-headshot}{Thomas Mcaloone}{PhD $\to$ Data Scientist}
    }
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/watkinson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mason-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/formanek-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mcaloone-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/stenczel-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/yallup-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/bex-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/claireburke-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/hobson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/lasenby-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mhandley-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/whandley-headshot.jpg}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        \includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/tdoa-cropped-1-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/tdoa-cropped-2-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/tdoa-cropped-3-crop.pdf}%
        \includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology {\small \arxiv{2210.07409}(NatAstro)}}
    \student{ian_roque}{Ian Roque}{PhD}
    \vspace{10pt}
    \begin{columns}
        \column{0.65\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
            \item Lead data analysis team (REACH first light in January)
            \item Nested sampling woven in from the ground up (calibrator, beam modelling, signal fitting, likelihood selection).
            \item All treated as parameterised model comparison problems.
        \includegraphics[height=0.3\textwidth]{figures/EDGES_antenna}
        \includegraphics[height=0.3\textwidth]{figures/SARAS}
        \includegraphics[height=0.3\textwidth]{figures/REACH_2.jpg}
        \end{itemize}
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT: combining particle physics \& cosmological data}
    \begin{columns}
        \column{0.52\textwidth}
        \begin{itemize}
            \item Multinational team of particle physicists, cosmologists and statisticians.
            \item Combine cosmological data, particle colliders, direct detection, \& neutrino detectors in a statistically principled manner~\arxiv{2205.13549}.
            \item Lead Cosmo/Dark Matter working group~\arxiv{2009.03286}.
            \item Nested sampling used for global fitting, and fine-tuning quantification~\arxiv{2101.00428}
        \end{itemize}
        \begin{center}
            \includegraphics[width=0.5\textwidth]{figures/gambit_logo.png}
        \end{center}
        \column{0.48\textwidth}
        \vspace{-40pt}
        \includegraphics[width=\textwidth]{figures/ALP_2.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Likelihood-free inference (aka SBI)}
\student{kilian_scheutwinkel}{Kilian Scheutwinkel}{PhD}

    \vspace{10pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item How do you do inference if you don't know the likelihood $P(D|\theta)$?
                \begin{itemize}
                    \item e.g.\ if you can simulate a disease outbreak, how can you infer a posterior on $R_0$, or select the most predictive model?
                \end{itemize}
            \item If you can forward simulate/model $\theta\to D$, then you have an implicit likelihood.
            \item LFI aims to (machine-)\emph{learn} the likelihood from forward simulations $\{(\theta,D)\}$.
            \item Nested sampling has much to offer
                \begin{itemize}
                    \item truncation strategies (\texttt{PolySwyft})
                    \item evidence driven compression
                    \item marginalised machine learning
                \end{itemize}
            \item In my view, LFI represents the future of inference -- in twenty years time this will be as well-used as MCMC techniques are today.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/noisy.pdf}
        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{unimpeded: PLA for the next generation}
\student{harry_bevins}{Harry Bevins}{PhD$\to$JRF}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item DiRAC 2020 RAC allocation of 30MCPUh
            \item Main goal: Planck Legacy Archive equivalent
            \item Parameter estimation $\to$ Model comparison
            \item MCMC $\to$ Nested sampling
            \item Planck $\to$ $\{\text{Planck}, \text{DESY1}, \text{BAO}, \ldots \}$
            \item Pairwise combinations
            \item Suite of tools for processing these 
                \begin{itemize}
                    \item \texttt{anesthetic} $2.0$
                    \item \texttt{unimpeded} $1.0$
                    \item \texttt{zenodo} archive
                    \item \texttt{margarine}
                \end{itemize}
            \item MCMC chains also available.
            \item Library of bijectors emulators for fast re-use
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{logos/dirac}
        \includegraphics[width=0.5\textwidth,page=21]{figures/himmelblau}%
        \includegraphics[width=0.5\textwidth,page=15]{figures/himmelblau}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{CosmoTension}
    \framesubtitle{Resolving cosmological tensions with diverse data, novel theories and Bayesian machine learning}
\student{will_barker}{Will Barker}{PhD$\to$JRF}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item ERC grant $\Rightarrow$ UKRI Frontier, commencing 2023.
            \item Funds 3 PDRAs and 4 PhDs over 5 years.
            \item Research programme centered around combining novel theories of gravity, Boltzmann solvers~\arxiv{1906.01421}, reconstruction~\arxiv{1908.00906}, nested sampling \& likelihood free inference.
            \item Aims to disentangle cosmological tensions $H_0$, $\sigma_8$, $\Omega_K$ with next-generation data analysis techniques.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/denseoutput.pdf}
        \includegraphics[width=\textwidth]{figures/pps.pdf}
        \includegraphics[width=\textwidth]{figures/gantt.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Fully Bayesian Forecasting~\arxiv{2309.06942}}
    \student{thomas_gessey-jones}{Thomas Gessey-Jones}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Experimental design necessitates forecasting the constraints that future data might give.
            \item Have you ever done a Fisher forecast, and then felt Bayesian guilt?
            \item Simulation based inference gives us the language to marginalise over parameters $\theta$ and possible future data $D$.
            \item Evidence networks~\arxiv{2305.11241} (Jeffreys \& Wandelt) give us the ability to do this at scale in the case of forecasting.
            \item Can answer questions such as ``Given current knowledge/theoretical uncertainty $\pi(\theta)$, how probable is a detection?''
            \item Re-usable package: \texttt{prescience}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fbf.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\href{https://www.github.com/handley-lab}{github.com/handley-lab}}
    \tikz[overlay,remember picture]
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
            \includegraphics[width=0.1\textheight]{figures/students/adam_ormondroyd.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/cole_meldorf.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/david_yallup.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/dily_ong.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/harry_bevins.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/ian_roque.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/george_carter.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/kilian_scheutwinkel.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/metha_prathaban.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/namu_kroupa.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/sinah_legner.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/thomas_gessey-jones.jpg}%
            \includegraphics[width=0.1\textheight]{figures/students/wei-ning_deng.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/will_barker.jpg}%
            %\includegraphics[width=0.1\textheight]{figures/students/zixiao_hu.jpg}%
    };
    \vspace{-0.1\textheight}
    \begin{columns}
        \column{0.65\textwidth}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Performing Bayesian inference and model comparison.
            \end{itemize}
        \item It is applied widely across cosmology, particle physics \& machine learning.
        \item It's unique traits as the only numerical Lebesgue integrator mean with compute it will continue to grow in importance.
    \end{itemize}
        \column{0.35\textwidth}
    \includegraphics[height=0.6\textwidth]{figures/pps_both}%
    \includegraphics[height=0.6\textwidth]{figures/REACH_2}%
    \end{columns}
    \includegraphics[height=0.2\textwidth]{figures/integration}%
    \includegraphics[height=0.2\textwidth]{figures/curvature_3}%
    \includegraphics[height=0.2\textwidth]{figures/ALP_3}%
\end{frame}


\appendix
\begin{frame}
    \frametitle{How does Nested Sampling compare to other approaches?}
    \begin{columns}
        \column{0.7\textwidth}
        \begin{itemize}
            \item In all cases:
                \begin{itemize}
                    \item[$+$] NS can handle multimodal functions
                    \item[$+$] NS computes evidences, partition functions and integrals
                    \item[$+$] NS is self-tuning/black-box
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
    \end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \begin{itemize}
                \item Gradient descent
                    \begin{itemize}
                        \item[$-$] NS cannot use gradients
                        \item[$+$] NS does not require gradients
                    \end{itemize}
                \item Genetic algorithms
                    \begin{itemize}
                        \item[$+$] NS discarded points have statistical meaning
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Sampling}
            \begin{itemize}
                \item Metropolis-Hastings?
                    \begin{itemize}
                        \item[$-$] Nothing beats well-tuned customised MH
                        \item[$+$] NS is self tuning
                    \end{itemize}
                \item Hamiltonian Monte Carlo?
                    \begin{itemize}
                        \item[$-$] In millions of dimensions, HMC is king
                        \item[$+$] NS does not require gradients
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \begin{itemize}
                \item Thermodynamic integration
                    \begin{itemize}
                        \item[$+$] protective against phase trasitions
                        \item[$+$] No annealing schedule tuning 
                    \end{itemize}
                \item Sequential Monte Carlo
                    \begin{itemize}
                        \item[$-$] SMC experts classify NS as a kind of SMC
                        \item[$+$] NS is athermal
                    \end{itemize}
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling: a user's guide}
    \begin{enumerate}
        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
            \begin{itemize}
                \item This means typically most of its time is spent on burn-in rather than posterior sampling.
                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable.
            \end{itemize}
        \item The number of live points $n_\text{live}$ is a resolution parameter.
            \begin{itemize}
                \item Run time is linear in $n_\text{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\text{live}}}$.
                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
            \end{itemize}
        \item Most algorithms come with additional reliability parameter(s).
            \begin{itemize}
                \item e.g. \texttt{MultiNest}: $\texttt{eff}$, \texttt{PolyChord}: $n_\text{repeats}$.
                \item These are parameters which have no gain if set too conservatively, but increase the reliability.
                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Time complexity of nested sampling}
    \includegraphics[width=\textwidth]{figures/run_prodecure}
    \vspace{-20pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-20pt}
    \begin{itemize}
        \item $x$-axis: log-compression of live points
        \item Area $\propto$ posterior mass
        \item Shows Bayesian balance of likelihood vs prior
        \item Run proceeds right to left
        \item Run finishes after bump (typical set)
    \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Time complexity\vspace{-5pt}
                \[ \boxed{T = n_\mathrm{live} \times T_\mathcal{L} \times T_\mathrm{sampler} \times \mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)}\vspace{-5pt}  \]
            \item Error complexity $\boxed{\sigma \propto \sqrt{\mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)/n_\mathrm{live}}}$
        \end{itemize}
    \end{columns}

\end{frame}




\begin{frame}
    \frametitle{Occam's Razor~\arxiv{2102.11511}}
    \begin{itemize}
        \item Bayesian inference quantifies Occam's Razor:
            \begin{itemize}
                \item \textit{``Entities are not to be multiplied without necessity''} \hfill --- William of Occam
                \item \textit{``Everything should be kept as simple as possible, but not simpler''} \hfill --- ``Albert Einstein''
            \end{itemize}
        %\item Consider the Evidence $\C[3]{\mathcal{Z}\equiv P(D|M)}$: 
        %    \begin{description}[Parameter estimation]
        %        \item [Parameter estimation] normalisation constant
        %        \item [Model comparison] critical update factor for \C[5]{model prior} to \C[4]{model posterior}
        %    \end{description}
        \item Properties of the evidence: rearrange Bayes' theorem for parameter estimation
            \[\C[0]{\mathcal{P}(\theta)} = \frac{\C[2]{\mathcal{L}(\theta)} \C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}}} \qquad\Rightarrow\qquad \C[3]{\log \mathcal{Z}} = \C[2]{\log\mathcal{L}(\theta)} - \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}}.\]  
        \item Evidence is composed of a ``goodness of fit'' term  and ``Occam Penalty''.
    \end{itemize}
    \begin{columns}[t]
        \column{0.5\textwidth}
    \begin{itemize}
        \item RHS true for all $\theta$. Take max likelihood value $\theta_*$:
            \[
                \log \mathcal{Z} = -\chi_\text{min}^2 - \text{Mackay penalty.}
            \]
    \end{itemize}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Be more Bayesian and take posterior average to get the ``Occam's razor equation''
            \[
                \boxed{
                    \log \mathcal{Z} = \av[\mathcal{P}]{\log\mathcal{L}} - \mathcal{D}_\text{KL}.
            }
            \]
    \end{itemize}
    \end{columns}
    \vfill
    \begin{itemize}
        \item Natural regularisation which penalises models with too many parameters.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kullback Liebler divergence}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is is defined as:
                \[\mathcal{D}_\text{KL} = \av[\mathcal{P}]{\log\frac{\mathcal{P}}{\pi}} = \int \mathcal{P}(\theta) \log \frac{\mathcal{P}(\theta)}{\pi(\theta)}d\theta.\]
            \item Whilst not a distance, $\mathcal{D}=0$ when $\mathcal{P}=\pi$.
            \item Occurs in the context of machine learning as an objective function for training functions.
            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
                \[ \mathcal{D}_\text{KL} \approx \log \frac{V_\pi}{V_\text{P}}.\]
                (this is exact for top-hat distributions).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Nested sampling for frequentist computation!?
            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
            \item Need insight/approximation to make efficient.
            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
    \end{columns}
    
\end{frame}

\begin{frame}
    \frametitle{Exploration of phase space~\arxiv{2106.02056}}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Nested sampling for cross section computation/event generation.
            \item Numerically compute collisional cross section 
                \vspace{-5pt}\[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2,\]
                $\Omega$ phase space of kinematic configurations $\Phi$, each with matrix element $\mathcal{M}(\Phi)$.
            \item Current state of the art e.g. HAAG (improvement on RAMBO) requires knowledge of $\mathcal{M}(\Phi)$.
            \item Nested sampling can explore the phase space and compute integral blind with comparable efficiency.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1.pdf}
        \includegraphics[width=\textwidth]{figures/phase_space.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Quantification of fine tuning~\arxiv{2101.00428}~\arxiv{2205.13549}}
    \vspace{-10pt}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Example: Cosmological constraints on decaying axion-like particles~\arxiv{2205.13549}.
            \item Subset of parameters $\xi,m_a,\tau,g_{a\gamma}$: ALP fraction, mass, lifetime and photon coupling.
                {(\small Also vary cosmology, $\tau_n$ and nuisance params)}
            \item Data: CMB, BBN, FIRAS, SMM, BAO.
            \item Standard profile likelihood fit shows ruled out regions and best-fit point.
            \item<2-> Nested sampling scan:
                \begin{itemize}
                    \item Quantifies amount of parameter space ruled out with Kullback-Liebler divergence $\mathcal{D}_\mathrm{KL}$.
                    \item Identifies best fit region as statistically irrelevant from information theory/Bayesian.
                    \item No evidence for decaying ALPs. Fit the data equally well: but more constrained parameters create Occam penalty.
                \end{itemize}
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/ALP_1.pdf}
        \includegraphics<2          >[width=\textwidth]{figures/ALP_2.pdf}
        \includegraphics<3|handout:0>[width=\textwidth]{figures/ALP_3.pdf}
    \end{columns}
    
\end{frame}

\end{document}
```
# branch: remotes/origin/zurich_2023
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usepackage{pythonhighlight}
\usepackage{tikz}
\newcommand{\av}[2][]{\left\langle #2\right\rangle_{#1}}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Nested Sampling:}
\subtitle{A multi-purpose numerical tool for science and machine learning  }
\date{20\textsuperscript{th} March 2023}

\begin{document}



\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Highlight: state-of-the-art Nature review [\href{https://www.nature.com/articles/s43586-022-00121-x}{NatRev}]}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Invented by John Skilling in 2004.
            \item Recent Nature review primer on nested sampling led by Andrew Fowlie and assembled by the community.
            \item Showcases the current set of tools, and applications from chemistry to cosmology.
            \item In this talk
                \begin{itemize}
                    \item User guide to nested sampling
                    \item Particle physics applications
                    \item Cosmology applications
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/nature1}
        \includegraphics[width=\textwidth]{figures/nature2}
        \column{0.2\textwidth}
        \includegraphics[width=\textwidth]{figures/nature4}
        \includegraphics[width=\textwidth]{figures/nature5}
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The three pillars of Bayesian inference}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Parameter estimation}
            What do the data tell us about the parameters of a model?

            \textit{e.g. the size or age of a $\Lambda$CDM universe}
            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Model comparison}
            How much does the data support a particular model?

            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}} \vspace{-7pt}\]
            \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[5]{\Pi_\mathcal{M}}}{\C[7]{\sum_m Z_m \Pi_m}} \]
            \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Tension quantification}
            Do different datasets make consistent predictions from the same model? 
            \textit{e.g. CMB vs Type IA supernovae data}
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why do sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item The cornerstone of numerical Bayesian inference is working with \textbf{samples}.
            \item Generate a set of representative parameters drawn in proportion a distributionto the posterior $\theta\sim\C[0]{\mathcal{P}}$.
            \item The magic of marginalisation $\Rightarrow$ perform usual analysis on each sample in turn.
            \item The golden rule is \C[3]{stay in samples} until the last moment before computing summary statistics/triangle plots because \[\boxed{f(\:\av{X}\:)\ne \av{\:f(X)\:}}\]
            \item Generally need $\sim\mathcal{O}(12)$ independent samples to compute a value and error bar.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/samples.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.48\textwidth}
        \begin{block}{\textbf{MCMC}}
            \only<16>{
                \begin{itemize}
                    \item Single ``walker''
                    \item Explores posterior
                    \item Fast, if proposal matrix is tuned
                    \item Parameter estimation, suspiciousness calculation
                    \item Channel capacity optimised for generating posterior samples
                \end{itemize}
            }
        \end{block}
            \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
            \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
            \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
            \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
            \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
            \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.48\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \only<16>{
                \begin{itemize}
                    \item Ensemble of ``live points''
                    \item Scans from prior to peak of likelihood
                    \item Slower, no tuning required
                    \item Parameter estimation, model comparison, tension quantification
                    \item Channel capacity optimised for computing partition function
                \end{itemize}
            }
        \end{block}
            \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
            \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
            \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
            \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
            \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
            \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
            \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
            \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
            \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} 
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Sequentially update a set $S$ of $n$ samples:
                \begin{itemize}
                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from the prior $\pi$). 

                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
                \end{itemize}
            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard likelihood constraint}:
                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence} \& \C[0]{posterior} weights.
            \item The evolving ensemble of live points allows algorithms to perform self-tuning and mode clustering.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth,page=4]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Integration in Physics}
    \begin{itemize}
        \item Integration is a fundamental concept in physics, statistics and data science:
    \end{itemize}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Partition functions}
            \vspace{-11pt}
            \[ Z(\beta) = \int e^{-\beta H(q,p)} dq dp \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Path integrals}
            \[ \Psi = \int e^{i S} \mathcal{D}x \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Bayesian marginals}
            \vspace{-11pt}
            \[ \mathcal{Z}(D) = \int \mathcal{L}(D|\theta) \pi(\theta) d\theta \]
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Need numerical tools if analytic solution unavailable.
            \item High-dimensional numerical integration is hard.
            \item Riemannian strategy estimates volumes geometrically:
                \[ \int f(x) d^nx \approx \sum_i f(x_i) \Delta V_i \sim \mathcal{O}(e^n) \]
            \item Curse of dimensionality $\Rightarrow$ exponential scaling.
            \item Nested sampling integrates \textbf{probabilistically}.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Integration in high dimensions}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Nested sampling can compute the \C[3]{Bayesian evidence} $\C[3]{\mathcal{Z}} = \int\C[2]{\mathcal{L}(\theta)}\C[1]{\pi(\theta)}d\theta$
            \item Numerical integration $\int f(x) dV$ in high dimensions is hard.
            \item \texttt{scipy.integrate(...)} is unusable in more than four dimensions.
            \item This is due to the curse of dimensionality: need to sum $\sim N^d$ units to compute $\approx \sum_i f(x_i) \Delta V_i$.
            \item Additionally, estimating volumes with geometry becomes exponentially hard as $d$ increases.
            \item \textit{Aside: \textbf{Riemannian integration} (blue) is taught as standard. An orthogonal approach (red) [usually theoretical] is \textbf{Lesbesgue integration}.}
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration}
        \includegraphics[width=\textwidth]{figures/2560px-Riemannvslebesgue.svg.png}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Probabalistic volume estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Key idea in NS: estimating volumes probabilistically
        \[
            \frac{\C[1]{V_\mathrm{after}}}{\C[0]{V_\mathrm{before}}} 
            \approx \frac{\C[1]{n_\mathrm{in}}}{\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}}
        \]
            \item This is the \textbf{only} way to calculate volume in high dimensions $d>3$.
                \begin{itemize}
                    \item Geometry is exponentially inefficient.
                \end{itemize}
            \item This really is the unique selling point of nested sampling.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/compression_1}%
        \includegraphics<2->[width=\textwidth]{figures/compression_2}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{(Lesbesgue) Integrating with nested sampling}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item At each iteration, the likelihood contour will shrink in volume $X$ by  $\approx 1/n$.
            \item Nested sampling zooms in to the peak of the function $\mathcal{L}$ {\em exponentially}.
                \vspace{-5pt}
                \[
                    \mathcal{Z} \approx \sum_i \Delta\mathcal{L}_i X_{i}, \quad
                    X_{i+1} \approx \frac{n}{n+1}X_i, \quad X_{0} = 1 .
                \]
                \vspace{-15pt}
            \item Although this is only approximate, we can quantify the error 
                \vspace{-10pt}
                \[
                    P(X_i|X_{i-1}) = \frac{X_{i}^{n-1}}{nX_{i-1}^n}\times[0<X_i<X_{i-1}].
                \]
                \vspace{-15pt}
            \item Integral can be discretised in several ways
                \vspace{-10pt}
                \[
                    \mathcal{Z} \approx \sum_i \Delta\mathcal{L}_i X_{i} = \sum_i \mathcal{L}_i \Delta X_{i} = \sum_i \tfrac{\mathcal{L}_i+\mathcal{L}_{i-1}}{2}{\small(X_{i-1}-X_i).}
                \]

        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/lesbesgue}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/lesbesgue}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/lesbesgue}%
        \includegraphics<4|handout:0>[width=\textwidth,page=4]{figures/lesbesgue}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/lesbesgue}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/lesbesgue}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/lesbesgue}%
        \includegraphics<8|handout:0>[width=\textwidth,page=8]{figures/lesbesgue}%
        \includegraphics<9|handout:0>[width=\textwidth,page=9]{figures/lesbesgue}%
        \includegraphics<10|handout:0>[width=\textwidth,page=10]{figures/lesbesgue}%
        \includegraphics<11|handout:0>[width=\textwidth,page=11]{figures/lesbesgue}%
        \includegraphics<12|handout:0>[width=\textwidth,page=12]{figures/lesbesgue}%
        \includegraphics<13|handout:0>[width=\textwidth,page=13]{figures/lesbesgue}%
        \includegraphics<14|handout:0>[width=\textwidth,page=14]{figures/lesbesgue}%
        \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/lesbesgue}%
        \includegraphics<16          >[width=\textwidth,page=16]{figures/lesbesgue}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Dead points: posteriors \& evidences}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded points.
            \item These may be weighted to form weighted posterior samples using $w_i = \mathcal{L}_i \Delta X_i$.
            \item They can also be used to calculate the integral $\mathcal{Z} = \sum \mathcal{L}_i \Delta X_i$, or more generally $\sum_i f(\mathcal{L}_i) \Delta X_i$.
                \begin{itemize}
                    \item Nested sampling probabilistically estimates the volume of the parameter space
                        \[X_i \approx {\left(\frac{n}{n+1}\right)} X_{i-1} \quad\Rightarrow\quad
                        X_i \approx {\left(\frac{n}{n+1}\right)}^i \approx e^{-i/n}, \]
                    \item Nested sampling thus estimates the density of states,
                    \item it is therefore a partition function calculator
                        $Z(\beta) = \sum_i \mathcal{L}_i^\beta \Delta X_i$.
                \end{itemize}
            \item The evolving ensemble of live points allows algorithms to perform self-tuning and mode clustering.
        \end{itemize}

        \column{0.4\textwidth}

        \includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        \includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%

    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Time complexity of nested sampling}
    \includegraphics[width=\textwidth]{figures/run_prodecure}
    \vspace{-20pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-20pt}
    \begin{itemize}
        \item $x$-axis: log-compression of live points
        \item Area $\propto$ posterior mass
        \item Shows Bayesian balance of likelihood vs prior
        \item Run proceeds right to left
        \item Run finishes after bump (typical set)
    \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Time complexity\vspace{-5pt}
                \[ \boxed{T = n_\mathrm{live} \times T_\mathcal{L} \times T_\mathrm{sampler} \times \mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)}\vspace{-5pt}  \]
            \item Error complexity $\boxed{\sigma \propto \sqrt{\mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)/n_\mathrm{live}}}$
        \end{itemize}
    \end{columns}

\end{frame}

\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 
  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item A large fraction of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \item \url{https://projecteuclid.org/euclid.ba/1340370944}.
    \item There has also been much work beyond this (focus of this talk).
  \end{itemize}
 
\end{frame}


\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{dynesty}~\arxiv{1904.02180} \texttt{nessai}~\arxiv{2102.11056}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Types of nested sampler}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
\item Constructs bounding region and draws many invalid points until $\mathcal{L}(\theta)>\mathcal{L}_*$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{5pt}
    \begin{itemize}
        \item Nested samplers usually come with:
            \begin{itemize}
                \item \emph{resolution} parameter $n_\mathrm{live}$ (which improve results as $\sim\mathcal{O}(n_\mathrm{live}^{-1/2})$.
                    \item set of \emph{reliability} parameters~\arxiv{2101.04525}, which don't improve results if set arbitrarily high, but introduce systematic errors if set too low.
                    \item e.g. \texttt{Multinest} efficiency \texttt{eff} or \texttt{PolyChord} chain length $n_\mathrm{repeats}$.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Cosmology}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Battle-tested in Bayesian cosmology on
                \begin{itemize}
                    \item Parameter estimation: multimodal alternative to MCMC samplers.
                    \item Model comparison: using integration to compute the Bayesian evidence
                    \item Tension quantification: using deep tail sampling and suspiciousness computations.
                \end{itemize}
            \item Plays a critical role in major cosmology pipelines: Planck, DES, KiDS, BAO, SNe.
            \item The default $\Lambda$CDM cosmology is well-tuned to have Gaussian-like posteriors for CMB data. 
            \item Less true for alternative cosmologies/models and orthogonal datasets, so nested sampling crucial.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/pps_both}
        \includegraphics[width=0.49\textwidth]{figures/reach_fit-cropped.pdf}
        %\includegraphics[width=0.49\textwidth]{figures/curvature_3}
        \includegraphics[width=\textwidth]{figures/omegak_H0_2.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Astrophysics}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item In exoplanets~\arxiv{1806.00518}
                \begin{itemize}
                    \item Parameter estimation: determining properties of planets.
                    \item Model comparison: how many planets? Stellar modelling~\arxiv{2007.07278}.
                    \item exoplanet problems regularly have posterior phase transitions \arxiv{2102.03387}
                \end{itemize}
            \item In gravitational waves
                \begin{itemize}
                    \item Parameter estimation: Binary merger properties
                    \item Model comparison: Modified theories of gravity, selecting phenomenological parameterisations~\arxiv{1803.10210}
                    \item Likelihood reweighting: fast slow properties
                \end{itemize}
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/rv_full.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Particle physics}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{columns}
            \column{0.67\textwidth}
            \begin{itemize}
                \item Nested sampling for cross section computation/event generation
            \end{itemize}
            \column{0.3\textwidth}
            \[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2.\]
        \end{columns}
        \begin{itemize}
            \item Nested sampling can explore the phase space $\Omega$ and compute integral blind with comparable efficiency to HAAG/RAMBO~\arxiv{2205.02030}.
            \item Bayesian sparse reconstruction~\arxiv{1809.04598} applied to bump hunting allows evidence-based detection of signals in phenomenological backgrounds~\arxiv{2211.10391}.
            \item Now applying to lattice field theory, and lattice gravity Lagrangians.
            \item Fine tuning quantification
        \end{itemize}
        \column{0.17\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1-pdfjam-crop.pdf}
        \column{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/psi_predict-crop.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Machine learning}
    \begin{columns}
        \column{0.62\textwidth}
        \begin{itemize}
            \item Machine learning requires:
                \begin{itemize}
                    \item Training to find weights
                    \item Choice of architecture/topology/hyperparameters
                \end{itemize}
            \item Bayesian NNs treat training as a model fitting problem
            \item Compute posterior of weights (parameter estimation), rather than optimisation (gradient descent)
            \item Use evidence to determine best architecture (model comparison), correlates with out-of-sample performance! 
            \item Solving the full ``shallow learning'' problem without compromise \arxiv{2004.12211}\arxiv{2211.10391}. 
            \item Promising work ongoing to extend this to transfer learning and deep nets.
        \end{itemize}
        \column{0.38\textwidth}
        \includegraphics[width=\textwidth]{figures/nn_posterior-cropped.pdf}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{Applications of nested sampling}
%    \framesubtitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
%    \begin{columns}
%        \column{0.57\textwidth}
%        \begin{itemize}
%            \item Nested sampling for frequentist computation!?
%            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
%            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
%            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
%            \item Need insight/approximation to make efficient.
%            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
%            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
%        \end{itemize}
%        \column{0.43\textwidth}
%        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{and beyond\ldots}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/watkinson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mason-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/formanek-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mcaloone-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/stenczel-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/yallup-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/bex-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/claireburke-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/hobson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/lasenby-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mhandley-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/whandley-headshot.jpg}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        \includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/tdoa-cropped-1-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/tdoa-cropped-2-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/tdoa-cropped-3-crop.pdf}%
        \includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{What is a model?}
    \begin{itemize}
        \item Model comparison in its purest form answers question such as:
            \begin{itemize}
                \item ``Is the universe $\Lambda$CDM?''
                \item ``Are neutrinos in a normal or inverted hierarchy?''
                \item ``Is there a detectable global signal in this data?''
            \end{itemize}
        \item However model $\mathcal{M}$ is likelihood $\C[2]{\mathcal{L}=P(D|\theta,M)}$ and priors $\C[1]{\pi=P(\theta|M)}$, $\C[5]{\Pi=P(M)}$.
        \item Can use the evidence \C[3]{$\mathcal{Z}$} to decide on which out of a set of likelihoods best describe data (e.g. Gaussian, Cauchy, Poisson, radiometric).
        \item Can also use it for antenna selection~\arxiv{2106.10193}~\arxiv{2109.10098}.
        \item In principle can use it to decide between theoretically motivated priors (care needed).
        \item It can also be used for non-parametric reconstruction:
            \begin{itemize}
                \item ``How many polynomial terms best describe the data?''
                \item ``How complicated a sky model do I need?''
                \item ``Which is the best sky model?''
            \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Model comparison and parameter estimation~\arxiv{1908.09139}}

    \begin{columns}

        \begin{column}{0.55\textwidth}

            \begin{itemize}
                \item If you allow $\Omega_K\ne0$, \textit{Planck} (\texttt{plikTTTEEE}) has a moderate preference for closed universes (50:1 betting odds on), $\Omega_K=-4.5\pm1.5\%$
                \item \textit{Planck}+lens+BAO strongly prefer $\Omega_K=0$.
                \item But, \textit{Planck} vs lensing is 2.5$\sigma$ in tension, and Planck vs BAO is 3$\sigma$.
                \item Reduced if $\texttt{plik}\to\texttt{camspec}$~\arxiv{2002.06892} 
                \item BAO and lensing summary assume $\Lambda$CDM.
                \item Doing this properly with BAO retains preference for closed universe (though closer to flat $\Omega_K =-0.4\pm0.2\%$)~\arxiv{2205.05892}.
                \item Present-day curvature has profound consequences for inflation~\arxiv{2205.07374}.
            \end{itemize}

        \end{column}

        \begin{column}{0.45\textwidth}
            \includegraphics<1|handout:0>[width=\textwidth]{figures/curvature_1}%
            \includegraphics<2|handout:0>[width=\textwidth]{figures/curvature_2}%
            \includegraphics<3          >[width=\textwidth]{figures/curvature_3}%

        \end{column}

    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{\texttt{unimpeded}: legacy suites for the next generation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item DiRAC 2020 RAC allocation of 30MCPUh
            \item Main goal: Planck Legacy Archive equivalent
            \item Parameter estimation $\to$ Model comparison
            \item MCMC $\to$ Nested sampling
            \item Planck $\to$ $\{\text{Planck}, \text{DESY1}, \text{BAO}, \ldots \}$
            \item Pairwise combinations
            \item Suite of tools for processing these 
                \begin{itemize}
                    \item \texttt{anesthetic} $2.0$
                    \item \texttt{unimpeded} $1.0$
                    \item \texttt{zenodo} archive
                    \item \texttt{margarine}
                \end{itemize}
            \item MCMC chains also available.
            \item Library of bijectors emulators for fast re-use
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{logos/dirac}
        \includegraphics[width=0.5\textwidth,page=21]{figures/himmelblau}%
        \includegraphics[width=0.5\textwidth,page=15]{figures/himmelblau}
    \end{columns}
\end{frame}



\newcommand{\movablecross}[1]{%
  \draw[->](#1) -- ++(0:\croslen);
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(180:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}

\newcommand{\movablevert}[1]{%
  \draw[->](#1) -- ++(90:\croslen);
  \draw[->](#1) -- ++(270:\croslen);
  \fill[red!70!black] (#1) circle (2pt);
}


\begin{frame}
    \frametitle<1-5>{Primordial power spectrum $\mathcal{P}_\mathcal{R}(k)$ reconstruction~\arxiv{1908.00906}}
  \frametitle<6|handout:0>{0 internal knots}
  \frametitle<7|handout:0>{1 internal knot}
  \frametitle<8|handout:0>{2 internal knots}
  \frametitle<9|handout:0>{3 internal knots}
  \frametitle<10|handout:0>{4 internal knots}
  \frametitle<11|handout:0>{5 internal knots}
  \frametitle<12|handout:0>{6 internal knots}
  \frametitle<13|handout:0>{7 internal knots}
  \frametitle<14|handout:0>{Bayes Factors}
  \frametitle<15|handout:0>{Marginalised plot}
  \frametitle<16|handout:0>{Kullback-Liebler divergences}
  %\framesubtitle{Primordial power spectrum $\mathcal{P}_\mathcal{R}(k)$ reconstruction}


  \begin{columns}
      \column{0.55\textwidth}
      \uncover<1->{
          \begin{itemize}
              \item Traditionally parameterise the primordial power spectrum with $(A_s,n_s)$
                  \[\mathcal{P}_\mathcal{R}(k) = A_s \left( \frac{k}{k_*} \right)^{n_s-1}\]
              \item To add more degrees of freedom, can add ``running'' parameters $n_\text{run}$ (higher order polynomial in index)
              \item Alternative non-parametric technique introduces a more flexible phenomenological parameterisation: ``FlexKnots''
              \item Let the Bayesian evidence decide when you've introduced too many parameters
          \end{itemize}
      }
      \column{0.45\textwidth}

      \only<1-5|handout:0>{
  \resizebox{\textwidth} {!} {%
    \begin{tikzpicture}
    % width of axes
      \def\xwidth{7}
      \def\ywidth{7}
    % min coordinate
      \def\xmn{0.5}
      \def\ymn{4}
    % start coordinate
      \def\xstart{2}
      \def\ystart{5}
    % middle coordinate
      \def\xmid{3}
      \def\ymid{3}
    % end coordinate
      \def\xend{5.5}
      \def\yend{5}
    % max coordinate
      \def\xmx{6.5}
      \def\ymx{3.5}

    % length of crosses
      \def\croslen{0.4}


    % Draw axes
      \draw<1-5|handout:0> [<->,thick] (0,\ywidth) node (yaxis) [above] {$\log\mathcal{P}_\mathcal{R}(k)$}
      |- (\xwidth,0) node (xaxis) [right] {$\log k$};
    % Draw limits
      %\draw [-,dashed] (\xmn,0) node[below] {$\log_{10}k_1$} -- (\xmn,\ywidth) ;
      %\draw [-,dashed] (\xmx,0) node[below] {$\log_{10}k_N$} -- (\xmx,\ywidth) ;

      \draw<1|handout:0> (\xmn,\ymn) -- (\xmx,\ymx);
      \draw<1|handout:0> (\xstart,\ystart) node[below right] {$A_s {\left(\frac{k}{k_*}\right)}^{n_s-1}$};

    % Draw the line joining start and end

      \coordinate (mn) at (\xmn,\ymn);
      \coordinate (start) at (\xstart,\ystart);
      \coordinate (mid) at (\xmid,\ymid);
      \coordinate (end) at (\xend,\yend);
      \coordinate (mx) at (\xmx,\ymx);
      \draw<2|handout:0> (mn) -- (mx);
      \draw<2-5|handout:0> (mn) node[below right]    {$(k_1,\mathcal{P}_{1})$};
      \draw<2|handout:0> (mx) node[below left]     {$(k_{2},\mathcal{P}_{{2}})$};
      \onslide<2-5|handout:0>{\movablevert{mn}};
      \onslide<2-5|handout:0>{\movablevert{mx}};

      \draw<3|handout:0> (mn) -- (start) -- (mx);
      \onslide<3-5|handout:0>{\movablecross{start}};
      \draw<3-5|handout:0> (start) node[above right] {$(k_2,\mathcal{P}_{2})$};
      \draw<3|handout:0> (mx) node[below left]     {$(k_{3},\mathcal{P}_{{3}})$};
 
      \draw<4|handout:0> (mn) -- (start) -- (mid) -- (mx);
      \onslide<4-5|handout:0>{\movablecross{mid}};
      \draw<4-5|handout:0> (mid) node[below right] {$(k_3,\mathcal{P}_{3})$};
      \draw<4|handout:0> (mx) node[below left]     {$(k_{4},\mathcal{P}_{{4}})$};

      \draw<5|handout:0> (mn) -- (start) -- (mid) -- (end) -- (mx);
      \onslide<5|handout:0>{\movablecross{end}};
      \draw<5|handout:0> (end) node[above right] {$(k_4,\mathcal{P}_{4})$};
      \draw<5|handout:0> (mx) node[below left]     {$(k_{N_\text{knots}},\mathcal{P}_{N_\text{knots}})$};


      %\draw<2-|handout:0> (\xmn,\ymn) coordinate (mn) -- (\xstart,\ystart) coordinate (start) -- (\xmid,\ymid) coordinate (mid) --  (\xend,\yend) coordinate(end) -- (\xmx,\ymx) coordinate(mx);

    % Draw the point labels
      %\draw<2-|handout:0> (mn) node[below right]    {$(k_1,\mathcal{P}_{1})$};
      %\draw<2-|handout:0> (start) node[above right] {$(k_2,\mathcal{P}_{2})$};
      %\draw<2-|handout:0> (mid) node[below right]   {$(k_3,\mathcal{P}_{3})$};
      %\draw<2-|handout:0> (end) node[above right]   {$(k_4,\mathcal{P}_{4})$};
      %\draw<2-|handout:0> (mx) node[below left]     {$(k_{N_\text{knots}},\mathcal{P}_{N_\text{knots}})$};

    % Draw a dashed line indicating the coordinate names
      %\draw[dashed] (yaxis |- start) node[left] {$y_{1}$}
      %-| (xaxis -| start) node[below] {$x_1$};
      %\draw[dashed] (yaxis |- mid) node[left] {$y_{2}$}
      %-| (xaxis -| mid) node[below] {$x_2$};
      %\draw[dashed] (yaxis |- end) node[left] {$y_{N}$}
      %-| (xaxis -| end) node[below] {$x_N$};
      %\draw  (xaxis -| start) node[below] {$\log_{10}k_2$};
      %\draw  (xaxis -| mid) node[below] {$\log_{10}k_3$};
      %\draw  (xaxis -| end) node[below] {$\log_{10}k_4$};

      % Draw the crosses
      %\onslide<2-|handout:0>{\movablevert{mn}
      %\movablecross{start}
      %\movablecross{mid}
      %\movablecross{end}
      %\movablevert{mx}
    %};

    % put some ellipses in between the start and end point

    \end{tikzpicture}

  }
  }




    \includegraphics<6|handout:0>[width=\textwidth]{figures/pps_both_1}
    \includegraphics<7|handout:0>[width=\textwidth]{figures/pps_both_2}
    \includegraphics<8|handout:0>[width=\textwidth]{figures/pps_both_3}
    \includegraphics<9|handout:0>[width=\textwidth]{figures/pps_both_4}
    \includegraphics<10|handout:0>[width=\textwidth]{figures/pps_both_5}
    \includegraphics<11|handout:0>[width=\textwidth]{figures/pps_both_6}
    \includegraphics<12|handout:0>[width=\textwidth]{figures/pps_both_7}
    \includegraphics<13|handout:0>[width=\textwidth]{figures/pps_both_8}
    \includegraphics<14|handout:0>[width=\textwidth]{figures/pps_evidence}
    \includegraphics<15          >[width=\textwidth]{figures/pps_both}
    \includegraphics<16|handout:0>[width=\textwidth]{figures/DKL.pdf}

  \end{columns}
\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology [\href{https://www.nature.com/articles/s41550-022-01709-9}{NatAstro}]}
    \begin{columns}
        \column{0.62\textwidth}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Gives a specific absorption trough, which if detected allows constraints on the physics of the dark ages decade(s) before SKA.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
        \includegraphics[height=0.3\textwidth]{figures/EDGES_antenna}
        \includegraphics[height=0.3\textwidth]{figures/SARAS}
        \includegraphics[height=0.3\textwidth]{figures/REACH_2.jpg}
        \end{itemize}

        \column{0.38\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}


    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology [\href{https://www.nature.com/articles/s41550-022-01709-9}{NatAstro}]}
    \begin{columns}
        \column{0.65\textwidth}
        \begin{itemize}
            \item EDGES~[\href{https://www.nature.com/articles/nature25792}{Nat}] claimed a controversial 2019 detection.
            \item SARAS3~[\href{https://www.nature.com/articles/s41550-022-01610-5}{NatAstro}]  would have detected this by 2021.
            \item REACH~[\href{https://www.nature.com/articles/s41550-022-01709-9}{NatAstro}] aims to settle the debate.
                \begin{itemize}
                    \item Broader band,
                    \item Honesty about systematic modelling,
                    \item State of the art inference.
                \end{itemize}
            \item Create parameterised models of sky, beam and signal, breaking degeneracy with a time-dependent likelihood to measure all three simultaneously.
            \item Use model comparison based reconstruction to determine complexity of parameterisation.
            \item Use model comparison to select likelihoods.
            \item A collaboration powered by nested sampling.
        \end{itemize}

        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/EDGES}
        \begin{center}
            \includegraphics[width=0.8\textwidth]{figures/REACH_1}
        \end{center}
    \end{columns}

\end{frame}

%\begin{frame}
%    \frametitle{Nested Sampling with Likelihood Free Inference}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}
%
%        \hfill Alsing \textit{et al.}~\arxiv{1903.00007}
%    \begin{itemize}
%        \item In density estimation likelihood free inference, the output is to learn one/all of:
%            \begin{description}
%                \item[Likelihood] $P(D|\theta)$,
%                \item[Posterior] $P(\theta|D)$,
%                \item[Joint] $P(D,\theta)$.
%            \end{description}
%        \item In the first instance, nested sampling can be used to scan these learnt functions.
%    \end{itemize}
%        \column{0.5\textwidth}
%    \begin{itemize}
%        \item Data are compressed, so joint space $(D,\theta)$ is navigable by off-the-shelf codes. 
%            \begin{itemize}
%                \item Sanity checking the solution,
%                \item Computing evidences/Kullback Liebler divergences from likelihoods.
%            \end{itemize}
%        \item Its self-tuning capacity and ability to handle multi-modal distributions can be very useful for diagnosing incompletely learnt functions.
%        \item Emulated likelihoods (e.g. normalising flows) are generally fast, so can deploy more likelihood hungry techniques like NS.
%        \item In principle can use it to train emulators by marginalisation rather than maximisation.
%    \end{itemize}
%    \end{columns}
%\end{frame}
%
%\begin{frame}
%    \frametitle{Nested Sampling for Approximate Bayesian Computation/SBI}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item Assume one has a generative model capable of turning parameters into mock data $D(\theta)$.
%            \item Given infinite computing power, ABC works by selecting $\{\theta : D(\theta)=D_\text{observed}\}$.
%            \item These are samples from the posterior, without using a likelihood.
%            \item In practice $D=D_\text{obs}$ becomes $D\approx D_\text{obs}$.
%            \item i.e. $|D-D_\text{obs}|<\varepsilon$, or more generally $\boxed{\rho(D,D_\text{obs})<\varepsilon}$, where $\rho$ is some suitably chosen objective function.
%            \item Main challenges are 
%                \begin{enumerate}
%                    \item Choice of $\rho$/summary stats,
%                    \item Choice of $\varepsilon$ schedule,
%                    \item Rejection sampling.
%                \end{enumerate}
%        \end{itemize}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item Nested sampling fits this well: In principle, can just change the usual hard likelihood constraints $\{\theta\sim\pi : \mathcal{L}(\theta)>\mathcal{L}_*\}$ to
%                \[\{\theta~\sim\pi : \rho(D(\theta),D_\text{obs})<\varepsilon\}\]
%            (Brewer \& Foreman-Mackey~\arxiv{1606.03757})
%            \item Ongoing work with Andrew Fowlie \& Sebastian Hoof
%                \begin{itemize}
%                    \item How to deal with nondeterminism,
%                    \item How to interpret $\rho$ as a ``likelihood'',
%                    \item How to interpret the evidence $\mathcal{Z}$.
%                \end{itemize}
%        \end{itemize}
%    \end{columns}
%\end{frame}

%\begin{frame}
%    \frametitle{Nested sampling for truncated methods}
%
%    \begin{columns}
%        \column{0.55\textwidth}
%        \begin{itemize}
%            \item Will hear more on this tomorrow from Christoph
%            \item Many Likelihood implicit approaches at the moment have some element of sampling direct from the prior
%            \item Inefficient if number of parameters $>\mathcal{O}(\text{a few})$
%            \item Can get round this by truncating to region:
%                \[ \Gamma\{ \theta\in \text{supp} p(\theta) \:|\: p(\theta|x_0)>\bar\varepsilon\} \]
%            \item At the moment regions defined by nested boxes
%            \item \textbf{Frontier:} This seems ripe for replacement by NS -- PhD student currently working on this (Kilian Scheutwinkel)
%        \end{itemize}
%        \column{0.45\textwidth}
%        \includegraphics[width=\textwidth]{figures/tmnre}
%
%        \hfill Cole \textit{et al.}~\arxiv{2111.08030}
%    \end{columns}
%    
%\end{frame}

\begin{frame}
    \frametitle{Beyond the meta-algorithm}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Dynamic nested sampling~\arxiv{1704.03459}
            \item Unwoven nested sampling~\arxiv{1703.09701}
            \item Accelerated nested sampling~\arxiv{2212.01760}
            \item Precision nested sampling~\arxiv{2006.03371}
            \item Multiobjective nested sampling
            \item Nested sampling with gradients?
            \item Reversible nested sampling?
            \item Transdimensional nested sampling?
            \item postprocessing: \texttt{anesthetic}~\arxiv{1905.04768}
            \item crosschecking: \texttt{nestcheck}~\arxiv{1804.06406}
            \item See ``Frontiers of nested sampling'' talk from last year: \href{https://www.willhandley.co.uk/talks}{willhandley.co.uk/talks}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/dynesty.pdf}
        \includegraphics[width=\textwidth]{figures/supernest.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Conclusions}
    \begin{columns}
        \column{0.65\textwidth}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Performing Bayesian inference and model comparison.
            \end{itemize}
        \item It is applied widely across cosmology and particle physics.
        \item It can be applied to both Bayesian and Frequentist inference.
    \end{itemize}
        \column{0.35\textwidth}
    \includegraphics[height=0.6\textwidth]{figures/pps_both}%
    \includegraphics[height=0.6\textwidth]{figures/REACH_2}%
    \end{columns}
    \includegraphics[height=0.2\textwidth]{figures/integration}%
    \includegraphics[height=0.2\textwidth]{figures/curvature_3}%
    \includegraphics[height=0.2\textwidth]{figures/ALP_3}%
\end{frame}


\appendix
\begin{frame}
    \frametitle{How does Nested Sampling compare to other approaches?}
    \begin{columns}
        \column{0.7\textwidth}
        \begin{itemize}
            \item In all cases:
                \begin{itemize}
                    \item[$+$] NS can handle multimodal functions
                    \item[$+$] NS computes evidences, partition functions and integrals
                    \item[$+$] NS is self-tuning/black-box
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
    \end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \begin{itemize}
                \item Gradient descent
                    \begin{itemize}
                        \item[$-$] NS cannot use gradients
                        \item[$+$] NS does not require gradients
                    \end{itemize}
                \item Genetic algorithms
                    \begin{itemize}
                        \item[$+$] NS discarded points have statistical meaning
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Sampling}
            \begin{itemize}
                \item Metropolis-Hastings?
                    \begin{itemize}
                        \item[$-$] Nothing beats well-tuned customised MH
                        \item[$+$] NS is self tuning
                    \end{itemize}
                \item Hamiltonian Monte Carlo?
                    \begin{itemize}
                        \item[$-$] In millions of dimensions, HMC is king
                        \item[$+$] NS does not require gradients
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \begin{itemize}
                \item Thermodynamic integration
                    \begin{itemize}
                        \item[$+$] protective against phase trasitions
                        \item[$+$] No annealing schedule tuning 
                    \end{itemize}
                \item Sequential Monte Carlo
                    \begin{itemize}
                        \item[$-$] SMC experts classify NS as a kind of SMC
                        \item[$+$] NS is athermal
                    \end{itemize}
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling: a user's guide}
    \begin{enumerate}
        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
            \begin{itemize}
                \item This means typically most of its time is spent on burn-in rather than posterior sampling.
                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable.
            \end{itemize}
        \item The number of live points $n_\text{live}$ is a resolution parameter.
            \begin{itemize}
                \item Run time is linear in $n_\text{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\text{live}}}$.
                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
            \end{itemize}
        \item Most algorithms come with additional reliability parameter(s).
            \begin{itemize}
                \item e.g. \texttt{MultiNest}: $\texttt{eff}$, \texttt{PolyChord}: $n_\text{repeats}$.
                \item These are parameters which have no gain if set too conservatively, but increase the reliability.
                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
            \end{itemize}
    \end{enumerate}
\end{frame}




\begin{frame}
    \frametitle{Occam's Razor~\arxiv{2102.11511}}
    \begin{itemize}
        \item Bayesian inference quantifies Occam's Razor:
            \begin{itemize}
                \item \textit{``Entities are not to be multiplied without necessity''} \hfill --- William of Occam
                \item \textit{``Everything should be kept as simple as possible, but not simpler''} \hfill --- ``Albert Einstein''
            \end{itemize}
        %\item Consider the Evidence $\C[3]{\mathcal{Z}\equiv P(D|M)}$: 
        %    \begin{description}[Parameter estimation]
        %        \item [Parameter estimation] normalisation constant
        %        \item [Model comparison] critical update factor for \C[5]{model prior} to \C[4]{model posterior}
        %    \end{description}
        \item Properties of the evidence: rearrange Bayes' theorem for parameter estimation
            \[\C[0]{\mathcal{P}(\theta)} = \frac{\C[2]{\mathcal{L}(\theta)} \C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}}} \qquad\Rightarrow\qquad \C[3]{\log \mathcal{Z}} = \C[2]{\log\mathcal{L}(\theta)} - \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}}.\]  
        \item Evidence is composed of a ``goodness of fit'' term  and ``Occam Penalty''.
    \end{itemize}
    \begin{columns}[t]
        \column{0.5\textwidth}
    \begin{itemize}
        \item RHS true for all $\theta$. Take max likelihood value $\theta_*$:
            \[
                \log \mathcal{Z} = -\chi_\text{min}^2 - \text{Mackay penalty.}
            \]
    \end{itemize}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Be more Bayesian and take posterior average to get the ``Occam's razor equation''
            \[
                \boxed{
                    \log \mathcal{Z} = \av[\mathcal{P}]{\log\mathcal{L}} - \mathcal{D}_\text{KL}.
            }
            \]
    \end{itemize}
    \end{columns}
    \vfill
    \begin{itemize}
        \item Natural regularisation which penalises models with too many parameters.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kullback Liebler divergence}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is is defined as:
                \[\mathcal{D}_\text{KL} = \av[\mathcal{P}]{\log\frac{\mathcal{P}}{\pi}} = \int \mathcal{P}(\theta) \log \frac{\mathcal{P}(\theta)}{\pi(\theta)}d\theta.\]
            \item Whilst not a distance, $\mathcal{D}=0$ when $\mathcal{P}=\pi$.
            \item Occurs in the context of machine learning as an objective function for training functions.
            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
                \[ \mathcal{D}_\text{KL} \approx \log \frac{V_\pi}{V_\text{P}}.\]
                (this is exact for top-hat distributions).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Nested sampling for frequentist computation!?
            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
            \item Need insight/approximation to make efficient.
            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
    \end{columns}
    
\end{frame}

\begin{frame}
    \frametitle{Exploration of phase space~\arxiv{2106.02056}}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Nested sampling for cross section computation/event generation.
            \item Numerically compute collisional cross section 
                \vspace{-5pt}\[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2,\]
                $\Omega$ phase space of kinematic configurations $\Phi$, each with matrix element $\mathcal{M}(\Phi)$.
            \item Current state of the art e.g. HAAG (improvement on RAMBO) requires knowledge of $\mathcal{M}(\Phi)$.
            \item Nested sampling can explore the phase space and compute integral blind with comparable efficiency.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1.pdf}
        \includegraphics[width=\textwidth]{figures/phase_space.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Quantification of fine tuning~\arxiv{2101.00428}~\arxiv{2205.13549}}
    \vspace{-10pt}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Example: Cosmological constraints on decaying axion-like particles~\arxiv{2205.13549}.
            \item Subset of parameters $\xi,m_a,\tau,g_{a\gamma}$: ALP fraction, mass, lifetime and photon coupling.
                {(\small Also vary cosmology, $\tau_n$ and nuisance params)}
            \item Data: CMB, BBN, FIRAS, SMM, BAO.
            \item Standard profile likelihood fit shows ruled out regions and best-fit point.
            \item<2-> Nested sampling scan:
                \begin{itemize}
                    \item Quantifies amount of parameter space ruled out with Kullback-Liebler divergence $\mathcal{D}_\mathrm{KL}$.
                    \item Identifies best fit region as statistically irrelevant from information theory/Bayesian.
                    \item No evidence for decaying ALPs. Fit the data equally well: but more constrained parameters create Occam penalty.
                \end{itemize}
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/ALP_1.pdf}
        \includegraphics<2          >[width=\textwidth]{figures/ALP_2.pdf}
        \includegraphics<3|handout:0>[width=\textwidth]{figures/ALP_3.pdf}
    \end{columns}
    
\end{frame}




\end{document}
```
# branch: remotes/origin/aachen_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usetikzlibrary{arrows,arrows.meta,automata,positioning}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{ Nested sampling: {\large powering next-generation inference and machine learning tools}}
\subtitle{for astrophysics, cosmology, particle physics and beyond}
\date{18\textsuperscript{th} July 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}


\begin{frame}
    \frametitle{LBI: Likelihood-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        The standard approach if you are fortunate enough to have a likelihood function $\only<1-2>{P(D|\theta)}\only<3->{\C[2]{\mathcal{L}(D|\theta)}}$: 
        \[
            \only<1-2>{
                P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}
            }
            \only<2>{
                \qquad
                \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}}\times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}
            }
            \only<3>{
                \C[0]{\mathcal{P}(\theta|D)} = \frac{\C[2]{\mathcal{L}(D|\theta)}\C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}(D)}}
                \qquad
                \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}}\times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}
            }
            \only<4>{
                P(\theta|D) P(D) = P(\theta,D) = P(D|\theta)P(\theta), \qquad
            }
            \only<5>{
                \C[0]{\mathcal{P}}\times\C[3]{\mathcal{Z}} = \C[4]{\mathcal{J}} = \C[2]{\mathcal{L}}\times\C[1]{\pi}, \qquad \C[4]{\text{Joint}} = \C[4]{\mathcal{J}} = P(\theta,D)
            }
        \]
        \vspace{-10pt}
        \begin{enumerate}
            \item Define \C[1]{prior $\pi(\theta)$} 
                \begin{itemize}
                    \item spend some time being philosophical
                \end{itemize}
            \item Sample \C[0]{posterior $\mathcal{P}(\theta|D)$} 
                \begin{itemize}
                    \item use out-of-the-box MCMC tools such as\\ \texttt{emcee} or \texttt{MultiNest}
                    \item make some triangle plots
                \end{itemize}
            \item Optionally compute \C[3]{evidence $\mathcal{Z}(D)$}
                \begin{itemize}
                    \item e.g. nested sampling or parallel tempering
                    \item do some model comparison (i.e. science)
                    \item talk about tensions
                \end{itemize}
        \end{enumerate}
        \column{0.5\textwidth}
        \hfill%
        \begin{overpic}[width=0.6\textwidth]{figures/des_parameters.pdf}
            \put(-40,90) {DES Y5 SN Ia}
            \put(-40,80) {\arxiv{2401.02929}}
        \end{overpic}
        \includegraphics[width=0.5\textwidth]{figures/des_model_comparison.pdf}%
        \includegraphics[width=0.5\textwidth]{figures/des_suspiciousness.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Where is Nested Sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item For many purposes, in your Neural Net you should group Nested Sampling with (MCMC) techniques such as:
                \begin{itemize}
                    \item Metropolis-Hastings (PyMC, MontePython)
                    \item Hamiltonian Monte Carlo (Stan, blackjax)
                    \item Ensemble sampling (emcee, zeus). 
                    \item Variational Inference (Pyro)
                    \item Sequential Monte Carlo 
                    \item Thermodynamic integration
                    \item Genetic algorithms
                \end{itemize}
            \item You may have heard of it branded form:
                \begin{itemize}
                    \item MultiNest
                    \item PolyChord
                    \item dynesty
                    \item ultranest
                \end{itemize}
            \end{itemize}
        \column{0.5\textwidth}
        \begin{columns}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/emcee}
        \includegraphics[width=\textwidth]{figures/metropolis-hastings}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/zeus}
        \end{columns}
        \includegraphics[width=\textwidth]{figures/hmc_explained}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Integration in Physics}
    \begin{itemize}
        \item Integration is a fundamental concept in physics, statistics and data science:
    \end{itemize}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Partition functions}
            \vspace{-11pt}
            \[ Z(\beta) = \int e^{-\beta H(q,p)} dq dp \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Path integrals}
            \[ \Psi = \int e^{i S} \mathcal{D}x \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Bayesian marginals}
            \vspace{-11pt}
            \[ \mathcal{Z}(D) = \int \mathcal{L}(D|\theta) \pi(\theta) d\theta \]
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Need numerical tools if analytic solution unavailable.
            \item High-dimensional numerical integration is hard.
            \item Riemannian strategy estimates volumes geometrically:
                \[ \int f(x) d^nx \approx \sum_i f(x_i) \Delta V_i \sim \mathcal{O}(e^n) \]
            \item Curse of dimensionality $\Rightarrow$ exponential scaling.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.48\textwidth}
        \begin{block}{\textbf{MCMC}}
            \only<16>{
                \begin{itemize}
                    \item Single ``walker''
                    \item Explores posterior
                    \item Fast, if proposal matrix is tuned
                    \item Parameter estimation, suspiciousness calculation
                    \item Channel capacity optimised for generating posterior samples
                \end{itemize}
            }
        \end{block}
            \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
            \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
            \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
            \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
            \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
            \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.48\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \only<16>{
                \begin{itemize}
                    \item Ensemble of ``live points''
                    \item Scans from prior to peak of likelihood
                    \item Slower, no tuning required
                    \item Parameter estimation, model comparison, tension quantification
                    \item Channel capacity optimised for computing partition function
                \end{itemize}
            }
        \end{block}
            \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
            \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
            \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
            \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
            \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
            \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
            \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
            \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
            \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} 
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: live points}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n$ random samples over the space.
            \item Delete outermost sample, and replace with a new random one at higher integrand value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n}\only<5->{\pm \frac{1}{n}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \int f(x) dV \approx \sum_i f(x_i) \Delta V_i, \quad V_i = V_0 e^{-\only<5->{(}i\only<5->{\pm\sqrt{i})}/n} \]
%            \item Nested sampling: completely different way to scan.
%            \item Ensemble sampling compresses entire space$\to$peak(s).
%            \item Sequentially update a set $S$ of $n$ samples:
%                \begin{itemize}
%                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from a measure $\pi$). 
%
%                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
%                \end{itemize}
%            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard constraint}:
%                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
%            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence}/integral of function \& \C[0]{posterior}/sample weights.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4-         >[width=\textwidth,page=4]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: dead points}
    \includegraphics[width=\textwidth]{figures/dead_measure}
    \begin{columns}
        \column{0.69\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Dead points have a unique scale-invariant distribution $\propto\: \tfrac{dV}{V}$.
            \item Uniform over original region, exponentially concentrating on region of interest (until termination volume).
            \item Good for training emulators (HERA~\arxiv{2108.07282}).
        \end{itemize}
        \column{0.3\textwidth}
        \begin{block}{Applications}
        \begin{itemize}
            \item training emulators.
            \item gridding simulations
            \item beta flows
            \item ``dead measure'' 
        \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \texttt{nautilus}~\arxiv{2306.16923} 
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.55\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267} \texttt{jaxnest}~\arxiv{2012.15286}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Types of nested sampler}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
\item Constructs bounding region and draws many invalid points until $\mathcal{L}(\theta)>\mathcal{L}_*$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{5pt}
    \begin{itemize}
        \item Nested samplers usually come with:
            \begin{itemize}
                \item \emph{resolution} parameter $n_\mathrm{live}$ (which improve results as $\sim\mathcal{O}(n_\mathrm{live}^{-1/2})$.
                    \item set of \emph{reliability} parameters~\arxiv{2101.04525}, which don't improve results if set arbitrarily high, but introduce systematic errors if set too low.
                    \item e.g. \texttt{Multinest} efficiency \texttt{eff} or \texttt{PolyChord} chain length $n_\mathrm{repeats}$.
            \end{itemize}
    \end{itemize}
\end{frame}

%\begin{frame}
%    \frametitle{Applications: The three pillars of Bayesian inference}
%    \begin{columns}[t]
%        \column{0.33\textwidth}
%        \begin{block}{Parameter estimation}
%            What do the data tell us about the parameters of a model?
%
%            \textit{e.g. the size or age of a $\Lambda$CDM universe}
%            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
%            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
%            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
%        \end{block}
%        \column{0.3\textwidth}
%        \begin{block}{Model comparison}
%            How much does the data support a particular model?
%
%            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
%            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}} \vspace{-7pt}\]
%            \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[5]{\Pi_\mathcal{M}}}{\C[7]{\sum_m Z_m \Pi_m}} \]
%            \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}\]
%        \end{block}
%        \column{0.33\textwidth}
%        \begin{block}{Tension quantification}
%            Do different datasets make consistent predictions from the same model? 
%            \textit{e.g. CMB vs Type IA supernovae data}
%            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
%            \[
%                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
%                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
%                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
%                \end{aligned}
%            \]
%        \end{block}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Cosmology}
    \student{adam_ormondroyd}{Adam Ormondroyd}{PhD}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Battle-tested in Bayesian cosmology on
                \begin{itemize}
                    \item Parameter estimation: multimodal alternative to MCMC samplers.
                    \item Model comparison: using integration to compute the Bayesian evidence
                    \item Tension quantification: using deep tail sampling and suspiciousness computations.
                \end{itemize}
            \item Plays a critical role in major cosmology pipelines: Planck, DES, KiDS, BAO, SNe.
            \item The default $\Lambda$CDM cosmology is well-tuned to have Gaussian-like posteriors for CMB data. 
            \item Less true for alternative cosmologies/models and orthogonal datasets, so nested sampling crucial.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/pps_both}
        \includegraphics[width=0.49\textwidth]{figures/reach_fit-cropped.pdf}
        %\includegraphics[width=0.49\textwidth]{figures/curvature_3}
        \includegraphics[width=\textwidth]{figures/omegak_H0_2.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Astrophysics}
\student{metha_prathaban}{Metha Prathaban}{PhD}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item In exoplanets~\arxiv{1806.00518}
                \begin{itemize}
                    \item Parameter estimation: determining properties of planets.
                    \item Model comparison: how many planets? Stellar modelling~\arxiv{2007.07278}.
                    \item exoplanet problems regularly have posterior phase transitions \arxiv{2102.03387}
                \end{itemize}
            \item In gravitational waves
                \begin{itemize}
                    \item Parameter estimation: Binary merger properties
                    \item Model comparison: Modified theories of gravity, selecting phenomenological parameterisations~\arxiv{1803.10210}
                    \item Likelihood reweighting: fast slow properties
                \end{itemize}
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/rv_full.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Particle physics}
    \student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{columns}
            \column{0.67\textwidth}
            \begin{itemize}
                \item Nested sampling for cross section computation/event generation
            \end{itemize}
            \column{0.3\textwidth}
            \[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2.\]
        \end{columns}
        \begin{itemize}
            \item Nested sampling can explore the phase space $\Omega$ and compute integral blind with comparable efficiency to HAAG/RAMBO~\arxiv{2205.02030}.
            \item Bayesian sparse reconstruction~\arxiv{1809.04598} applied to bump hunting allows evidence-based detection of signals in phenomenological backgrounds~\arxiv{2211.10391}.
            \item Fine tuning quantification
            \item Fast estimation of small $p$-values \arxiv{2106.02056}(PRL), just make switch:\\ $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.

        \end{itemize}
        \column{0.17\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1-pdfjam-crop.pdf}
        \column{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/psi_predict-crop.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Lattice field theory}
\student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Consider standard field theory Lagrangian:
                \[ Z(\beta) = \int D\phi e^{-\beta S(\phi)}, \quad S(\phi) = \int dx^\mu \mathcal{L}(\phi) \]
            \item Discretize onto spacetime grid.
            \item Compute partition function
            \item NS unique traits:
                \begin{itemize}
                    \item Get full partition function for free
                    \item allows for critical tuning
                    \item avoids critical slowing down
                \end{itemize}
            \item Applications in lattice gravity, QCD, condensed matter physics
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/potential_shape}
        \includegraphics[width=0.49\textwidth]{figures/2d_phase}
        \includegraphics[width=\textwidth]{figures/lattice_field_theory.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Machine learning}
\student{kamran_javid}{Kamran Javid}{PDRA}
    \begin{columns}
        \column{0.62\textwidth}
        \begin{itemize}
            \item Machine learning requires:
                \begin{itemize}
                    \item Training to find weights
                    \item Choice of architecture/topology/hyperparameters
                \end{itemize}
            \item Bayesian NNs treat training as a model fitting problem
            \item Compute posterior of weights (parameter estimation), rather than optimisation (gradient descent)
            \item Use evidence to determine best architecture (model comparison), correlates with out-of-sample performance! 
            \item Solving the full ``shallow learning'' problem without compromise \arxiv{2004.12211}\arxiv{2211.10391}. 
                \begin{itemize}
                    \item Promising work ongoing to extend this to transfer learning and deep nets.
                \end{itemize}
            \item More generally, dead points are optimally spaced  for training traditional ML approaches e.g. \arxiv{2309.05697}
        \end{itemize}
        \column{0.38\textwidth}
        \includegraphics[width=\textwidth]{figures/nn_posterior-cropped.pdf}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{Applications of nested sampling}
%    \framesubtitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
%    \begin{columns}
%        \column{0.57\textwidth}
%        \begin{itemize}
%            \item Nested sampling for frequentist computation!?
%            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
%            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
%            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
%            \item Need insight/approximation to make efficient.
%            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
%            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
%        \end{itemize}
%        \column{0.43\textwidth}
%        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{And beyond\ldots}
    \only<1-4>{
        \student{watkinson-headshot}{Catherine Watkinson}{Senior Data Scientist}
    }
    \only<5->{
        \student{mcaloone-headshot}{Thomas Mcaloone}{PhD $\to$ Data Scientist}
    }
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
            \includegraphics[width=0.08\textwidth]{figures/headshots/catherine-watkinson.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/thomas-macaloone.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/parul-janagal.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/angus-peters.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/tamas-stenzel.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/david-yallup.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/rebecca-handley.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/adam-ormondroyd.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/mike-hobson.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/anthony-lasenby.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/mike-handley.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/will-handley.pdf}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        \includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/tdoa-cropped-1-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/tdoa-cropped-2-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/tdoa-cropped-3-crop.pdf}%
        \includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology {\small \arxiv{2210.07409}(NatAstro)}}
    \student{ian_roque}{Ian Roque}{PhD}
    \vspace{10pt}
    \begin{columns}
        \column{0.65\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
            \item Lead data analysis team (REACH first light in January)
            \item Nested sampling woven in from the ground up (calibrator, beam modelling, signal fitting, likelihood selection).
            \item All treated as parameterised model comparison problems.
        \includegraphics[height=0.3\textwidth]{figures/EDGES_antenna}
        \includegraphics[height=0.3\textwidth]{figures/SARAS}
        \includegraphics[height=0.3\textwidth]{figures/REACH_2.jpg}
        \end{itemize}
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT: combining particle physics \& cosmological data}
    \begin{columns}
        \column{0.52\textwidth}
        \begin{itemize}
            \item Multinational team of particle physicists, cosmologists and statisticians.
            \item Combine cosmological data, particle colliders, direct detection, \& neutrino detectors in a statistically principled manner~\arxiv{2205.13549}.
            \item Lead Cosmo/Dark Matter working group~\arxiv{2009.03286}.
            \item Nested sampling used for global fitting, and fine-tuning quantification~\arxiv{2101.00428}
        \end{itemize}
        \begin{center}
            \includegraphics[width=0.5\textwidth]{figures/gambit_logo.png}
        \end{center}
        \column{0.48\textwidth}
        \vspace{-40pt}
        \includegraphics[width=\textwidth]{figures/ALP_2.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{SBI: Simulation-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item What do you do if you don't know \C[2]{$\mathcal{L}(D|\theta)$}?
            \item If you have a simulator/forward model $\theta \rightarrow D$
                defines an \C[2]{\emph{implicit} likelihood~$\mathcal{L}$}.
            \item Simulator generates samples from $\C[2]{\mathcal{L}(\cdot|\theta)}$.
            \item With a prior $\C[1]{\pi}(\theta)$ can generate samples from \C[4]{joint distribution}~$\C[4]{\mathcal{J}(\theta,D)}=\C[2]{\mathcal{L}(D|\theta)}\C[1]{\pi(\theta)}$\\\hfill \emph{the ``probability of everything''}.
            \item Task of SBI is take joint~$\C[4]{\mathcal{J}}$ samples and learn \C[0]{posterior $\mathcal{P}(\theta|D)$} and \C[3]{evidence $\mathcal{Z}(D)$} \\\hfill and possibly \C[2]{likelihood $\mathcal{L}(D|\theta)$}.
            \item Present state of the art achieves this using \emph{machine learning} (neural networks).
                \begin{itemize}
                    \item My group's research tries to removes machine learning \tthref{github.com/handley-lab/lsbi}.

                \end{itemize}
            %\item Present SotA: NPE, NLE, NJE, NRE
            %\item SBI \& forward modelling force us to think about data space~$D$ \& parameter space~$\theta$.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[page=1, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<2>[page=2, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<3>[page=3, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<4>[page=4, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<5>[page=5, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<6>[page=6, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<7>[page=7, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<8>[page=8, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<9>[page=9, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<10>[page=10, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<11>[page=11, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<12>[page=12, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<13>[page=13, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<14>[page=14, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<15>[page=15, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<16>[page=16, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<17>[page=17, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<18>[page=18, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<19>[page=19, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<20>[page=20, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<21>[page=21, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Neural Ratio Estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item SBI flavours: {\small \hfill\tthref{github.com/sbi-dev/sbi}}
                {\small
                    \begin{description}
                        \item[NPE] Neural posterior estimation
                        \item[NLE] Neural likelihood estimation
                        \item[NJE] Neural joint estimation
                        \item[NRE] Neural ratio estimation
                    \end{description}
                }
            \item NRE recap:
                \begin{enumerate}
                    \item Generate joint samples $(\theta,D)\sim\C[4]{\mathcal{J}}$
                        \label{step:joint}
                        \begin{itemize}
                            \item \textit{straightforward if you have a simulator:\\ $\theta\sim\C[1]{\pi(\cdot)}$, $D\sim\C[2]{\mathcal{L}(\cdot|\theta)}$}
                        \end{itemize}
                    \item Generate separated samples $\theta\sim\C[1]{\pi}$, $D\sim\C[3]{\mathcal{Z}}$\label{step:sep}
                        \begin{itemize}
                            \item \textit{aside: can shortcut step~\ref{step:sep} by scrambling the $(\theta,D)$ pairings from step~\ref{step:joint}}
                        \end{itemize}
                    \item Train probabilistic classifier $p$ to distinguish whether $(\theta,D)$ came from $\C[4]{\mathcal{J}}$ or $\C[1]{\pi}\times\C[3]{\mathcal{Z}}$.
                    \item $\frac{p}{1-p} = \C[5]{r} = \frac{P(\theta,D)}{P(\theta)P(D)} 
                        =
                        \frac{\C[4]{\mathcal{J}}}{\C[1]{\pi}\times\C[3]{\mathcal{Z}}} = \frac{\C[2]{\mathcal{L}}}{\C[3]{\mathcal{Z}}} = \frac{\C[0]{\mathcal{P}}}{\C[1]{\pi}}$.
                    \item Use ratio $\C[5]{r}$ for parameter estimation $\C[0]{\mathcal{P}} = \C[5]{r}\C[1]\times {\pi}$
                \end{enumerate}
        \end{itemize}
        \column{0.5\textwidth}
        \only<1|handout:0>{
            \begin{tikzpicture}[node distance=1cm, every neuron/.style={circle, draw, minimum size=1cm},]
                \node[every neuron/.try] (j2)  {};
                \node[every neuron/.try, above left = 0cm and 0.5cm of j2] (theta) { $\theta$};
                \node[every neuron/.try, below left = 0cm and 0.5cm of j2] (D) { $D$};
                \node[every neuron/.try, above = 0.5cm of j2] (j1) {};
                \node[every neuron/.try, below = 0.5cm of j2] (j3) {};
                \node[every neuron/.try, above right = 0cm and 0.5cm of j2] (h1) {};
                \node[every neuron/.try, below right = 0cm and 0.5cm of j2] (h2) {};
                \node[every neuron/.try, right = 1.3cm of j2] (p) { $p$};
                \node[every neuron/.try, right = 0.5cm of p] (logr) { $\C[5]{r}$};
                \draw[-] (theta) -- (j1);
                \draw[-] (D) -- (j1);
                \draw[-] (theta) -- (j2);
                \draw[-] (D) -- (j2);
                \draw[-] (theta) -- (j3);
                \draw[-] (D) -- (j3);
                \draw[-] (j1) -- (h1);
                \draw[-] (j1) -- (h2);
                \draw[-] (j2) -- (h1);
                \draw[-] (j2) -- (h2);
                \draw[-] (j3) -- (h1);
                \draw[-] (j3) -- (h2);
                \draw[-] (h1) -- (p);
                \draw[-] (h2) -- (p);
                \draw[-] (p) -- (logr);
                \node[below =0.5cm of logr] {\includegraphics[page=22, width=0.5\textwidth]{figures/sbi_parameter_estimation.pdf}};
            \end{tikzpicture}
        }
        \only<2>{
            \begin{exampleblock}{Bayesian proof}
                \begin{itemize}
                    \item Let $M_{\C[4]{\mathcal{J}}}$: $(\theta,D)\sim\C[4]{\mathcal{J}}$, $M_{\C[1]{\pi}\C[3]{\mathcal{Z}}}$: $(\theta,D)\sim\C[1]{\pi}\times\C[3]{\mathcal{Z}}$
                    \item Classifier gives
                        ${p(\theta,D) = P(M_{\C[4]{\mathcal{J}}}|\theta,D) = 1- P(M_{\C[1]{\pi}\C[3]{\mathcal{Z}}}|\theta,D)}$
                    \item Bayes theorem then shows
                        ${\frac{p}{1-p}=\frac{P(M_{\C[4]{\mathcal{J}}}|\theta,D)}{P(M_{\C[1]{\pi}\C[3]{\mathcal{Z}}}|\theta,D)} = \frac{P(\theta,D|M_{\C[4]{\mathcal{J}}})P(M_{\C[4]{\mathcal{J}}})}{P(\theta,D|M_{\C[1]{\pi}\C[3]{\mathcal{Z}}})P(M_{\C[1]{\pi}\C[3]{\mathcal{Z}}})} = 
                        \frac{\C[4]{\mathcal{J}}}{\C[1]{\pi}\C[3]{\mathcal{Z}}}}$, \\
                        where we have assumed 
                        \begin{itemize}
                            \item $P(M_{\C[4]{\mathcal{J}}}) = P(M_{\C[1]{\pi}\C[3]{\mathcal{Z}}})$,
                        \end{itemize}
                        and by definition
                        \begin{itemize}
                            \item $\C[4]{\mathcal{J}(\theta,D)} = P(\theta,D|M_{\C[4]{\mathcal{J}}})$
                            \item $\C[1]{\pi(\theta)}\C[3]{\mathcal{Z}(D)} = P(\theta,D|M_{\C[1]{\pi}\C[3]{\mathcal{Z}}})$.
                        \end{itemize}
                \end{itemize}
            \end{exampleblock}
        }
        \only<3|handout:0>{
            \begin{block}{Why I like NRE}
                \begin{itemize}
                    \item The link between classification and inference is profound.
                    \item Density estimation is hard -- Dimensionless $r$ divides out the hard-to-calculate parts.
                \end{itemize}
            \end{block}
            \begin{block}{Why I don't like NRE}
                \begin{itemize}
                    \item Practical implementations require marginalisation~\arxiv{2107.01214}, or autoregression~\arxiv{2308.08597}.
                    \item Model comparison and parameter estimation are separate~\arxiv{2305.11241}.
                \end{itemize}
            \end{block}
        }
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{TMNRE: Truncated Marginal Neural Ratio Estimation}
    \framesubtitle{\texttt{swyft}: \tthref{github.com/undark-lab/swyft}}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Two tricks for practical NRE:
        \end{itemize}
        \begin{block}{Marginalisation}
            \begin{itemize}
                \item Only consider one or two parameters at a time.
                \item Fine if your goal is to produce triangle plots.
                \item Problematic if information is contained jointly in more than two parameters.
            \end{itemize}
        \end{block}
        \begin{block}{Truncation}
            \begin{itemize}
                \item focus parameters $\theta$ on a subset of the prior which reproduces observed data $D_\text{obs}$
                \item region is somewhat arbitrary (usually a box)
                \item not amortised, sounds a bit like ABC
            \end{itemize}
        \end{block}
        \column{0.45\textwidth}
        \begin{overpic}[width=\textwidth]{figures/tmnre}
            \put(70,0) {\arxiv{2111.08030}}
        \end{overpic}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Similarities}
    \begin{columns}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth,page=15]{figures/himmelblau}% 
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/tmnre.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Sequential NRE with nested sampling}
    \begin{tikzpicture}[
            node distance=1cm,
            >=stealth, auto,
            every state/.style={
                rectangle, rounded corners, minimum width=2em,
                text width=6.8cm, align=center
            }
        ]
        \node[state, fill=C3!20, minimum height=3cm] (q14) {
            \textbf{NS}\\
            Run nested sampling on $\log r_{i}(\theta, D_\text{obs})$ \\
            to generate $\theta$ dead samples between prior and posterior targeted at $D_\text{obs}$.
        };
        \node[state, fill=C0!20, minimum height=1cm] (q121) [above=of q14] {
            \textbf{Terminate} if $\mathcal{D}_\text{KL}$ has converged
        };
        \node[state, fill=C0!20, minimum height=1cm] (q12) [above=of q121] {
            \textbf{Initialise} $\theta\sim\pi$\\
        };
        \node[state, fill=C2!20, minimum height=3cm] (q34) [right=of q14] {
            \textbf{NRE}\\
            Train Neural ratio estimator $\log r_i$
            with weights initialised from previous run
        };
        \node[state, fill=C1!20, minimum height=3cm] (q23) [above=of q34] {
            \textbf{Simulate}\\
            Generate simulations $\theta\to D$ from all discarded points
        };
        \begin{scope}[bend left]%
            \path[thick,-{Latex[width=2mm]}]   (q14.north) edge node {} (q121.south)
            (q121.east) edge node {}($(q23.south west)!0.16!(q23.north west)$) 
            (q23.south) edge node {} (q34.north)
            (q34.west) edge node {} (q14.east)
            (q12.east) edge node {}($(q23.south west)!0.84!(q23.north west)$) ;
        \end{scope}
        \node[align=center] (e) at (barycentric cs:q121=0.5,q12=0.5,q23=1,q34=1,q14=1) {\Large\textbf{NSNRE}};
    \end{tikzpicture}
\end{frame}

\begin{frame}
    \frametitle{\texttt{PolySwyft}}
    \begin{columns}[t]
        \column{0.45\textwidth}
        \begin{block}{\texttt{PolyChord}}
            \tthref{github.com/PolyChord/PolyChordLite}
            \includegraphics[width=\textwidth]{figures/polychord.png}
            \begin{itemize}
                \item Widely used high-performance nested sampling tool (implementing slice sampling \& clustering in MPI Fortran)
            \end{itemize}
        \end{block}
        \column{0.45\textwidth}
        \begin{block}{\texttt{Swyft}}
            \tthref{github.com/undark-lab/swyft}
            \includegraphics[width=\textwidth]{figures/swyft_logo_wide.png}
            \begin{itemize}
                \item Widely used TMNRE tool in cosmology/astrophysics.
            \end{itemize}
        \end{block}
        However, NSNRE is general, and not specific to these choices.
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Convergence diagnostics}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Example for a $n=5$ dimensional parameter space, with $d=100$ data points, (\texttt{lsbi} gaussian mixture model).
            \item This is the regime for cosmological scale problems.
            \item To determine convergence we track:
                \begin{itemize}
                    \item The change in KL divergence between rounds (\C[0]{blue}), and check when this goes to zero.
                    \item The total KL divergence between prior and posterior estimate (\C[1]{orange}), and check when this levels off (ground truth in \C[3]{red}).
                    \item Also shown is the KL divergence between the estimate and the ground truth (\C[2]{green}).
                \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/GMM_posterior_estimates.pdf}%
        \includegraphics<2|handout:0>[width=\textwidth]{figures/GMM_KL_div_per_round.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Posterior repartitioning}
    \student{aleksandr_petrosyan}{Aleksandr Petrosyan}{MSci 2021}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Almost all sampling algorithms (MH, HMC, SMC) are only sensitive to the product of likelihood and prior $\boxed{\C[2]{\mathcal{L}}\times\C[1]{\pi}}$.
            \item In practice this manifests as many codes implementing ``prior terms'' in the likelihood (or visa-versa) with no ill effect
        \item Nested sampling is unusual, in that one ``samples from the prior $\C[1]{\pi}$, subject to a hard constraint $\C[2]{\mathcal{L}}>\C[2]{\mathcal{L}_*}$.'':
                \[\{\theta\sim \C[1]{\pi} : \C[2]{\mathcal{L}}(\theta)>\C[2]{\mathcal{L}_*} \}\] 
            \item This separates the prior from the likelihood at an algorithmic level.
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
\item One can therefore repartition the likelihood and prior $(\C[2]{\mathcal{L}},\C[1]{\pi})\to(\C[2]{\tilde{\mathcal{L}}},\C[1]{\tilde{\pi}})$, providing
                \[ \boxed{\C[2]{\mathcal{L}}\times\C[1]{\pi} = \C[2]{\tilde{\mathcal{L}}}\times\C[1]{\tilde{\pi}}}\]
            \item This moves pieces between likelihood and prior that you have analytic control over.
                \includegraphics[width=\textwidth]{figures/repartition}
            \item Chen, Feroz \& Hobson~\arxiv{1908.04655} invented this to deal with misspecified priors
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Posterior repartitioning for acceleration}
    \student{metha_prathaban}{Metha Prathaban}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{exampleblock}{Key idea}
Whilst posteriors and evidences are invariant to repartitioning $\C[2]{\mathcal{L}}\to\C[2]{\tilde{\mathcal{L}}}$, the KL divergence is not.
        \end{exampleblock}
    \begin{itemize}
            \item For constant ``run quality'' $\sigma$, 
            \begin{gather*} 
                {\scriptstyle
                    T = T_{\C[2]{\mathcal{L}}} \times n_\text{live} \times \mathcal{D}_\text{KL} \times f_\text{sampler}, \quad
\sigma \approx \sqrt{\mathcal{D}_\text{KL}/n_\text{live}} }
    \\
\Rightarrow\boxed{T = T_{\C[2]{\mathcal{L}}} \times \sigma \times \mathcal{D}_\text{KL}^2 \times f_\text{sampler}} 
        \end{gather*}
            so if you can reduce the KL divergence, then quadratic gains to be made
        \item \texttt{SuperNest}~\arxiv{2212.01760}
        \item Ongoing work with Metha Prathaban \& Harry Bevins.
    \end{itemize}
        \column{0.5\textwidth}
        \vspace{8pt}
        \includegraphics[width=\textwidth]{figures/supernest.pdf}
        \begin{itemize}
            \item Easy to get 40x performance normalised speed-up on cosmological scale cases
            \item Paper $\sim\mathcal{O}(\text{next month})$.
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\tthref{github.com/handley-lab}}
    \tikz[overlay,remember picture]
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
        \includegraphics[width=0.09\textheight]{figures/students/adam_ormondroyd.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/charlotte_priestley.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/david_yallup.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/dily_ong.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/george_carter.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/harry_bevins.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/ian_roque.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/kilian_scheutwinkel.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/metha_prathaban.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/namu_kroupa.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sinah_legner.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sam_leeney.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/thomas_gessey-jones.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/wei-ning_deng.jpg}%
    };
    \vspace{-0.1\textheight}
    \begin{columns}
        \column{0.65\textwidth}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Performing Bayesian inference and model comparison.
            \end{itemize}
        \item It is applied widely across cosmology, particle physics \& machine learning.
        \item It's unique traits as the only numerical Lebesgue integrator mean with compute it will continue to grow in importance.
    \end{itemize}
        \column{0.35\textwidth}
    \includegraphics[height=0.6\textwidth]{figures/pps_both}%
    \includegraphics[height=0.6\textwidth]{figures/REACH_2}%
    \end{columns}
    \includegraphics[height=0.2\textwidth]{figures/integration}%
    \includegraphics[height=0.2\textwidth]{figures/curvature_3}%
    \includegraphics[height=0.2\textwidth]{figures/ALP_3}%
\end{frame}



\appendix

\end{document}
```
# branch: remotes/origin/cairns_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}

% Commands
% --------
% - \arxiv{arxiv number}
% - \arxiv{<number>}            arxiv.org/abs/<number>
% - \oldarxiv{<arxiv number>}   arxiv.org/<number>
% - \doi{<doi>}                 doi.org/<doi>
% - \xkcd{<number>}             xkcd.com/<number>
% - \email{<email>}             <<email>>
% - \tthref{<website>}          <website>
% - \av[list]{<quantity>}       <quantity>_{list}
% - \student{<name>}{<detail>}{<photo>}

% Talk details
% ------------
\title{Sampling methods for high energy physics \& particle astrophysics}
%\subtitle{<+subtitle+>}
\date{19\textsuperscript{th} August 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

%\begin{frame}
%    \frame title{The nature of sampling}
%    <++>
%\end{frame}
%
%\begin{frame}
%    \frame title{Monte Carlo integration}
%    <++>
%\end{frame}

\begin{frame}
    \frametitle{Numerical inference tasks}
    \begin{itemize}
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, one might want to:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimise}
            \[\theta_\text{Max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Explore}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integrate}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{(incomplete) list of techniques}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Optimisers
                \begin{itemize}
                    \item Gradient descent (ADAM, BFGS)
                    \item simplex method (Nelder-Mead)
                    \item Genetic algorithms (Diver)
                \end{itemize}
            \item Samplers
                \begin{itemize}
                    \item \textbf{Metropolis-Hastings} (PyMC, MontePython)
                    \item Hamiltonian Monte Carlo (Stan, blackjax)
                    \item Ensemble sampling (emcee, zeus). 
                    \item Variational Inference (Pyro, NIFTY)
                \end{itemize}
            \item Integrators
                \begin{itemize}
                    \item \textbf{Nested sampling} (MultiNest, dynesty)
                    \item Thermodynamic integration
                    \item Sequential Monte Carlo (pocomc)
                \end{itemize}
            \end{itemize}
        \column{0.5\textwidth}
        \begin{columns}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/emcee}
        \includegraphics[width=\textwidth]{figures/metropolis-hastings}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/zeus}
        \end{columns}
        \includegraphics[width=\textwidth]{figures/hmc_explained}
    \end{columns}
\end{frame}



\begin{frame}
    \begin{columns}
        \column{0.48\textwidth}
        \begin{block}{\textbf{MCMC}}
            \only<16>{
                \begin{itemize}
                    \item Single ``walker''
                    \item Explores posterior
                    \item Fast, if proposal matrix is tuned
                    \item Parameter estimation, suspiciousness calculation
                    \item Channel capacity optimised for generating posterior samples
                \end{itemize}
            }
        \end{block}
            \includegraphics<1|handout:0>[width=\textwidth,page=16]{figures/himmelblau.pdf}%
            \includegraphics<2|handout:0>[width=\textwidth,page=17]{figures/himmelblau.pdf}%
            \includegraphics<3|handout:0>[width=\textwidth,page=18]{figures/himmelblau.pdf}%
            \includegraphics<4|handout:0>[width=\textwidth,page=19]{figures/himmelblau.pdf}%
            \includegraphics<5|handout:0>[width=\textwidth,page=20]{figures/himmelblau.pdf}%
            \includegraphics<6-15|handout:0>[width=\textwidth,page=21]{figures/himmelblau.pdf}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau.pdf}}
        \column{0.48\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \only<16>{
                \begin{itemize}
                    \item Ensemble of ``live points''
                    \item Scans from prior to peak of likelihood
                    \item Slower, no tuning required
                    \item Parameter estimation, model comparison, tension quantification
                    \item Channel capacity optimised for computing partition function
                \end{itemize}
            }
        \end{block}
            \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau.pdf}%
            \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau.pdf}%
            \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau.pdf}%
            \includegraphics<10|handout:0>[width=\textwidth,page=4]{figures/himmelblau.pdf}%
            \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau.pdf}%
            \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau.pdf}%
            \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau.pdf}%
            \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau.pdf}%
            \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau.pdf}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau.pdf}} 
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling: numerical Lebesgue integration}
    \begin{columns}
        \column{0.5\textwidth}
        \fbox{\parbox{\textwidth}{
                \begin{itemize}
                    \item[0.] Start with $N$ random samples over the space.
                    \item[i.] Delete outermost sample, and replace with a new random one at higher integrand value.
        \end{itemize}}}
        \vspace{-5pt}
        \begin{itemize}
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item Discarded ``dead points'' can be weighted to form posterior, prior, or anything in between.
            \item Estimates the \textbf{density of states} and \textbf{partition function} $\C[3]{\log \mathcal{Z}(\beta)}$.
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune,
                    \item exploration of multimodal functions,
                    \item global and local optimisation.
                \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4|handout:0>[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<8|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        \includegraphics<9->[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling zoo \hspace{170pt} \arxiv{2205.15570}}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \texttt{nautilus}~\arxiv{2306.16923} 
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.55\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267} \texttt{jaxnest}~\arxiv{2012.15286}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Cross sections \& Bayesian detection}
    \framesubtitle{Applications of nested sampling}
    \student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{columns}
            \column{0.67\textwidth}
            \begin{itemize}
                \item Nested sampling for cross section computation/event generation
            \end{itemize}
            \column{0.3\textwidth}
            \[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2.\]
        \end{columns}
        \begin{itemize}
            \item Nested sampling can explore the phase space $\Omega$ and compute integral blind with comparable efficiency to HAAG/RAMBO~\arxiv{2205.02030}.
            \item Bayesian sparse reconstruction~\arxiv{1809.04598} applied to bump hunting allows evidence-based detection of signals in phenomenological backgrounds~\arxiv{2211.10391}.
        \end{itemize}
        \column{0.17\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1-pdfjam-crop.pdf}
        \column{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/psi_predict-crop.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Lattice field theory}
    \framesubtitle{Applications of nested sampling}
\student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Consider standard field theory Lagrangian:
                \[ Z(\beta) = \int D\phi e^{-\beta S(\phi)}, \quad S(\phi) = \int dx^\mu \mathcal{L}(\phi) \]
            \item Discretize onto spacetime grid.
            \item Compute partition function
            \item NS unique traits:
                \begin{itemize}
                    \item Get full partition function for free
                    \item allows for critical tuning
                    \item avoids critical slowing down
                \end{itemize}
            \item Applications in lattice gravity, QCD, condensed matter physics
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/potential_shape}
        \includegraphics[width=0.49\textwidth]{figures/2d_phase}
        \includegraphics[width=\textwidth]{figures/lattice_field_theory.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
    \framesubtitle{Applications of nested sampling}
    \student{andrew_fowlie}{Andrew Fowlie}{}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
            \item Need insight/approximation to make efficient.
            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Quantification of fine tuning~\arxiv{2101.00428}~\arxiv{2205.13549}}
    \framesubtitle{Applications of nested sampling}
    \student{gambit}{GAMBIT}{dm-cosmo WG}
    \vspace{8pt}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Example: Cosmological constraints on decaying axion-like particles~\arxiv{2205.13549}.
            %\item Subset of parameters $\xi,m_a,\tau,g_{a\gamma}$: ALP fraction, mass, lifetime and photon coupling.
                {(\small Also vary cosmology, $\tau_n$ and nuisance params)}
            \item Data: CMB, BBN, FIRAS, SMM, BAO.
            \item Standard profile likelihood fit shows ruled out regions and best-fit point.
            \item Nested sampling scan:
                \begin{itemize}
                    \item Quantifies amount of parameter space ruled out with Kullback-Liebler divergence $\mathcal{D}_\mathrm{KL}$.
                    \item Identifies best fit region as statistically irrelevant from information theory/Bayesian.
                    \item No evidence for decaying ALPs. Fit the data equally well: but more constrained parameters create Occam penalty.
                \end{itemize}
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/ALP_1.pdf}%
        \includegraphics<2          >[width=\textwidth]{figures/ALP_2.pdf}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/ALP_3.pdf}%
    \end{columns}
    
\end{frame}


%\begin{frame}
%    \frametitle{GAMBIT}
%    <++>
%\end{frame}

\begin{frame}
    \frametitle{Model comparison $\C[3]{\mathcal{Z}=P(D|M)}$}
    \begin{itemize}
        \item Bayesian model comparison allows mathematical derivation of key philosophical principles.
    \end{itemize}
    \begin{columns}[t]
        \column{0.47\textwidth}
        Viewed from data-space $D$:
        \begin{block}{Popper's falsificationism}
            \begin{itemize}
                \item Prefer models that make bold predictions.
                \item if proven true, model more likely correct.
            \end{itemize}
        \end{block}
        \includegraphics<1|handout:0>[width=\textwidth, page=1]{figures/popper}%
        \includegraphics<2|handout:0>[width=\textwidth, page=2]{figures/popper}%
        \includegraphics<3>[width=\textwidth, page=3]{figures/popper}%
        \begin{itemize}
            \item Falsificationism comes from normalisation
        \end{itemize}
        \column{0.47\textwidth}
        Viewed from parameter-space $\theta$:
        \begin{block}{Occam's razor}
            \begin{itemize}
                \item Models should be as simple as possible
                \item \ldots but no simpler
            \end{itemize}
        \end{block}
        \begin{itemize}
            \item Occam's razor equation:
                \[\C[3]{\log\mathcal{Z}} = \av[{\C[0]{\mathcal{P}}}]{\C[2]{\log\mathcal{L}}} - \mathcal{D}_\text{KL}\]
            \item ``Occam penalty'': KL divergence between \C[1]{prior~$\pi$} and \C[0]{posterior~$\mathcal{P}$}.
                \[ \mathcal{D}_\text{KL}\sim \log\frac{\text{\C[1]{Prior volume} }}{\text{\C[0]{Posterior volume}}}\]
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\tthref{github.com/handley-lab}}
    \tikz[overlay,remember picture]
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
        \includegraphics[width=0.09\textheight]{figures/students/adam_ormondroyd.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/charlotte_priestley.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/david_yallup.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/dily_ong.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/george_carter.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/harry_bevins.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/kilian_scheutwinkel.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/metha_prathaban.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/namu_kroupa.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sinah_legner.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sam_leeney.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/wei-ning_deng.jpg}%
    };
    \vspace{-0.1\textheight}
    \begin{columns}
        \column{0.65\textwidth}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Quantifying fine-tuning with Bayesian theory
            \end{itemize}
        \item It is applied widely across cosmology \& particle physics.
        \item It's unique traits as the only numerical Lebesgue integrator mean with compute it will continue to grow in importance.
    \end{itemize}
        \column{0.35\textwidth}
    \includegraphics[width=\textwidth]{figures/multinest}

    %\includegraphics[height=0.6\textwidth]{figures/pps_both}%
    %\includegraphics[height=0.6\textwidth]{figures/REACH_2}%
    \end{columns}
    %\includegraphics[height=0.2\textwidth]{figures/integration}%
    %\includegraphics[height=0.2\textwidth]{figures/curvature_3}%
    %\includegraphics[height=0.2\textwidth]{figures/ALP_3}%
    \includegraphics[height=0.2\textwidth]{figures/potential_shape}%
    \includegraphics[height=0.2\textwidth]{figures/2d_phase}%
    \includegraphics[height=0.2\textwidth]{figures/popper}
\end{frame}

\end{document}
```
# branch: remotes/origin/cavendish_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Nested sampling: powering next-generation Bayesian inference tools}
\subtitle{for cosmology, particle physics and beyond}
\date{14\textsuperscript{th} December 2022}

% Weds 14^th December 2022 : Day 2 - candidates present a research seminar to
% the department – this will be in hybrid format and recorded in order that
% members of the selection panel and other members of the department who are
% unable to be present in person to view the lectures.  Your seminar
% presentation should be of no more than 30 minutes' duration, with a further
% 5-10 minutes allotted for questions and  discussion.  The audience will
% include colleagues from several Departments, therefore the first half of
% your presentation should be a broad introduction to your area of research
% and view to the future, and must be accessible to faculty across all
% research  disciplines. The second half of your presentation may be as
% technical  as you wish.

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{General setting}
    \begin{itemize}
        \item Integration is a fundamental concept in physics, statistics and data science:
    \end{itemize}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Partition functions}
            \vspace{-11pt}
            \[ Z(\beta) = \int e^{-\beta H(q,p)} dq dp \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Path integrals}
            \[ \Psi = \int e^{i S} \mathcal{D}x \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Bayesian marginals}
            \vspace{-11pt}
            \[ \mathcal{Z}(D) = \int \mathcal{L}(D|\theta) \pi(\theta) d\theta \]
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Need numerical tools if analytic solution unavailable.
            \item High-dimensional numerical integration is hard.
            \item Riemannian strategy estimates volumes geometrically:
                \[ \int f(x) d^nx \approx \sum_i f(x_i) \Delta V_i \sim \mathcal{O}(e^n) \]
            \item Curse of dimensionality $\Rightarrow$ exponential scaling.
            \item Nested sampling integrates \textbf{probabilistically}.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n$ random samples over the space.
            \item Delete outermost sample, and replace with a new random one at higher integrand value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n}\only<9->{\pm \frac{1}{n}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \sum_i f(x_i) \Delta V_i, \qquad V_i = V_0 e^{-\only<9->{(}i\only<9->{\pm\sqrt{i})}/n} \]
%            \item Nested sampling: completely different way to scan.
%            \item Ensemble sampling compresses entire space$\to$peak(s).
%            \item Sequentially update a set $S$ of $n$ samples:
%                \begin{itemize}
%                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from a measure $\pi$). 
%
%                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
%                \end{itemize}
%            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard constraint}:
%                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
%            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence}/integral of function \& \C[0]{posterior}/sample weights.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4          >[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<8-|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Nested sampling estimates the \textbf{density of states} and calculates partition functions
                \[Z(\beta) = \sum_i f(x_i)^\beta \Delta V_i\]
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune
                    \item exploration of multimodal functions
                    \item global and local optimisation
                \end{itemize}
            \item For this kind of numerical, generic, high-dimensional integration, it is the only game in town.
            %\item Interpreted as a Bayesian algorithm, it
            %    \begin{itemize}
            %        \item Computes the Bayesian evidence (model comparison)
            %        \item Produces (weighted) posterior samples (parameter estimation)
            %    \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''\\
   {\hfill --- John Skilling}
  \end{quote}
  \begin{itemize}
      \item A large fraction of the work in nested sampling to date has been in attempting to implement a hard-edged sampler in the nested sampling meta-algorithm.
      \item \url{https://projecteuclid.org/euclid.ba/1340370944}.
      \item There has also been much work beyond this (focus of this talk).
  \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \column{0.3\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{dynesty}~\arxiv{1904.02180}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Cosmology}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Battle-tested in Bayesian cosmology on
                \begin{itemize}
                    \item Parameter estimation: multimodal alternative to MCMC samplers.
                    \item Model comparison: using integration to compute the Bayesian evidence
                    \item Tension quantification: using deep tail sampling and suspiciousness computations.
                \end{itemize}
            \item Plays a critical role in major cosmology pipelines: Planck, DES, KiDS, BAO, SNe.
            \item The default $\Lambda$CDM cosmology is well-tuned to have Gaussian-like posteriors for CMB data. 
            \item Less true for alternative cosmologies/models and orthogonal datasets, so nested sampling crucial.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/pps_both}
        \includegraphics[width=0.49\textwidth]{figures/reach_fit-cropped.pdf}
        %\includegraphics[width=0.49\textwidth]{figures/curvature_3}
        \includegraphics[width=\textwidth]{figures/omegak_H0_2.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Astrophysics}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item In exoplanets~\arxiv{1806.00518}
                \begin{itemize}
                    \item Parameter estimation: determining properties of planets.
                    \item Model comparison: how many planets? Stellar modelling~\arxiv{2007.07278}.
                    \item exoplanet problems regularly have posterior phase transitions \arxiv{2102.03387}
                \end{itemize}
            \item In gravitational waves
                \begin{itemize}
                    \item Parameter estimation: Binary merger properties
                    \item Model comparison: Modified theories of gravity, selecting phenomenological parameterisations~\arxiv{1803.10210}
                    \item Likelihood reweighting: fast slow properties
                \end{itemize}
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/rv_full.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Particle physics}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{columns}
            \column{0.67\textwidth}
            \begin{itemize}
                \item Nested sampling for cross section computation/event generation
            \end{itemize}
            \column{0.3\textwidth}
            \[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2.\]
        \end{columns}
        \begin{itemize}
            \item Nested sampling can explore the phase space $\Omega$ and compute integral blind with comparable efficiency to HAAG/RAMBO~\arxiv{2205.02030}.
            \item Bayesian sparse reconstruction~\arxiv{1809.04598} applied to bump hunting allows evidence-based detection of signals in phenomenological backgrounds~\arxiv{2211.10391}.
            \item Now applying to lattice field theory, and lattice gravity Lagrangians.
            \item Particle statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL).
        \end{itemize}
        \column{0.17\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1-pdfjam-crop.pdf}
        \column{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/psi_predict-crop.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Machine learning}
    \begin{columns}
        \column{0.62\textwidth}
        \begin{itemize}
            \item Machine learning requires:
                \begin{itemize}
                    \item Training to find weights
                    \item Choice of architecture/topology/hyperparameters
                \end{itemize}
            \item Bayesian NNs treat training as a model fitting problem
            \item Compute posterior of weights (parameter estimation), rather than optimisation (gradient descent)
            \item Use evidence to determine best architecture (model comparison), correlates with out-of-sample performance! 
            \item Solving the full ``shallow learning'' problem without compromise \arxiv{2004.12211}\arxiv{2211.10391}. 
            \item Promising work ongoing to extend this to transfer learning and deep nets.
        \end{itemize}
        \column{0.38\textwidth}
        \includegraphics[width=\textwidth]{figures/nn_posterior-cropped.pdf}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{Applications of nested sampling}
%    \framesubtitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
%    \begin{columns}
%        \column{0.57\textwidth}
%        \begin{itemize}
%            \item Nested sampling for frequentist computation!?
%            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
%            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
%            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
%            \item Need insight/approximation to make efficient.
%            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
%            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
%        \end{itemize}
%        \column{0.43\textwidth}
%        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{and beyond\ldots}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/watkinson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mason-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/formanek-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mcaloone-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/stenczel-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/yallup-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/bex-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/claireburke-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/hobson-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/lasenby-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/mhandley-headshot.jpg}%
        \includegraphics<1->[width=0.082\textwidth]{figures/headshots/whandley-headshot.jpg}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        \includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/tdoa-cropped-1-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/tdoa-cropped-2-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/tdoa-cropped-3-crop.pdf}%
        \includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Beyond the meta-algorithm}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Dynamic nested sampling~\arxiv{1704.03459}
            \item Unwoven nested sampling~\arxiv{1703.09701}
            \item Accelerated nested sampling~\arxiv{2212.01760}
            \item Precision nested sampling~\arxiv{2006.03371}
            \item Multiobjective nested sampling
            \item Nested sampling with gradients?
            \item Reversible nested sampling?
            \item Transdimensional nested sampling?
            \item postprocessing: \texttt{anesthetic}~\arxiv{1905.04768}
            \item crosschecking: \texttt{nestcheck}~\arxiv{1804.06406}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/dynesty.pdf}
        \includegraphics[width=\textwidth]{figures/supernest.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology \arxiv{2210.07409}(NatAstro)}
    \begin{columns}
        \column{0.65\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
            \item Lead data analysis team (REACH first light in January)
            \item Nested sampling woven in from the ground up (calibrator, beam modelling, signal fitting, likelihood selection).
            \item All treated as parameterised model comparison problems.
        \includegraphics[height=0.3\textwidth]{figures/EDGES_antenna}
        \includegraphics[height=0.3\textwidth]{figures/SARAS}
        \includegraphics[height=0.3\textwidth]{figures/REACH_2.jpg}
        \end{itemize}
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT: combining particle physics \& cosmological data}
    \begin{columns}
        \column{0.52\textwidth}
        \begin{itemize}
            \item Multinational team of particle physicists, cosmologists and statisticians.
            \item Combine cosmological data, particle colliders, direct detection, \& neutrino detectors in a statistically principled manner~\arxiv{2205.13549}.
            \item Lead Cosmo/Dark Matter working group~\arxiv{2009.03286}.
            \item Nested sampling used for global fitting, and fine-tuning quantification~\arxiv{2101.00428}
        \end{itemize}
        \begin{center}
            \includegraphics[width=0.5\textwidth]{figures/gambit_logo.png}
        \end{center}
        \column{0.48\textwidth}
        \vspace{-40pt}
        \includegraphics[width=\textwidth]{figures/ALP_2.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Likelihood-free inference (\& nested sampling)}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item How do you do inference if you don't know the likelihood $P(D|\theta)$?
                \begin{itemize}
                    \item e.g.\ if you can simulate a disease outbreak, how can you infer a posterior on $R_0$, or select the most predictive model?
                \end{itemize}
            \item If you can forward simulate/model $\theta\to D$, then you have an implicit likelihood.
            \item LFI aims to (machine-)\emph{learn} the likelihood from carefully chosen training data $\{(\theta,D)\}$.
            \item Nested sampling has much to offer
                \begin{itemize}
                    \item truncation strategies
                    \item evidence driven compression
                    \item marginalised machine learning
                \end{itemize}
            \item In my view, LFI represents the future of inference -- in twenty years time this will be as well-used as MCMC techniques are today.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/noisy.pdf}
        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{CosmoTension}
    \framesubtitle{Resolving cosmological tensions with diverse data, novel theories and Bayesian machine learning}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item ERC grant $\Rightarrow$ UKRI Frontier, commencing 2023.
            \item Funds 3 PDRAs and 4 PhDs over 5 years.
            \item Research programme centered around combining novel theories of gravity, Boltzmann solvers~\arxiv{1906.01421}, reconstruction~\arxiv{1908.00906}, nested sampling \& likelihood free inference.
            \item Aims to disentangle cosmological tensions $H_0$, $\sigma_8$, $\Omega_K$ with next-generation data analysis techniques.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/denseoutput.pdf}
        \includegraphics[width=\textwidth]{figures/pps.pdf}
        \includegraphics[width=\textwidth]{figures/gantt.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{unimpeded: legacy suites for the next generation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item DiRAC 2020 RAC allocation of 30MCPUh
            \item Main goal: Planck Legacy Archive equivalent
            \item Parameter estimation $\to$ Model comparison
            \item MCMC $\to$ Nested sampling
            \item Planck $\to$ $\{\text{Planck}, \text{DESY1}, \text{BAO}, \ldots \}$
            \item Pairwise combinations
            \item Suite of tools for processing these 
                \begin{itemize}
                    \item \texttt{anesthetic} $2.0$
                    \item \texttt{unimpeded} $1.0$
                    \item \texttt{zenodo} archive
                    \item \texttt{margarine}
                \end{itemize}
            \item MCMC chains also available.
            \item Library of bijectors emulators for fast re-use
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{logos/dirac}
        \includegraphics[width=0.5\textwidth,page=21]{figures/himmelblau}%
        \includegraphics[width=0.5\textwidth,page=15]{figures/himmelblau}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Summary}
    \begin{itemize}
        \item Nested sampling is a unique, multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Performing Bayesian inference: parameter estimation, model comparison \& tension quantification.
            \end{itemize}
        \item It already forms the cornerstone of many data-intensive science analyses
        \item My research innovates at the frontier of this field, and applies these techniques to a wide variety of core scientific problems working with international teams.
    \end{itemize}
\end{frame}
\appendix
\begin{frame}
    \frametitle{How does Nested Sampling compare to other approaches?}
    \begin{columns}
        \column{0.7\textwidth}
        \begin{itemize}
            \item In all cases:
                \begin{itemize}
                    \item[$+$] NS can handle multimodal functions
                    \item[$+$] NS computes evidences, partition functions and integrals
                    \item[$+$] NS is self-tuning/black-box
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
    \end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \begin{itemize}
                \item Gradient descent
                    \begin{itemize}
                        \item[$-$] NS cannot use gradients
                        \item[$+$] NS does not require gradients
                    \end{itemize}
                \item Genetic algorithms
                    \begin{itemize}
                        \item[$+$] NS discarded points have statistical meaning
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Sampling}
            \begin{itemize}
                \item Metropolis-Hastings?
                    \begin{itemize}
                        \item[$-$] Nothing beats well-tuned customised MH
                        \item[$+$] NS is self tuning
                    \end{itemize}
                \item Hamiltonian Monte Carlo?
                    \begin{itemize}
                        \item[$-$] In millions of dimensions, HMC is king
                        \item[$+$] NS does not require gradients
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \begin{itemize}
                \item Thermodynamic integration
                    \begin{itemize}
                        \item[$+$] protective against phase trasitions
                        \item[$+$] No annealing schedule tuning 
                    \end{itemize}
                \item Sequential Monte Carlo
                    \begin{itemize}
                        \item[$-$] SMC experts classify NS as a kind of SMC
                        \item[$+$] NS is athermal
                    \end{itemize}
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Curvature tension: evidence for a closed universe(?)~\arxiv{1908.09139}}
    \begin{columns}
        \begin{column}{0.55\textwidth}
            \begin{itemize}
                \item If you allow $\Omega_K\ne0$, \textit{Planck} (\texttt{plikTTTEEE}) has a moderate preference for closed universes (50:1 betting odds on), $\Omega_K=-4.5\pm1.5\%$
                \item \textit{Planck}+lens+BAO strongly prefer $\Omega_K=0$.
                \item But, \textit{Planck} vs lensing is 2.5$\sigma$ in tension, and Planck vs BAO is 3$\sigma$.
                \item Reduced if $\texttt{plik}\to\texttt{camspec}$~\arxiv{2002.06892} 
                \item BAO and lensing summary assume $\Lambda$CDM.
                \item Doing this properly with BAO retains preference for closed universe (though closer to flat $\Omega_K =-0.4\pm0.2\%$)~\arxiv{2205.05892}.
                \item Present-day curvature has profound consequences for inflation~\arxiv{2205.07374}.
            \end{itemize}
        \end{column}
        \begin{column}{0.45\textwidth}
            \includegraphics<1|handout:0>[width=\textwidth]{figures/curvature_1}%
            \includegraphics<2|handout:0>[width=\textwidth]{figures/curvature_2}%
            \includegraphics<3          >[width=\textwidth]{figures/curvature_3}%
        \end{column}
    \end{columns}
\end{frame}

\end{document}
```
# branch: remotes/origin/cosmoverse_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usetikzlibrary{arrows,arrows.meta,automata,positioning}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{\texttt{PolySwyft}}
\subtitle{a sequential simulation-based nested sampler}
\date{10\textsuperscript{th} July 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Contents}
    \begin{columns}
        \column{0.77\textwidth}
        \begin{enumerate}
            \item Likelihood- vs Simulation-based inference (LBI vs SBI) 
            \item Neural Ratio estimation (NRE)
            \item Nested sampling (NS)
            \item NS+NRE
            \item Future prospects
        \end{enumerate}
        Stems from over a year of discussion, with the majority of the work done by Kilian Scheutwinkel (PhD student).
        \column{0.23\textwidth}
        \begin{overpic}[width=\textwidth]{figures/students/kilian_scheutwinkel.jpg}
            \put(5,5) {\textcolor{white}{Kilian Scheutwinkel}}
        \end{overpic}
        \begin{overpic}[width=\textwidth]{figures/students/christoph_weniger.jpg}
            \put(10,5) {\textcolor{white}{Christoph Weniger}}
        \end{overpic}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{LBI: Likelihood-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        The standard approach if you are fortunate enough to have a likelihood function $\only<1-2>{P(D|\theta)}\only<3->{\C[2]{\mathcal{L}(D|\theta)}}$: 
        \[
            \only<1-2>{
                P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}
            }
            \only<2>{
                \qquad
                \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}}\times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}
            }
            \only<3>{
                \C[0]{\mathcal{P}(\theta|D)} = \frac{\C[2]{\mathcal{L}(D|\theta)}\C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}(D)}}
                \qquad
                \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}}\times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}
            }
            \only<4>{
                P(\theta|D) P(D) = P(\theta,D) = P(D|\theta)P(\theta), \qquad
            }
            \only<5>{
                \C[0]{\mathcal{P}}\times\C[3]{\mathcal{Z}} = \C[4]{\mathcal{J}} = \C[2]{\mathcal{L}}\times\C[1]{\pi}, \qquad \C[4]{\text{Joint}} = \C[4]{\mathcal{J}} = P(\theta,D)
            }
        \]
        \vspace{-10pt}
        \begin{enumerate}
            \item Define \C[1]{prior $\pi(\theta)$} 
                \begin{itemize}
                    \item spend some time being philosophical
                \end{itemize}
            \item Sample \C[0]{posterior $\mathcal{P}(\theta|D)$} 
                \begin{itemize}
                    \item use out-of-the-box MCMC tools such as\\ \texttt{emcee} or \texttt{MultiNest}
                    \item make some triangle plots
                \end{itemize}
            \item Optionally compute \C[3]{evidence $\mathcal{Z}(D)$}
                \begin{itemize}
                    \item e.g. nested sampling or parallel tempering
                    \item do some model comparison (i.e. science)
                    \item talk about tensions
                \end{itemize}
        \end{enumerate}
        \column{0.5\textwidth}
        \hfill%
        \begin{overpic}[width=0.6\textwidth]{figures/des_parameters.pdf}
            \put(-40,90) {DES Y5 SN Ia}
            \put(-40,80) {\arxiv{2401.02929}}
        \end{overpic}
        \includegraphics[width=0.5\textwidth]{figures/des_model_comparison.pdf}%
        \includegraphics[width=0.5\textwidth]{figures/des_suspiciousness.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{SBI: Simulation-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item What do you do if you don't know \C[2]{$\mathcal{L}(D|\theta)$}?
            \item If you have a simulator/forward model $\theta \rightarrow D$
                defines an \C[2]{\emph{implicit} likelihood~$\mathcal{L}$}.
            \item Simulator generates samples from $\C[2]{\mathcal{L}(\cdot|\theta)}$.
            \item With a prior $\C[1]{\pi}(\theta)$ can generate samples from \C[4]{joint distribution}~$\C[4]{\mathcal{J}(\theta,D)}=\C[2]{\mathcal{L}(D|\theta)}\C[1]{\pi(\theta)}$\\\hfill \emph{the ``probability of everything''}.
            \item Task of SBI is take joint~$\C[4]{\mathcal{J}}$ samples and learn \C[0]{posterior $\mathcal{P}(\theta|D)$} and \C[3]{evidence $\mathcal{Z}(D)$} \\\hfill and possibly \C[2]{likelihood $\mathcal{L}(D|\theta)$}.
            \item Present state of the art achieves this using \emph{machine learning} (neural networks).
                \begin{itemize}
                    \item My group's research tries to removes machine learning \tthref{github.com/handley-lab/lsbi}.

                \end{itemize}
            %\item Present SotA: NPE, NLE, NJE, NRE
            %\item SBI \& forward modelling force us to think about data space~$D$ \& parameter space~$\theta$.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[page=1, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<2>[page=2, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<3>[page=3, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<4>[page=4, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<5>[page=5, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<6>[page=6, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<7>[page=7, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<8>[page=8, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<9>[page=9, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<10>[page=10, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<11>[page=11, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<12>[page=12, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<13>[page=13, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<14>[page=14, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<15>[page=15, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<16>[page=16, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<17>[page=17, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<18>[page=18, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<19>[page=19, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<20>[page=20, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<21>[page=21, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why SBI?}
    \begin{columns}
        \column{0.6\textwidth}
        SBI is useful because:
        \begin{enumerate}
            \item If you don't have a likelihood, you can still do inference
                \begin{itemize}
                    \item This is the usual case beyond CMB cosmology
                \end{itemize}
            \item Faster than LBI
                \begin{itemize}
                    \item emulation -- also applies to LBI in principle
                \end{itemize}
            \item No need to pragmatically encode fiducial cosmologies
                \begin{itemize}
                    \item Covariance computation implicitly encoded in simulations
                    \item Highly relevant for disentangling tensions \& systematics
                \end{itemize}
            \item Equips AI/ML with Bayesian interpretability
            \item Lower barrier to entry than LBI
                \begin{itemize}
                    \item Much easier to forward model a systematic
                    \item Emerging set of plug-and-play packages
                    \item For this reason alone, it will come to dominate scientific inference
                \end{itemize}
        \end{enumerate}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/sbi_screenshot}
        \href{https://github.com/sbi-dev}{github.com/sbi-dev}
        \includegraphics[width=\textwidth]{figures/swyft_screenshot}
        \href{https://github.com/undark-lab/swyft}{github.com/undark-lab/swyft}
        \includegraphics[width=\textwidth]{figures/selfi_screenshot}
        \href{https://github.com/florent-leclercq/pyselfi}{github.com/florent-leclercq/pyselfi}
        \includegraphics[width=\textwidth]{figures/delfi_screenshot}
        \href{https://github.com/justinalsing/pydelfi}{github.com/justinalsing/pydelfi}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why aren't we currently using SBI in cosmology?}
    \begin{columns}
        \column{0.4\textwidth}
        \begin{itemize}
            \item Short answer: we are!
                \begin{itemize}
                    \item Mostly for weak lensing
                    \item 2024 has been the year it has started to be applied to real data.
                \end{itemize}
            \item Longer answer: SBI requires mock data generation code
            \item Most data analysis codes were built before the generative paradigm.
            \item It's still a lot of work to upgrade cosmological likelihoods  to be able to do this (e.g.\ \texttt{plik} \& \texttt{camspec}).
        \end{itemize}
        \column{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/sbi_papers/clusters.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/des.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/gw.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/center.pdf}
        \column{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/sbi_papers/kidsdes.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/kids.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/population.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/simbig.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Neural Ratio Estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item SBI flavours: {\small \hfill\tthref{github.com/sbi-dev/sbi}}
                {\small
                    \begin{description}
                        \item[NPE] Neural posterior estimation
                        \item[NLE] Neural likelihood estimation
                        \item[NJE] Neural joint estimation
                        \item[NRE] Neural ratio estimation
                    \end{description}
                }
            \item NRE recap:
                \begin{enumerate}
                    \item Generate joint samples $(\theta,D)\sim\C[4]{\mathcal{J}}$
                        \label{step:joint}
                        \begin{itemize}
                            \item \textit{straightforward if you have a simulator:\\ $\theta\sim\C[1]{\pi(\cdot)}$, $D\sim\C[2]{\mathcal{L}(\cdot|\theta)}$}
                        \end{itemize}
                    \item Generate separated samples $\theta\sim\C[1]{\pi}$, $D\sim\C[3]{\mathcal{Z}}$\label{step:sep}
                        \begin{itemize}
                            \item \textit{aside: can shortcut step~\ref{step:sep} by scrambling the $(\theta,D)$ pairings from step~\ref{step:joint}}
                        \end{itemize}
                    \item Train probabilistic classifier $p$ to distinguish whether $(\theta,D)$ came from $\C[4]{\mathcal{J}}$ or $\C[1]{\pi}\times\C[3]{\mathcal{Z}}$.
                    \item $\frac{p}{1-p} = \C[5]{r} = \frac{P(\theta,D)}{P(\theta)P(D)} 
                        =
                        \frac{\C[4]{\mathcal{J}}}{\C[1]{\pi}\times\C[3]{\mathcal{Z}}} = \frac{\C[2]{\mathcal{L}}}{\C[3]{\mathcal{Z}}} = \frac{\C[0]{\mathcal{P}}}{\C[1]{\pi}}$.
                    \item Use ratio $\C[5]{r}$ for parameter estimation $\C[0]{\mathcal{P}} = \C[5]{r}\C[1]\times {\pi}$
                \end{enumerate}
        \end{itemize}
        \column{0.5\textwidth}
        \only<1|handout:0>{
            \begin{tikzpicture}[node distance=1cm, every neuron/.style={circle, draw, minimum size=1cm},]
                \node[every neuron/.try] (j2)  {};
                \node[every neuron/.try, above left = 0cm and 0.5cm of j2] (theta) { $\theta$};
                \node[every neuron/.try, below left = 0cm and 0.5cm of j2] (D) { $D$};
                \node[every neuron/.try, above = 0.5cm of j2] (j1) {};
                \node[every neuron/.try, below = 0.5cm of j2] (j3) {};
                \node[every neuron/.try, above right = 0cm and 0.5cm of j2] (h1) {};
                \node[every neuron/.try, below right = 0cm and 0.5cm of j2] (h2) {};
                \node[every neuron/.try, right = 1.3cm of j2] (p) { $p$};
                \node[every neuron/.try, right = 0.5cm of p] (logr) { $\C[5]{r}$};
                \draw[-] (theta) -- (j1);
                \draw[-] (D) -- (j1);
                \draw[-] (theta) -- (j2);
                \draw[-] (D) -- (j2);
                \draw[-] (theta) -- (j3);
                \draw[-] (D) -- (j3);
                \draw[-] (j1) -- (h1);
                \draw[-] (j1) -- (h2);
                \draw[-] (j2) -- (h1);
                \draw[-] (j2) -- (h2);
                \draw[-] (j3) -- (h1);
                \draw[-] (j3) -- (h2);
                \draw[-] (h1) -- (p);
                \draw[-] (h2) -- (p);
                \draw[-] (p) -- (logr);
                \node[below =0.5cm of logr] {\includegraphics[page=22, width=0.5\textwidth]{figures/sbi_parameter_estimation.pdf}};
            \end{tikzpicture}
        }
        \only<2>{
            \begin{exampleblock}{Bayesian proof}
                \begin{itemize}
                    \item Let $M_{\C[4]{\mathcal{J}}}$: $(\theta,D)\sim\C[4]{\mathcal{J}}$, $M_{\C[1]{\pi}\C[3]{\mathcal{Z}}}$: $(\theta,D)\sim\C[1]{\pi}\times\C[3]{\mathcal{Z}}$
                    \item Classifier gives
                        ${p(\theta,D) = P(M_{\C[4]{\mathcal{J}}}|\theta,D) = 1- P(M_{\C[1]{\pi}\C[3]{\mathcal{Z}}}|\theta,D)}$
                    \item Bayes theorem then shows
                        ${\frac{p}{1-p}=\frac{P(M_{\C[4]{\mathcal{J}}}|\theta,D)}{P(M_{\C[1]{\pi}\C[3]{\mathcal{Z}}}|\theta,D)} = \frac{P(\theta,D|M_{\C[4]{\mathcal{J}}})P(M_{\C[4]{\mathcal{J}}})}{P(\theta,D|M_{\C[1]{\pi}\C[3]{\mathcal{Z}}})P(M_{\C[1]{\pi}\C[3]{\mathcal{Z}}})} = 
                        \frac{\C[4]{\mathcal{J}}}{\C[1]{\pi}\C[3]{\mathcal{Z}}}}$, \\
                        where we have assumed 
                        \begin{itemize}
                            \item $P(M_{\C[4]{\mathcal{J}}}) = P(M_{\C[1]{\pi}\C[3]{\mathcal{Z}}})$,
                        \end{itemize}
                        and by definition
                        \begin{itemize}
                            \item $\C[4]{\mathcal{J}(\theta,D)} = P(\theta,D|M_{\C[4]{\mathcal{J}}})$
                            \item $\C[1]{\pi(\theta)}\C[3]{\mathcal{Z}(D)} = P(\theta,D|M_{\C[1]{\pi}\C[3]{\mathcal{Z}}})$.
                        \end{itemize}
                \end{itemize}
            \end{exampleblock}
        }
        \only<3|handout:0>{
            \begin{block}{Why I like NRE}
                \begin{itemize}
                    \item The link between classification and inference is profound.
                    \item Density estimation is hard -- Dimensionless $r$ divides out the hard-to-calculate parts.
                \end{itemize}
            \end{block}
            \begin{block}{Why I don't like NRE}
                \begin{itemize}
                    \item Practical implementations require marginalisation~\arxiv{2107.01214}, or autoregression~\arxiv{2308.08597}.
                    \item Model comparison and parameter estimation are separate~\arxiv{2305.11241}.
                \end{itemize}
            \end{block}
        }
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{TMNRE: Truncated Marginal Neural Ratio Estimation}
    \framesubtitle{\texttt{swyft}: \tthref{github.com/undark-lab/swyft}}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Two tricks for practical NRE:
        \end{itemize}
        \begin{block}{Marginalisation}
            \begin{itemize}
                \item Only consider one or two parameters at a time.
                \item Fine if your goal is to produce triangle plots.
                \item Problematic if information is contained jointly in more than two parameters.
            \end{itemize}
        \end{block}
        \begin{block}{Truncation}
            \begin{itemize}
                \item focus parameters $\theta$ on a subset of the prior which reproduces observed data $D_\text{obs}$
                \item region is somewhat arbitrary (usually a box)
                \item not amortised, sounds a bit like ABC
            \end{itemize}
        \end{block}
        \column{0.45\textwidth}
        \begin{overpic}[width=\textwidth]{figures/tmnre}
            \put(70,0) {\arxiv{2111.08030}}
        \end{overpic}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling: numerical Lebesgue integration}
    \begin{columns}
        \column{0.5\textwidth}
        \fbox{\parbox{\textwidth}{
                \begin{itemize}
                    \item[0.] Start with $N$ random samples over the space.
                    \item[i.] Delete outermost sample, and replace with a new random one at higher integrand value.
        \end{itemize}}}
        \vspace{-5pt}
        \begin{itemize}
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item Discarded ``dead points'' can be weighted to form posterior, prior, or anything in between.
            \item Estimates the \textbf{density of states} and calculates evidences \& partition functions.
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune,
                    \item exploration of multimodal functions,
                    \item global and local optimisation.
                \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4|handout:0>[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<8|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        \includegraphics<9->[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \texttt{nautilus}~\arxiv{2306.16923} 
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.55\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267} \texttt{jaxnest}~\arxiv{2012.15286}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: dead points}
    \includegraphics[width=\textwidth]{figures/dead_measure}
    \begin{columns}
        \column{0.69\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Dead points have a unique scale-invariant distribution $\propto\: \tfrac{dV}{V}$.
            \item Uniform over original region, exponentially concentrating on region of interest (until termination volume).
            \item Good for training emulators (HERA~\arxiv{2108.07282}).
        \end{itemize}
        \column{0.3\textwidth}
        \begin{block}{Applications}
            \begin{itemize}
                \item training emulators.
                \item gridding simulations
                \item beta flows
                \item ``dead measure'' 
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Similarities}
    \begin{columns}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth,page=15]{figures/himmelblau}% 
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/tmnre.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why it's hard to do SBI with nested sampling}
    \begin{itemize}
        \item At each iteration $i$, nested sampling requires you to be able to generate a new live point from the prior, subject to a hard likelihood constraint
            \[ \theta\sim\pi : \mathcal{L}(\theta)>\mathcal{L}_i \]
        \item This is hard if you don't have a likelihood!
        \item In addition, nested sampling does not do well if the likelihood is non-deterministic
        \item Previous attempts:
            \begin{itemize}
                \item DNest paper \arxiv{1606.03757}(Section 10: Nested sampling for ABC)
                \item ANRE~\arxiv{2308.08597} using non-box priors driven by current ratio estimate with slice sampling re-population.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Sequential NRE with nested sampling}
    \begin{tikzpicture}[
            node distance=1cm,
            >=stealth, auto,
            every state/.style={
                rectangle, rounded corners, minimum width=2em,
                text width=6.8cm, align=center
            }
        ]
        \node[state, fill=C3!20, minimum height=3cm] (q14) {
            \textbf{NS}\\
            Run nested sampling on $\log r_{i}(\theta, D_\text{obs})$ \\
            to generate $\theta$ dead samples between prior and posterior targeted at $D_\text{obs}$.
        };
        \node[state, fill=C0!20, minimum height=1cm] (q121) [above=of q14] {
            \textbf{Terminate} if $\mathcal{D}_\text{KL}$ has converged
        };
        \node[state, fill=C0!20, minimum height=1cm] (q12) [above=of q121] {
            \textbf{Initialise} $\theta\sim\pi$\\
        };
        \node[state, fill=C2!20, minimum height=3cm] (q34) [right=of q14] {
            \textbf{NRE}\\
            Train Neural ratio estimator $\log r_i$
            with weights initialised from previous run
        };
        \node[state, fill=C1!20, minimum height=3cm] (q23) [above=of q34] {
            \textbf{Simulate}\\
            Generate simulations $\theta\to D$ from all discarded points
        };
        \begin{scope}[bend left]%
            \path[thick,-{Latex[width=2mm]}]   (q14.north) edge node {} (q121.south)
            (q121.east) edge node {}($(q23.south west)!0.16!(q23.north west)$) 
            (q23.south) edge node {} (q34.north)
            (q34.west) edge node {} (q14.east)
            (q12.east) edge node {}($(q23.south west)!0.84!(q23.north west)$) ;
        \end{scope}
        \node[align=center] (e) at (barycentric cs:q121=0.5,q12=0.5,q23=1,q34=1,q14=1) {\Large\textbf{NSNRE}};
    \end{tikzpicture}
\end{frame}

\begin{frame}
    \frametitle{\texttt{PolySwyft}}
    \begin{columns}[t]
        \column{0.45\textwidth}
        \begin{block}{\texttt{PolyChord}}
            \tthref{github.com/PolyChord/PolyChordLite}
            \includegraphics[width=\textwidth]{figures/polychord.png}
            \begin{itemize}
                \item Widely used high-performance nested sampling tool (implementing slice sampling \& clustering in MPI Fortran)
            \end{itemize}
        \end{block}
        \column{0.45\textwidth}
        \begin{block}{\texttt{Swyft}}
            \tthref{github.com/undark-lab/swyft}
            \includegraphics[width=\textwidth]{figures/swyft_logo_wide.png}
            \begin{itemize}
                \item Widely used TMNRE tool in cosmology/astrophysics.
            \end{itemize}
        \end{block}
        However, NSNRE is general, and not specific to these choices.
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Convergence diagnostics}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Example for a $n=5$ dimensional parameter space, with $d=100$ data points, (\texttt{lsbi} gaussian mixture model).
            \item This is the regime for cosmological scale problems.
            \item To determine convergence we track:
                \begin{itemize}
                    \item The change in KL divergence between rounds (\C[0]{blue}), and check when this goes to zero.
                    \item The total KL divergence between prior and posterior estimate (\C[1]{orange}), and check when this levels off (ground truth in \C[3]{red}).
                    \item Also shown is the KL divergence between the estimate and the ground truth (\C[2]{green}).
                \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/GMM_posterior_estimates.pdf}%
        \includegraphics<2|handout:0>[width=\textwidth]{figures/GMM_KL_div_per_round.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\tthref{github.com/handley-lab}}
    \tikz[overlay,remember picture]
    \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
        \includegraphics[width=0.09\textheight]{figures/students/adam_ormondroyd.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/david_yallup.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/dily_ong.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/george_carter.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/harry_bevins.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/ian_roque.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/kilian_scheutwinkel.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/metha_prathaban.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/namu_kroupa.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sinah_legner.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sam_leeney.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/thomas_gessey-jones.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/wei-ning_deng.jpg}%
    };
    \begin{itemize}
        %\item What does NSNRE give you that TMNRE doesn't?
        %    \begin{itemize}
        %        \item Use of dead points which scan from prior to logr peak avoids risk of 'trimming' important regions of the space
        %        \item Likelihood-driven contours allow more parameters $n>2$ to be considered in comparison with box priors
        %    \end{itemize}
        \item \texttt{PolySwyft} can perform NRE on $n\sim 6$ parameter spaces and $d\sim100$ data spaces.
        \item This makes it relevant for cosmological applications.
        \item Look out for imminent paper (post Kilian's thesis hand-in in $\sim\mathcal{O}(1 \text{month}))$
        %\item Investigating this raises (?existential) questions regarding NRE.
        %\item Is the $-5 <\log r < 5$ saturation of NREs a fundamental problem, or an engineering one?
        %\item Is there a ``nested'' approach to crossing this range in larger parameter spaces $n\gg 5$?
        \item Examples produced using \texttt{lsbi} package: \tthref{github.com/handley-lab/lsbi}\\
    \end{itemize}
\end{frame}

\appendix

\begin{frame}
    \frametitle{Considerations of ratio estimation}
    \student{zixiao_hu}{Zixiao Hu}{MPhil}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Neural REs can in practice only estimate in a band of $\log r$ before the activation function saturates (typically $-5 < \log r < 5$).
            \item Consider a posterior $\mathcal{P}$ well approximated by a Gaussian profile in an $n$-dimensional parameter space~\arxiv{2312.00294}
            \item If $\mathcal{D}_\text{KL}\gg1$ between prior and posterior:
                \begin{gather*}
                    \log r = \frac{n}{2} + \mathcal{D}_\text{KL} + \chi^2_{n} \\
            %\[\mathcal{P}(\log r) = \frac{1}{\Gamma(\tfrac{n}{2})} e^{\log r-\frac{n}{2}-\mathcal{D}_\text{KL}} \left(\tfrac{n}{2}+\mathcal{D}_\text{KL}-\log r\right)^{\frac{n}{2}-1}\]
            %i.e. $\log r$ has an offset \& rescaled chi-squared distribution:
            %\[n + 2\mathcal{D}_\text{KL} - 2\log r \sim \chi^2_{n} \]
                    \left\langle \log r \right\rangle_\mathcal{P} = \mathcal{D}_\text{KL}, \qquad \sigma(\log r)_\mathcal{P} = \sqrt{\frac{n}{2}}
                \end{gather*}
            \item Truncation ({\bf T}MNRE) reduces $\mathcal{D}_\text{KL}$, focusing the distribution into the $[-5,5]$ band.
            \item Marginalisation (T{\bf M}NRE) reduces $n$ \& $\sigma$.
        \end{itemize}
        \column{0.5\textwidth}
        \vspace{10pt}
        \includegraphics{figures/anatomy}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Cosmological forecasting}
    \framesubtitle{Have you ever done a Fisher forecast, and then felt Bayesian guilt?}
    \vspace{-20pt}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item Cosmologists are interested in forecasting what a Bayesian analysis of future data might produce.
            \item Useful for:
                \begin{itemize}
                    \item white papers/grants,
                    \item optimising existing instruments/strategies,
                    \item picking theory/observation to explore next.
                \end{itemize}
            \item To do this properly:
                \begin{enumerate}
                    \item start from current knowledge $\pi(\theta)$, derived from current data
                    \item Pick potential dataset $D$ that might be collected from $P(D)\: (=\mathcal{Z})$
                    \item Derive posterior $P(\theta|D)$
                    \item Summarise science (e.g. constraint on $\theta$, ability to perform model comparison)
                \end{enumerate}
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item This procedure should be marginalised over:
                \begin{enumerate}
                    \item All possible parameters $\theta$ (consistent with prior knowledge)
                    \item All possible data $D$
                \end{enumerate}
            \item i.e. marginalised over the joint $P(\theta,D)=P(D|\theta)P(\theta)$.
            \item Historically this has proven very challenging.
            \item Most analyses assume a fiducial cosmology $\theta_*$, and/or a Gaussian likelihood/posterior (c.f. Fisher forecasting).
            \item This runs the risk of biasing forecasts by baking in a given theory/data realisation.
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Fully Bayesian Forecasting~\arxiv{2309.06942}}
    \student{thomas_gessey-jones}{Thomas Gessey-Jones}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Simulation based inference gives us the language to marginalise over parameters $\theta$ and possible future data $D$.
            \item Evidence networks give us the ability to do this at scale for forecasting~\arxiv{2305.11241}.
            \item Demonstrated in 21cm global experiments, marginalising over:
                \begin{itemize}
                    \item theoretical uncertainty
                    \item foreground uncertainty
                    \item systematic uncertainty
                \end{itemize}
            \item Able to say ``at 67mK radiometer noise'', have a 50\% chance of 5$\sigma$ Bayes factor detection.
            \item Can use to optimise instrument design
            \item Re-usable package: \texttt{prescience}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fbf.pdf}
    \end{columns}
\end{frame}

\end{document}
```
# branch: remotes/origin/gatsby_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{ Nested sampling: {\large powering next-generation inference and machine learning tools}}
\subtitle{for astrophysics, cosmology, particle physics and beyond}
\date{20\textsuperscript{th} March 2024}

\begin{document}



\begin{frame}
    \titlepage
\end{frame}

%\begin{frame}
%    \frametitle{Highlight: state-of-the-art Nature review [\href{https://www.nature.com/articles/s43586-022-00121-x}{NatRev}]}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item Invented by John Skilling in 2004.
%            \item Recent Nature review primer on nested sampling led by Andrew Fowlie and assembled by the community.
%            \item Showcases the current set of tools, and applications from chemistry to cosmology.
%            \item Recent 1.5 day conference in Munich: ``Frontiers of Nested Sampling''
%            \item Planned week-long NSCON 2024
%            \item In this talk:
%                \begin{itemize}
%                    \item User guide to nested sampling
%                    \item Particle physics applications
%                    \item Cosmology applications
%                    \item Machine learning applications
%                \end{itemize}
%        \end{itemize}
%        \column{0.3\textwidth}
%        \includegraphics[width=\textwidth]{figures/nature1}
%        \includegraphics[width=\textwidth]{figures/nature2}
%        \column{0.2\textwidth}
%        \includegraphics[width=\textwidth]{figures/nature4}
%        \includegraphics[width=\textwidth]{figures/nature5}
%    \end{columns}
%
%\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Where is Nested Sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item For many purposes, in your Neural Net you should group Nested Sampling with (MCMC) techniques such as:
                \begin{itemize}
                    \item Metropolis-Hastings (PyMC, MontePython)
                    \item Hamiltonian Monte Carlo (Stan, blackjax)
                    \item Ensemble sampling (emcee, zeus). 
                    \item Variational Inference (Pyro)
                    \item Sequential Monte Carlo 
                    \item Thermodynamic integration
                    \item Genetic algorithms
                \end{itemize}
            \item You may have heard of it branded form:
                \begin{itemize}
                    \item MultiNest
                    \item PolyChord
                    \item dynesty
                    \item ultranest
                \end{itemize}
            \end{itemize}
        \column{0.5\textwidth}
        \begin{columns}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/emcee}
        \includegraphics[width=\textwidth]{figures/metropolis-hastings}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/zeus}
        \end{columns}
        \includegraphics[width=\textwidth]{figures/hmc_explained}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Integration in Physics}
    \begin{itemize}
        \item Integration is a fundamental concept in physics, statistics and data science:
    \end{itemize}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Partition functions}
            \vspace{-11pt}
            \[ Z(\beta) = \int e^{-\beta H(q,p)} dq dp \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Path integrals}
            \[ \Psi = \int e^{i S} \mathcal{D}x \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Bayesian marginals}
            \vspace{-11pt}
            \[ \mathcal{Z}(D) = \int \mathcal{L}(D|\theta) \pi(\theta) d\theta \]
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Need numerical tools if analytic solution unavailable.
            \item High-dimensional numerical integration is hard.
            \item Riemannian strategy estimates volumes geometrically:
                \[ \int f(x) d^nx \approx \sum_i f(x_i) \Delta V_i \sim \mathcal{O}(e^n) \]
            \item Curse of dimensionality $\Rightarrow$ exponential scaling.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Probabalistic volume estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Key idea in NS: estimating volumes probabilistically
                \[
                    \only<-2>{
                    \frac{\C[1]{V_\mathrm{after}}}{\C[0]{V_\mathrm{before}}} 
                    \approx \frac{\C[1]{n_\mathrm{in}}}{\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}}
                }
                    \only<3>{
                    \frac{\C[1]{V_\mathrm{after}}}{\C[0]{V_\mathrm{before}}} 
                    \approx \frac{\C[1]{n_\mathrm{in}}+1}{\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}+2}
                }
                \only<4>{\hspace{-15pt}
                    \frac{\C[1]{V_\mathrm{after}}}{\C[0]{V_\mathrm{before}}} 
\sim \frac{\C[1]{n_\mathrm{in}}+1}{\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}+2} \pm \sqrt{\tfrac{(\C[1]{n_\mathrm{in}}+1)(\C[0]{n_\mathrm{out}}+1)}{(\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}+2)^2(\C[0]{n_\mathrm{out}}+\C[1]{n_\mathrm{in}}+3)}}
                }
                \]
            \item This is the \textbf{only} way to calculate volume in high dimensions $d>3$.
                \begin{itemize}
                    \item Geometry is exponentially inefficient.
                \end{itemize}
            \item This estimation process does not depend on geometry, topology or dimensionality
            \item Basis of all Monte-Carlo integration
            \item Nested Sampling uniquely uses a nested framework to couple together MC integrals in a robust, scalable manner.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/compression_1}%
        \includegraphics<2->[width=\textwidth]{figures/compression_2}%
    \end{columns}
\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.48\textwidth}
        \begin{block}{\textbf{MCMC}}
            \only<16>{
                \begin{itemize}
                    \item Single ``walker''
                    \item Explores posterior
                    \item Fast, if proposal matrix is tuned
                    \item Parameter estimation, suspiciousness calculation
                    \item Channel capacity optimised for generating posterior samples
                \end{itemize}
            }
        \end{block}
            \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
            \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
            \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
            \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
            \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
            \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.48\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \only<16>{
                \begin{itemize}
                    \item Ensemble of ``live points''
                    \item Scans from prior to peak of likelihood
                    \item Slower, no tuning required
                    \item Parameter estimation, model comparison, tension quantification
                    \item Channel capacity optimised for computing partition function
                \end{itemize}
            }
        \end{block}
            \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
            \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
            \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
            \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
            \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
            \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
            \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
            \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
            \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} 
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: live points}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n$ random samples over the space.
            \item Delete outermost sample, and replace with a new random one at higher integrand value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n}\only<5->{\pm \frac{1}{n}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \int f(x) dV \approx \sum_i f(x_i) \Delta V_i, \quad V_i = V_0 e^{-\only<5->{(}i\only<5->{\pm\sqrt{i})}/n} \]
%            \item Nested sampling: completely different way to scan.
%            \item Ensemble sampling compresses entire space$\to$peak(s).
%            \item Sequentially update a set $S$ of $n$ samples:
%                \begin{itemize}
%                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from a measure $\pi$). 
%
%                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
%                \end{itemize}
%            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard constraint}:
%                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
%            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence}/integral of function \& \C[0]{posterior}/sample weights.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4-         >[width=\textwidth,page=4]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: dead points}
    \includegraphics[width=\textwidth]{figures/dead_measure}
    \begin{columns}
        \column{0.69\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Dead points have a unique scale-invariant distribution $\propto\: \tfrac{dV}{V}$.
            \item Uniform over original region, exponentially concentrating on region of interest (until termination volume).
            \item Good for training emulators (HERA~\arxiv{2108.07282}).
        \end{itemize}
        \column{0.3\textwidth}
        \begin{block}{Applications}
        \begin{itemize}
            \item training emulators.
            \item gridding simulations
            \item beta flows
            \item ``dead measure'' 
        \end{itemize}
        \end{block}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{The nested sampling meta-algorithm: Lebesgue integration}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Full dead-point coverage of tails enables integration.
            \item Can be weighted to form posterior samples, prior samples, or anything in between.
            \item Nested sampling estimates the \textbf{density of states} and calculates partition functions
                \[Z(\beta) = \sum_i f(x_i)^\beta \Delta V_i.\]
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune
                    \item exploration of multimodal functions
                    \item global and local optimisation
                \end{itemize}
            %\item Interpreted as a Bayesian algorithm, it
            %    \begin{itemize}
            %        \item Computes the Bayesian evidence (model comparison)
            %        \item Produces (weighted) posterior samples (parameter estimation)
            %    \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Sampling from a hard likelihood constraint} 
  
  \begin{quote}
    ``It is not the purpose of this introductory paper to develop the technology of navigation within such a volume. We merely note that exploring a hard-edged likelihood-constrained domain should prove to be neither more nor less demanding than exploring a likelihood-weighted space.''
    
   {\hfill --- John Skilling}
  \end{quote}

  \begin{itemize}
      
    \item A large fraction of the work in NS to date has been in attempting to implement a hard-edged sampler in the NS meta-algorithm $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \item \url{https://projecteuclid.org/euclid.ba/1340370944}.
    \item There has also been much work beyond this (see 'Frontiers of nested sampling' talk)
        \begin{itemize}
            \item \href{https://www.willhandley.co.uk/talks}{willhandley.co.uk/talks}
        \end{itemize}
  \end{itemize}
 
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \texttt{nautilus}~\arxiv{2306.16923} 
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.55\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267} \texttt{jaxnest}~\arxiv{2012.15286}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Types of nested sampler}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
\item Constructs bounding region and draws many invalid points until $\mathcal{L}(\theta)>\mathcal{L}_*$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{5pt}
    \begin{itemize}
        \item Nested samplers usually come with:
            \begin{itemize}
                \item \emph{resolution} parameter $n_\mathrm{live}$ (which improve results as $\sim\mathcal{O}(n_\mathrm{live}^{-1/2})$.
                    \item set of \emph{reliability} parameters~\arxiv{2101.04525}, which don't improve results if set arbitrarily high, but introduce systematic errors if set too low.
                    \item e.g. \texttt{Multinest} efficiency \texttt{eff} or \texttt{PolyChord} chain length $n_\mathrm{repeats}$.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Applications: The three pillars of Bayesian inference}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Parameter estimation}
            What do the data tell us about the parameters of a model?

            \textit{e.g. the size or age of a $\Lambda$CDM universe}
            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Model comparison}
            How much does the data support a particular model?

            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}} \vspace{-7pt}\]
            \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[5]{\Pi_\mathcal{M}}}{\C[7]{\sum_m Z_m \Pi_m}} \]
            \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Tension quantification}
            Do different datasets make consistent predictions from the same model? 
            \textit{e.g. CMB vs Type IA supernovae data}
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Cosmology}
    \student{adam_ormondroyd}{Adam Ormondroyd}{PhD}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Battle-tested in Bayesian cosmology on
                \begin{itemize}
                    \item Parameter estimation: multimodal alternative to MCMC samplers.
                    \item Model comparison: using integration to compute the Bayesian evidence
                    \item Tension quantification: using deep tail sampling and suspiciousness computations.
                \end{itemize}
            \item Plays a critical role in major cosmology pipelines: Planck, DES, KiDS, BAO, SNe.
            \item The default $\Lambda$CDM cosmology is well-tuned to have Gaussian-like posteriors for CMB data. 
            \item Less true for alternative cosmologies/models and orthogonal datasets, so nested sampling crucial.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/pps_both}
        \includegraphics[width=0.49\textwidth]{figures/reach_fit-cropped.pdf}
        %\includegraphics[width=0.49\textwidth]{figures/curvature_3}
        \includegraphics[width=\textwidth]{figures/omegak_H0_2.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Astrophysics}
\student{metha_prathaban}{Metha Prathaban}{PhD}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item In exoplanets~\arxiv{1806.00518}
                \begin{itemize}
                    \item Parameter estimation: determining properties of planets.
                    \item Model comparison: how many planets? Stellar modelling~\arxiv{2007.07278}.
                    \item exoplanet problems regularly have posterior phase transitions \arxiv{2102.03387}
                \end{itemize}
            \item In gravitational waves
                \begin{itemize}
                    \item Parameter estimation: Binary merger properties
                    \item Model comparison: Modified theories of gravity, selecting phenomenological parameterisations~\arxiv{1803.10210}
                    \item Likelihood reweighting: fast slow properties
                \end{itemize}
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/rv_full.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Particle physics}
    \student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{columns}
            \column{0.67\textwidth}
            \begin{itemize}
                \item Nested sampling for cross section computation/event generation
            \end{itemize}
            \column{0.3\textwidth}
            \[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2.\]
        \end{columns}
        \begin{itemize}
            \item Nested sampling can explore the phase space $\Omega$ and compute integral blind with comparable efficiency to HAAG/RAMBO~\arxiv{2205.02030}.
            \item Bayesian sparse reconstruction~\arxiv{1809.04598} applied to bump hunting allows evidence-based detection of signals in phenomenological backgrounds~\arxiv{2211.10391}.
            \item Fine tuning quantification
            \item Fast estimation of small $p$-values \arxiv{2106.02056}(PRL), just make switch:\\ $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.

        \end{itemize}
        \column{0.17\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1-pdfjam-crop.pdf}
        \column{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/psi_predict-crop.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Lattice field theory}
\student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Consider standard field theory Lagrangian:
                \[ Z(\beta) = \int D\phi e^{-\beta S(\phi)}, \quad S(\phi) = \int dx^\mu \mathcal{L}(\phi) \]
            \item Discretize onto spacetime grid.
            \item Compute partition function
            \item NS unique traits:
                \begin{itemize}
                    \item Get full partition function for free
                    \item allows for critical tuning
                    \item avoids critical slowing down
                \end{itemize}
            \item Applications in lattice gravity, QCD, condensed matter physics
            \item Publication imminent (next week)
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/potential_shape}
        \includegraphics[width=0.49\textwidth]{figures/2d_phase}
        \includegraphics[width=\textwidth]{figures/lattice_field_theory.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Machine learning}
\student{kamran_javid}{Kamran Javid}{PDRA}
    \begin{columns}
        \column{0.62\textwidth}
        \begin{itemize}
            \item Machine learning requires:
                \begin{itemize}
                    \item Training to find weights
                    \item Choice of architecture/topology/hyperparameters
                \end{itemize}
            \item Bayesian NNs treat training as a model fitting problem
            \item Compute posterior of weights (parameter estimation), rather than optimisation (gradient descent)
            \item Use evidence to determine best architecture (model comparison), correlates with out-of-sample performance! 
            \item Solving the full ``shallow learning'' problem without compromise \arxiv{2004.12211}\arxiv{2211.10391}. 
                \begin{itemize}
                    \item Promising work ongoing to extend this to transfer learning and deep nets.
                \end{itemize}
            \item More generally, dead points are optimally spaced  for training traditional ML approaches e.g. \arxiv{2309.05697}
        \end{itemize}
        \column{0.38\textwidth}
        \includegraphics[width=\textwidth]{figures/nn_posterior-cropped.pdf}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{Applications of nested sampling}
%    \framesubtitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
%    \begin{columns}
%        \column{0.57\textwidth}
%        \begin{itemize}
%            \item Nested sampling for frequentist computation!?
%            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
%            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
%            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
%            \item Need insight/approximation to make efficient.
%            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
%            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
%        \end{itemize}
%        \column{0.43\textwidth}
%        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{And beyond\ldots}
    \only<1-4>{
        \student{watkinson-headshot}{Catherine Watkinson}{Senior Data Scientist}
    }
    \only<5->{
        \student{mcaloone-headshot}{Thomas Mcaloone}{PhD $\to$ Data Scientist}
    }
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
            \includegraphics[width=0.08\textwidth]{figures/headshots/catherine-watkinson.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/thomas-macaloone.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/parul-janagal.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/angus-peters.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/tamas-stenzel.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/david-yallup.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/rebecca-handley.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/adam-ormondroyd.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/mike-hobson.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/anthony-lasenby.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/mike-handley.pdf}%
            \includegraphics[width=0.08\textwidth]{figures/headshots/will-handley.pdf}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        \includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/tdoa-cropped-1-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/tdoa-cropped-2-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/tdoa-cropped-3-crop.pdf}%
        \includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology {\small \arxiv{2210.07409}(NatAstro)}}
    \student{ian_roque}{Ian Roque}{PhD}
    \vspace{10pt}
    \begin{columns}
        \column{0.65\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
            \item Lead data analysis team (REACH first light in January)
            \item Nested sampling woven in from the ground up (calibrator, beam modelling, signal fitting, likelihood selection).
            \item All treated as parameterised model comparison problems.
        \includegraphics[height=0.3\textwidth]{figures/EDGES_antenna}
        \includegraphics[height=0.3\textwidth]{figures/SARAS}
        \includegraphics[height=0.3\textwidth]{figures/REACH_2.jpg}
        \end{itemize}
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT: combining particle physics \& cosmological data}
    \begin{columns}
        \column{0.52\textwidth}
        \begin{itemize}
            \item Multinational team of particle physicists, cosmologists and statisticians.
            \item Combine cosmological data, particle colliders, direct detection, \& neutrino detectors in a statistically principled manner~\arxiv{2205.13549}.
            \item Lead Cosmo/Dark Matter working group~\arxiv{2009.03286}.
            \item Nested sampling used for global fitting, and fine-tuning quantification~\arxiv{2101.00428}
        \end{itemize}
        \begin{center}
            \includegraphics[width=0.5\textwidth]{figures/gambit_logo.png}
        \end{center}
        \column{0.48\textwidth}
        \vspace{-40pt}
        \includegraphics[width=\textwidth]{figures/ALP_2.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Likelihood-free inference (aka SBI)}
\student{kilian_scheutwinkel}{Kilian Scheutwinkel}{PhD}

    \vspace{10pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item How do you do inference if you don't know the likelihood $P(D|\theta)$?
                \begin{itemize}
                    \item e.g.\ if you can simulate a disease outbreak, how can you infer a posterior on $R_0$, or select the most predictive model?
                \end{itemize}
            \item If you can forward simulate/model $\theta\to D$, then you have an implicit likelihood.
            \item LFI aims to (machine-)\emph{learn} the likelihood from forward simulations $\{(\theta,D)\}$.
            \item Nested sampling has much to offer
                \begin{itemize}
                    \item truncation strategies (\texttt{PolySwyft})
                    \item evidence driven compression
                    \item marginalised machine learning
                \end{itemize}
            \item In my view, LFI represents the future of inference -- in twenty years time this will be as well-used as MCMC techniques are today.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/noisy.pdf}
        \includegraphics[width=\textwidth]{figures/three_ways_II.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{unimpeded: PLA for the next generation}
\student{harry_bevins}{Harry Bevins}{PhD$\to$JRF}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item DiRAC 2020 RAC allocation of 30MCPUh
            \item Main goal: Planck Legacy Archive equivalent
            \item Parameter estimation $\to$ Model comparison
            \item MCMC $\to$ Nested sampling
            \item Planck $\to$ $\{\text{Planck}, \text{DESY1}, \text{BAO}, \ldots \}$
            \item Pairwise combinations
            \item Suite of tools for processing these 
                \begin{itemize}
                    \item \texttt{anesthetic} $2.0$
                    \item \texttt{unimpeded} $1.0$
                    \item \texttt{zenodo} archive
                    \item \texttt{margarine}
                \end{itemize}
            \item MCMC chains also available.
            \item Library of bijectors emulators for fast re-use
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{logos/dirac}
        \includegraphics[width=0.5\textwidth,page=21]{figures/himmelblau}%
        \includegraphics[width=0.5\textwidth,page=15]{figures/himmelblau}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{CosmoTension}
    \framesubtitle{Resolving cosmological tensions with diverse data, novel theories and Bayesian machine learning}
\student{will_barker}{Will Barker}{PhD$\to$JRF}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item ERC grant $\Rightarrow$ UKRI Frontier, commencing 2023.
            \item Funds 3 PDRAs and 4 PhDs over 5 years.
            \item Research programme centered around combining novel theories of gravity, Boltzmann solvers~\arxiv{1906.01421}, reconstruction~\arxiv{1908.00906}, nested sampling \& likelihood free inference.
            \item Aims to disentangle cosmological tensions $H_0$, $\sigma_8$, $\Omega_K$ with next-generation data analysis techniques.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/denseoutput.pdf}
        \includegraphics[width=\textwidth]{figures/pps.pdf}
        \includegraphics[width=\textwidth]{figures/gantt.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\href{https://www.github.com/handley-lab}{github.com/handley-lab}}
    \tikz[overlay,remember picture]
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
        \includegraphics[width=0.09\textheight]{figures/students/adam_ormondroyd.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/david_yallup.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/dily_ong.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/felicity_ibrahim.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/george_carter.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/harry_bevins.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/ian_roque.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/kilian_scheutwinkel.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/metha_prathaban.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/namu_kroupa.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sinah_legner.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/thomas_gessey-jones.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/tze_goh.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/wei-ning_deng.jpg}%
    };
    \vspace{-0.1\textheight}
    \begin{columns}
        \column{0.65\textwidth}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Performing Bayesian inference and model comparison.
            \end{itemize}
        \item It is applied widely across cosmology, particle physics \& machine learning.
        \item It's unique traits as the only numerical Lebesgue integrator mean with compute it will continue to grow in importance.
    \end{itemize}
        \column{0.35\textwidth}
    \includegraphics[height=0.6\textwidth]{figures/pps_both}%
    \includegraphics[height=0.6\textwidth]{figures/REACH_2}%
    \end{columns}
    \includegraphics[height=0.2\textwidth]{figures/integration}%
    \includegraphics[height=0.2\textwidth]{figures/curvature_3}%
    \includegraphics[height=0.2\textwidth]{figures/ALP_3}%
\end{frame}


\appendix
\begin{frame}
    \frametitle{How does Nested Sampling compare to other approaches?}
    \begin{columns}
        \column{0.7\textwidth}
        \begin{itemize}
            \item In all cases:
                \begin{itemize}
                    \item[$+$] NS can handle multimodal functions
                    \item[$+$] NS computes evidences, partition functions and integrals
                    \item[$+$] NS is self-tuning/black-box
                \end{itemize}
        \end{itemize}
        \column{0.3\textwidth}
        Modern Nested Sampling algorithms can do this in $\sim\mathcal{O}(100s)$ dimensions
    \end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \begin{itemize}
                \item Gradient descent
                    \begin{itemize}
                        \item[$-$] NS cannot use gradients
                        \item[$+$] NS does not require gradients
                    \end{itemize}
                \item Genetic algorithms
                    \begin{itemize}
                        \item[$+$] NS discarded points have statistical meaning
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Sampling}
            \begin{itemize}
                \item Metropolis-Hastings?
                    \begin{itemize}
                        \item[$-$] Nothing beats well-tuned customised MH
                        \item[$+$] NS is self tuning
                    \end{itemize}
                \item Hamiltonian Monte Carlo?
                    \begin{itemize}
                        \item[$-$] In millions of dimensions, HMC is king
                        \item[$+$] NS does not require gradients
                    \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \begin{itemize}
                \item Thermodynamic integration
                    \begin{itemize}
                        \item[$+$] protective against phase trasitions
                        \item[$+$] No annealing schedule tuning 
                    \end{itemize}
                \item Sequential Monte Carlo
                    \begin{itemize}
                        \item[$-$] SMC experts classify NS as a kind of SMC
                        \item[$+$] NS is athermal
                    \end{itemize}
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested Sampling: a user's guide}
    \begin{enumerate}
        \item Nested sampling is a likelihood scanner, rather than posterior explorer.
            \begin{itemize}
                \item This means typically most of its time is spent on burn-in rather than posterior sampling.
                \item Changing the stopping criterion from $10^{-3}$ to $0.5$ does little to speed up the run, but can make results very unreliable.
            \end{itemize}
        \item The number of live points $n_\text{live}$ is a resolution parameter.
            \begin{itemize}
                \item Run time is linear in $n_\text{live}$, posterior and evidence accuracy goes as $\frac{1}{\sqrt{n_\text{live}}}$.
                \item Set low for exploratory runs $\sim\mathcal{O}(10)$ and increased to $\sim\mathcal{O}(1000)$ for production standard.
            \end{itemize}
        \item Most algorithms come with additional reliability parameter(s).
            \begin{itemize}
                \item e.g. \texttt{MultiNest}: $\texttt{eff}$, \texttt{PolyChord}: $n_\text{repeats}$.
                \item These are parameters which have no gain if set too conservatively, but increase the reliability.
                \item Check that results do not degrade if you reduce them from defaults, otherwise increase.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Time complexity of nested sampling}
    \includegraphics[width=\textwidth]{figures/run_prodecure}
    \vspace{-20pt}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-20pt}
    \begin{itemize}
        \item $x$-axis: log-compression of live points
        \item Area $\propto$ posterior mass
        \item Shows Bayesian balance of likelihood vs prior
        \item Run proceeds right to left
        \item Run finishes after bump (typical set)
    \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Time complexity\vspace{-5pt}
                \[ \boxed{T = n_\mathrm{live} \times T_\mathcal{L} \times T_\mathrm{sampler} \times \mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)}\vspace{-5pt}  \]
            \item Error complexity $\boxed{\sigma \propto \sqrt{\mathcal{D}_\mathrm{KL}(\mathcal{P}||\pi)/n_\mathrm{live}}}$
        \end{itemize}
    \end{columns}

\end{frame}




\begin{frame}
    \frametitle{Occam's Razor~\arxiv{2102.11511}}
    \begin{itemize}
        \item Bayesian inference quantifies Occam's Razor:
            \begin{itemize}
                \item \textit{``Entities are not to be multiplied without necessity''} \hfill --- William of Occam
                \item \textit{``Everything should be kept as simple as possible, but not simpler''} \hfill --- ``Albert Einstein''
            \end{itemize}
        %\item Consider the Evidence $\C[3]{\mathcal{Z}\equiv P(D|M)}$: 
        %    \begin{description}[Parameter estimation]
        %        \item [Parameter estimation] normalisation constant
        %        \item [Model comparison] critical update factor for \C[5]{model prior} to \C[4]{model posterior}
        %    \end{description}
        \item Properties of the evidence: rearrange Bayes' theorem for parameter estimation
            \[\C[0]{\mathcal{P}(\theta)} = \frac{\C[2]{\mathcal{L}(\theta)} \C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}}} \qquad\Rightarrow\qquad \C[3]{\log \mathcal{Z}} = \C[2]{\log\mathcal{L}(\theta)} - \log \frac{\C[0]{\mathcal{P}(\theta)}}{\C[1]{\pi(\theta)}}.\]  
        \item Evidence is composed of a ``goodness of fit'' term  and ``Occam Penalty''.
    \end{itemize}
    \begin{columns}[t]
        \column{0.5\textwidth}
    \begin{itemize}
        \item RHS true for all $\theta$. Take max likelihood value $\theta_*$:
            \[
                \log \mathcal{Z} = -\chi_\text{min}^2 - \text{Mackay penalty.}
            \]
    \end{itemize}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Be more Bayesian and take posterior average to get the ``Occam's razor equation''
            \[
                \boxed{
                    \log \mathcal{Z} = \av[\mathcal{P}]{\log\mathcal{L}} - \mathcal{D}_\text{KL}.
            }
            \]
    \end{itemize}
    \end{columns}
    \vfill
    \begin{itemize}
        \item Natural regularisation which penalises models with too many parameters.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Kullback Liebler divergence}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is is defined as:
                \[\mathcal{D}_\text{KL} = \av[\mathcal{P}]{\log\frac{\mathcal{P}}{\pi}} = \int \mathcal{P}(\theta) \log \frac{\mathcal{P}(\theta)}{\pi(\theta)}d\theta.\]
            \item Whilst not a distance, $\mathcal{D}=0$ when $\mathcal{P}=\pi$.
            \item Occurs in the context of machine learning as an objective function for training functions.
            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
                \[ \mathcal{D}_\text{KL} \approx \log \frac{V_\pi}{V_\text{P}}.\]
                (this is exact for top-hat distributions).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Nested sampling for frequentist computation!?
            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
            \item Need insight/approximation to make efficient.
            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
    \end{columns}
    
\end{frame}

\begin{frame}
    \frametitle{Exploration of phase space~\arxiv{2106.02056}}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Nested sampling for cross section computation/event generation.
            \item Numerically compute collisional cross section 
                \vspace{-5pt}\[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2,\]
                $\Omega$ phase space of kinematic configurations $\Phi$, each with matrix element $\mathcal{M}(\Phi)$.
            \item Current state of the art e.g. HAAG (improvement on RAMBO) requires knowledge of $\mathcal{M}(\Phi)$.
            \item Nested sampling can explore the phase space and compute integral blind with comparable efficiency.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1.pdf}
        \includegraphics[width=\textwidth]{figures/phase_space.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Quantification of fine tuning~\arxiv{2101.00428}~\arxiv{2205.13549}}
    \vspace{-10pt}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Example: Cosmological constraints on decaying axion-like particles~\arxiv{2205.13549}.
            \item Subset of parameters $\xi,m_a,\tau,g_{a\gamma}$: ALP fraction, mass, lifetime and photon coupling.
                {(\small Also vary cosmology, $\tau_n$ and nuisance params)}
            \item Data: CMB, BBN, FIRAS, SMM, BAO.
            \item Standard profile likelihood fit shows ruled out regions and best-fit point.
            \item<2-> Nested sampling scan:
                \begin{itemize}
                    \item Quantifies amount of parameter space ruled out with Kullback-Liebler divergence $\mathcal{D}_\mathrm{KL}$.
                    \item Identifies best fit region as statistically irrelevant from information theory/Bayesian.
                    \item No evidence for decaying ALPs. Fit the data equally well: but more constrained parameters create Occam penalty.
                \end{itemize}
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/ALP_1.pdf}
        \includegraphics<2          >[width=\textwidth]{figures/ALP_2.pdf}
        \includegraphics<3|handout:0>[width=\textwidth]{figures/ALP_3.pdf}
    \end{columns}
    
\end{frame}

\end{document}
```
# branch: remotes/origin/imperial_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}

% Commands
% --------
% - \arxiv{arxiv number}
% - \arxiv{<number>}            arxiv.org/abs/<number>
% - \oldarxiv{<arxiv number>}   arxiv.org/<number>
% - \doi{<doi>}                 doi.org/<doi>
% - \xkcd{<number>}             xkcd.com/<number>
% - \email{<email>}             <<email>>
% - \tthref{<website>}          <website>
% - \av[dist]{<quantity>}       <quantity>_{dist}
% - \student{<name>}{<detail>}{<photo>}

% Talk details
% ------------
\title{\texttt{lsbi}: linear simulation based inference }
\date{11\textsuperscript{th} September 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Context from phystat}
    \begin{itemize}
        \item From \textbf{Jesse Thaler}'s talk on Interpretable Machine Learning:
            \begin{quote}
                ``If asked what is the most under-used Machine Learning technique in physics\ldots\\ \hfill\ldots my answer is only half-jokingly \textbf{linear regression}.''
            \end{quote}
        \item sbi mentioned in (so far)
            \begin{itemize}
                \item Monday
                    \begin{description}
                        \item[Ben Wandelt] Cosmology and machine learning¶
                        \item[Maximilian Dax] Simulation-based machine learning for gravitational-wave analysis
                        \item[Andre Scaffidi] Anomaly aware machine learning for dark matter direct detection at the DARWIN experiment
                        \item[Joshua Villarrea] Feldman-Cousins’ ML Cousin
                    \end{description}
                \item Tuesday
                    \begin{description}
                        \item[Aishik Ghosh] Simulation-based Inference (SBI)
                    \end{description}
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Who?}
    \begin{columns}
        \column{0.8\textwidth}
        Idea I've been working on/talking about for the better part of 18 months,
        \begin{itemize}
            \item Nicolas Mediato Diaz (MSci project)
            \item David Yallup (Postdoc)
            \item Thomas Gessey Jones (Postdoc)
        \end{itemize}

        Many others have also presented this idea independently
        \begin{itemize}
            \item \texttt{SELFI} incorporates much of this idea: Leclercq~\arxiv{1902.10149}
            \item some of these ideas are in \texttt{MOPED}: Heavens~\oldarxiv{astro-ph/9911102}
            \item Also appears in Häggström~\arxiv{2403.07454}
        \end{itemize}

        \column{0.2\textwidth}
        \includegraphics[width=\textwidth]{people/david_yallup}
        \includegraphics[width=\textwidth]{people/thomas_gessey-jones}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{SBI: Simulation-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item What do you do if you don't know \C[2]{$\mathcal{L}(D|\theta)$}?
            \item If you have a simulator/forward model $\theta \rightarrow D$
                defines an \C[2]{\emph{implicit} likelihood~$\mathcal{L}$}.
            \item Simulator generates samples from $\C[2]{\mathcal{L}(\cdot|\theta)}$.
            \item With a \C[1]{prior $\pi(\theta)$} can generate samples from \C[4]{joint distribution}~$\C[4]{\mathcal{J}(\theta,D)}=\C[2]{\mathcal{L}(D|\theta)}\C[1]{\pi(\theta)}$\\\hfill \emph{the ``probability of everything''}.
            \item Task of SBI is take joint~$\C[4]{\mathcal{J}}$ samples and learn \C[0]{posterior $\mathcal{P}(\theta|D)$},  and \C[3]{evidence $\mathcal{Z}(D)$} \\ or even \C[2]{likelihood~$\mathcal{L}(D|\theta)$} or \C[4]{joint~$\mathcal{J}(\theta,D)$}.
            \item Present state of the art achieves this using \emph{machine learning} (neural networks).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[page=1, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<2|handout:0>[page=2, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<3|handout:0>[page=3, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<4|handout:0>[page=4, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<5|handout:0>[page=5, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<6|handout:0>[page=6, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<7|handout:0>[page=7, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<8|handout:0>[page=8, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<9|handout:0>[page=9, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<10|handout:0>[page=10, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<11|handout:0>[page=11, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<12|handout:0>[page=12, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<13|handout:0>[page=13, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<14|handout:0>[page=14, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<15|handout:0>[page=15, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<16|handout:0>[page=16, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<17|handout:0>[page=17, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<18|handout:0>[page=18, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<19|handout:0>[page=19, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<20|handout:0>[page=20, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<21>[page=21, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why \textit{linear} SBI?}
    If neural networks are all that, why should we consider the regressive step of going back to linear versions of this problem?

    \begin{itemize}
        \item It is pedagogically helpful 
            \begin{itemize}
                \item separates general principles of SBI from the details of neural networks
                \item (particularly for ML skeptics)
            \end{itemize}
        \item It is practically useful
            \begin{itemize}
                \item for producing expressive examples with known ground truths. 
            \end{itemize}
        \item It is pragmatically useful
            \begin{itemize}
                \item competitive with neural approaches in terms of accuracy,
                \item faster and more interpretable. 
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Linear Simulation Based Inference}
    \framesubtitle{Mathematical setup}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item Linear generative model $(m,M,C)$
                \[ 
                    \only<1>{D = m + M \theta \pm \sqrt{C}}%
                    \only<2>{D \sim \mathcal{N}(m + M \theta, C)}%D \sim 
                    %\only<3>{p = \mathcal{N}(D|m + M \theta, C)}
                \]%
                where:
                \begin{description}
                    \item[$\theta$]: $n$ dimensional parameters
                    \item[$D$]: $d$ dimensional data
                    \item[$M$]: $d\times n$ transfer matrix
                    \item[$m$]: $d$-dimensional shift
                    \item[$C$]:  $d\times d$ data covariance
                \end{description}
        \end{itemize}

        \pause
        \column{0.5\textwidth}
        \begin{itemize}
            \item $k$ Simulations 
                \[
                    S=\{ (\theta_i,D_i): i=1,\ldots,k\}
                \]
            \item Define simulation statistics\footnotemark:

                \begin{description}
                    \item[$\bar\theta$] $= \tfrac{1}{k}\sum_k \theta_i$
                    \item[$\bar D$] $= \tfrac{1}{k}\sum_k D_i$
                    \item[$\Theta$] $=\tfrac{1}{k-1}\sum_i (\theta_i-\bar\theta)(\theta_i-\bar\theta)'$
                    \item[$\Delta$] $=\tfrac{1}{k-1}\sum_i (D_i-\bar D)(D_i-\bar D)'$
                    \item[$\Psi$] $=\tfrac{1}{k-1}\sum_i (D_i-\bar D)(\theta_i-\bar \theta)'$
                \end{description}
        \end{itemize}
    \end{columns}
    \footnotetext[1]{N.B. using matrix variate notation where primes denote transposes $M' = M^T$}
\end{frame}

\begin{frame}
    \frametitle{Linear Simulation Based Inference: gory mathematical details}
    \begin{columns}
        \column{0.7\textwidth}
        \begin{itemize}
            \item We now wish to infer the parameters of the linear model $(m,M,C)$ from simulations $S$ (which define $\bar\theta,\bar D, \Theta, \Delta, \Psi$)
            \item The likelihood for this problem is:
                \[P(\{D_i\}|\{\theta_i\}|m, M, C) = \prod_i \mathcal{N}(D_i|m+M\theta_i,C)\]
            \item It can be shown the \C[0]{posterior $\mathcal{P}$} is\ldots
                \onslide<3>{
                    \begin{align*}
                        m&|M,C,S \sim \mathcal{N}(\tfrac{k}{k+1}(\bar D - M \bar \theta), \tfrac{C}{k+1}), \\
                        M&|C,S \sim \mathcal{MN}(\Psi \Theta^{-1}_*, \tfrac{C}{k-1}, \Theta^{-1}_*), \\
                        C&|S\sim \mathcal{W}^{-1}_\nu(C_0 + (k-1)(\Delta-\Psi \Theta^{-1} \Psi')),
                    \end{align*}%
                    where $\Theta_* = \tfrac{1}{k-1}\Theta_0 + \Theta$, $\nu=\nu_0+k$, and $C_0$ define conjugate \C[2]{prior $\pi$} on $m,M,C$
                }
        \end{itemize}


        \column{0.3\textwidth}
        \includegraphics<2->[width=\textwidth]{figures/matrix_variate_distributions.jpg}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Sequential LSBI}
    \begin{itemize}
        \item As we shall see, for non-linear problems, a linear approximation is unlikely to be a good one.
        \item Sequential methods iteratively improve by focussing effort around observed data~$D_\text{obs}$.
            \begin{itemize}
                \item This is orthogonal to amortised approaches
                \item More appropriate to cosmology, where there is only one dataset
                \item Less appropriate to particle physics/GW
            \end{itemize}
        \item We are free to choose where to place simulation parameters $\{\theta_i\}$, so it makes sense to choose these so that they generate simulations close to the observed data
        \item Our current approximation to the posterior is a natural choice.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Example of this on our toy model}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item<1-> Same model as before
            \item<2-> Mark the observed data~$D_\text{obs}$
            \item<3-> Fit a model using \texttt{lsbi}
            \item<4-> Evaluate the posterior (cheap as linear)
            \item<5-> Now use this posterior to pick $\{\theta_i\}$
            \item<5-> Generate $\{D_i\}$ from original simulator
            \item<6-> Fit \texttt{lsbi} to these
            \item<7-> Evaluate the new posterior
            \item<8-> Iterate
        \end{itemize}

        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth, page=1]{figures/lsbi_plot.pdf}%
        \includegraphics<2|handout:0>[width=\textwidth, page=2]{figures/lsbi_plot.pdf}%
        \includegraphics<3|handout:0>[width=\textwidth, page=3]{figures/lsbi_plot.pdf}%
        \includegraphics<4|handout:0>[width=\textwidth, page=4]{figures/lsbi_plot.pdf}%
        \includegraphics<5|handout:0>[width=\textwidth, page=5]{figures/lsbi_plot.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth, page=6]{figures/lsbi_plot.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth, page=7]{figures/lsbi_plot.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth, page=8]{figures/lsbi_plot.pdf}%
        \includegraphics<9|handout:0>[width=\textwidth, page=9]{figures/lsbi_plot.pdf}%
        \includegraphics<10|handout:0>[width=\textwidth, page=10]{figures/lsbi_plot.pdf}%
        \includegraphics<11|handout:0>[width=\textwidth, page=11]{figures/lsbi_plot.pdf}%
        \includegraphics<12|handout:0>[width=\textwidth, page=12]{figures/lsbi_plot.pdf}%
        \includegraphics<13|handout:0>[width=\textwidth, page=13]{figures/lsbi_plot.pdf}%
        \includegraphics<14|handout:0>[width=\textwidth, page=14]{figures/lsbi_plot.pdf}%
        \includegraphics<15|handout:0>[width=\textwidth, page=15]{figures/lsbi_plot.pdf}%
        \includegraphics<16>[width=\textwidth, page=16]{figures/lsbi_plot.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Example of this on the CMB}
    \begin{columns}
        \column{0.52\textwidth}
        \begin{itemize}
            \item Now apply this to a ``real'' cosmology example, inferring $\Lambda$CDM from the CMB
            \item Unfortunately generative planck likelihoods do not exist yet
            \item Consider a cosmic-variance limited, temperature-only, full sky CMB experiment with no foregrounds
            \item This is a $n=6$, $d=2500$ non-linear problem
                \begin{itemize}
                    \item No compression needed
                \end{itemize}
            \item Apply the above procedure
            \item Slight bias these results, but this can be fixed by marginalising over $m,M,C$, rather than taking point estimates.
        \end{itemize}

        \column{0.48\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/cosmo_update.pdf}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/cosmo_update.pdf}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/cosmo_update.pdf}%
        \includegraphics<4|handout:0>[width=\textwidth,page=4]{figures/cosmo_update.pdf}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/cosmo_update.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/cosmo_update.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/cosmo_update.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth,page=8]{figures/cosmo_update.pdf}%
        \includegraphics<9|handout:0>[width=\textwidth,page=9]{figures/cosmo_update.pdf}%
        \includegraphics<10|handout:0>[width=\textwidth,page=10]{figures/cosmo_update.pdf}%
        \includegraphics<11|handout:0>[width=\textwidth,page=11]{figures/cosmo_update.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{\texttt{lsbi}: linear simulation based inference}
    \framesubtitle{Code details}
    \begin{itemize}
        \item \texttt{lsbi} is a pip-installable python package
        \item it extends \texttt{scipy.stats.multivariate\_normal}
            \begin{itemize}
                \item vectorised distributions with (broadcastable) arrays of \texttt{mean} and \texttt{cov}
                \item \texttt{.marginalise(...)} and \texttt{.condition(...)} methods
                \item Plotting functionality
            \end{itemize}
        \item Implements \texttt{LinearModel} class with \texttt{.prior()}, \texttt{.likelihood(theta)}, \texttt{.posterior(D)} \& \texttt{.evidence()} methods which return distributions
        \item Also implement \texttt{MixtureModel}
        \item Under active develpoment
            \begin{itemize}
                \item Open source
                \item Continuous integration
            \end{itemize}
        \item \tthref{github.com/handley-lab/lsbi}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Where next?}
    \begin{itemize}
        \item Paper being written up 
            \begin{itemize}
                \item soft deadline for Nicolas' MPhil start in October
                \item hard deadline for PhD applications
            \end{itemize}
        \item Include realistic CMB simulation effects (foregrounds)
        \item Extend to more examples (BAO, SNe)
        \item How does LSBI contribute to the question of compression
        \item Explore limits of $d$ and $n$
        \item Explore mixture modelling for real nonlinear effects
        \item If the posterior is the answer, what is the question?
        \item Importance sampling?
        \item Model comparison?
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\tthref{github.com/handley-lab}}
    \tikz[overlay,remember picture]
    \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
        \includegraphics[width=0.10\textheight]{people/adam_ormondroyd.jpg}%
        \includegraphics[width=0.10\textheight]{people/charlotte_priestley.jpg}%
        \includegraphics[width=0.10\textheight]{people/david_yallup.jpg}%
        \includegraphics[width=0.10\textheight]{people/dily_ong.jpg}%
        \includegraphics[width=0.10\textheight]{people/george_carter.jpg}%
        \includegraphics[width=0.10\textheight]{people/harry_bevins.jpg}%
        \includegraphics[width=0.10\textheight]{people/kilian_scheutwinkel.jpg}%
        \includegraphics[width=0.10\textheight]{people/metha_prathaban.jpg}%
        \includegraphics[width=0.10\textheight]{people/namu_kroupa.jpg}%
        \includegraphics[width=0.10\textheight]{people/sinah_legner.jpg}%
        \includegraphics[width=0.10\textheight]{people/sam_leeney.jpg}%
        \includegraphics[width=0.10\textheight]{people/wei-ning_deng.jpg}%
        \includegraphics[width=0.10\textheight]{people/will_handley.jpg}%
    };
    \begin{itemize}
        \item \textbf{Introduction to \texttt{lsbi}:} A linear simulation-based inference method developed over 18 months by the speaker and collaborators.
        \item \textbf{Benefits of Linear SBI:} Pedagogical value, practical examples with known ground truths, competitive accuracy, speed, and interpretability compared to neural networks.
        \item \textbf{Mathematical Setup:} Uses a linear generative model to fit simulation data and iteratively refine posterior estimations, demonstrated through toy and cosmology examples.
        \item \textbf{\texttt{lsbi} Python Package:} Extends \texttt{scipy.stats.multivariate\_normal} with functionalities for marginalization, conditioning, and plotting; under active development and open source.
        \item \textbf{Future Directions:} Include realistic CMB simulations, extend to other examples (BAO, SNe), explore parameter limits, mixture modeling, and integrate importance sampling and model comparison.
    \end{itemize}
\end{frame}

\end{document}
```
# branch: remotes/origin/india_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usetikzlibrary{arrows,arrows.meta,automata,positioning}

% Commands
% --------
% - \arxiv{arxiv number}
% - \arxiv{<number>}            arxiv.org/abs/<number>
% - \oldarxiv{<arxiv number>}   arxiv.org/<number>
% - \doi{<doi>}                 doi.org/<doi>
% - \xkcd{<number>}             xkcd.com/<number>
% - \email{<email>}             <<email>>
% - \tthref{<website>}          <website>
% - \av[dist]{<quantity>}       <quantity>_{dist}
% - \student{<name>}{<detail>}{<photo>}

% Talk details
% ------------
\title{\texttt{PolySwyft}}
\subtitle{a sequential simulation-based nested sampler}
\date{30\textsuperscript{th} October 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Contents}
    \begin{columns}
        \column{0.77\textwidth}
        \begin{enumerate}
            \item Likelihood- vs Simulation-based inference (LBI vs SBI) 
            \item Neural Ratio estimation (NRE)
            \item Nested sampling (NS)
            \item NS+NRE
            \item Future prospects
        \end{enumerate}
        Stems from over a year of discussion, with the majority of the work done by Kilian Scheutwinkel (PhD student).
        \column{0.23\textwidth}
        \begin{overpic}[width=\textwidth]{people/kilian_scheutwinkel.jpg}
            \put(5,5) {\textcolor{white}{Kilian Scheutwinkel}}
        \end{overpic}
        \begin{overpic}[width=\textwidth]{people/christoph_weniger.jpg}
            \put(10,5) {\textcolor{white}{Christoph Weniger}}
        \end{overpic}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{LBI: Likelihood-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        The standard approach if you are fortunate enough to have a likelihood function $\only<1-2>{P(D|\theta)}\only<3->{\C[2]{\mathcal{L}(D|\theta)}}$: 
        \[
            \only<1-2>{
                P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}
            }
            \only<2>{
                \qquad
                \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}}\times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}
            }
            \only<3>{
                \C[0]{\mathcal{P}(\theta|D)} = \frac{\C[2]{\mathcal{L}(D|\theta)}\C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}(D)}}
                \qquad
                \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}}\times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}
            }
            \only<4>{
                P(\theta|D) P(D) = P(\theta,D) = P(D|\theta)P(\theta), \qquad
            }
            \only<5>{
                \C[0]{\mathcal{P}}\times\C[3]{\mathcal{Z}} = \C[4]{\mathcal{J}} = \C[2]{\mathcal{L}}\times\C[1]{\pi}, \qquad \C[4]{\text{Joint}} = \C[4]{\mathcal{J}} = P(\theta,D)
            }
        \]
        \vspace{-10pt}
        \begin{enumerate}
            \item Define \C[1]{prior $\pi(\theta)$} 
                \begin{itemize}
                    \item spend some time being philosophical
                \end{itemize}
            \item Sample \C[0]{posterior $\mathcal{P}(\theta|D)$} 
                \begin{itemize}
                    \item use out-of-the-box MCMC tools such as\\ \texttt{emcee} or \texttt{MultiNest}
                    \item make some triangle plots
                \end{itemize}
            \item Optionally compute \C[3]{evidence $\mathcal{Z}(D)$}
                \begin{itemize}
                    \item e.g. nested sampling or parallel tempering
                    \item do some model comparison (i.e. science)
                    \item talk about tensions
                \end{itemize}
        \end{enumerate}
        \column{0.5\textwidth}
        \hfill%
        \begin{overpic}[width=0.6\textwidth]{figures/des_parameters.pdf}
            \put(-40,90) {DES Y5 SN Ia}
            \put(-40,80) {\arxiv{2401.02929}}
        \end{overpic}
        \includegraphics[width=0.5\textwidth]{figures/des_model_comparison.pdf}%
        \includegraphics[width=0.5\textwidth]{figures/des_suspiciousness.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{SBI: Simulation-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item What do you do if you don't know \C[2]{$\mathcal{L}(D|\theta)$}?
            \item If you have a simulator/forward model $\theta \rightarrow D$
                defines an \C[2]{\emph{implicit} likelihood~$\mathcal{L}$}.
            \item Simulator generates samples from $\C[2]{\mathcal{L}(\cdot|\theta)}$.
            \item With a \C[1]{prior $\pi(\theta)$} can generate samples from \C[4]{joint distribution}~$\C[4]{\mathcal{J}(\theta,D)}=\C[2]{\mathcal{L}(D|\theta)}\C[1]{\pi(\theta)}$\\\hfill \emph{the ``probability of everything''}.
            \item Task of SBI is take \C[4]{joint~$\mathcal{J}$} samples and learn \C[0]{posterior $\mathcal{P}(\theta|D)$},  and \C[3]{evidence $\mathcal{Z}(D)$} \\ or even \C[2]{likelihood~$\mathcal{L}(D|\theta)$} or \C[4]{joint~$\mathcal{J}(\theta,D)$}.
            \item Present state of the art achieves this using \emph{machine learning} (neural networks).
                \begin{itemize}
                    \item My group's research tries to removes machine learning \tthref{github.com/handley-lab/lsbi}.
                \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[page=1, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<2|handout:0>[page=2, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<3|handout:0>[page=3, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<4|handout:0>[page=4, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<5|handout:0>[page=5, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<6|handout:0>[page=6, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<7|handout:0>[page=7, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<8|handout:0>[page=8, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<9|handout:0>[page=9, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<10|handout:0>[page=10, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<11|handout:0>[page=11, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<12|handout:0>[page=12, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<13|handout:0>[page=13, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<14|handout:0>[page=14, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<15|handout:0>[page=15, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<16|handout:0>[page=16, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<17|handout:0>[page=17, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<18|handout:0>[page=18, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<19|handout:0>[page=19, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<20|handout:0>[page=20, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<21>[page=21, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Why SBI?}
    \begin{columns}
        \column{0.6\textwidth}
        SBI is useful because:
        \begin{enumerate}
            \item If you don't have a likelihood, you can still do inference
                \begin{itemize}
                    \item This is the usual case beyond CMB cosmology
                \end{itemize}
            \item Faster than LBI
                \begin{itemize}
                    \item emulation -- also applies to LBI in principle
                \end{itemize}
            \item No need to pragmatically encode fiducial cosmologies
                \begin{itemize}
                    \item Covariance computation implicitly encoded in simulations
                    \item Highly relevant for disentangling tensions \& systematics
                \end{itemize}
            \item Equips AI/ML with Bayesian interpretability
            \item Lower barrier to entry than LBI
                \begin{itemize}
                    \item Much easier to forward model a systematic
                    \item Emerging set of plug-and-play packages
                    \item For this reason alone, it will come to dominate scientific inference
                \end{itemize}
        \end{enumerate}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/sbi_screenshot}
        \href{https://github.com/sbi-dev}{github.com/sbi-dev}
        \includegraphics[width=\textwidth]{figures/swyft_screenshot}
        \href{https://github.com/undark-lab/swyft}{github.com/undark-lab/swyft}
        \includegraphics[width=\textwidth]{figures/selfi_screenshot}
        \href{https://github.com/florent-leclercq/pyselfi}{github.com/florent-leclercq/pyselfi}
        \includegraphics[width=\textwidth]{figures/delfi_screenshot}
        \href{https://github.com/justinalsing/pydelfi}{github.com/justinalsing/pydelfi}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why aren't we currently using SBI in cosmology?}
    \begin{columns}
        \column{0.4\textwidth}
        \begin{itemize}
            \item Short answer: we are!
                \begin{itemize}
                    \item Mostly for weak lensing
                    \item 2024 has been the year it has started to be applied to real data.
                \end{itemize}
            \item Longer answer: SBI requires mock data generation code
            \item Most data analysis codes were built before the generative paradigm.
            \item It's still a lot of work to upgrade cosmological likelihoods  to be able to do this (e.g.\ \texttt{plik} \& \texttt{camspec}).
        \end{itemize}
        \column{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/sbi_papers/clusters.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/des.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/gw.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/center.pdf}
        \column{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/sbi_papers/kidsdes.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/kids.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/population.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/simbig.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why aren't we currently using SBI in 21cm cosmology?}
    \begin{itemize}
        \item Search for \texttt{21} in list of SBI references:~\tthref{github.com/smsharma/awesome-neural-sbi}
            \begin{enumerate}
                \item \arxiv{2203.15734} Implicit Likelihood Inference of Reionization Parameters from the 21 cm Power Spectrum, \textbf{Xiaosheng Zhao, Yi Mao, Benjamin D. Wandelt}
                \item \arxiv{2303.07339} Constraining the X-ray heating and reionization using 21-cm power spectra with Marginal Neural Ratio Estimation \textbf{Anchal Saxena, et al}
                \item \arxiv{2305.03074} Exploring the likelihood of the 21-cm power spectrum with simulation-based inference, \textbf{David Prelogović, Andrei Mesinger}
                \item \arxiv{2310.17602} Simulation-based Inference of Reionization Parameters from 3D Tomographic 21 cm Light-cone Images -- II: Application of Solid Harmonic Wavelet Scattering Transform, \textbf{Xiaosheng Zhao, Yi Mao, Shifan Zuo, Benjamin D. Wandelt}
                \item \arxiv{2401.04174} Optimal, fast, and robust inference of reionization-era cosmology with the 21cmPIE-INN, \textbf{Benedikt Schosser, Caroline Heneka, Tilman Plehn}
                \item \arxiv{2403.14618} Simulation-Based Inference of the sky-averaged 21-cm signal from CD-EoR with REACH, \textbf{Anchal Saxena et al}
            \end{enumerate}
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Neural Ratio Estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item SBI flavours: {\small \hfill\tthref{github.com/sbi-dev/sbi}}
                {\small
                    \begin{description}
                        \item[NPE] Neural posterior estimation
                        \item[NLE] Neural likelihood estimation
                        \item[NJE] Neural joint estimation
                        \item[NRE] Neural ratio estimation
                    \end{description}
                }
            \item NRE recap:
                \begin{enumerate}
                    \item Generate joint samples $(\theta,D)\sim\C[4]{\mathcal{J}}$
                        \label{step:joint}
                        \begin{itemize}
                            \item \textit{straightforward if you have a simulator:\\ $\theta\sim\C[1]{\pi(\cdot)}$, $D\sim\C[2]{\mathcal{L}(\cdot|\theta)}$}
                        \end{itemize}
                    \item Generate separated samples $\theta\sim\C[1]{\pi}$, $D\sim\C[3]{\mathcal{Z}}$\label{step:sep}
                        \begin{itemize}
                            \item \textit{aside: can shortcut step~\ref{step:sep} by scrambling the $(\theta,D)$ pairings from step~\ref{step:joint}}
                        \end{itemize}
                    \item Train probabilistic classifier $p$ to distinguish whether $(\theta,D)$ came from $\C[4]{\mathcal{J}}$ or $\C[1]{\pi}\times\C[3]{\mathcal{Z}}$.
                    \item $\frac{p}{1-p} = \C[5]{r} = \frac{P(\theta,D)}{P(\theta)P(D)} 
                        =
                        \frac{\C[4]{\mathcal{J}}}{\C[1]{\pi}\times\C[3]{\mathcal{Z}}} = \frac{\C[2]{\mathcal{L}}}{\C[3]{\mathcal{Z}}} = \frac{\C[0]{\mathcal{P}}}{\C[1]{\pi}}$.
                    \item Use ratio $\C[5]{r}$ for parameter estimation $\C[0]{\mathcal{P}} = \C[5]{r}\C[1]\times {\pi}$
                \end{enumerate}
        \end{itemize}
        \column{0.5\textwidth}
        \only<1|handout:0>{
            \begin{tikzpicture}[node distance=1cm, every neuron/.style={circle, draw, minimum size=1cm},]
                \node[every neuron/.try] (j2)  {};
                \node[every neuron/.try, above left = 0cm and 0.5cm of j2] (theta) { $\theta$};
                \node[every neuron/.try, below left = 0cm and 0.5cm of j2] (D) { $D$};
                \node[every neuron/.try, above = 0.5cm of j2] (j1) {};
                \node[every neuron/.try, below = 0.5cm of j2] (j3) {};
                \node[every neuron/.try, above right = 0cm and 0.5cm of j2] (h1) {};
                \node[every neuron/.try, below right = 0cm and 0.5cm of j2] (h2) {};
                \node[every neuron/.try, right = 1.3cm of j2] (p) { $p$};
                \node[every neuron/.try, right = 0.5cm of p] (logr) { $\C[5]{r}$};
                \draw[-] (theta) -- (j1);
                \draw[-] (D) -- (j1);
                \draw[-] (theta) -- (j2);
                \draw[-] (D) -- (j2);
                \draw[-] (theta) -- (j3);
                \draw[-] (D) -- (j3);
                \draw[-] (j1) -- (h1);
                \draw[-] (j1) -- (h2);
                \draw[-] (j2) -- (h1);
                \draw[-] (j2) -- (h2);
                \draw[-] (j3) -- (h1);
                \draw[-] (j3) -- (h2);
                \draw[-] (h1) -- (p);
                \draw[-] (h2) -- (p);
                \draw[-] (p) -- (logr);
                \node[below =0.5cm of logr] {\includegraphics[page=22, width=0.5\textwidth]{figures/sbi_parameter_estimation.pdf}};
            \end{tikzpicture}
        }
        \only<2>{
            \begin{exampleblock}{Bayesian proof}
                \begin{itemize}
                    \item Let $M_{\C[4]{\mathcal{J}}}$: $(\theta,D)\sim\C[4]{\mathcal{J}}$, $M_{\C[1]{\pi}\C[3]{\mathcal{Z}}}$: $(\theta,D)\sim\C[1]{\pi}\times\C[3]{\mathcal{Z}}$
                    \item Classifier gives
                        ${p(\theta,D) = P(M_{\C[4]{\mathcal{J}}}|\theta,D) = 1- P(M_{\C[1]{\pi}\C[3]{\mathcal{Z}}}|\theta,D)}$
                    \item Bayes theorem then shows
                        ${\frac{p}{1-p}=\frac{P(M_{\C[4]{\mathcal{J}}}|\theta,D)}{P(M_{\C[1]{\pi}\C[3]{\mathcal{Z}}}|\theta,D)} = \frac{P(\theta,D|M_{\C[4]{\mathcal{J}}})P(M_{\C[4]{\mathcal{J}}})}{P(\theta,D|M_{\C[1]{\pi}\C[3]{\mathcal{Z}}})P(M_{\C[1]{\pi}\C[3]{\mathcal{Z}}})} = 
                        \frac{\C[4]{\mathcal{J}}}{\C[1]{\pi}\C[3]{\mathcal{Z}}}}$, \\
                        where we have assumed 
                        \begin{itemize}
                            \item $P(M_{\C[4]{\mathcal{J}}}) = P(M_{\C[1]{\pi}\C[3]{\mathcal{Z}}})$,
                        \end{itemize}
                        and by definition
                        \begin{itemize}
                            \item $\C[4]{\mathcal{J}(\theta,D)} = P(\theta,D|M_{\C[4]{\mathcal{J}}})$
                            \item $\C[1]{\pi(\theta)}\C[3]{\mathcal{Z}(D)} = P(\theta,D|M_{\C[1]{\pi}\C[3]{\mathcal{Z}}})$.
                        \end{itemize}
                \end{itemize}
            \end{exampleblock}
        }
        \only<3|handout:0>{
            \begin{block}{Why I like NRE}
                \begin{itemize}
                    \item The link between classification and inference is profound.
                    \item Density estimation is hard -- Dimensionless $r$ divides out the hard-to-calculate parts.
                \end{itemize}
            \end{block}
            \begin{block}{Why I don't like NRE}
                \begin{itemize}
                    \item Practical implementations require marginalisation~\arxiv{2107.01214}, or autoregression~\arxiv{2308.08597}.
                    \item Model comparison and parameter estimation are separate~\arxiv{2305.11241}.
                \end{itemize}
            \end{block}
        }
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{TMNRE: Truncated Marginal Neural Ratio Estimation}
    \framesubtitle{\texttt{swyft}: \tthref{github.com/undark-lab/swyft}}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Two tricks for practical NRE:
        \end{itemize}
        \begin{block}{Marginalisation}
            \begin{itemize}
                \item Only consider one or two parameters at a time.
                \item Fine if your goal is to produce triangle plots.
                \item Problematic if information is contained jointly in more than two parameters.
            \end{itemize}
        \end{block}
        \begin{block}{Truncation}
            \begin{itemize}
                \item focus parameters $\theta$ on a subset of the prior which reproduces observed data $D_\text{obs}$
                \item region is somewhat arbitrary (usually a box)
                \item not amortised, sounds a bit like ABC
            \end{itemize}
        \end{block}
        \column{0.45\textwidth}
        \begin{overpic}[width=\textwidth]{figures/tmnre}
            \put(70,0) {\arxiv{2111.08030}}
        \end{overpic}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling: numerical Lebesgue integration}
    \begin{columns}
        \column{0.5\textwidth}
        \fbox{\parbox{\textwidth}{
                \begin{itemize}
                    \item[0.] Start with $N$ random samples over the space.
                    \item[i.] Delete outermost sample, and replace with a new random one at higher integrand value.
        \end{itemize}}}
        \vspace{-5pt}
        \begin{itemize}
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item Discarded ``dead points'' can be weighted to form posterior, prior, or anything in between.
            \item Estimates the \textbf{density of states} and calculates evidences \& partition functions.
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune,
                    \item exploration of multimodal functions,
                    \item global and local optimisation.
                \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4|handout:0>[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<8|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        \includegraphics<9->[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \texttt{nautilus}~\arxiv{2306.16923} 
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.55\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267} \texttt{jaxnest}~\arxiv{2012.15286}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: dead points}
    \includegraphics[width=\textwidth]{figures/dead_measure}
    \begin{columns}
        \column{0.69\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Dead points have a unique scale-invariant distribution $\propto\: \tfrac{dV}{V}$.
            \item Uniform over original region, exponentially concentrating on region of interest (until termination volume).
            \item Good for training emulators (HERA~\arxiv{2108.07282}).
        \end{itemize}
        \column{0.3\textwidth}
        \begin{block}{Applications}
            \begin{itemize}
                \item training emulators.
                \item gridding simulations
                \item beta flows
                \item ``dead measure'' 
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Similarities}
    \begin{columns}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth,page=15]{figures/himmelblau}% 
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/tmnre.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why it's hard to do SBI with nested sampling}
    \begin{itemize}
        \item At each iteration $i$, nested sampling requires you to be able to generate a new live point from the prior, subject to a hard likelihood constraint
            \[ \theta\sim\pi : \mathcal{L}(\theta)>\mathcal{L}_i \]
        \item This is hard if you don't have a likelihood!
        \item In addition, nested sampling does not do well if the likelihood is non-deterministic
        \item Previous attempts:
            \begin{itemize}
                \item DNest paper \arxiv{1606.03757}(Section 10: Nested sampling for ABC)
                \item ANRE~\arxiv{2308.08597} using non-box priors driven by current ratio estimate with slice sampling re-population.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Sequential NRE with nested sampling}
    \begin{tikzpicture}[
            node distance=1cm,
            >=stealth, auto,
            every state/.style={
                rectangle, rounded corners, minimum width=2em,
                text width=6.8cm, align=center
            }
        ]
        \node[state, fill=C3!20, minimum height=3cm] (q14) {
            \textbf{NS}\\
            Run nested sampling on $\log r_{i}(\theta, D_\text{obs})$ \\
            to generate $\theta$ dead samples between prior and posterior targeted at $D_\text{obs}$.
        };
        \node[state, fill=C0!20, minimum height=1cm] (q121) [above=of q14] {
            \textbf{Terminate} if $\mathcal{D}_\text{KL}$ has converged
        };
        \node[state, fill=C0!20, minimum height=1cm] (q12) [above=of q121] {
            \textbf{Initialise} $\theta\sim\pi$\\
        };
        \node[state, fill=C2!20, minimum height=3cm] (q34) [right=of q14] {
            \textbf{NRE}\\
            Train Neural ratio estimator $\log r_i$
            with weights initialised from previous run
        };
        \node[state, fill=C1!20, minimum height=3cm] (q23) [above=of q34] {
            \textbf{Simulate}\\
            Generate simulations $\theta\to D$ from all discarded points
        };
        \begin{scope}[bend left]%
            \path[thick,-{Latex[width=2mm]}]   (q14.north) edge node {} (q121.south)
            (q121.east) edge node {}($(q23.south west)!0.16!(q23.north west)$) 
            (q23.south) edge node {} (q34.north)
            (q34.west) edge node {} (q14.east)
            (q12.east) edge node {}($(q23.south west)!0.84!(q23.north west)$) ;
        \end{scope}
        \node[align=center] (e) at (barycentric cs:q121=0.5,q12=0.5,q23=1,q34=1,q14=1) {\Large\textbf{NSNRE}};
    \end{tikzpicture}
\end{frame}

\begin{frame}
    \frametitle{\texttt{PolySwyft}}
    \begin{columns}[t]
        \column{0.45\textwidth}
        \begin{block}{\texttt{PolyChord}}
            \tthref{github.com/PolyChord/PolyChordLite}
            \includegraphics[width=\textwidth]{figures/polychord.png}
            \begin{itemize}
                \item Widely used high-performance nested sampling tool (implementing slice sampling \& clustering in MPI Fortran)
            \end{itemize}
        \end{block}
        \column{0.45\textwidth}
        \begin{block}{\texttt{Swyft}}
            \tthref{github.com/undark-lab/swyft}
            \includegraphics[width=\textwidth]{figures/swyft_logo_wide.png}
            \begin{itemize}
                \item Widely used TMNRE tool in cosmology/astrophysics.
            \end{itemize}
        \end{block}
        However, NSNRE is general, and not specific to these choices.
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Convergence diagnostics}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Example for a $n=5$ dimensional parameter space, with $d=100$ data points, (\texttt{lsbi} gaussian mixture model).
            \item This is the regime for cosmological scale problems.
            \item To determine convergence we track:
                \begin{itemize}
                    \item The change in KL divergence between rounds (\C[0]{blue}), and check when this goes to zero.
                    \item The total KL divergence between prior and posterior estimate (\C[1]{orange}), and check when this levels off (ground truth in \C[3]{red}).
                    \item Also shown is the KL divergence between the estimate and the ground truth (\C[2]{green}).
                \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/GMM_posterior_estimates.pdf}%
        \includegraphics<2|handout:0>[width=\textwidth]{figures/GMM_KL_div_per_round.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\tthref{github.com/handley-lab}}
    \tikz[overlay,remember picture]
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
        \includegraphics[width=0.10\textheight]{people/adam_ormondroyd.jpg}%
        \includegraphics[width=0.10\textheight]{people/charlotte_priestley.jpg}%
        \includegraphics[width=0.10\textheight]{people/david_yallup.jpg}%
        \includegraphics[width=0.10\textheight]{people/dily_ong.jpg}%
        \includegraphics[width=0.10\textheight]{people/george_carter.jpg}%
        \includegraphics[width=0.10\textheight]{people/harry_bevins.jpg}%
        \includegraphics[width=0.10\textheight]{people/kilian_scheutwinkel.jpg}%
        \includegraphics[width=0.10\textheight]{people/metha_prathaban.jpg}%
        \includegraphics[width=0.10\textheight]{people/namu_kroupa.jpg}%
        \includegraphics[width=0.10\textheight]{people/sinah_legner.jpg}%
        \includegraphics[width=0.10\textheight]{people/sam_leeney.jpg}%
        \includegraphics[width=0.10\textheight]{people/wei-ning_deng.jpg}%
        \includegraphics[width=0.10\textheight]{people/will_handley.jpg}%
    };
    \begin{itemize}
        %\item What does NSNRE give you that TMNRE doesn't?
        %    \begin{itemize}
        %        \item Use of dead points which scan from prior to logr peak avoids risk of 'trimming' important regions of the space
        %        \item Likelihood-driven contours allow more parameters $n>2$ to be considered in comparison with box priors
        %    \end{itemize}
        \item \texttt{PolySwyft} can perform NRE on $n\sim 6$ parameter spaces and $d\sim100$ data spaces.
        \item This makes it relevant for cosmological applications.
        \item Look out for imminent paper (post Kilian's thesis hand-in in $\sim\mathcal{O}(1 \text{month}))$
        %\item Investigating this raises (?existential) questions regarding NRE.
        %\item Is the $-5 <\log r < 5$ saturation of NREs a fundamental problem, or an engineering one?
        %\item Is there a ``nested'' approach to crossing this range in larger parameter spaces $n\gg 5$?
        \item Examples produced using \texttt{lsbi} package: \tthref{github.com/handley-lab/lsbi}\\
    \end{itemize}
\end{frame}

\appendix

\begin{frame}
    \frametitle{Considerations of ratio estimation}
    \student{zixiao_hu}{Zixiao Hu}{MPhil}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Neural REs can in practice only estimate in a band of $\log r$ before the activation function saturates (typically $-5 < \log r < 5$).
            \item Consider a posterior $\mathcal{P}$ well approximated by a Gaussian profile in an $n$-dimensional parameter space~\arxiv{2312.00294}
            \item If $\mathcal{D}_\text{KL}\gg1$ between prior and posterior:
                \begin{gather*}
                    \log r = \frac{n}{2} + \mathcal{D}_\text{KL} + \chi^2_{n} \\
            %\[\mathcal{P}(\log r) = \frac{1}{\Gamma(\tfrac{n}{2})} e^{\log r-\frac{n}{2}-\mathcal{D}_\text{KL}} \left(\tfrac{n}{2}+\mathcal{D}_\text{KL}-\log r\right)^{\frac{n}{2}-1}\]
            %i.e. $\log r$ has an offset \& rescaled chi-squared distribution:
            %\[n + 2\mathcal{D}_\text{KL} - 2\log r \sim \chi^2_{n} \]
                    \left\langle \log r \right\rangle_\mathcal{P} = \mathcal{D}_\text{KL}, \qquad \sigma(\log r)_\mathcal{P} = \sqrt{\frac{n}{2}}
                \end{gather*}
            \item Truncation ({\bf T}MNRE) reduces $\mathcal{D}_\text{KL}$, focusing the distribution into the $[-5,5]$ band.
            \item Marginalisation (T{\bf M}NRE) reduces $n$ \& $\sigma$.
        \end{itemize}
        \column{0.5\textwidth}
        \vspace{10pt}
        \includegraphics{figures/anatomy}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Cosmological forecasting}
    \framesubtitle{Have you ever done a Fisher forecast, and then felt Bayesian guilt?}
    \vspace{-20pt}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item Cosmologists are interested in forecasting what a Bayesian analysis of future data might produce.
            \item Useful for:
                \begin{itemize}
                    \item white papers/grants,
                    \item optimising existing instruments/strategies,
                    \item picking theory/observation to explore next.
                \end{itemize}
            \item To do this properly:
                \begin{enumerate}
                    \item start from current knowledge $\pi(\theta)$, derived from current data
                    \item Pick potential dataset $D$ that might be collected from $P(D)\: (=\mathcal{Z})$
                    \item Derive posterior $P(\theta|D)$
                    \item Summarise science (e.g. constraint on $\theta$, ability to perform model comparison)
                \end{enumerate}
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item This procedure should be marginalised over:
                \begin{enumerate}
                    \item All possible parameters $\theta$ (consistent with prior knowledge)
                    \item All possible data $D$
                \end{enumerate}
            \item i.e. marginalised over the joint $P(\theta,D)=P(D|\theta)P(\theta)$.
            \item Historically this has proven very challenging.
            \item Most analyses assume a fiducial cosmology $\theta_*$, and/or a Gaussian likelihood/posterior (c.f. Fisher forecasting).
            \item This runs the risk of biasing forecasts by baking in a given theory/data realisation.
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Fully Bayesian Forecasting~\arxiv{2309.06942}}
    \student{thomas_gessey-jones}{Thomas Gessey-Jones}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Simulation based inference gives us the language to marginalise over parameters $\theta$ and possible future data $D$.
            \item Evidence networks give us the ability to do this at scale for forecasting~\arxiv{2305.11241}.
            \item Demonstrated in 21cm global experiments, marginalising over:
                \begin{itemize}
                    \item theoretical uncertainty
                    \item foreground uncertainty
                    \item systematic uncertainty
                \end{itemize}
            \item Able to say ``at 67mK radiometer noise'', have a 50\% chance of 5$\sigma$ Bayes factor detection.
            \item Can use to optimise instrument design
            \item Re-usable package: \texttt{prescience}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fbf.pdf}
    \end{columns}
\end{frame}

\end{document}
```
# branch: remotes/origin/ioa_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usepackage{slashed}
\usepackage{tikz}
\usetikzlibrary{arrows,arrows.meta,automata,positioning}
\usepackage{bm}
\usepackage[percent]{overpic}


% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Next-generation astrophysical inference\\across the interdisciplinary frontier}
\date{29\textsuperscript{th} April 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{The future of astronomy is interdisciplinary}
    \begin{itemize}
        \item Across astronomy, combining data and disciplines will be the key to the next breakthroughs.
    \end{itemize}
    \vspace{-15pt}
    \begin{columns}[t]
        \column{0.72\textwidth}
        \begin{columns}
            \column{0.4\textwidth}
            \begin{block}{CMB+BAO}
                \begin{overpic}[width=\textwidth]{figures/desi_w_constraints.pdf} 
                    \put(70,15) {\tiny \arxiv{2404.03002}}
                \end{overpic}
            \end{block}
            \column{0.5\textwidth}
            \begin{block}{HEP+Astro}
                \begin{overpic}[width=\textwidth]{figures/cosmoalp.pdf}
                    \put(15,12) {\tiny \arxiv{2205.13549}}
                \end{overpic}
            \end{block}
        \end{columns}
        \begin{itemize}
            \item We have spent the last 5 years hair-splitting ``parkable'' tensions.
            \item The next 25 years of data confront the real tensions in our understanding of the Universe.
        \end{itemize}
        \column{0.25\textwidth}
        \begin{block}{GW170817}
            \begin{overpic}[width=\textwidth]{figures/mma_2.pdf}
                \put(50,103) {\tiny \arxiv{1710.05833}}
            \end{overpic}
        \end{block}
    \end{columns}
    \begin{itemize}
        \item I aim to show how my research programme is preparing us for this interdisciplinary challenge.
    \end{itemize}


\end{frame}

\begin{frame}
    \frametitle{\textit{Planck}: Inflation \& primordial power spectrum}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-5pt}
        \begin{itemize}
            \item Began theoretical PhD in 2012 investigating initial conditions for inflation.
            \item Joined Planck inflation team, working on Bayesian model fitting alongside theory.
            \item Found I enjoyed the observation \& inference as much as the theory.
            \item \texttt{FlexKnots} were used to reconstruct the primordial power spectrum, inflationary potential \& reionisation history (now used by Fialkov group)~\arxiv{1908.00906}.
            \item \texttt{PolyChord} developed for model comparison, particularly axion monodromy.
                \begin{itemize}
                    \item Now used widely within cosmology (DES, DESI, CMB) and beyond (exoplanets, GW, galaxies, 21cm, \ldots ) [\textcolor{C0}{\texttt{\href{https://ui.adsabs.harvard.edu/abs/2015MNRAS.453.4384H/citations}{ADS}}}]
                \end{itemize}
        \end{itemize}

        \column{0.5\textwidth}
        \begin{overpic}[height=0.4\textwidth]{figures/kinetic_dominance.pdf}%
                \put(68,-2) {\tiny \arxiv{1809.07185}}
        \end{overpic}
        \begin{overpic}[height=0.4\textwidth]{figures/axion_monodromy.pdf}
                \put(70,0) {\tiny \arxiv{1502.02114}}
        \end{overpic}


        \begin{overpic}[height=0.4\textwidth]{figures/flexknots.pdf}%
        \end{overpic}
        \begin{overpic}[height=0.4\textwidth]{figures/pps_planck.pdf}
                \put(70,0) {\tiny \arxiv{1908.00906}}
        \end{overpic}


    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Analytic innovation: from \texttt{MultiNest} to \texttt{PolyChord}}
    \begin{columns}
        \column{0.66\textwidth}
        \begin{itemize}
            \item \texttt{MultiNest}~\arxiv{0809.3437} was the leading Bayesian numerical model comparison tool in 2013.
                \begin{itemize}
                    \item A general purpose \& performant implementation of John Skilling's nested sampling meta-algorithm.
                    \item Remains the leader of the pack in $n\sim\mathcal{O}(10)$ parameter fits.
                    \item Careful testing in \textit{Planck} showed that it couldn't handle the many fast-slow nuisance parameters needed for systematics .
                \end{itemize}
            \item I analytically developed and numerically implemented \texttt{PolyChord}, which has polynomial scaling efficiency $f_{\texttt{PC}}\sim \frac{1}{n}$ with model parameters (c.f.\ exponential $f_{\texttt{MN}}\sim e^{-n/n_0}$).
            \item Exemplifies theoretical innovation \& numerical implementation driven by astrophysical challenges.
        \end{itemize}
        \texttt{PolyChord} inspired a new generation of nested samplers (\texttt{dynesty}, \texttt{UltraNest}, \texttt{nessai}\ldots), but remains the state of the art in high-dimensional model comparison.
        \column{0.33\textwidth}
        \vspace{-10pt}
        %\includegraphics[width=\textwidth,page=14]{figures/himmelblau.pdf}%
        \begin{block}{\texttt{MultiNest}~\arxiv{0809.3437}}
            \includegraphics[width=\textwidth]{figures/multinest.pdf}
        \end{block}%
        \vspace{-5pt}
        \begin{block}{\texttt{PolyChord}~\arxiv{1506.00171}}
            \includegraphics[width=\textwidth]{figures/polychord.png}
        \end{block}



    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Aside: theoretical work}
    \begin{columns}
        \column{0.5\textwidth}

        This talk will focus on my interdisciplinary work, but I have theoretical interests in:
        \begin{itemize}
            \item Quantum fields in curved spacetime \\\hfill {\small (Mary Letey, Zak Shumaylov, Fruzsina Agocs)}
            \item Poincar\'{e} Gauge Theory \\\hfill {\small (Sinah Legner, Will Barker)}
            \item Future conformal boundary/CPT universes \\\hfill {\small (Metha Prathaban, Wei-Ning Deng)}
            \item Curved finite inflation \hfill {\small (Lukas Hergt)}
        \end{itemize}
        \includegraphics[width=0.125\textwidth]{figures/students/mary_letey.jpg}%
        \includegraphics[width=0.125\textwidth]{figures/students/zak_shumaylov.jpg}%
        \includegraphics[width=0.125\textwidth]{figures/students/fruzsina_agocs.jpg}%
        \includegraphics[width=0.125\textwidth]{figures/students/sinah_legner.jpg}%
        \includegraphics[width=0.125\textwidth]{figures/students/will_barker.jpg}%
        \includegraphics[width=0.125\textwidth]{figures/students/metha_prathaban.jpg}%
        \includegraphics[width=0.125\textwidth]{figures/students/wei-ning_deng.jpg}%
        \includegraphics[width=0.125\textwidth]{figures/students/lukas_hergt.jpg}%

        \column{0.5\textwidth}
        \begin{columns}
            \column{0.5\textwidth}
            \begin{overpic}[width=\textwidth]{figures/qic.pdf}
                \put(65,-5) {\tiny \arxiv{1607.04148}}
            \end{overpic}
            \vspace{-5pt}

            \begin{overpic}[width=\textwidth]{figures/pgt.pdf}
                \put(65,-2) {\tiny \arxiv{2003.02690}}
            \end{overpic}
            \column{0.5\textwidth}
            \begin{overpic}[width=\textwidth]{figures/fcb.pdf}
                \put(65,0) {\tiny \arxiv{2111.14588}}
            \end{overpic}

            \begin{overpic}[width=\textwidth]{figures/pps_analytic.pdf}
                \put(65,20) {\tiny \arxiv{1907.08524}}
            \end{overpic}
        \end{columns}
        \vspace{5pt}
        \begin{overpic}[width=\textwidth]{figures/hergt.pdf}
            \put(45,0) {\tiny \arxiv{2211.17248}}
        \end{overpic}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Interdisciplinary work to date}
    \begin{columns}
        \column{0.35\textwidth}
        \begin{itemize}
            \item CMB cosmology
            \item Cosmological tension quantification
            \item \textbf{21cm cosmology}
            \item Radio Instrumentation
            \item \textbf{Gravitational waves}
            \item \textbf{Exoplanets}
            \item \textbf{Particle physics}
            \item Theory of machine learning
            \item Nested sampling theory
            \item Atomistic chemistry
            \item \textbf{Industrial applications}
            \item \ldots
        \end{itemize}
        \column{0.65\textwidth}
        50 minutes is not enough time to cover a decade's publishing.

        \vspace{10pt}

        \includegraphics[width=\textwidth]{figures/publishing.png}

        \hfill\textcolor{C0}{\texttt{\href{https://arxiv.org/a/handley_w_1.html}{arxiv.org/a/handley\_w\_1.html}}} 

        Will showcase a targeted subset.
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{21cm cosmology}
    \student{harry_bevins}{Harry Bevins}{PhD$\to$KICC fellow}

    \begin{columns}
        \column{0.6\textwidth}
        Transferring interdisciplinary ideas into 21cm cosmology with Anastasia Fialkov \& Eloy de Lera Acedo.
        \begin{itemize}
            \item \texttt{maxsmooth} \arxiv{2007.14970}
                \begin{itemize}
                    \item quadratic programming choice arose from quantitative finance consultancy work
                \end{itemize}
            \item \texttt{FlexKnots}
                \begin{itemize}
                    \item importing ideas from inflationary reconstruction into reionisation \arxiv{2310.05608}(Heimersheim) \& ionospheric reconstruction \arxiv{2311.14537}(Shen).
                \end{itemize}
            \item \texttt{margarine} \arxiv{2205.12841} \arxiv{2207.11457}
                \begin{itemize}
                    \item combination of ideas from interdisciplinary fields (emulators, nested sampling, marginal density estimation)
                \end{itemize}
        \end{itemize}
        These techniques are now widely used beyond the Cambridge 21cm community.

        \column{0.4\textwidth}
        \begin{overpic}[height=0.45\textwidth]{figures/maxsmooth_1.pdf}
            \put(0,0) {\tiny \arxiv{2007.14970}}
        \end{overpic}%
        \includegraphics[height=0.45\textwidth]{figures/maxsmooth_2.pdf}
        \vspace{5pt}

        \begin{overpic}[width=\textwidth]{figures/margarine_data.pdf}
            \put(0,0) {\tiny \arxiv{2301.03298}}
        \end{overpic}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology {\small \arxiv{2210.07409}}}
    \vspace{10pt}
    \begin{columns}
        \column{0.65\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
            \item Lead data analysis team (REACH first light in January)
            \item Nested sampling woven in from the ground up (calibrator, beam modelling, signal fitting, likelihood selection).
            \item All treated as parameterised model comparison problems.
                \includegraphics[height=0.3\textwidth]{figures/EDGES_antenna.JPG}
                \includegraphics[height=0.3\textwidth]{figures/SARAS.jpg}
                \includegraphics[height=0.3\textwidth]{figures/REACH_2.jpg}
        \end{itemize}
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Gravitational waves}
    \student{metha_prathaban}{Metha Prathaban}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Nested sampling has been used in GW since the beginning [\textcolor{C0}{\href{https://arxiv.org/abs/1602.03840}{GW150914}}]
            \item Work with Alvin Chua \& Chris Moore on transdimensional sampling for EMRI~\arxiv{1803.10210}
            \item Recent work with Metha Prathaban showing new chain-based approaches for improving precision \arxiv{2404.16428}

                \includegraphics[width=\textwidth]{figures/gw_chua.pdf}

                \vspace{8pt}
                \includegraphics[width=\textwidth]{figures/gw_prathaban.pdf}
        \end{itemize}
        \column{0.5\textwidth}
        \vspace{4pt}

        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}%
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
        \vspace{5pt}
        \begin{overpic}[height=0.27\textwidth]{figures/product_space.pdf}%
            \put(12,10) {\tiny \arxiv{1803.10210}}
        \end{overpic}%
        \begin{overpic}[height=0.27\textwidth]{figures/gw_chains.pdf}
            \put(65,-4) {\tiny \arxiv{2404.16428}}
        \end{overpic}
        \begin{itemize}
            \item Discussed use of \texttt{margarine}~\arxiv{2207.11457} as alternative to GW hierarchical modelling at inaugural data science discussion group
        \end{itemize}
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Exoplanets}
    \student{namu_kroupa}{Namu Kroupa}{PhD}

    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Exoplanet science requires solution of subtle inference problems
                \begin{itemize}
                    \item Inference from RV data~\arxiv{1806.00518}
                    \item Survey challenges~\arxiv{2007.07278}
                    \item Stellar activity~\arxiv{2102.03387} 
                \end{itemize}
            \item Gaussian processes+Nested Sampling for transit astronomy~\arxiv{2311.04153}
            \item Potential for further collaboration with Madhu's group who are seeking beyond \texttt{MultiNest} solutions as their problems scale in dimensionality
            \item Ongoing cross-disciplinary theoretical chemistry work may be useful in Paul Rimmer's group.
        \end{itemize}

        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/exoplanet_kernel.pdf}%
        \vspace{10pt}
        \includegraphics[width=\textwidth]{figures/harps_headline.pdf}%
        \vspace{10pt}
        \includegraphics[width=\textwidth]{figures/trappist_headline.pdf}%
        \vspace{10pt}
        \includegraphics[width=\textwidth]{figures/rv_headline.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT}
    \framesubtitle{Interdisciplinary case studies}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item GAMBIT is an interdisciplinary community and software framework.
            \item Like \texttt{CosmoMC}/\texttt{Cobaya}/\texttt{Bilby}, an organiser of data, likelihoods \& theory, including:
                \begin{itemize}
                    \item Collider data (e.g. LHC)
                    \item Direct detections (e.g. XENON1T)
                    \item Cosmology (\texttt{MontePython})
                    \item Astrophysics (e.g. Bullet Cluster, Supernovae)
                    \item Pulsar timing
                    \item \ldots \& much more
                \end{itemize}
            \item \texttt{GravBit} and \texttt{LowEnergyBit} arising from GAMBIT@KICC workshop
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[height=0.423\textwidth]{figures/students/gambit.png}
        \includegraphics[height=0.423\textwidth]{figures/gambit_kicc.jpg}
        \includegraphics[width=\textwidth]{figures/gambit_meetingbanner.jpg}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT: sub-GeV Dark matter constraints}
    \framesubtitle{Interdisciplinary case studies}
    \student{gambit}{Felix Kahlhoefer et al}{GAMBIT cosmo/DM working group}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{itemize}
            \item Physical model of sub-GeV thermal dark matter with a dark photon mediator~$A$:
        \end{itemize}
        \vspace{-10pt}
        \begin{align*}
            \small
            \mathcal{L}_\text{int} =& -\frac{1}{2} m_{A'}^2 A'^\mu A'_\mu - \frac{1}{4} A'^{\mu\nu}A'_{\mu\nu} -\kappa e A'^\mu \sum_{f} q_f \overline{f} \gamma_\mu f \,,
            \normalsize
        \end{align*}
        \vspace{-15pt}
        \begin{itemize}
            \item Constrain using cosmological, astrophysical, accelerator \& direct detection data.
            \item Bayesian Model comparison of Fermion~$\psi$ vs scalar~$\Phi$ models (scalar preferred).
        \end{itemize}
        \vspace{-10pt}
        \begin{align*}
            \small
            \mathcal{L}_\psi  =& \bar{\psi}(i \slashed{\partial} - m_\text{DM}) \psi + g_\text{DM} A'^\mu \bar{\psi} \gamma_\mu \psi \,,\\
            \mathcal{L}_\Phi  =& |\partial_\mu \Phi|^2 - m_\text{DM}^2 |\Phi|^2 - g_\text{DM}^2 A'_\mu A'^\mu |\Phi|^2 \\ &+ i g_\text{DM} A'^\mu \left[\Phi^\ast (\partial_\mu \Phi) - (\partial_\mu \Phi^\ast) \Phi\right]\,,
            \normalsize
        \end{align*}
        \column{0.44\textwidth}
        \vspace{10pt}
        \includegraphics[width=\textwidth]{figures/Bayes_SubGeVDM_fermion_RDprior_allDM_asym_observables.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{PolyChord Ltd: interdisciplinary R\&D spinout}
    \framesubtitle{Interdisciplinary case studies}
    \only<1-4>{
        \student{watkinson-headshot}{Catherine Watkinson}{Senior Data Scientist}
    }
    \only<5->{
        \student{mcaloone-headshot}{Thomas Mcaloone}{PhD $\to$ Data Scientist}
    }
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
        \includegraphics[width=0.08\textwidth]{figures/headshots/catherine-watkinson-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/thomas-macaloone-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/angus-peters-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/tamas-stenzel-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/david-yallup-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/rebecca-handley-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/adam-ormondroyd-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/mike-hobson-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/anthony-lasenby-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/mike-handley-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/will-handley-polychord.jpg}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        \includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/tdoa-cropped-1-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/tdoa-cropped-2-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/tdoa-cropped-3-crop.pdf}%
        \includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{DSTL: Bayesian OODA loops}
    \framesubtitle{Interdisciplinary case studies}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Work through Isaac Newton Institute with Defence Science \& Technology Laboratory.
            \item Quantification of ``OODA'' loop concept from litigation, business, law enforcement, management and military strategy
        \end{itemize}


        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/midas.png}%
            \column{0.5\textwidth}
            \begin{itemize}
                \item Two-way research interaction between government and academia.
                \item techniques now being used in REACH antenna design~\arxiv{2309.06942}
            \end{itemize}
        \end{columns}


        \column{0.5\textwidth}
        \begin{minipage}[]{0.6\textwidth}
            \begin{tikzpicture}[
                    node distance=5mm,
                    >=stealth, auto,
                    every state/.style={
                        rectangle, rounded corners, minimum height=6em, minimum width=2em,
                        text width=3cm, align=center
                    }
                ]


                \node[state, fill=C3!20] (q14) {{\textbf{Act}}
                    \\ 
                    \small
                    Execute strategy $\bm{s}$\\
                    Replace prior $\pi$ with posterior $\mathcal{P}$
                };
                \node[state, fill=C0!20] (q12) [above=of q14] { {\textbf{Observe}} 
                    \\
                    \small
                    Collect data $\bm{D}$ about system $\bm{\theta}$ using strategy $\bm{s}$.
                };
                \node[state, fill=C1!20] (q23) [right=of q12] { {\textbf{Orient}} 
                    \\
                    \small
                    Update knowledge from prior~$\pi(\bm{\theta})$ to posterior~$\mathcal{P}(\bm{\theta}|\bm{D})$.
                };
                \node[state, fill=C2!20] (q34) [below=of q23] { {\textbf{Decide}} 
                    \\
                    \small
                    choose strategy $\bm{s}$ using information $\hat{\mathcal{D}}_\mathrm{KL}(\bm{s}|\mathcal{P})$.
                };
                \begin{scope}[bend left]%
                    \path[thick,-{Latex[width=2mm]}]   (q14.north) edge node {} (q12.south)
                    (q12.east) edge node {} (q23.west)
                    (q23.south) edge node {} (q34.north)
                    (q34.west) edge node {} (q14.east);
                \end{scope}

                \node[align=center] (e) at (barycentric cs:q12=1,q23=1,q34=1,q14=1) {\Large\textbf{MIDAS}};

            \end{tikzpicture}
        \end{minipage}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Beginning the golden age of astronomy data}
    \begin{columns}
        \column{0.47\textwidth}
        \begin{itemize}
            \item Over our research lifetimes we will see next-generation data rates across the electromagnetic spectrum \& beyond:
                \begin{description}
                    \item[Radio] SKA \textit{et al}
                    \item[Micro] SO/CMB-S4/LiteBIRD
                    \item[IR] JWST, Roman
                    \item[Optical] Euclid, DESI, Rubin, EELT
                    \item[X-ray] Athena
                    \item[Gamma-ray] e-ASTROGAM
                    \item[Gravitational] LIGO/LVK$^+$/LISA
                    \item[Particle] CTA, IceCube, KM3NeT
                \end{description}
        \end{itemize}
        \column{0.55\textwidth}

        \includegraphics[height=0.145\textwidth]{figures/telescopes/jwst.png}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/roman.jpg}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/euclid.jpeg}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/athena.jpg}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/lisa.jpg}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/e-ASTROGAM.pdf}%
        \vspace{-1pt}

        \includegraphics[height=0.15183\textwidth]{figures/telescopes/desi.jpg}%
        \includegraphics[height=0.15183\textwidth]{figures/telescopes/eelt.jpg}%
        \includegraphics[height=0.15183\textwidth]{figures/telescopes/ska.jpg}%
        \includegraphics[height=0.15183\textwidth]{figures/telescopes/SO.jpg}%
        \vspace{-1pt}

        \includegraphics[height=0.18428\textwidth]{figures/telescopes/ligo.jpg}%
        \includegraphics[height=0.18428\textwidth]{figures/telescopes/km3n.jpg}%
        \includegraphics[height=0.18428\textwidth]{figures/telescopes/icecube.jpg}%
        \includegraphics[height=0.18428\textwidth]{figures/telescopes/CTA.jpg}%

        \begin{itemize}
            \item This ever-increasing statistical weight will mean true accuracy demands rigorous attention to systematics.
            \item This applies to all of cosmology, astrophysics, particle physics and beyond.
        \end{itemize}

    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Tensions in cosmology}
    \begin{columns}
        \column{0.38\textwidth}
        \begin{block}{Hubble}
            \begin{overpic}[width=\textwidth]{figures/hubble_tension.pdf}
                \put(72,37) {\tiny \arxiv{1907.05922}}
            \end{overpic}
        \end{block}
        \column{0.17\textwidth}
        \begin{block}{Weak lensing}
            \begin{overpic}[width=\textwidth]{figures/kids_tension.pdf}
                \put(13,18) {\tiny \arxiv{2007.15632}}
            \end{overpic}%
        \end{block}
        \column{0.36\textwidth}
        \begin{block}{\hfill other $w_0$/$\Omega_K$/$\nu$?}
            \begin{overpic}[width=0.48\textwidth]{figures/desi_tension.pdf}
                \put(0,90) {\tiny \arxiv{2404.03002}}
            \end{overpic}%
            \begin{overpic}[width=0.48\textwidth]{figures/curvature_3.pdf}
                \put(54,20) {\tiny \arxiv{1908.09139}}
            \end{overpic}%
        \end{block}
    \end{columns}

    \begin{columns}
        \column{0.75\textwidth}
        \begin{itemize}
            \item Tensions have been appearing in cosmology over the last five years.
            \item Though their significance may be debatable, they have revealed a gap in the armour of modern data analysis.
            \item Likelihood-based methods have to assume a fiducial cosmology. 
            \item This will cast greater doubt on the robustness of our conclusions from stage 4 surveys and beyond.
        \end{itemize}
        \column{0.25\textwidth}
        \begin{block}{\tiny DESI BAO paper: \arxiv{2404.03000}}
            \includegraphics[width=\textwidth]{figures/desi_fiducial.pdf}
        \end{block}
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{The real tensions in the room}


    \begin{columns}
        \column{0.34\textwidth}
        \begin{block}{Dark tension}
            \includegraphics[width=\textwidth]{figures/pie_chart.jpg}
        \end{block}
        \column{0.26\textwidth}
        \begin{block}{Initial conditions}
            \includegraphics[width=\textwidth]{figures/multiverse.jpg}
        \end{block}
        \column{0.32\textwidth}
        \begin{block}{Quantum gravity}
            \includegraphics[width=\textwidth]{figures/quantum_gravity.jpeg}
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item These are existential questions that don't go away if $\Lambda$CDM is phenomenologically correct\ldots
            \item \ldots and if it is, we will need an interdisciplinary approach that goes far beyond cosmology (using astronomy, collider \& experimental physics).
        \end{itemize}
        \column{0.4\textwidth}
        \begin{itemize}
            \item The GW redshift frontier, time domain astronomy \& 21cm SKA will be the key tools for challenging our astronomical understanding.
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The future: simulation-based inference}
    \student{kilian_scheutwinkel}{Kilian Scheutwinkel}{PhD}
    \vspace{5pt}
    \begin{itemize}
        \item Traditional Likelihood-based inference (LBI) requires knowledge of the likelihood $P(D|\theta)$
            \begin{itemize}
                \item For the CMB, it is possible to compute Probability(Sky$|\Lambda$CDM) [with caveats]
                \item For almost everybody else (different models, more evolved systems), the likelihood is approximate
            \end{itemize}
        \item Simulation-based inference learns the likelihood from physical simulations $\theta\to D$.
    \end{itemize}
    \begin{columns}
        \column{0.65\textwidth}
        \begin{itemize}
            \item Can extract nonlinear information from data.
            \item Do not need to assume a fiducial model, since data covariance is implicit in simulations.
            \item Users do not need to know advanced statistics
                \begin{itemize}
                    \item For this reason alone, it will come to dominate the next generation $\Rightarrow$ imperative to build a principled understanding!
                \end{itemize}
            %\item The current state of the art is achieved with \textit{Neural}~estimation
            \item My interest has been in understanding how essential machine learning is by building analytics \tthref{github.com/handley-lab/lsbi}
            \item Have also made progress on the first simulation-based nested sampler \texttt{PolySwyft} (presenting at EuCAIFCon)
        \end{itemize}
        \column{0.35\textwidth}
        \begin{overpic}[width=\textwidth]{figures/des_sbi.pdf}
            \put(65,80) {\tiny \arxiv{2403.02314}}
            \put(65,73) {\tiny (Jeffrey et DES)}
        \end{overpic}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{ERC grant: COSMOTENSION \hspace{95pt}\small \tthref{willhandley.co.uk/ERC.pdf}}
    \framesubtitle{Resolving cosmological tensions with diverse data, novel theories and Bayesian machine learning}

    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item ERC starting grant $\Rightarrow$ UKRI Frontier, commencing October 2024.
            \item Funds 3 PDRAs and 4 PhDs over 5 years.
            \item Research programme centered around combining novel theories of gravity, Boltzmann solvers~\arxiv{1906.01421}, reconstruction~\arxiv{1908.00906}, nested sampling \& simulation-based inference (n\'{e}e LFI).
            \item Aims to disentangle cosmological tensions $H_0$, $\sigma_8$, $\Omega_K$ with next-generation data analysis techniques.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/denseoutput.pdf}
        \includegraphics[width=\textwidth]{figures/pps.pdf}
        \includegraphics[width=\textwidth]{figures/gantt.pdf}
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\tthref{github.com/handley-lab}}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item The astronomy challenges of our generation are interdisciplinary.
            \item Interdisciplinary approaches are two-way streets for innovation.
            \item Simulation-based inference represents the frontier of how we will all be doing data analysis.
            \item Our research programme at the interface between theory, observation and inference is well-placed to explore the interdisciplinary frontier.
        \end{itemize}

        \column{0.4\textwidth}
        \includegraphics[height=0.32\textwidth]{logos/ukri}\hfill
        \includegraphics[height=0.32\textwidth]{logos/ukerc}\hfill
        \includegraphics[height=0.32\textwidth]{logos/royalsociety}

        \centerline{
            \includegraphics[width=0.8\textwidth]{logos/dirac.png}%
        }
    \end{columns}
    \vspace{10pt}


    \tikz[overlay,remember picture]
    \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
        \includegraphics[width=0.09\textheight]{figures/students/adam_ormondroyd.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/david_yallup.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/dily_ong.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/felicity_ibrahim.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/george_carter.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/harry_bevins.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/ian_roque.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/kilian_scheutwinkel.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/metha_prathaban.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/namu_kroupa.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sinah_legner.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/thomas_gessey-jones.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/tze_goh.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/wei-ning_deng.jpg}%
    };
\end{frame}


%\begin{frame}
%    \frametitle{unimpeded: PLA for the next generation}
%    \student{dily_ong}{Dily Ong}{PhD}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item DiRAC 2020 RAC allocation of 30MCPUh
%            \item Main goal: Planck Legacy Archive equivalent
%            \item Parameter estimation $\to$ Model comparison
%            \item MCMC $\to$ Nested sampling
%            \item Planck $\to$ $\{\text{Planck}, \text{DESY1}, \text{BAO}, \ldots \}$
%            \item Pairwise combinations
%            \item Suite of tools for processing these 
%                \begin{itemize}
%                    \item \texttt{anesthetic} $2.0$
%                    \item \texttt{unimpeded} $1.0$
%                    \item \texttt{zenodo} archive
%                    \item \texttt{margarine}
%                \end{itemize}
%            \item MCMC chains also available.
%            \item Library of bijectors emulators for fast re-use
%        \end{itemize}
%        \column{0.5\textwidth}
%        \includegraphics[width=\textwidth]{logos/dirac.png}
%        \includegraphics[width=0.5\textwidth,page=21]{figures/himmelblau.pdf}%
%        \includegraphics[width=0.5\textwidth,page=15]{figures/himmelblau.pdf}
%    \end{columns}
%\end{frame}




%\begin{frame}
%    \frametitle{Other frontiers}
%    Copilot
%
%    ChatGPT
%
%    LLMs/translation between disciplines
%    
%\end{frame}

\end{document}
```
# branch: remotes/origin/kicc_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usepackage{slashed}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage[percent]{overpic}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Resonant or asymmetric: The status of sub-GeV dark matter }
\subtitle{\arxiv{2405.17548}}
\date{21\textsuperscript{st} June 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Background: Dark Matter}
    \student{gambit}{}{}

    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item We assume dark matter (DM) is a particle.
            \item If there is weak interaction with the standard model, it can be probed by  any/all of:
                \begin{itemize}
                    \item \textbf{Direct detection:} underground detector
                    \item \textbf{Indirect detection:} telescopes
                    \item \textbf{Collider searches:} missing energy
                    \item \textbf{Thermalisation:} cosmological equilibrium
                \end{itemize}
            \item Thermalisation: the same thing we search for in the lab would mean DM is in equilibrium with standard model particles at early times, but freezes out at some point.
            \item This allows us to link particles physics modelling and cosmology.
        \end{itemize}
%        \begin{tikzpicture}
%            %Diagram showing two arrows going into a circle and two arrows going out of a circle
%            \node[circle, fill=black, minimum size=3pt] (O){};
%            \node[above left = of O] (A) {};
%            \node[below left = of O] (B) {};
%            \node[above right = of O] (C) {};
%            \node[below right = of O] (D) {};
%            \draw[<->] (O) -- (A);
%            \draw[<->] (O) -- (B);
%            \draw[<->] (O) -- (C);
%            \draw[<->] (O) -- (D);
%
%            \node[rotate=-90] (X) at ($(A)!0.5!(B)$) {Normal matter};
%            \node[rotate=-90] (Y) at ($(C)!0.5!(D)$) {Dark matter};
%
%            \draw[->] (B) -- (D);
%            \draw[->] (A) -- (C);
%
%            \draw[->] (D)+(0.5,0) -- ($(C)+(0.5,0)$);
%            %\node (Z) at ($(A)!0.5!(C)$) {};
%            %\node [above = of Z] {Indirect detection};
%
%
%        \end{tikzpicture}

        
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/DM_diagram.png}
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Summary}
    \student{gambit}{}{}

    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Why sub-GeV?}
            \begin{itemize}
                \item If DM is a particle, $m_\text{DM}\in[10^{-30},10^{19}]~\text{GeV}$, \\\hfill($\lambda<H_0^{-1}$ and $m_\text{DM} <m_\text{p}$).
                \item If DM is thermal, $m > 10~\text{MeV}$ from CMB
                \item Direct detection means $m<1~\text{GeV}$.
                \item sub-GeV DM was thought ruled out due to Lee-Weinberg bound (known standard model interactions can't produce enough DM).
                \item However, Dark Matter + ``Dark Photons'' ($U(1)'$ gauge group) escapes this bound.
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{\arxiv{2405.17548}$=[\psi,\Phi]\times[\eta_\text{DM},\cdot]\times[f_\text{DM},\cdot]$}
            Switches we consider:
            \begin{itemize}
                \item Fermionic $\psi$ or Scalar $\Phi$
                \item symmetric $\eta_\text{DM}=0$ or asymmetric $\eta_\text{DM}\ne0$
                \item Dominant $f_\text{DM}=1$ or sub-dominant $f_\text{DM}<1$
            \end{itemize}
            We find that Fermionic, symmetric, dominant DM is disfavoured.

            The other three alternatives separately still allowed in much of the parameter space.
        \end{block}
    \end{columns}


\end{frame}

\begin{frame}
    \frametitle{Physics}
    \student{gambit}{}{}
    \begin{itemize}
        \item Dark photon $A'$ with mass $\boxed{m_{A'}}$ (via Stueckelberg mechanism) with kinetic mixing $\boxed{\kappa}$.
            \[\mathcal{L} = -\frac{1}{2}m_{A'}^2 A'_\mu A'^\mu - \frac{1}{4}A'_{\mu\nu}A'^{\mu\nu} - \kappa e A'^\mu \sum_{f\in\text{SM}} q_f \bar f \gamma_\mu f\]
        \item Dark matter candidate $\chi$ with mass $\boxed{m_\text{DM}}$, with $\chi\in\{\Phi, \psi\}$ complex scalar or Dirac fermion, coupled to the dark photon with $\boxed{g_\text{DM}}$.
        \begin{align*}
            \mathcal{L}_\psi  =& \bar{\psi}(i \slashed{\partial} - m_\text{DM}) \psi + g_\text{DM} A'^\mu \bar{\psi} \gamma_\mu \psi \\
            \mathcal{L}_\Phi  =& |\partial_\mu \Phi|^2 - m_\text{DM}^2 |\Phi|^2 - g_\text{DM}^2 A'_\mu A'^\mu |\Phi|^2 + i g_\text{DM} A'^\mu \left[\Phi^\ast (\partial_\mu \Phi) - (\partial_\mu \Phi^\ast) \Phi\right],
        \end{align*}
    \item Dark matter different from its anti-particle, there may be an asymmetry $\boxed{\eta_\text{DM}}$
    \item Also have local halo DM density $\rho_0$, velocity dispersion $v_0$ and escape velocity $v_\text{esc}$ as nuisance parameters determined by Gaia data~\arxiv{1901.02016} \textit{et al}.
    \item Focus on sub GeV region $\text{MeV} < m_\text{DM} < \text{GeV}$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Data}
    \student{gambit}{}{}
    \begin{columns}
        \column{0.49\textwidth}
        \begin{block}{Cosmological constraints}
                CMB \& BBN

            $\Omega_c h^2 \approx 0.120\pm0.001$ from \textit{Planck}

            $N_\text{eff}\approx 2.99\pm 0.17$ via \texttt{AlterBBN}
            %Exotic energy injections $p_\text{ann}$ affect recombination
        \end{block}
        \begin{block}{Astrophysical constraints}
            X-rays \& Bullet cluster

            %MeV gap (INTEGRAL $<$MeV, Fermi-LAT $>$GeV)

            %secondary signals of DM annihilation (inverse compton scattering ) producing keV X-Rays

            INTEGRAL, NuStar, XMM-Newton, Suzaku

            Bullet cluster gives self-interaction constraints%, since interactions make friction, and would cause subcluster to lose DM. (focus on the latter due to need for SBI for the former).
        \end{block}
        \column{0.49\textwidth}
        \begin{block}{Accelerator constraints}
            Beam dumps \& electron-positron colliders

            %dark photon decays into pairs of DM particles
            %constraints come from 'missing energy searches' 

            beam dump: LSND, MiniBooNE
            missing energy searches: NA64, BaBar
        \end{block}
        \begin{block}{Direct detection constraints}
            searches for electron \& nuclear recoils

            Xenon1T, SENSEI, DarkSide50, PandaX-4T, DAMIC-M, SuperCDMS HV
        \end{block}
    \end{columns}
\end{frame}

{
    \usebackgroundtemplate{\includegraphics[width=\paperwidth]{gambit_ad_slide}}
    \begin{frame}[plain]
    \end{frame}
}

%\begin{frame}
%    \frametitle{GAMBIT: combining datasets}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item GAMBIT is an interdisciplinary community and software framework.
%            \item Like \texttt{CosmoMC}/\texttt{Cobaya}/\texttt{Bilby}, an organiser of data, likelihoods \& theory, including:
%                \begin{itemize}
%                    \item Collider data (e.g. LHC)
%                    \item Direct detections (e.g. XENON1T)
%                    \item Cosmology (\texttt{MontePython})
%                    \item Astrophysics (e.g. Bullet Cluster, Supernovae)
%                    \item Pulsar timing
%                    \item \ldots \& much more
%                \end{itemize}
%            \item \texttt{GravBit} and \texttt{LowEnergyBit} arising from GAMBIT@KICC workshop
%        \end{itemize}
%        \column{0.5\textwidth}
%        \includegraphics[height=0.423\textwidth]{figures/students/gambit.png}
%        \includegraphics[height=0.423\textwidth]{figures/gambit_kicc.jpg}
%        \includegraphics[width=\textwidth]{figures/gambit_meetingbanner.jpg}
%    \end{columns}
%\end{frame}


%\begin{frame}
%    \frametitle{What is GAMBIT?}
%    \student{gambit}{}{}
%    \vspace{-10pt}
%    \begin{columns}
%        \column{0.48\textwidth}
%        \begin{block}{GAMBIT as a software framework}
%            \begin{itemize}
%                \item Combines collider, direct detection, neutrino \& telescope data.
%                \item Allows joint analysis of dark matter, neutrinos \& BSM physics
%                \item MPI + OpenMP parallelisation \\(record of 115,000 CPUs)
%                \item Combines libraries \& codes written in:\\
%                    \texttt{C++}, \texttt{Fortran}, \texttt{Python}, \texttt{Mathematica}\ldots
%                \item Highly modular ``Bits''
%                \item Often with several alternatives
%            \end{itemize}
%        \end{block}
%        \column{0.48\textwidth}
%        \begin{block}<only@1>{GAMBIT as a community}
%            \begin{itemize}
%                \item Particle physicists, cosmologists \& statisticians ($>$80 members)
%                \item Generates interdisciplinary expertise \& inspires new techniques
%                \item Open source software
%                \item Access to large community computing resources (40MCPUh/y)
%                \item Modularity allows parallel teams
%                \item Short-author papers by default
%                \item in-person meetings every 9 months
%            \end{itemize}
%        \end{block}%
%        \includegraphics<2>[width=\textwidth]{figures/gambit_kicc.jpg}
%    \end{columns}
%\end{frame}


\begin{frame}
    \frametitle{Parameter estimation}
    \student{gambit}{}{}
    \begin{columns}
        \column{0.4\textwidth}
        \begin{itemize}
            \item Frequentist analysis: Profile likelihood plots optimise unseen parameters 
            \item Can build intuition for how data constrain parameters
            \item Global fit extracts more information than traditional particle physics approach
            %    \begin{itemize}
            %        \item Useful for ``worst case''/guarantee statements
            %    \end{itemize}
            %\item Bayesian analysis: Posterior probability plots marginalise  unseen parameters
            \item Grey contours have $f_\text{DM}\le1$.
            \item Similar plots for fermions.
            \item Asymmetric dark matter less constrained.
            \item GAMBIT can do Bayesian \& Frequentist analyses.
        \end{itemize}
        
        \column{0.6\textwidth}
        \only<1>{
            \begin{overpic}[width=\textwidth]{figures/fermion_mDM_epsR.pdf} 
            \put(45,70) {\small Indirect detection}
            \put(55,65) {\small $\epsilon_R\lesssim 0.4$}
            \end{overpic}
        }%
        \includegraphics<2>[width=\textwidth]{figures/fermion_mDM_gDM.pdf}%
        \only<3>{
            \begin{overpic}[width=\textwidth]{figures/fermion_mDM_kappa.pdf} 
            \put(45,25) {\tiny Relic density requirements}
            \end{overpic}
        }%
        \only<4>{
            \begin{overpic}[width=\textwidth]{figures/fermion_gDM_kappa.pdf} 
            \put(45,35) {\tiny Relic density requirements}
            \end{overpic}
        }
        \only<5>{
            \includegraphics[width=0.79\textwidth]{figures/Bayes_SubGeVDM_fermion_allDM_sym.pdf}
        }
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Model comparison}
    \student{gambit}{}{}
    \includegraphics<1>[width=\textwidth]{figures/allDM.pdf}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Fermionic symmetric dark matter disfavoured
                \includegraphics<2->[width=\textwidth]{figures/allDM.pdf}
            \item<2-> This is because the the cross sections with SM need to be more fine-tuned.
            \item<2-> contrasts with scalar as it's easier to generate dark matter without asymmetry ($p$-wave suppression of annihilation cross section).
        \end{itemize}
        \column{0.5\textwidth}
        \vspace{0.6em}
        \includegraphics<2>[width=\textwidth]{figures/Bayes_SubGeVDM_fermion_allDM_sym_observables.pdf}%
        \includegraphics<3>[width=\textwidth]{figures/Bayes_SubGeVDM_fermion_RDprior_allDM_asym_observables.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \student{gambit}{}{}
    \framesubtitle{\tthref{gambitbsm.org}}
    \begin{columns}
        \column{0.5\textwidth}
            \begin{itemize}
                \item GAMBIT \texttt{DarkBit}, \texttt{CosmoBit} \& \texttt{ScannerBit} modules now mature enough to fit principled dark matter models to a wide variety of data.
                \item Unique combination of tools \& expertise that can systematically test dark matter models.
                \item Future detectors will constrain these further.
                \item Looking for other possible contributions to GAMBIT physics-driven global fits
            \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/fermion_asym_mAp_kappa_sensitivities.pdf}%
        \includegraphics<2>[width=\textwidth]{figures/fermion_asym_mDM_fsigmap_sensitivities.pdf}%
        \includegraphics<3>[width=\textwidth]{figures/fermion_asym_mDM_fsigmae_sensitivities.pdf}
    \end{columns}
\end{frame}

\end{document}
```
# branch: remotes/origin/maxent_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}

\title{The scaling frontier of nested sampling}
\date{1\textsuperscript{st} July 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Recap: the nested sampling meta-algorithm}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n_\text{live}$ random prior samples $\theta\sim\C[1]{\pi}$.
            \item Delete lowest likelihood sample, and replace with a new one at higher value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n_\text{live}}\only<5->{\pm \frac{1}{n_\text{live}}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \C[3]{\mathcal{Z}} = \int \C[2]{\mathcal{L}} \C[1]{\pi} d\theta \approx \sum_{\mathclap{i\in\text{dead}}} \C[2]{\mathcal{L}}_i \Delta X_i, \quad X_i = e^{-\only<5->{(}i\only<5->{\pm\sqrt{i})}/n_\text{live}} \]
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4-         >[width=\textwidth,page=4]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Kullback Leibler divergence (or entropy $H$ or, information gain)}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The KL divergence between \C[1]{prior $\pi$} and \C[0]{posterior $\mathcal{P}$} is is defined as:
                \[\mathcal{D}_\text{KL} = \av[{\C[0]{\mathcal{P}}}]{\log\frac{\C[0]{\mathcal{P}}}{\C[1]{\pi}}} = \int \C[0]{\mathcal{P}}(\theta) \log \frac{\C[0]{\mathcal{P}}(\theta)}{\C[1]{\pi}(\theta)}d\theta.\]
            \item Whilst not a distance, $\mathcal{D}=0$ when $\C[0]{\mathcal{P}}=\C[1]{\pi}$.
            \item Occurs in the context of machine learning as an objective function for training functions.
            \item In Bayesian inference it can be understood as a log-ratio of ``volumes'':
                \[ \mathcal{D}_\text{KL} \approx \log \frac{V_{\C[1]{\pi}}}{V_{\C[0]{\mathcal{P}}}}.\]
                (this is exact for top-hat distributions).
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics{figures/volumes.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{How fast in nested sampling?}
    \student{zixiao_hu}{Zixiao Hu}{MSci 2023}
    \begin{columns}
        \column{0.75\textwidth}
        \[
            \boxed{
                T = T_{\C[2]{\mathcal{L}}} \times \underbrace{
                    \overbrace{
                        n_\text{live}  \times \mathcal{D}_\text{KL}
                    }^{\displaystyle n_\text{dead}}
                    \times f_\text{sampler}
                }_{\displaystyle n_{\C[2]{\mathcal{L}}}},
            }
        \]
        \vspace{-10pt}
        \begin{itemize}
            \item NS compresses the volume $X$ of the live points from prior $X=1$ to posterior $X = e^{-\mathcal{D}_\text{KL}}$.
            \item It does this exponentially so at iteration $i$, $X_i = e^{-i/n_\text{live}}$
            \item Need $e^{-n_\text{dead}/n_\text{live}}\sim e^{-\mathcal{D}_\text{KL}}\Rightarrow \boxed{n_\text{dead}\sim n_\text{live}\times\mathcal{D}_\text{KL}}$.
            \item It takes a $f_\text{sampler}$ likelihood evaluations to generate a new live point, so $n_{\C[2]{\mathcal{L}}} = n_\text{dead}\times f_\text{sampler}$
            \item It takes $T_{\C[2]{\mathcal{L}}}$ seconds to call the likelihood, so $T=T_{\C[2]{\mathcal{L}}}\times n_{\C[2]{\mathcal{L}}}$
        \end{itemize}
        \begin{exampleblock}{Further reading~\arxiv{2312.00294}}
            \texttt{aeons}: approximating the end of nested sampling, (Zixiao Hu et al) 
        \end{exampleblock}
        \column{0.25\textwidth}
        \vspace{8pt}
        \includegraphics[width=\textwidth,page=1]{figures/himmelblau}
        \includegraphics[width=\textwidth,page=6]{figures/himmelblau}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{How accurate is nested sampling?}
    \student{lukas_hergt}{Lukas Hergt}{PhD 2020}
    \begin{columns}
        \column{0.75\textwidth}
        \[ \boxed{\sigma(\log\C[3]{\mathcal{Z}}) \approx \sqrt{\mathcal{D}_\text{KL}/n_\text{live}}} \]
        \begin{itemize}
            \item From the Occam's razor equation~\arxiv{2102.11511}:
                \[\log \C[3]{\mathcal{Z}} = \av[{\C[0]{\mathcal{P}}}]{\log\C[2]{\mathcal{L}}} - \mathcal{D}_\text{KL}\]
            \item The dominant error is in the estimation of compression $\mathcal{D}_\text{KL}$
            \item At each iteration, compress by $\exp(-\frac{1}{n_\text{live}} \pm \frac{1}{n_\text{live}})$
            \item After $n_\text{dead}$ iterations, compressed by $e^{-\frac{n_\text{dead}}{n_\text{live}} \pm \frac{\sqrt{n_\text{dead}}}{n_\text{live}}} = e^{-\mathcal{D}_\text{KL}}$
            \item As before $n_\text{dead}\sim n_\text{live}\times\mathcal{D}_\text{KL}$
            \item $\sigma(\log\C[3]{\mathcal{Z}}) = \sigma (\mathcal{D}_\text{KL}) = \frac{\sqrt{n_\text{dead}}}{n_\text{live}}= \sqrt{\frac{\mathcal{D}_\text{KL}}{n_\text{live}}}$
        \end{itemize}
        \column{0.25\textwidth}
        \vspace{8pt}
        \includegraphics[width=\textwidth,page=1]{figures/himmelblau}
        \includegraphics[width=\textwidth,page=6]{figures/himmelblau}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The scaling frontier of nested sampling}
    \begin{columns}[t]
        \column{0.47\textwidth}
        \begin{block}{How fast in nested sampling?}
            \[ \boxed{T = T_{\C[2]{\mathcal{L}}} \times n_\text{live} \times \mathcal{D}_\text{KL} \times f_\text{sampler}} \]
        \end{block}
        \column{0.43\textwidth}
        \begin{block}{How accurate is nested sampling?}
            \[ \boxed{\sigma(\log\C[3]{\mathcal{Z}}) \approx \sqrt{\mathcal{D}_\text{KL}/n_\text{live}}} \]
        \end{block}
    \end{columns}
    \vspace{10pt}
    \begin{columns}[t]
        \column{0.5\textwidth}
        in $d$ dimensional parameter space:
        \begin{description}
\item[$T_{\C[2]{\mathcal{L}}}$:] likelihood eval time \hfill$\sim\mathcal{O}(d)$
            \item[$n_\text{live}$:] number of live points\hfill$\sim\mathcal{O}(d)$
            \item[$\mathcal{D}_\text{KL}$:] KL divergence from prior to posterior $\approx\log{V_{\C[1]{\pi}}}/{V_{\C[0]{\mathcal{P}}}}$ \hfill$\sim\mathcal{O}(d)$
            \item[$f_\text{sampler}$:] efficiency of point generation \\ region$\sim\mathcal{O}(e^{d/d_0})$ or path$\sim\mathcal{O}(d)$
        \end{description}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Algorithmically improving $f_\text{sampler}$ is only a fraction of the story!
            \item $\mathcal{D}_\text{KL}$ appears twice, so improvements here are quadratically important.
            \item Gradients give you $d$ more information.
        \end{itemize}
    \end{columns}\vspace{10pt}
    \begin{itemize}
        \item $T\sim\mathcal{O}(d^4)$ whilst polynomial is far from ideal! -- Let's unpack the caveats.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$f_\text{sampler}$: live-point generation efficiency}
    \begin{columns}[t]
        \column{0.25\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \texttt{nautilus}~\arxiv{2306.16923} 
        \texttt{nessai}~\arxiv{2102.11056}
        \texttt{nora}~\arxiv{2305.19267}
        \column{0.43\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.55\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        {\small
            \texttt{jaxns}~\arxiv{2012.15286} \texttt{dynesty}~\arxiv{1904.02180} 
            \texttt{BayesicFitting}~\arxiv{2109.11976}
        }
        \vfill
        \column{0.25\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{nested\_fit}~\arxiv{1611.10189}
        \texttt{pymatnext}~\arxiv{0906.3544}
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{$f_\text{sampler}$: live-point generation efficiency}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \C[1]{\pi} : \C[2]{\mathcal{L}}(\theta)>\C[2]{\mathcal{L}_*} \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
                \item Constructs bounding region and draws many invalid points until $\C[2]{\mathcal{L}}(\theta)>\C[2]{\mathcal{L}_*}$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{5pt}
    \begin{itemize}
        \item for high dimensions, state of the art is linear
        \item Recent innovations in \texttt{UltraNest} allow self-tuning of efficiency for rejection sampling
    \end{itemize}
    \begin{alertblock}{Frontier}
        Can chain-based NS self-tune chain length to be sub-linear at run time?
    \end{alertblock}
\end{frame}

\begin{frame}
    \frametitle{$n_\text{live}$: the number of live points}
    \begin{columns}
        \column{0.75\textwidth}
        There are two reasons one might increase the number of live points:
        \begin{block}{Multimodality}
            At each iteration, if a mode has a basin of attraction $<1/n_\text{live}$ of the space you are more likely than not to miss it
            {\small
                \begin{itemize}
                    \item No reason to expect multimodality to have anything so simple as linear scaling with dimension.
                \end{itemize}
            }
        \end{block}
        \vspace{-5pt}
        \begin{block}{Accuracy}
            Evidence error $\sigma(\log \C[3]{\mathcal{Z}}) = \sqrt{\mathcal{D}_\text{KL}/n_\text{live}} $
            {\small
                \begin{itemize}
                    \item So if $\mathcal{D}_\text{KL} \sim \mathcal{O}(d)$, then to maintain the same quality of inference have to scale $n_\text{live}$ at the same rate.
                \end{itemize}
            }
        \end{block}
        \begin{alertblock}{Frontier}
            Can we make this ``mode missing'' argument more quantitative?
        \end{alertblock}
        \column{0.25\textwidth}
        \includegraphics[width=\textwidth,page=1]{figures/himmelblau}
        \includegraphics[width=\textwidth,page=6]{figures/himmelblau}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{$T_{\C[2]{\mathcal{L}}}$: the time to evaluate the likelihood}
    \begin{columns}
        \column{0.42\textwidth}
        \begin{block}{Acceleration/emulation}
            \begin{itemize}
                \item Increasing emphasis of using machine learning/AI emulators to accelerate slow components of pipelines
                \item Nested sampling dead points are good at training emulators of likelihoods, due to their nose-to-tail sampling (Brewer's ``Dead measure'')
                \item \texttt{BAMBI}~\arxiv{1110.2997} does this at run-time to accelerate nested sampling
            \end{itemize}
        \end{block}
        \begin{alertblock}{Frontier}
            Is BAMBI the best we can do?
        \end{alertblock}
        \column{0.135\textwidth}
        \includegraphics[angle=-90,width=\textwidth]{figures/dead_measure}
        \column{0.42\textwidth}
        \begin{block}{Fast-slow hierarchies}
            \begin{itemize}
                \item Success in cosmology where some parameters take longer than others
                \item reduces $T_{\C[2]{\mathcal{L}}}\sim\mathcal{O}(d_\text{slow})$.
            \end{itemize}
        \end{block}
        \includegraphics[width=\textwidth]{figures/fast_slow.pdf}
        \begin{alertblock}{Frontier}
            Can fast-slow hierarchies be better automated within algorithms?
        \end{alertblock}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{$\mathcal{D}_\text{KL}$: The Kullback Leibler divergence}
    \begin{columns}
        \column{0.75\textwidth}
        \begin{itemize}
            \item Naively $\mathcal{D}_\text{KL} \sim \mathcal{O}(d)$, since if you have $d$ \emph{useful} parameters, you would expect to need to compress each of them, so each contribute linearly to the volume contribution
            \item From the Occam's razor equation however:
                \[\log \C[3]{\mathcal{Z}} = \av[{\mathcal{P}}]{\log\C[2]{\mathcal{L}}} - \mathcal{D}_\text{KL}\]
                a large $\mathcal{D}_\text{KL}$ means a ``bad'' model
            \item Methods which explore models weighted by their evidence won't spend as much time in high $\mathcal{D}_\text{KL}$ regions~\arxiv{1506.09024}.
            \item There's also an intuition of the KL being ``information'', so increasing the number of parameters shouldn't extract more content.
            \item The KL divergence is a property of the model, so one would initially consider this to be something that the user has no control over\ldots
        \end{itemize}
        \column{0.25\textwidth}
        \includegraphics[width=\textwidth]{figures/volumes.pdf}
        \[ \mathcal{D}_\text{KL} = \log \frac{V_{\C[1]{\pi}}}{V_{\C[0]{\mathcal{P}}}} \]
        \[ \mathcal{D}_\text{KL} = \av[{\C[0]{\mathcal{P}}}]{\log\frac{\C[0]{\mathcal{P}}}{\C[1]{\pi}}} \]
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Posterior repartitioning}
    \student{aleksandr_petrosyan}{Aleksandr Petrosyan}{MSci 2021}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Almost all sampling algorithms (MH, HMC, SMC) are only sensitive to the product of likelihood and prior $\boxed{\C[2]{\mathcal{L}}\times\C[1]{\pi}}$.
            \item In practice this manifests as many codes implementing ``prior terms'' in the likelihood (or visa-versa) with no ill effect
        \item Nested sampling is unusual, in that one ``samples from the prior $\C[1]{\pi}$, subject to a hard constraint $\C[2]{\mathcal{L}}>\C[2]{\mathcal{L}_*}$.'':
                \[\{\theta\sim \C[1]{\pi} : \C[2]{\mathcal{L}}(\theta)>\C[2]{\mathcal{L}_*} \}\] 
            \item This separates the prior from the likelihood at an algorithmic level.
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
\item One can therefore repartition the likelihood and prior $(\C[2]{\mathcal{L}},\C[1]{\pi})\to(\C[2]{\tilde{\mathcal{L}}},\C[1]{\tilde{\pi}})$, providing
                \[ \boxed{\C[2]{\mathcal{L}}\times\C[1]{\pi} = \C[2]{\tilde{\mathcal{L}}}\times\C[1]{\tilde{\pi}}}\]
            \item This moves pieces between likelihood and prior that you have analytic control over.
                \includegraphics[width=\textwidth]{figures/repartition}
            \item Chen, Feroz \& Hobson~\arxiv{1908.04655} invented this to deal with misspecified priors
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Posterior repartitioning for acceleration}
    \student{metha_prathaban}{Metha Prathaban}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{exampleblock}{Key idea}
Whilst posteriors and evidences are invariant to repartitioning $\C[2]{\mathcal{L}}\to\C[2]{\tilde{\mathcal{L}}}$, the KL divergence is not.
        \end{exampleblock}
    \begin{itemize}
            \item For constant ``run quality'' $\sigma$, 
            \begin{gather*} 
                {\scriptstyle
                    T = T_{\C[2]{\mathcal{L}}} \times n_\text{live} \times \mathcal{D}_\text{KL} \times f_\text{sampler}, \quad
\sigma \approx \sqrt{\mathcal{D}_\text{KL}/n_\text{live}} }
    \\
\Rightarrow\boxed{T = T_{\C[2]{\mathcal{L}}} \times \sigma \times \mathcal{D}_\text{KL}^2 \times f_\text{sampler}} 
        \end{gather*}
            so if you can reduce the KL divergence, then quadratic gains to be made
        \item \texttt{SuperNest}~\arxiv{2212.01760}
        \item Ongoing work with Metha Prathaban \& Harry Bevins.
    \end{itemize}
        \column{0.5\textwidth}
        \vspace{8pt}
        \includegraphics[width=\textwidth]{figures/supernest.pdf}
    \begin{alertblock}{Frontier}
        Can we use posterior repartitioning to quadratically reduce the run time of nested sampling?
    \end{alertblock}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Parallelisation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Nested sampling is well-suited to parallelisation, up to the number of live points, in contrast with MH/HMC
            \item I've known people run nested sampling on 10,000 CPUs
            \item This is in addition to any existing parallelisation in the likelihood
            \item Dynamic nested sampling very useful here adding machinery for throwing away multiple points~\arxiv{1704.03459}
        \end{itemize}
        \begin{alertblock}{Frontier}
            Are there any other parallelisation strategies, e.g. clustering/divide \& conquer?
        \end{alertblock}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/parallelisation}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling with gradients}
    \student{namu_kroupa}{Namu Kroupa}{PhD (with Gábor Csányi)}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item  In principle, gradients give you $\sim\mathcal{O}(d)$ pieces of information to help with exploration
            \item With modern automatic differentiation codes (\texttt{JAX}, \texttt{PyTorch}, \texttt{TensorFlow}, \texttt{Enzyme} \ldots)
                    \\ these now come ``for free''.
            \item However nested sampling has unique challenges, so plugging in default (e.g. HMC/Langevin) algorithms does not work
        \end{itemize}
        \begin{alertblock}{Frontier}
            Can you come up with a nested sampling strategy that uses gradients and scales pass $d\sim\mathcal{O}(100)$?
        \end{alertblock}
        \column{0.5\textwidth}
        \vspace{6pt}
        \includegraphics<1|handout:3>[width=\textwidth,page=1]{figures/himmelblau_gradient}%
        \includegraphics<2|handout:3>[width=\textwidth,page=2]{figures/himmelblau_gradient}%
        \includegraphics<3|handout:3>[width=\textwidth,page=3]{figures/himmelblau_gradient}%
        \includegraphics<4|handout:3>[width=\textwidth,page=4]{figures/himmelblau_gradient}%
        \includegraphics<5|handout:3>[width=\textwidth,page=5]{figures/himmelblau_gradient}%
        \includegraphics<6|handout:3>[width=\textwidth,page=6]{figures/himmelblau_gradient}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling with gradients}
    \student{namu_kroupa}{Namu Kroupa}{PhD (with Gábor Csányi)}
    \begin{description}
        \item[Betancourt] Hamiltonian constrained nested sampling~\arxiv{1005.0157}.
        \item[Feroz] Galilean Nested Sampling~\arxiv{1312.5638}.
        \item[Skilling] Galilean and Hamiltonian Monte Carlo~\doi{10.3390/proceedings2019033019}
        \item[Speagle] Incorporated into \texttt{dynesty}~\arxiv{1904.02180}.
        \item[Habeck] Habeck Demonic nested sampling -- uses thermodynamic analogy to soften the hard boundary with a Maxwell daemon~\doi{10.1063/1.4905971}.
        \item[Baldock] Total Enthalpy HMC, incorporating momenta in a more HMC like way, but specialised to materials science~\arxiv{1710.11085}.
        \item[Cai] ProxNest for high-dimensional convex imaging problems~\arxiv{2106.03646}.
        \item[Lemos] Updated existing HMC/Galilean nested sampling to use differentiable programming (jax/torch) ICML 2024~\arxiv{2312.03911}
    \end{description}
    \begin{itemize}
        \item See also 2023 MIAPbP talk ``Gradients and nested sampling: The present state of the art''
        \item Come talk to me afterwards for further detail on what I'm working on with Namu Kroupa.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Reversible nested sampling}
    \begin{columns}
        \column{0.5\textwidth}
\vspace{-10pt}
        \begin{itemize}
            \item One could in principle reverse the direction of travel, and move outward from a peak.
            \item Need to know the initial volume $X_\text{start}\ne1$
            \item e.g. if a Laplace estimation were good enough at the peak.
            \item The same trick in normal nested sampling would reduce volume estimation error and hence improve scaling.
        \end{itemize}
        \begin{alertblock}{Frontier}
            \begin{itemize}
                \item Can you come up with a reversible nested sampling algorithm?
                \item Can you reduce the volume estimation error by clamping the end point?
            \end{itemize}
        \end{alertblock}
        
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<4|handout:0>[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<5|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<7-|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Memory scaling}
    \student{adam_ormondroyd}{Adam Ormondroyd}{PhD}
    \begin{columns}
        \column{0.4\textwidth}
    \begin{itemize}
        \item Memory is driven by needing to store:
            \begin{itemize}
                \item dead points $n_\text{dead}=n_\text{live}\times\mathcal{D}_\text{KL}\sim \mathcal{O}(d^2)$
                \item optionally their coordinates $\sim\mathcal{O}(d)$
                \item optionally further machinery for generating new live points $\sim\mathcal{O}(d)$
            \end{itemize}      
        \item \texttt{anesthetic}~\arxiv{1905.04768} as a performant tool for post-processing nested sampling runs
    \end{itemize}
        
        \column{0.6\textwidth}
        \includegraphics[width=\textwidth]{figures/dataframe}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\tthref{github.com/handley-lab}}

    \begin{columns}[t]
        \column{0.47\textwidth}
        \begin{block}{How fast in nested sampling?}
            \[ \boxed{T = T_{\C[2]{\mathcal{L}}} \times n_\text{live} \times \mathcal{D}_\text{KL} \times f_\text{sampler}} \]
        \end{block}
        \column{0.43\textwidth}
        \begin{block}{How accurate is nested sampling?}
            \[ \boxed{\sigma(\log\C[3]{\mathcal{Z}}) \approx \sqrt{\mathcal{D}_\text{KL}/n_\text{live}}} \]
        \end{block}
    \end{columns}

    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Scaling on the face of it $T\sim\mathcal{O}(d^4)$
            \item In reality can be anywhere between $T\sim\mathcal{O}(d_\text{slow})$ and $T\sim\mathcal{O}(d^4)$
            \item Memory scaling $\sim\mathcal{O}(d^{2-4})$
            \item Most people focus on improving $f_\text{sampler}$, but this is only a fraction of the story.
            \item Large accelerations to be made in the other terms
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Technical Review paper: Buchner \arxiv{2101.09675}
            \item Nature Review paper: \arxiv{2205.15570}
            \item \texttt{aeons}~\arxiv{2312.00294}
            \item \texttt{SuperNest}~\arxiv{2212.01760}
        \end{itemize}
    \end{columns}


    \tikz[overlay,remember picture]
    \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
        \includegraphics[width=0.09\textheight]{figures/students/adam_ormondroyd.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/david_yallup.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/dily_ong.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/george_carter.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/harry_bevins.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/kilian_scheutwinkel.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/metha_prathaban.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/namu_kroupa.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sinah_legner.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/thomas_gessey-jones.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/wei-ning_deng.jpg}%
    };
\end{frame}

\appendix
\begin{frame}
    \frametitle{Probabalistic volume estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Key idea in NS: estimating volumes probabilistically
                \[
                    \only<-2>{
                        \frac{\C[1]{V_\text{after}}}{\C[0]{V_\text{before}}} 
                        \approx \frac{\C[1]{n_\text{in}}}{\C[0]{n_\text{out}}+\C[1]{n_\text{in}}}
                    }
                    \only<3>{
                        \frac{\C[1]{V_\text{after}}}{\C[0]{V_\text{before}}} 
                        \approx \frac{\C[1]{n_\text{in}}+1}{\C[0]{n_\text{out}}+\C[1]{n_\text{in}}+2}
                    }
                    \only<4>{\hspace{-15pt}
                        \frac{\C[1]{V_\text{after}}}{\C[0]{V_\text{before}}} 
                        \sim \frac{\C[1]{n_\text{in}}+1}{\C[0]{n_\text{out}}+\C[1]{n_\text{in}}+2} \pm \sqrt{\tfrac{(\C[1]{n_\text{in}}+1)(\C[0]{n_\text{out}}+1)}{(\C[0]{n_\text{out}}+\C[1]{n_\text{in}}+2)^2(\C[0]{n_\text{out}}+\C[1]{n_\text{in}}+3)}}
                    }
                \]
            \item This is the \textbf{only} way to calculate volume in high dimensions $d>3$.
                \begin{itemize}
                    \item Geometry is exponentially inefficient.
                \end{itemize}
            \item This estimation process does not depend on geometry, topology or dimensionality
            \item Basis of all Monte-Carlo integration
            \item Nested Sampling uniquely uses a nested framework to couple together MC integrals in a robust, scalable manner.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/compression_1}%
        \includegraphics<2->[width=\textwidth]{figures/compression_2}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why dynamic nested sampling doesn't help}
    \includegraphics[width=\textwidth]{figures/dynamic}
    \vspace{-20pt}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item In dynamic nested sampling~\arxiv{1704.03459}, live points are varied by choosing to optionally create/delete at each iteration.
            \item Can use this to ``focus'' computational power on posterior.
        \end{itemize}
        \column{0.5\textwidth}
        \begin{itemize}
            \item However, this is at the cost of reducing evidence accuracy.
            \item Equivalent to saying ``We could just set $n_\text{live}$ very low'' as a solution to scaling.
        \end{itemize}
    \end{columns}
\end{frame}

\end{document}
```
# branch: remotes/origin/munich_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{white},   
    commentstyle=\color{C2},
    keywordstyle=\color{C3},
    numberstyle=\tiny\color{C0},
    stringstyle=\color{C1},
    basicstyle=\ttfamily\tiny,
	identifierstyle=\color{black},
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


% Commands
% --------
% - \arxiv{arxiv number}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Sampling techniques in high-dimensional parameter spaces}
\subtitle{with ScannerBit 2.0}
\date{23\textsuperscript{rd} Feb 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{What do I mean by parameter spaces?}
    \begin{itemize}
        \item In science we build models $M$ to describe data $D$
        \item These models usually come with a vector of tunable unknown parameters $\theta$
        \item The general task of science is to use data to define regions of ``good'' $\theta$
        \item This talks considers the (increasingly common) case where $\theta$ is high-dimensional
            \begin{itemize}
                \item High-dimensional: $d\ge4$
            \end{itemize}
        \item Let's look at some concrete examples across fields
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Cosmology \& Astrophysics}
    \begin{columns}
        \column{0.4\textwidth}
        \begin{itemize}
            \item Cosmological parameters:%
                \begin{itemize}
                    \item matter fraction $\Omega_m$
                    \item dark matter $\Omega_b$
                    \item dark energy $\Omega_\Lambda$
                    \item Hubble constant $H_0$
                    \item Initial conditions $(A_s, n_s)$
                    \item optical depth $\tau$
                \end{itemize}
            \item LIGO/Virgo/Kagra event:%
                \begin{itemize}
                    \item masses $m_1, m_2$
                    \item spins $\chi_1, \chi_2$
                    \item sky position $(\theta, \phi)$
                    \item orientation $(\iota, \psi)$
                    \item distance $d_L$
                    \item time \& phase $t_c, \Phi_c$
                \end{itemize}
            \item Bayesian framework: $P(\theta|D)$ defines ``credible regions''
        \end{itemize}
        
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/old_parameters}
        \column{0.25\textwidth}
        \includegraphics[width=\textwidth]{figures/darkmatterchart_squared}
        \includegraphics[width=\textwidth]{figures/universe_constraints_zoom}
        %TODO put in LIGO event
    \end{columns}

    
\end{frame}

\begin{frame}
    \frametitle{Particle physics}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The standard model has a several physical/phenomenological parameters
            \item Frequentist framework: $P(D|\theta)$ defines ``confidence intervals''
            \item topical: pMSSM ATLAS scans $d=19$
        \end{itemize}
        \centerline{
            \includegraphics[width=\textwidth]{figures/pmssm}
        }
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/gambit_examples}
    \end{columns}
    
\end{frame}

\begin{frame}
    \frametitle{Materials/Chemistry}
    \begin{columns}
        \column{0.65\textwidth}
        \begin{itemize}
            \item Here the relevant function is the free energy $F(\theta)$ as a function of system configuration (bond angles, bond lengths).
            \item Naturally very high dimensional $d\sim\mathcal{O}(10^2\to10^5)$.
            \item Complicated parameter spaces with many solutions of interest
            \item Boltzmann distribution $P(\theta) = e^{-\beta E(\theta)}$ defines the thermodnamic ensemble.
            \item Added challenge of temperature free parameter, meaning ideally one wants $Z(\beta) = e^{-\beta F(\beta)} = \int P(\theta|\beta) d\theta$
        \end{itemize}
        
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/materials}
        
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Biophysics}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item A specific molecular example is protein molecules
            \item Alphafold made waves in this space in 2022
            \item Essentially a powerful machine learning optimisation solution
            \item Full problem is far from solved!
            \item Ideally one would compute misfolds \& partition functions
        \end{itemize}
        
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/proteins}
        
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Machine learning}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Standard machine learning training setup:
                \[ \hat{\theta} = \arg\min_\theta L(\theta) \]
                \[ L(\theta) = \min_{\theta} \sum_{i\in\text{train}} |f(x_i;\theta)-y_i|^2 \]
                \begin{itemize}
                    \item $x$: input data
                    \item $y$: target data
                    \item $f$: neural network
                    \item $\theta$: model parameters (weights and biases)
                \end{itemize}
            \item The ``Loss function'' $L$ of the training/testing data defines the ``good'' $\theta$
            \item Usually optimised with gradient descent (adam).
        \end{itemize}
        
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/loss_landscape}

        \hfill\href{https://losslandscape.com}{losslandscape.com}
    \end{columns}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Why is high-dimensional $d\ge4$?}
    \begin{columns}
        \column{0.45\textwidth}
        \begin{itemize}
            \item 4D is the crossover where the ``curse of dimensionality'' kicks in.
            \item We can see this effect in practiceif we consider trying to do a 1, 2, 3 \& 4D integral numerically.
            \item Integration grinds to a halt as the number of hyperboxes in the grid becomes exponntially large.
            \item Visualisation requires a 2D projection
            \item 2D has very specific properties which do not lend themselves well to intuiting the behaviour of the full parameter space
        \end{itemize}
        \column{0.5\textwidth}
    \begin{lstlisting}[language=Python]
import numpy as np
from scipy.integrate import nquad

def f(*x):
    x = np.array([*x])
    return np.exp(-(x**2).sum())

nquad(f, [[-5, 5]])     # 322  micro s
nquad(f, [[-5, 5]] * 2) # 33.4 ms
nquad(f, [[-5, 5]] * 3) # 3.4  s 
nquad(f, [[-5, 5]] * 4) # ...
    \end{lstlisting}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
    
\end{frame}

\begin{frame}
    \frametitle{Observation \#1: The typical set is small}

    \begin{columns}
        \column{0.6\textwidth}
    \begin{itemize}
        \item The non-zero region of the function (the typical set) occupies a tiny fraction of the space
        \item From a 1D/2D projected view, this is not obvious
        \item If each parameter has a compressed region $\delta\theta_i$ relative to the initial bounds $\Delta\theta_i$, then the total fraction of the compressed region is
            $\prod_i^d \frac{\delta \theta_i}{\Delta \theta_i}$
        which is exponential in the number of parameters $d$
    \item This is generalised by the KL divergence:
\[\mathcal{D}_\mathrm{KL} = \int \mathcal{P}(\theta) \log\frac{\mathcal{P}(\theta)}{\pi(\theta)} d\theta \sim \log \frac{V_\pi}{V_\mathcal{P}}\]
    \item e.g. $6d$ cosmological Planck $\Lambda$CDM $e^{-\mathcal{D}_\mathrm{KL}}\sim 10^{-14}$
    \end{itemize}
        
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/volumes.pdf}
    \end{columns}

\end{frame}



\begin{frame}
    \frametitle{Observation \#2: The typical set is not at the peak}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item In low dimensions, the ``best fit point'' is in the ``best fit region''.
            \item This is not true in high dimensions.
            \item This can be thought of as a balance of:
                \begin{itemize}
                    \item probability density and volume
                    \item energy and entropy
                    \item likelihood and prior
                \end{itemize}
        \end{itemize}
        
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth, page=1]{figures/anatomy}%
        \includegraphics<2>[width=\textwidth, page=2]{figures/anatomy}%
        \includegraphics<3>[width=\textwidth, page=3]{figures/anatomy}%
        \includegraphics<4>[width=\textwidth, page=4]{figures/anatomy}%
        \includegraphics<5>[width=\textwidth, page=5]{figures/anatomy}%
        \includegraphics<6>[width=\textwidth, page=6]{figures/anatomy}%
        \includegraphics<7>[width=\textwidth, page=7]{figures/anatomy}%
        \includegraphics<8>[width=\textwidth, page=8]{figures/anatomy}%
        \includegraphics<9>[width=\textwidth, page=9]{figures/anatomy}%
        \includegraphics<10>[width=\textwidth, page=10]{figures/anatomy}%
        \includegraphics<11>[width=\textwidth, page=11]{figures/anatomy}%
        \includegraphics<12>[width=\textwidth, page=12]{figures/anatomy}%
        \includegraphics<13>[width=\textwidth, page=13]{figures/anatomy}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Principles of parameter exploration}
    \begin{enumerate}
        \item \textbf{What do I want to do?}  \textit{(Optimise/Profile/Sample)}
            \begin{itemize}
                \item Optimisation can be done with very few function calls
                \item exploration algorithms require more: $\mathcal{O}(10^4\to10^8)$ in practice
            \end{itemize}
        \item \textbf{How long does each function call take?} \textit{(Microseconds/Seconds/Days)}
            \begin{itemize}
                \item The first can be done on a laptop, the second on HPC, the third restricts one to optimisation
            \end{itemize}
        \item \textbf{How high dimensional is the problem?} \textit{($4\to10$, $10-500$, $500-10^6$, $>10^6$)}
            \begin{itemize}
                \item rejection/importance algorithms up to $\sim$10 dimensions (normalising flows, multinest, VEGAS)
                \item many statistical approaches can work in 100s (Gibbs, Slice, NS)
                \item Beyond that HMC is king up to millions.
                \item Current deep learning record training is 530 billion parameters.
            \end{itemize}
        \item \textbf{Do I have the gradient of the function?}
            \begin{itemize}
                \item autodiff from differentiable programming languages (\texttt{jax},\texttt{pytorch} etc) makes this more common
                \item some things are not differentiable even in principle
            \end{itemize}
        \item \textbf{Is the function an energy$\equiv$log probability?}
            \begin{itemize}
                \item Sampling is well defined for energies
                \item If you're optimising something non-probabilistic e.g.\ financial return, resolution of beam etc then it is more involved (but not impossible) to turn this into a probabilistic problem.
            \end{itemize}
    \end{enumerate}
\end{frame}

%vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv

\begin{frame}
    \frametitle{Exploring parameter spaces}
    \begin{description}
        \item[Option 1] Optimize
            \begin{itemize}
                \item Solve $\theta = \arg\max_\theta f(\theta)$
            \end{itemize}
        \item[Option 2] Sample
            \begin{itemize}
                \item Generate a set of samples $\{\theta_i\}$ from the distribution $\mathcal{P}(\theta)$
                \item Works if your function is a probability  in $\theta$ e.g.\ a Bayesian posterior  $P(\theta|D)$
            \end{itemize}
        \item[Option 3] Profile
            \begin{itemize}
                \item Solve $f_i = \max_{\theta_{j\ne i}} f(\theta)$ for a range of $\theta_i$
                \item i.e. fix one parameter, and maximise over the rest, to give a 1D profile
                \item Also works for 2D profiles.
            \end{itemize}
    \end{description}
    
\end{frame}


\begin{frame}
    \frametitle{Optimisation: Gradient descent (for highest dimensions)}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The first thing one thinks of.
            \item Fastest class of algorithms (used in deep learning e.g. Adam)
            \item Can get stuck in local minima
            \item Finds a maximum, but hard to use the samples it's calculated along the way.
            \item Nontrivial consideration:
                \begin{quote}
                    ``While finding the gradient of an objective function is a splendid
                    idea, ascending the gradient directly may not be.'' (MacKay)
                \end{quote}
                (Most gradient descent algorithms are not covariant out-of-the-box).
        \end{itemize}
        
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/gradient_descent}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Optimisation: Bayesian optimisation (for expensive functions)}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Best approach for punitively expensive functions
            \item Uses a Gaussian process to model the function, and propose the optimal new point which achieves an objective (e.g. maximisation), taking into account all uncertanties
            \item These are used to e.g. optimise hyperparameters in machine learning/N body simulations
        \end{itemize}
        
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/bayesian_optimisation}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Optimisation: Genetic Algorithms (for profiling)}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Algorithms use an evolutionary process:
                \begin{itemize}
                    \item Generate a population of points
                    \item Reproduce: Recombine, mutate
                    \item Kill off low ranking points
                \end{itemize}
            \item \texttt{diver} is the genetic algorithm for differential evolution preferred by GAMBIT
            \item Generally accepted as being good for solving multimodal functions
            \item Not statistically principled or interpretable if it goes wrong
            \item These in practice are good for profiling in medium dimensions, since the evolutionary process generates many good candidate points -- but technically a global optimiser!
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=0.5\textwidth]{figures/diver_1}%
        \includegraphics[width=0.5\textwidth]{figures/diver_2}
        \includegraphics[width=0.5\textwidth]{figures/diver_3}%
        \includegraphics[width=0.5\textwidth]{figures/diver_4}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{GAMBIT}
%    \begin{columns}
%        \column{0.4\textwidth}
%        \column{0.6\textwidth}
%        \includegraphics[width=\textwidth]{figures/gambit_layout}
%    \end{columns}
%\end{frame}



%\begin{frame}
%    \frametitle{Notation}
%    \begin{itemize}
%        \item Space of parameters $\theta$
%        \item Probability distribution/function $\mathcal{P}(\theta)$
%        \item Region of valid parameters/support of function/prior/measure $\pi(\theta)$
%            \begin{itemize}
%                \item Note that for any numerical method even if not specified there is an implicit measure imposed by floating point arithmetic
%            \end{itemize}
%    \end{itemize}
%\end{frame}

\begin{frame}
    \frametitle{Why do sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item The cornerstone of numerical Bayesian inference is working with \C[3]{samples}.
            \item Generate a set of representative parameters drawn in proportion to the posterior $\theta\sim\mathcal{P}$.
            \item The magic of marginalisation $\Rightarrow$ perform usual analysis on each sample in turn.
            \item The golden rule is \C[1]{stay in samples} until the last moment before computing summary statistics/triangle plots because \[\boxed{f(\:\av{X}\:)\ne \av{\:f(X)\:}}\]
            \item Generally need $\sim\mathcal{O}(12)$ independent samples to compute a value and error bar.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>{figures/volumes.pdf}%
        \includegraphics<2>{figures/samples.pdf}
    \end{columns}
\end{frame}


%\begin{frame}
%    \frametitle{Why do sampling}
%    \begin{itemize}
%        \item $\theta\sim\mathcal{P}$ drawn is optimally concentrated around the typical set of $\mathcal{P}$
%        \item This represents an optimal compression of the space
%        \item If you have generated a set of samples drawn from a distribution $S=\{\theta_i : i=1\cdots N,\theta_i\sim\mathcal{P}\}$, then one can compute integrals
%            \[ \langle f(\theta) \rangle_\mathcal{P} = \int f(\theta) \mathcal{P}(\theta) d\theta \approx \sum_{\theta_i\in S} f(\theta_i) \]
%    \end{itemize}
%\end{frame}

%\begin{frame}
%    \frametitle{The name of the game}
%    \begin{itemize}
%        \item Many considerations in this talk applicable to Bayesian, Frequentist or ``Monte Carlo'' 
%        \item I acknowledge my Bayesian bias!
%        \item In general consider a function $\mathcal{P}(\theta)$ over some $d$-dimensional space
%        \item This function can be ``forward calculated'': given any location $\theta$ in space, can compute $P$
%        \item Analytic pen-and-paper results assumed unavailable/impossible
%        \item We wish to:
%            \begin{itemize}
%                \item Explore the region(s) of high $\mathcal{P}$
%                \item Generate representative samples, either in this region, or in the tails
%                \item find the hypervolume under the curve $\int P(\theta) d\theta$
%            \end{itemize}
%        \item Examples include
%            \begin{itemize}
%                \item Generating samples from a Bayesian posterior distribution
%                \item Generating datasets for computing test statistics
%                \item Generating Monte Carlo events 
%                \item Computing Bayesian evidences for model comparison
%                \item Computing cross sections
%            \end{itemize}
%    \end{itemize}
%\end{frame}

%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

\begin{frame}
    \frametitle{Random scanning: how not to do it}
    \begin{itemize}
        \item The worst way to explore/integrate a probability distribution/generate samples is to randomly sample the space using $\pi(\theta)$~\arxiv{2012.09874}
        \item Gridding is also equivalently bad
        \item Whilst this works in low dimensions, if each parameter is confined within some fraction $f\sim\ell_\mathcal{P}/\ell_\pi$ of the space, then the volume fraction $\sim\mathcal{O}(f^d)$, or equivalently $\mathcal{D}\sim d\log f$ 
        \item Random sampling has an efficiency of $\boxed{\approx e^{-\mathcal{D}} \sim e^{-d\log f} = f^{-d}}$
        \item If you find that naive tail sampling is performant for e.g. importance sampling and unweighting, then your function likely has sublinear $\mathcal{D}$ scaling with $d$. 
        \item Turning this around, you can use the inefficiency of random sampling to estimate $\mathcal{D}$

        \item \emph{``Exploring phase space with Nested Sampling''} Handley, \textbf{Jan{\ss}en}, Schumann \& \textbf{Yallup} \arxiv{2205.02030}
            Also \textbf{Carragher} et al~\arxiv{2101.00428}
    \end{itemize}
\end{frame}




\begin{frame}
    \frametitle{Metropolis Hastings (baseline sampling algorithm)} 
    \begin{itemize}
        \item Turn the $N$-dimensional problem into a one-dimensional one.
        \item Pick start point $\theta_0$.
        \item At step $i$:
            \begin{enumerate}
                \item Propose a new point $\theta_{i+1}$ a small step away from $\theta_{i}$
                \item If uphill $\mathcal{P}(\theta_{i+1}) > \mathcal{P}(\theta_i)$, make step\ldots
                \item \ldots otherwise make step with probability $\alpha = \mathcal{P}(\theta_{i+1}) / \mathcal{P}(\theta_i)$. 
            \end{enumerate}
        \item Requires a proposal distribution $\mathcal{Q}(\theta_{i+1}|\theta_i)$
        \item In general case where $\mathcal{Q}$ is not symmetric, need acceptance ratio:
            \begin{equation*}
                \alpha = \frac{\mathcal{P}(\theta_{i+1})\mathcal{Q}(\theta_{i}|\theta_{i+1})}{\mathcal{P}(\theta_{i})\mathcal{Q}(\theta_{i+1}|\theta_{i})}
            \end{equation*}
    \end{itemize}
    \begin{block}{TOOLS}
        Whilst many exist: $\texttt{PyMC3}$, $\texttt{cobaya}$, $\texttt{MontePython}$,\ldots, in practice the algo is so simple, and the proposal distribution so problem-specific, it's usually relatively efficient to write your own.
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Metropolis Hastings } 
    \framesubtitle{Where can this go wrong?}
    \begin{itemize}
        \item Burn in
            \begin{itemize}
                \item It can take a while for the chain to equilibriate to the typical set
                \item It is hard to diagnose burn in, particularly in high dimensions
            \end{itemize}
        \item Multimodality
            \begin{itemize}
                \item If the function has multiple separated peaks, despite mathematical guarantees of convergence it will take a Hubble time to move between modes.
            \end{itemize}
        \item Correlated distributions
            \begin{itemize}
                \item In practice the peak(s) of the distribution have nontrivial structure (e.g. narrow ridges)
                \item Very hard to create a flexible enough proposal to accomodate all, and not strictly Markovian
            \end{itemize}
        \item Phase transitions
            \begin{itemize}
                \item A different kind of multi-modality, which can occur if the function is a ``slab and spike''
                \item Two regions -- one high-volume $V$ lower $\mathcal{P}$, the other low $V$ high $\mathcal{P}$. Difficult to transition
            \end{itemize}
        \item Poor parallelisation
            \begin{itemize}
                \item In practice it is not well parallelised, since majority of time is spent in burn-in
            \end{itemize}
    \end{itemize}
\end{frame}



\begin{frame}
    \frametitle{Hamiltonian Monte-Carlo (ultra high dimensional sampling)} 
    \begin{itemize}
        \item Key idea: Treat $\log L(\Theta)$ as a potential energy
        \item Guide walker under ``force'': \[F(\Theta) =-\nabla \log L(\Theta)\]
        \item Walker is naturally ``guided'' uphill
        \item Conserved quantities mean efficient acceptance ratios.
        \item Whilst the recieved wisdom is that this is ``tuning parameter free'', in practice the mass matrix has similar degrees of tuning unless the problem is already normalised (physicists naturally do this, which explains why it works so well out of the box).
            \begin{block}{TOOLS}
                \item \texttt{stan} is a fully fledged, mature programming language with HMC as a default sampler.
                \item \texttt{TensorFlow}, \texttt{PyTorch} \& \texttt{JAX} all have HMC implementations.
            \end{block}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Ensemble sampling (robust alternative to MH)} 
    \begin{itemize}
        \item Instead of one walker, evolve a set of $n$ walkers.
        \item Can use information present in ensemble to guide proposals.
        \item Generally tuning parameter free
        \item Struggles with multimodal distributions
        \item Strive to be affine invariant
    \begin{block}{TOOLS}
        \begin{itemize}
            \item \texttt{emcee}: The MCMC Hammer~\arxiv{1202.3665}
            \item \texttt{zeus}: Ensemble slice sampling~\arxiv{2002.06212}
        \end{itemize}
    \end{block}
    \end{itemize}
\end{frame}

%\begin{frame}
%    \frametitle{The fundamental issue with all of the above} 
%
%    \begin{itemize}
%        \item They can't integrate functions over the space
%            \begin{align}
%                Z
%                &= P(D|M) 
%                \nonumber\\
%                &= \int P(D|\Theta,M)P(\Theta|M) d\Theta 
%                \nonumber\\
%                &= \left\langle L \right\rangle_\pi
%                \nonumber
%            \end{align}
%        \item MCMC fundamentally explores the posterior, and cannot average over the prior.
%        \item Simulated annealing gives one possibility for ``tricking'' MCMC into computing evidences.
%            \begin{itemize}
%                \item Inspired by thermodynamics.
%                \item Suffers from similar issues to MCMC.
%                \item Unclear how to choose correct annealing schedule
%            \end{itemize}
%    \end{itemize}
%
%\end{frame}


\begin{frame}
    \frametitle{Nested sampling (high dimensional integration)}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n$ random samples over the space.
            \item Delete outermost sample, and replace with a new random one at higher integrand value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n}\only<9->{\pm \frac{1}{n}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \int f(x) dV \approx \sum_i f(x_i) \Delta V_i, \quad V_i = V_0 e^{-\only<9->{(}i\only<9->{\pm\sqrt{i})}/n} \]
%            \item Nested sampling: completely different way to scan.
%            \item Ensemble sampling compresses entire space$\to$peak(s).
%            \item Sequentially update a set $S$ of $n$ samples:
%                \begin{itemize}
%                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from a measure $\pi$). 
%
%                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
%                \end{itemize}
%            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard constraint}:
%                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
%            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence}/integral of function \& \C[0]{posterior}/sample weights.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4|handout:0>[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<8-         >[width=\textwidth,page=8]{figures/himmelblau}%
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{The nested sampling meta-algorithm: dead points}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item At the end, one is left with a set of discarded ``dead'' points.
%            \item Can be weighted to form posterior samples, prior samples, or anything in between.
%            \item Nested sampling estimates the \textbf{density of states} and calculates partition functions
%                \[Z(\beta) = \sum_i f(x_i)^\beta \Delta V_i.\]
%            \item The evolving ensemble of live points allows:
%                \begin{itemize}
%                    \item implementations to self-tune
%                    \item exploration of multimodal functions
%                    \item global and local optimisation
%                \end{itemize}
%            %\item Interpreted as a Bayesian algorithm, it
%            %    \begin{itemize}
%            %        \item Computes the Bayesian evidence (model comparison)
%            %        \item Produces (weighted) posterior samples (parameter estimation)
%            %    \end{itemize}
%        \end{itemize}
%        \column{0.5\textwidth}
%        \includegraphics[width=\textwidth,page=14]{figures/himmelblau}%
%        %\includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
%        %\includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{The dead measure~\arxiv{2312.00294}}
    \includegraphics[width=\textwidth]{figures/dead_measure}
    \begin{columns}
        \column{0.69\textwidth}
        \begin{itemize}
            \item Dead points have a unique scale-invariant distribution $\propto\: \tfrac{dV}{V}$.
            \item Uniform over original region, exponentially concentrating on region of interest (until termination volume).
            \item Full coverage of tails enables integration.
            \item Good for training emulators (HERA~\arxiv{2108.07282}).
        \end{itemize}
        \column{0.3\textwidth}
        \begin{block}{Applications}
        \begin{itemize}
            \item training emulators.
            \item gridding simulations
            \item beta flows\ldots
            \item good name for a band
        \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}


%\begin{frame}
%    \frametitle{Likelihood free inference}
%    <+Content+>
%\end{frame}






\begin{frame}
    \frametitle{Software frameworks}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Principled scientific data analysis requires a lot of different components:
                \begin{itemize}
                    \item Data
                    \item Theory
                    \item Statistics
                    \item Likelihoods
                    \item Samplers/scanners
                    \item Post-processing
                \end{itemize}
            \item Mature communities develop frameworks to do all of the above in one go:
                \begin{itemize}
                    \item Cosmology: \texttt{CosmoMC}, \texttt{MontePython}, \texttt{Cobaya}, \texttt{CosmoSIS}, \texttt{CosmoLike}
                    \item Gravitational Waves : \texttt{LAL}, \texttt{Bilby}, \texttt{PyCBC}
                    \item Particle physics: \texttt{GAMBIT}, \texttt{MasterCode}
                \end{itemize}
            \item Much harder if you want to combine data from multiple communities
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=0.5\textwidth]{figures/cmb}%
        \includegraphics[width=0.5\textwidth]{figures/theory}
        \begin{columns}
            \column{0.55\textwidth}
        \includegraphics[width=\textwidth]{figures/coadded}%
            \column{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/multinest}%

        \includegraphics[width=\textwidth]{figures/parameters}
            
        \end{columns}
    \end{columns}
\end{frame}

{
    \usebackgroundtemplate{\includegraphics[width=\paperwidth]{gambit_ad_slide}}
    \begin{frame}[plain]
    \end{frame}
}

\begin{frame}
    \frametitle{What is GAMBIT?}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{GAMBIT as a software framework}
            \begin{itemize}
                \item Combines collider, direct detection, neutrino \& telescope data.
                \item Allows joint analysis of dark matter, neutrinos \& BSM physics
                \item MPI + OpenMP parallelisation \\(record of 115,000 CPUs)
                \item Combines libraries \& codes written in:\\
                    \texttt{C++}, \texttt{Fortran}, \texttt{Python}, \texttt{Mathematica}\ldots
                \item Highly modular ``Bits''
                \item Often with several alternatives
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{GAMBIT as a community}
            \begin{itemize}
                \item Particle physicists, cosmologists \& statisticians ($>$80 members)
                \item Generates interdisciplinary expertise \& inspires new techniques
                \item Open source software
                \item Access to large community computing resources (40MCPUh/y)
                \item Modularity allows parallel teams
                \item Short-author papers by default
                \item in-person meetings every 9 months
            \end{itemize}
        \end{block}
    \end{columns}


%Greg

% More slides
% https://www.dropbox.com/s/2c7c475t6traj42/Data_at_UiO_2022_Kvellestad.pdf?dl=0

%Anders:
    % talk to atlas susy group
    % trivial point which landed well: if we're going to use a smart sampling algorithm, then parallelisation will be non-trivial
    % atlas recently released pmssm, completely random sampling, so they can do sampling, and then parallelised physics computations in increasing levels of complexity offline after the sampling
    % adaptive sampling algorithms _require_ all likelihood computations on the fly.
    % at the end of the day they just classify points as excluded or not
    % https://ui.adsabs.harvard.edu/abs/2022AAS...24030220M/abstract
    % https://www.overleaf.com/9325782714mghdvzmzxtpt#ba83d1
    
\end{frame}

\begin{frame}
    \frametitle{What is GAMBIT not?}
    \begin{columns}
        \column{0.65\textwidth}
        \begin{itemize}
            \item GAMBIT is not pip-installable
            \item It should be viewed as a ``power tool'' for heavy tasks
            \item Designed with HPC in mind 
                \begin{itemize}
                    \item expectation that forward modelling for frontier science takes seconds to hours
                \end{itemize}
            \item Combining data, theory \& code across multiple communities is non trivial
            \item Installing on a new system takes patience
                \begin{itemize}
                    \item Stickers for newcomers!
                \end{itemize}
        \end{itemize}
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/gambit_sticker}
    \end{columns}

\end{frame}

\begin{frame}[fragile]
    \frametitle{ScannerBit~\arxiv{1705.07959}}
    \begin{columns}
        \column{0.5\textwidth}
        Original paper:
        \begin{itemize}
            \item describes code design \& structure
            \item compares performance of the main two scanning algorithms: \texttt{diver} \& \texttt{MultiNest}
                \begin{itemize}
                    \item also compared \texttt{Great}(MH) and \texttt{Twalk}(affine~invariant) 
                \end{itemize}
            \item Discusses Frequentist \& Bayesian analyses
            \item 15D MSSM scalar singlet (2+13 parameters)
            \item Modular \& flexible plugins homogenise scanning algorithms (agnostic of model/prior)
        \end{itemize}
        
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/scannerbit_header}
        \includegraphics[width=0.5\textwidth]{figures/scannerbit_15d_diver_profile}%
        \includegraphics[width=0.5\textwidth]{figures/scannerbit_15d_mn_posterior}
    \end{columns}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ScannerBit 2.0}
    \begin{columns}
        \column{0.5\textwidth}
        Developments since original ScannerBit paper:
        \begin{enumerate}
            \item New scanning algorithms
                \begin{itemize}
                    \item \texttt{MINUIT2}
                    \item \texttt{PolyChord}
                    \item particle swarm
                \end{itemize}
            \item \texttt{PyScannerBit}
                \begin{itemize}
                    \item Isolation of scanners from GAMBIT
                    \item documentation: \href{https://pyscannerbit.readthedocs.io}{pyscannerbit.readthedocs.io}
                \end{itemize}
            \item Python Scanners
                \begin{itemize}
                    \item Allows easy incorporation of new scanning algorithms into GAMBIT
                    \item Modern machine learning techniques
                    \item (embarassingly) easy to incorporate
                \end{itemize}
        \end{enumerate}
        \begin{itemize}
            \item Paper release later this year.
            \item Beta testers welcome! \tiny{\href{mailto:gambit-scan@projects.hepforge.org}{gambit-scan@projects.hepforge.org}}
        \end{itemize}
        \column{0.5\textwidth}
\begin{lstlisting}[language=Python]
"""scipy.optimize scanner."""
import scanner_plugin as splug
import scipy.optimize
import numpy as np


class Minimize(splug.scanner):
    __version__ = scipy_version

    def __init__(self, **kwargs):
        super().__init__(use_mpi=False,
                         use_resume=False)

    def run(self):
        start = np.random.rand(self.dim)
        bounds = [(0., 1.)] * self.dim

        def neg_loglike_hypercube(x):
            return -self.loglike_hypercube(x)

        res = scipy.optimize.minimize(
                  neg_loglike_hypercube,
                  start, bounds=bounds)

        return 0

__plugins__ = {"scipy_minimize": Minimize}
\end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}[fragile]
    \frametitle{ScannerBit 2.0}
    \begin{columns}
        \column{0.5\textwidth}
        Developments since original ScannerBit paper:
        \begin{enumerate}
            \item New scanning algorithms
                \begin{itemize}
                    \item \texttt{MINUIT2}
                    \item \texttt{PolyChord}
                    \item particle swarm
                \end{itemize}
            \item \texttt{PyScannerBit}
                \begin{itemize}
                    \item Isolation of scanners from GAMBIT
                    \item documentation: \href{https://pyscannerbit.readthedocs.io}{pyscannerbit.readthedocs.io}
                \end{itemize}
            \item Python Scanners
                \begin{itemize}
                    \item Allows easy incorporation of new scanning algorithms into GAMBIT
                    \item Modern machine learning techniques
                    \item (embarassingly) easy to incorporate
                \end{itemize}
        \end{enumerate}
        \begin{itemize}
            \item Paper release later this year.
            \item Beta testers welcome! \tiny{\href{mailto:gambit-scan@projects.hepforge.org}{gambit-scan@projects.hepforge.org}}
        \end{itemize}
        \column{0.5\textwidth}
\begin{lstlisting}[language=Python]
"""Grid scanner with MPI."""
import scanner_plugin as splug
from utils import MPI
import numpy as np

class Grid(splug.scanner):

    __version__="1.0.0"

    def __init__(self, grid_pts=10, parameters=[],
                 **kwargs):
        super().__init__(use_mpi=True, use_resume=False)
        # Set up grid
        for param in self.parameter_names:
            n = grid_pts[parameters.index(param)]
            self.vecs.append(np.linspace(0.0, 1.0, n))
    
    def run(self):
        # Loop over grid with MPI
        grid = np.meshgrid(*self.vecs)
        grid = grid.reshape(self.dim, -1).T
        grid = grid[self.mpi_rank:self.size:
                    self.mpi_size]
        for pt in grid:
            self.loglike_hypercube(pt)
        return 0

__plugins__={"python_grid": Grid}
\end{lstlisting}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT highlights}
    \begin{itemize}
        \item Projects I have been involved in:
            \begin{description}
                \item[CosmoBit] \arxiv{2009.03286}(main paper) \arxiv{2009.03287}(neutrinos)
                \item[DMEFT] \arxiv{2106.02056}
                \item[CosmoALP] \arxiv{2205.13549}
            \end{description}
        \item Projects I'm currently involved in:
            \begin{description}
                \item[AnnualModulation] DAMA/LIBRA, COSINE-100, ANAIS, SABRE comparison + EFT
                \item[SubGeVDM] Global analysis of sub-GeV dark matter
            \end{description}
        \item GAMBIT Role: CosmoBit convener
    \end{itemize}

\end{frame}

    % UPCOMING:

\begin{frame}
    \frametitle{Upcoming GAMBIT developments}
    \begin{itemize}
        \item ScannerBit 2.0
            \begin{itemize}
                \item Next-generation scanning algorithms
            \end{itemize}
        \item GAMBIT-light
            \begin{itemize}
                \item Installing GAMBIT framework independent of heavyweight HEP codes
            \end{itemize}
        \item GravBit
            \begin{itemize}
                \item Beginning with PTAs
                \item Moving on to GW events
            \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Conclusions}
    \begin{itemize}
        \item Exploring high dimensional parameter spaces is challenging, and an area of active research
        \item If you would like to learn more about GAMBIT:\\
            \href{https://gambitbsm.org/}{gambitbsm.org}
        \item If you would like to find out about joining GAMBIT:
            email \href{mailto:gambit-community@projects.hepforge.org}{gambit-community@projects.hepforge.org}
    \end{itemize}
\end{frame}

\appendix
\begin{frame}
    \frametitle{FAQs}
    
    \begin{itemize}
    \item What was that awesome website? \\
    \hfill Full credit to Chi-feng for this incredible online demonstration tool\\
    \hfill \href{https://chi-feng.github.io/mcmc-demo/}{chi-feng.github.io/mcmc-demo/}

    \item How do you make your plots look hand-drawn? \\
        \hfill \texttt{import matplotlib.pyplot as plt; plt.xkcd()}
    \end{itemize}
\end{frame}



\begin{frame}
    \frametitle{A note on adaptive scanning algorithms}
    Some analyses (e.g. ATLAS) have a legitimate preference for random sampling because it allows parallelised physics computations offline.

    Smart sampling algorithms require all likelihood computations on the fly, so need machinery like GAMBIT
\end{frame}


\end{document}
```
# branch: remotes/origin/oxford_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{fit}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Next generation inference tools for cosmology and beyond}
%\subtitle{}
\date{23\textsuperscript{rd} Jan 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{The golden age of cosmology data}
    \begin{columns}
        \column{0.45\textwidth}
        \begin{itemize}
            \item Over our research lifetimes we will see next-generation data rates across the electromagnetic spectrum \& beyond:
                \begin{description}
                    \item[Radio] SKA \textit{et al}
                    \item[Micro] SO/CMB-S4
                    \item[IR] JWST, Roman (WFIRST)
                    \item[Optical] Euclid, DESI, Rubin (LSST), EELT
                    \item[X-ray] Athena
                    \item[Gamma-ray] e-ASTROGAM
                    \item[Gravitational] LIGO/Virgo/Kagra + LISA
                    \item[Particle] CTA, IceCube, KM3NeT
                \end{description}
        \end{itemize}
        \column{0.55\textwidth}

        \includegraphics[height=0.145\textwidth]{figures/telescopes/jwst}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/roman}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/euclid}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/athena}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/lisa}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/e-ASTROGAM}%
        \vspace{-1pt}

        \includegraphics[height=0.15183\textwidth]{figures/telescopes/desi}%
        \includegraphics[height=0.15183\textwidth]{figures/telescopes/eelt}%
        \includegraphics[height=0.15183\textwidth]{figures/telescopes/ska}%
        \includegraphics[height=0.15183\textwidth]{figures/telescopes/SO}%
        \vspace{-1pt}

        \includegraphics[height=0.18428\textwidth]{figures/telescopes/ligo}%
        \includegraphics[height=0.18428\textwidth]{figures/telescopes/km3n}%
        \includegraphics[height=0.18428\textwidth]{figures/telescopes/icecube}%
        \includegraphics[height=0.18428\textwidth]{figures/telescopes/CTA}%

        \begin{itemize}
            \item We are moving from an age of \textbf{precision} cosmology to \textbf{accurate} cosmology.
            \item \textbf{Systematics} $\gtrsim$ \textbf{statistics}.
            \item Tools risk lagging behind hardware
        \end{itemize}

    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Tensions in cosmology}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item \textbf{Data}:
                $H_0$, $S_8$, $A_L/\Omega_K$, Li
            \item \textbf{Theory}:
                Initial conditions, Entropy, Dark energy, dark matter, quantum gravity
            \item \textbf{Analysis}:
            \item Disentangling systematics from new physics is challenging!
            \item Almost all cosmological analyses pragmatically assume a fiducial flat $\Lambda$CDM assumption during their analyses.
            \item Unless this is resolved, we risk confirmation bias in the analysis of next-generation data.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[height=0.45\textwidth]{figures/hubble_tension}%
        \includegraphics[height=0.45\textwidth]{figures/hubble_survey}
        \includegraphics[height=0.5\textwidth]{figures/S8}%
        \includegraphics[height=0.5\textwidth]{figures/curvature_3}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Structure of talk}
    \begin{enumerate}
        \item Likelihood-based inference
        \item Nested sampling
        \item Marginal hierarchical inference
        \item Simulation-based inference
        \item Fully Bayesian Forecasts
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Bayesian notation}
    \begin{itemize}
        \item A ``generative'' model $M$, with tunable parameters $\theta$, describing (compressed) data $D$.
            \begin{itemize}
                \item e.g. $M=\Lambda$CDM, $\theta=\{\Omega_b,\Omega_c, \tau, H_0, A_s, n_s\}$, $D=\{C_\ell\}$.
            \end{itemize}
        \item Described by simulation process $\theta\to D$, or likelihood $P(D|\theta,M)$.
        \item Frequentists \& Bayesians agree on the likelihood.
        \item Bayesians treat parameter space $\theta$ the same as data space $D$.
        \item Quantifying uncertainty with probability using Bayes theorem:
            \[
                \C[0]{P}(\theta|D,M) = \frac{\C[2]{P}(D|\theta,M)\C[1]{P}(\theta|M)}{\C[3]{P}(D|M)},
                \quad
                \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}}\times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}},
                \quad
                \C[0]{\mathcal{P}}(\theta|D) = \frac{\C[2]{\mathcal{L}}(D|\theta)\C[1]{\pi}(\theta)}{\C[3]{\mathcal{Z}}(D)}.
            \]
        \item Follows from the oft-forgotten Joint (the probability of everything):
            \[
                \C[0]{\mathcal{P}}\times\C[3]{\mathcal{Z}} = \C[4]{\mathcal{J}} = \C[2]{\mathcal{L}}\times\C[1]{\pi}, \qquad \C[4]{P}(D,\theta|M) = \C[4]{\text{Joint}} = \C[4]{\mathcal{J}}  
            \]
        \item Also relevant (in many overlapping contexts) is the dimensionless ratio
            \[\C[5]{r} = \frac{\C[4]{\mathcal{J}}}{\C[1]{\pi}\times\C[3]{\mathcal{Z}}} = \frac{\C[2]{\mathcal{L}}}{\C[3]{\mathcal{Z}}} = \frac{\C[0]{\mathcal{P}}}{\C[1]{\pi}}\]
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The three pillars of Bayesian inference}
    \vspace{-30pt}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Parameter estimation}
            What do the data tell us about the parameters of a model?

            \textit{e.g. the size or age of a $\Lambda$CDM universe}
            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Model comparison}
            How much does the data support a particular model?

            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
            \[ \C[6]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[7]{P(M)}}{\C[8]{P(D)}} \vspace{-7pt}\]
            \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \C[7]{\Pi_\mathcal{M}}}{\C[8]{\sum_m Z_m \Pi_m}} \]
            \[ \C[6]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[7]{\text{Prior}}}{\C[8]{\text{Normalisation}}}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Tension quantification}
            Do different datasets make consistent predictions from the same model? 
            \textit{e.g. CMB vs Type IA supernovae data}
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
        \end{block}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{LBI: Likelihood-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        The standard approach if you are fortunate enough to have a likelihood function $\mathcal{L}(D|\theta)$: 
        e.g recent DES analysis \arxiv{2401.02929}
        \begin{enumerate}
            \item Define prior $\pi(\theta)$ 
                \begin{itemize}
                    \item spend some time being philosophical
                \end{itemize}
            \item Sample posterior $\mathcal{P}(\theta|D)$ 
                \begin{itemize}
                    \item use out-of-the-box MCMC tools such as\\ \texttt{emcee} or \texttt{MultiNest}
                    \item make some triangle plots
                \end{itemize}
            \item Optionally compute evidence $\mathcal{Z}(D)$
                \begin{itemize}
                    \item e.g. nested sampling or parallel tempering
                    \item do some model comparison (i.e. science)
                \end{itemize}
            \item Optionally talk about tensions
                \begin{itemize}
                    \item Bayes ratio (Nested Sampling)
                    \item Suspiciousness (MCMC)~\arxiv{2007.08496}
                \end{itemize}
        \end{enumerate}
        \column{0.5\textwidth}
        \hfill\includegraphics[width=0.6\textwidth]{figures/des_parameters.pdf}
        \includegraphics[width=0.5\textwidth]{figures/des_model_comparison.pdf}%
        \includegraphics[width=0.5\textwidth]{figures/des_suspiciousness.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.48\textwidth}
        \begin{block}{\textbf{MCMC}}
%            \only<16>{
%                \begin{itemize}
%                    \item Single ``walker''
%                    \item Explores posterior
%                    \item Fast, if proposal matrix is tuned
%                    \item Parameter estimation, suspiciousness calculation
%                    \item Channel capacity optimised for generating posterior samples
%                \end{itemize}
%            }
        \end{block}
        \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
        \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
        \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
        \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
        \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
        \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
%        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.48\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
%            \only<16>{
%                \begin{itemize}
%                    \item Ensemble of ``live points''
%                    \item Scans from prior to peak of likelihood
%                    \item Slower, no tuning required
%                    \item Parameter estimation, model comparison, tension quantification
%                    \item Channel capacity optimised for computing partition function
%                \end{itemize}
%            }
        \end{block}
        \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
        \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
%        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} 
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: Lebesgue integration}
    \begin{columns}
        \column{0.52\textwidth}
        \begin{itemize}
            \item Full dead-point coverage of tails enables integration.
            \item Can be weighted to form posterior samples, prior samples, or anything in between.
            \item Nested sampling estimates the \textbf{density of states} and calculates partition functions
                \[Z(\beta) = \sum_i f(x_i)^\beta \Delta V_i.\]
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune
                    \item exploration of multimodal functions
                    \item global and local optimisation
                \end{itemize}
            %\item Interpreted as a Bayesian algorithm, it
            %    \begin{itemize}
            %        \item Computes the Bayesian evidence (model comparison)
            %        \item Produces (weighted) posterior samples (parameter estimation)
            %    \end{itemize}
        \end{itemize}
        \column{0.48\textwidth}
        \includegraphics[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<1|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        %\includegraphics<2          >[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Standard uses of nested sampling}
    %\only<1>{\student{adam_ormondroyd}{Adam Ormondroyd}{PhD}}
    %\only<2>{\student{metha_prathaban}{Metha Prathaban}{PhD}}
    \begin{columns}
        \column{0.55\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Battle-tested in Bayesian cosmology on
                \begin{itemize}
                    \item Parameter estimation: multimodal alternative to MCMC samplers.
                    \item Model comparison: using integration to compute the Bayesian evidence
                    \item Tension quantification: using deep tail sampling and suspiciousness computations.
                \end{itemize}
            \item Plays a critical role in major cosmology pipelines: Planck, DES, KiDS, BAO, SNe.
            \item The default $\Lambda$CDM cosmology is well-tuned to have Gaussian-like posteriors for CMB data. 
            \item Less true for alternative cosmologies/models and orthogonal datasets, so nested sampling crucial.
            \item Also used in Gravitational Waves \& Exoplanets
            \item Often taken as ``ground truth to beat''.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/pps_both}
        \includegraphics[width=0.49\textwidth]{figures/reach_fit-cropped.pdf}
        %\includegraphics[width=0.49\textwidth]{figures/curvature_3}
        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Marginal inference}
    \student{harry_bevins}{Harry Bevins}{PhD$\to$JRF}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Many cosmological likelihoods come with nuisance parameters that have limited relevance for onward inference.
            \item Notation: \only<1>{CMB cosmology} \only<2>{GW cosmology}
                \begin{itemize}
                    \item[$\mathcal{L}$] Likelihood \hfill (e.g. \only<1>{\texttt{plik}}\only<2>{LAL}),
                    \item[$D$] Data \hfill (e.g. \only<1>{CMB}\only<2>{GW170817}),
                    \item[$\theta$] Cosmological parameters \hfill (e.g. \only<1>{$\Omega_m$}, $H_0$\ldots),
                    \item[$\alpha$] Nuisance parameters \hfill (e.g. \only<1>{$A_\text{planck}$}\only<2>{$m_1$, $m_2$}\ldots),
                    \item[$M$] Model \hfill (e.g. $\Lambda$CDM).
                \end{itemize}
            \item Some marginal statistics (e.g. marginal means, posteriors\ldots) are easy to compute.
            \item More machinery is needed for e.g. nuisance marginalised likelihoods and marginal KL divergences $\mathcal{D}_\text{KL}$.
        \end{itemize}
        \column{0.5\textwidth}
        \vspace{10pt}
        \includegraphics<1>{figures/planck_2018_plik.pdf}%
        \includegraphics<2>[width=\textwidth]{figures/standardsirens}
        \includegraphics<2>[width=\textwidth]{figures/graveyard}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nuisance marginalised likelihoods: Theory {\small\arxiv{2207.11457}}}
    \student{harry_bevins}{Harry Bevins}{PhD$\to$JRF}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item Bayes theorem
                \begin{align}
                    \C[2]{\mathcal{L}}(\theta,\alpha) 
                    \times 
                    \C[1]{\pi}(\theta,\alpha) &= 
                    \C[0]{\mathcal{P}}(\theta,\alpha)
                    \times
                    \C[3]{\mathcal{Z}}\\
                    \C[2]{\text{Likelihood}}
                    \times
                    \C[1]{\text{Prior}}
                    &=
                    \C[0]{\text{Posterior}}
                    \times
                    \C[3]{\text{Evidence}}
                    \nonumber
                \end{align}
                \small{$\alpha$: nuisance parameters, $\theta$: cosmo parameters.}
            \item Marginal Bayes theorem
                \begin{equation}
                    \C[2]{\mathcal{L}}(\theta) 
                    \times 
                    \C[1]{\pi}(\theta) = 
                    \C[0]{\mathcal{P}}(\theta)
                    \times
                    \C[3]{\mathcal{Z}}
                \end{equation}
            \item Non-trivially gives \textbf{nuisance-free likelihood}
                \begin{equation}
                    \boxed{
                        \C[2]{\mathcal{L}}(\theta) 
                        = 
                        \frac{
                            \C[0]{\mathcal{P}}(\theta)
                            \C[3]{\mathcal{Z}}
                        }{
                            \C[1]{\pi}(\theta)
                        }
                    }
                    =
                    \frac{
                        \int \C[2]{\mathcal{L}}(\theta,\alpha) \C[1]{\pi}(\theta,\alpha) d{\alpha}
                    }
                    {
                        \int \C[1]{\pi}(\theta,\alpha) d{\alpha}
                    }
                \end{equation}
        \end{itemize}
        \column{0.5\textwidth}
        \textbf{Key properties}
        \begin{itemize}
            \item Given datasets $A$ and $B$, each with own nuisance parameters $\alpha_A$ and $\alpha_B$:
            \item If you use $\mathcal{L}_A(\theta)$, you get the same (marginal) posterior and evidence if you had run with nuisance parameters $\alpha_A$ (ditto $B$).
            \item If you run inference on $\mathcal{L}_A(\theta)\times\mathcal{L}_B(\theta)$, you get the same (marginal) posterior and evidence if you had run with all nuisance parameters $\alpha_A$, $\alpha_B$ on.
            \item[] \textit{(weak marginal consistency requirements on joint $\pi(\theta,\alpha_A,\alpha_B)$ and marginal priors)}
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nuisance marginalised likelihoods: Practice~{\small\arxiv{2205.12841}}}
    \student{harry_bevins}{Harry Bevins}{PhD$\to$JRF}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{columns}
            \column{0.3\textwidth}
            \[
                \boxed{
                    \C[2]{\mathcal{L}}(\theta) 
                    = 
                    \frac{
                        \C[0]{\mathcal{P}}(\theta)
                        \C[3]{\mathcal{Z}}
                    }{
                        \C[1]{\pi}(\theta)
                    }
                }
            \]
            \column{0.7\textwidth}
            \begin{itemize}
                \item To compute the nuisance marginalised likelihood, need:
                    \begin{enumerate}
                        \item Bayesian evidence $\C[3]{\mathcal{Z}}$
                        \item Marginal prior and posterior \textbf{densities}
                    \end{enumerate}
            \end{itemize}
        \end{columns}
        \begin{enumerate}
            \item Bayesian evidence $\C[3]{\mathcal{Z}}$:                              g
                \begin{itemize}
                    \item Nested sampling
                    \item Parallel tempering (pocomc, ptmcmc)
                    \item Sequential Monte Carlo (SMC)
                    \item MCEvidence
                \end{itemize}
            \item Marginal prior $\C[1]{\pi}(\theta)$ and posterior $\C[0]{\mathcal{P}}(\theta)$ densities:
                \begin{itemize}
                    \item Histograms of samples
                    \item Kernel density estimation
                    \item Normalising flows / Diffusion models
                    \item \ldots
                \end{itemize}
        \end{enumerate}
        \begin{itemize}
            %\item Combination termed \texttt{margarine} \arxiv{2205.12841}
            \item Emulators usually much faster than original likelihoods
            \item \texttt{margarine}: PyPI, \href{https://github.com/htjb/margarine}{github.com/htjb/margarine}
        \end{itemize}

        \column{0.4\textwidth}
        \begin{tikzpicture}[
                rednode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
                bluenode/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
                greennode/.style={rectangle, draw=green!60, very thick, minimum size=5mm},
                node distance=0.5cm,
                remember picture, overlay
            ]
            \node<1->[bluenode, xshift=0.5\textwidth, yshift=-0.25\textwidth](likelihood) at (current page.north)  {$ \mathcal{L}(\theta,\alpha)$};
            \node<1->[bluenode, right = of likelihood.east](prior) {$ \pi(\theta,\alpha)$};

            \coordinate<1-> (likelihoodprior) at ($(likelihood.south)!0.5!(prior.south)$);

            \node<2->[rednode, below = of likelihoodprior](nestedsampling) {Nested Sampling};
            \draw<2->[->](likelihood.south) -- (likelihood|-nestedsampling.north);
            \draw<2->[->](prior.south) -- (prior|-nestedsampling.north);

            \node<3->[bluenode, below = of nestedsampling](posterior) {$ \{\theta,\alpha\}_\mathcal{P}$};
            \draw<3->[->](nestedsampling.south-|posterior) -- (posterior.north);
            \node<4->[bluenode, left = of posterior.west](evidence) {$ \mathcal{Z}$};
            \draw<4->[->](nestedsampling.south-|likelihood) -- (evidence.north);
            \node<5->[bluenode, right = of posterior.east](priorSamples) {$ \{\theta,\alpha\}_\pi$};
            \draw<5->[->](nestedsampling.south-|prior) -- (priorSamples.north);

            \coordinate<5-> (posteriorprior) at ($(posterior.south)!0.5!(priorSamples.south)$);

            \node<6->[rednode, below = of posteriorprior](margarine)  {Density Estimation};

            \draw<6->[->](posterior.south) -- (margarine.north-|posterior.east);
            \draw<6->[->](priorSamples.south) -- (margarine.north-|priorSamples.west);

            \node<7->[bluenode, below = of posterior|-margarine.south](marginalPosterior) {$ \mathcal{P}(\theta)$};


            \draw<7->[->](margarine.south-|marginalPosterior.east) -- (marginalPosterior.north);


            \node<8->[bluenode, below = of marginalPosterior.south-|margarine.south-|priorSamples](marginalPrior) {$ \pi(\theta)$};
            \draw<8->[->](margarine.south-|priorSamples.west) -- (marginalPrior.north);


            \node<9->[bluenode, below = of marginalPosterior](marginalLikelihood) {$ \mathcal{L}(\theta)$};


            \draw<9->[->](evidence.south) -- (marginalLikelihood.west);
            \draw<9->[->](marginalPosterior.south) -- (marginalLikelihood.north);
            \draw<9->[->](marginalPrior.west) -- (marginalLikelihood.east);

            \node<10->[greennode,behind path,fit=(nestedsampling) (marginalPosterior) (priorSamples) (evidence),] {};

        \end{tikzpicture}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nuisance marginalised likelihoods: Example uses}
    \begin{itemize}
        \item Library of pre-trained bijectors to be used as priors/emulators/nuisance marginalised likelihoods (DiRAC allocation \texttt{unimpeded})
        \item e.g. easy to apply a \textit{Planck}/DES/HERA/JWST prior or likelihood to your existing MCMC chains without needing to install the whole cosmology machinery.
        \item Hierarchical modelling:
            \begin{itemize}
                \item Usually, have $N$ objects, each with nuisance parameters $\alpha_i$, and shared parameters of interest $\theta$.

                \item Likelihood $\C[2]{\mathcal{L}}(\{D_i\}|\theta,\{\alpha_i\}) = \prod_i^N \C[2]{\mathcal{L}}_i(D_i|\theta,\alpha_i)$ has  $N\times \texttt{len}(\alpha_i) + \texttt{len}(\theta)$ parameters
                \item Instead, break problem down into $N$ runs on $\texttt{len}(\theta) + \texttt{len}(\alpha_i)$ parameters, and one final one on $\texttt{len}(\theta)$ parameters, using nuisance marginal likelihoods $\mathcal{L}_i(\theta)$.
                \item In addition to computational tractability, also can perform model comparison with nuisance marginalised likelihoods.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{SBI: Simulation-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Only have access to a forward model $\theta \rightarrow D$.
            \item $(\theta,D)$ plane gives a more expansive theoretical view of inference.
            \item Forward model defines \emph{implicit} likelihood~$\C[2]{\mathcal{L}}$:
            \item Simulator generates samples from $\C[2]{\mathcal{L}}(D|\theta)$.
            \item With a prior $\C[1]{\pi}(\theta)$ can generate samples from joint distribution~$\C[4]{\mathcal{J}}(\theta,D)=\C[2]{\mathcal{L}}(D|\theta)\pi(\theta)$\\\hfill \emph{the ``probability of everything''}.
            \item Task of SBI is then to go from joint~$\C[4]{\mathcal{J}}$ samples to posterior $\C[0]{\mathcal{P}}(\theta|D)$ and evidence $\C[3]{\mathcal{Z}}(D)$ -- and possibly likelihood $\C[2]{\mathcal{L}}(D|\theta)$.
            \item Present SotA: NPE, NLE, NJE, NRE
            \item SBI \& forward modelling force us to think about data space~$D$ \& parameter space~$\theta$.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[page=1, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<2>[page=2, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<3>[page=3, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<4>[page=4, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<5>[page=5, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<6>[page=6, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<7>[page=7, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<8>[page=8, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<9>[page=9, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<10>[page=10, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<11>[page=11, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<12>[page=12, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<13>[page=13, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<14>[page=14, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<15>[page=15, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<16>[page=16, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<17>[page=17, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<18>[page=18, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<19>[page=19, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<20>[page=20, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<21>[page=21, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why SBI?}
    \begin{columns}
        \column{0.6\textwidth}
        SBI is useful because:
        \begin{enumerate}
            \item If you don't have a likelihood, you can still do inference
                \begin{itemize}
                    \item The usual case beyond CMB cosmology
                \end{itemize}
            \item Faster than LBI
                \begin{itemize}
                    \item emulation -- also applies to LBI in principle
                \end{itemize}
            \item No need to pragmatically encode fiducial cosmologies
                \begin{itemize}
                    \item Covariance computation implicitly encoded in simulations
                \end{itemize}
            \item Equips AI/ML with Bayesian interpretability
            \item Lower barrier to entry than LBI
                \begin{itemize}
                    \item Much easier to forward model a systematic
                    \item Emerging set of plug-and-play packages
                    \item For this reason alone, it will come to dominate scientific inference
                \end{itemize}
        \end{enumerate}

        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/sbi_screenshot}
        \href{https://github.com/sbi-dev}{github.com/sbi-dev}
        \includegraphics[width=\textwidth]{figures/swyft_screenshot}
        \href{https://github.com/undark-lab/swyft}{github.com/undark-lab/swyft}
        \includegraphics[width=\textwidth]{figures/selfi_screenshot}
        \href{https://github.com/florent-leclercq/pyselfi}{github.com/florent-leclercq/pyselfi}
        \includegraphics[width=\textwidth]{figures/delfi_screenshot}
        \href{https://github.com/justinalsing/pydelfi}{github.com/justinalsing/pydelfi}



    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{SBI \& model comparison}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{itemize}
            \item Extend: models $A$ and $B$.
            \item Each with own separate parameters $\theta_A$ and $\theta_B$ (can be same).
            \item The evidence $\C[3]{\mathcal{Z}}(D|M)$ compares models
            \item Occams razor: more~predictive $\equiv$~more~probable \\(due to normalisation).
        \end{itemize}

        \column{0.7\textwidth}
        \includegraphics<1>[page=1, width=\textwidth]{figures/sbi_model_comparison.pdf}%
        \includegraphics<2>[page=2, width=\textwidth]{figures/sbi_model_comparison.pdf}%
        \includegraphics<3>[page=3, width=\textwidth]{figures/sbi_model_comparison.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling and Simulation based inference}
    \student{kilian_scheutwinkel}{Kilian Scheutwinkel}{PhD}
    \begin{columns}[t]
        \column{0.45\textwidth}
        \begin{block}{Weak SBI}
            \begin{itemize}
                \item Use model comparison to choose between likelihoods
                \item Use flexible likelihood (e.g. unknown noise scale $\sigma$, non-gaussian shape, mixture components~\arxiv{1809.04598})
                \item 21cm~\arxiv{2204.04491}, SNe~\arxiv{2312.02075}
            \end{itemize}

        \end{block}
        \column{0.45\textwidth}
        \begin{block}{Strong SBI}
            \begin{itemize}
                \item Develop ``likelihood-free nested sampling''
                \item Use dead points to train NRE
                \item Replaces/enhances current SotA of truncation techniques
            \end{itemize}
        \end{block}
    \end{columns}
    \centerline{
        \includegraphics[width=0.8\textwidth]{figures/dead_measure}
    }
\end{frame}



\begin{frame}
    \frametitle{Cosmological forecasting}
    \framesubtitle{Have you ever done a Fisher forecast, and then felt Bayesian guilt?}
    \vspace{-20pt}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item Cosmologists are interested in forecasting what a Bayesian analysis of future data might produce.
            \item Useful for:
                \begin{itemize}
                    \item white papers/grants,
                    \item optimising existing instruments/strategies,
                    \item picking theory/observation to explore next.
                \end{itemize}
            \item To do this properly:
                \begin{enumerate}
                    \item start from current knowledge $\pi(\theta)$, derived from current data
                    \item Pick potential dataset $D$ that might be collected from $P(D)\: (=\mathcal{Z})$
                    \item Derive posterior $P(\theta|D)$
                    \item Summarise science (e.g. constraint on $\theta$, ability to perform model comparison)
                \end{enumerate}
        \end{itemize}

        \column{0.5\textwidth}
        \begin{itemize}
            \item This procedure should be marginalised over:
                \begin{enumerate}
                    \item All possible parameters $\theta$ (consistent with prior knowledge)
                    \item All possible data $D$
                \end{enumerate}
            \item i.e. marginalised over the joint $P(\theta,D)=P(D|\theta)P(\theta)$.
            \item Historically this has proven very challenging.
            \item Most analyses assume a fiducial cosmology $\theta_*$, and/or a Gaussian likelihood/posterior (c.f. Fisher forecasting).
            \item This runs the risk of biasing forecasts by baking in a given theory/data realisation.
        \end{itemize}

    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Fully Bayesian Forecasting~\arxiv{2309.06942}}
    \student{thomas_gessey-jones}{Thomas Gessey-Jones}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Simulation based inference gives us the language to marginalise over parameters $\theta$ and possible future data $D$.
            \item Evidence networks give us the ability to do this at scale for forecasting~\arxiv{2305.11241}.
            \item Demonstrated in 21cm global experiments, marginalising over:
                \begin{itemize}
                    \item theoretical uncertainty
                    \item foreground uncertainty
                    \item systematic uncertainty
                \end{itemize}
            \item Able to say ``at 67mK radiometer noise'', have a 50\% chance of 5$\sigma$ Bayes factor detection.
            \item Can use to optimise instrument design
            \item Re-usable package: \texttt{prescience}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fbf.pdf}
    \end{columns}
\end{frame}



\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\href{https://www.github.com/handley-lab}{github.com/handley-lab}}
    \tikz[overlay,remember picture]
    \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
        \includegraphics[width=0.09\textheight]{figures/students/adam_ormondroyd.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/david_yallup.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/dily_ong.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/felicity_ibrahim.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/george_carter.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/harry_bevins.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/ian_roque.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/kilian_scheutwinkel.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/metha_prathaban.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/namu_kroupa.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sinah_legner.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/thomas_gessey-jones.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/tze_goh.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/wei-ning_deng.jpg}%
    };
    Covered a suite of tools for next-generation ``generative cosmology''
    \begin{itemize}
        \item Nested sampling for cosmological\ldots
            \begin{itemize}
                \item model comparison
                \item parameter estimation
                \item tension quantification
            \end{itemize}
        \item Nuisance-marginalised cosmology
            \boxed{
                \C[2]{\mathcal{L}}(\theta) 
                = 
                \frac{
                    \C[0]{\mathcal{P}}(\theta)
                    \C[3]{\mathcal{Z}}
                }{
                    \C[1]{\pi}(\theta)
                }
            }
        \item Simulation-based inference
        \item Fully Bayesian forecasting
    \end{itemize}
\end{frame}

\end{document}
```
# branch: remotes/origin/phystat_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usetikzlibrary{arrows,arrows.meta,automata,positioning}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{\texttt{PolySwyft}}
\subtitle{a sequential simulation-based nested sampler}
\date{16\textsuperscript{th} May 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Contents}
    \begin{columns}
        \column{0.77\textwidth}
        \begin{enumerate}
            \item Notation
            \item Neural Ratio estimation (NRE)
            \item Nested sampling (NS)
            \item NS+NRE
            \item Future prospects
        \end{enumerate}
        Stems from over a year of discussion, with the majority of the work done by Kilian Scheutwinkel (PhD student).
        \column{0.23\textwidth}
        \begin{overpic}[width=\textwidth]{figures/students/kilian_scheutwinkel.jpg}
            \put(5,5) {\textcolor{white}{Kilian Scheutwinkel}}
        \end{overpic}
        \begin{overpic}[width=\textwidth]{figures/students/christoph_weniger.jpg}
            \put(10,5) {\textcolor{white}{Christoph Weniger}}
        \end{overpic}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Notation}
    \begin{itemize}
        \item A ``generative'' model $M$, with tunable parameters $\theta$, describing (compressed) data $D$.
            \begin{itemize}
                \item e.g. $M=\Lambda$CDM, $\theta=\{\Omega_b,\Omega_c, \tau, H_0, A_s, n_s\}$, $D=\{C_\ell\}$.
            \end{itemize}
        \item Described by simulation process $\theta\to D$, or likelihood $\C[2]{P}(D|\theta,M)$.
        %\item Frequentists \& Bayesians agree on the likelihood.
        %\item Bayesians treat parameter space $\theta$ the same as data space $D$.
        \item Quantifying uncertainty with probability using Bayes theorem:
            \[
                \C[0]{P}(\theta|D,M) = \frac{\C[2]{P}(D|\theta,M)\C[1]{P}(\theta|M)}{\C[3]{P}(D|M)},
                \quad
                \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}}\times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}},
                \quad
                \C[0]{\mathcal{P}}(\theta|D) = \frac{\C[2]{\mathcal{L}}(D|\theta)\C[1]{\pi}(\theta)}{\C[3]{\mathcal{Z}}(D)}.
            \]
        \item Follows from the oft-forgotten Joint (the probability of everything):
            \[
                \C[0]{\mathcal{P}}\times\C[3]{\mathcal{Z}} = \C[4]{\mathcal{J}} = \C[2]{\mathcal{L}}\times\C[1]{\pi}, \qquad \C[4]{P}(D,\theta|M) = \C[4]{\text{Joint}} = \C[4]{\mathcal{J}}  
            \]
        \item Highly relevant (in many overlapping contexts) is the dimensionless ratio
            \[\C[5]{r} = \frac{\C[0]{\mathcal{P}}}{\C[1]{\pi}} = \frac{\C[4]{\mathcal{J}}}{\C[1]{\pi}\times\C[3]{\mathcal{Z}}} = \frac{\C[2]{\mathcal{L}}}{\C[3]{\mathcal{Z}}}\]
        \item e.g.\ $r$ occurs in neural ratio estimation, and has the properties that
            \[ 
            \mathcal{D}_\mathrm{KL}(\C[0]{\mathcal{P}}||\C[1]{\pi}) = \langle \log \C[5]{r} \rangle_{\C[0]{\mathcal{P}}}, \qquad                  \mathcal{I}(\theta,D))= \langle \mathcal{D}_\mathrm{KL} \rangle_{\C[3]{\mathcal{Z}}}  = \langle \log \C[5]{r} \rangle_{\C[4]{\mathcal{J}}}
        \]
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Neural Ratio Estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item SBI flavours:
                {\small
                    \begin{description}
                        \item[NPE] Neural posterior estimation
                        \item[NLE] Neural likelihood estimation
                        \item[NJE] Neural joint estimation
                        \item[NRE] Neural ratio estimation
                    \end{description}
                }
            \item NRE recap:
                \begin{enumerate}
                    \item Generate joint samples $(\theta,D)\sim\C[4]{\mathcal{J}}$
                        \label{step:joint}
                        \begin{itemize}
                            \item \textit{straightforward if you have a simulator:\\ $\theta\sim\C[1]{\pi}(\cdot)$, $D\sim\C[2]{\mathcal{L}}(\cdot|\theta)$}
                        \end{itemize}
                    \item Generate separated samples $\theta\sim\C[1]{\pi}$, $D\sim\C[3]{\mathcal{Z}}$\label{step:sep}
                        \begin{itemize}
                            \item \textit{aside: can shortcut step~\ref{step:sep} by scrambling the $(\theta,D)$ pairings from step~\ref{step:joint}}
                        \end{itemize}
                    \item Train probabilistic classifier $p$ to distinguish whether $(\theta,D)$ came from $\C[4]{\mathcal{J}}$ or $\C[1]{\pi}\times\C[3]{\mathcal{Z}}$.
                    \item $\log \C[5]{r} = p/(1-p)$.
                \end{enumerate}
        \end{itemize}
        \column{0.5\textwidth}
        \only<1>{
        \begin{tikzpicture}[node distance=1cm, every neuron/.style={circle, draw, minimum size=1cm},]

            \node[every neuron/.try] (j2)  {};
            \node[every neuron/.try, above left = 0cm and 0.5cm of j2] (theta) { $\theta$};
            \node[every neuron/.try, below left = 0cm and 0.5cm of j2] (D) { $D$};
            \node[every neuron/.try, above = 0.5cm of j2] (j1) {};
            \node[every neuron/.try, below = 0.5cm of j2] (j3) {};
            \node[every neuron/.try, above right = 0cm and 0.5cm of j2] (h1) {};
            \node[every neuron/.try, below right = 0cm and 0.5cm of j2] (h2) {};
            \node[every neuron/.try, right = 1.3cm of j2] (p) { $p$};
            \node[every neuron/.try, right = 0.5cm of p] (logr) { $logr$};

            \draw[-] (theta) -- (j1);
            \draw[-] (D) -- (j1);
            \draw[-] (theta) -- (j2);
            \draw[-] (D) -- (j2);
            \draw[-] (theta) -- (j3);
            \draw[-] (D) -- (j3);

            \draw[-] (j1) -- (h1);
            \draw[-] (j1) -- (h2);
            \draw[-] (j2) -- (h1);
            \draw[-] (j2) -- (h2);
            \draw[-] (j3) -- (h1);
            \draw[-] (j3) -- (h2);

            \draw[-] (h1) -- (p);
            \draw[-] (h2) -- (p);

            \draw[-] (p) -- (logr);
        \end{tikzpicture}
        }
        \only<2|handout:0>{
        \begin{block}{Why I like NRE}
            \begin{itemize}
                \item Link between classification and inference is profound.
                \item Density estimation is hard -- Dimensionless $r$ divides out the hard-to-calculate bits.
            \end{itemize}
        \end{block}
        \begin{block}{Why I don't like NRE}
            \begin{itemize}
                \item Practical implementations require marginalisation~\arxiv{2107.01214}, or autoregression~\arxiv{2308.08597}.
                \item Model comparison and parameter estimation are separate~\arxiv{2305.11241}.
            \end{itemize}
        \end{block}
    }
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{TMNRE: Truncated Marginal Neural Ratio Estimation}
    \framesubtitle{\texttt{swyft}: \tthref{github.com/undark-lab/swyft}}

    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Two tricks for practical NRE:
        \end{itemize}
        \begin{block}{Marginalisation}
            \begin{itemize}
                \item Only consider one or two parameters at a time.
                \item Fine if your goal is to produce triangle plots.
                \item Problematic if information is contained jointly in more than two parameters.
            \end{itemize}
        \end{block}
        \begin{block}{Truncation}
                \begin{itemize}
                    \item focus parameters $\theta$ on a subset of the prior which reproduces observed data $D_\mathrm{obs}$
                    \item region is somewhat arbitrary (usually a box)
                    \item not amortised, sounds a bit like ABC
                \end{itemize}
        \end{block}
        \column{0.45\textwidth}
        \begin{overpic}[width=\textwidth]{figures/tmnre}
            \put(70,0) {\arxiv{2111.08030}}
        \end{overpic}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Nested sampling: numerical Lebesgue integration}
    \begin{columns}
        \column{0.5\textwidth}
        \fbox{\parbox{\textwidth}{
        \begin{itemize}
            \item[0.] Start with $N$ random samples over the space.
            \item[i.] Delete outermost sample, and replace with a new random one at higher integrand value.
        \end{itemize}}}
        \vspace{-5pt}
    \begin{itemize}



            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item Discarded ``dead points'' can be weighted to form posterior, prior, or anything in between.
            \item Estimates the \textbf{density of states} and calculates evidences \& partition functions.
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune,
                    \item exploration of multimodal functions,
                    \item global and local optimisation.
                \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4|handout:0>[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<8|handout:0>[width=\textwidth,page=14]{figures/himmelblau}%
        \includegraphics<9->[width=\textwidth,page=15]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \texttt{nautilus}~\arxiv{2306.16923} 
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.55\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267} \texttt{jaxnest}~\arxiv{2012.15286}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{The nested sampling meta-algorithm: dead points}
    \includegraphics[width=\textwidth]{figures/dead_measure}
    \begin{columns}
        \column{0.69\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Dead points have a unique scale-invariant distribution $\propto\: \tfrac{dV}{V}$.
            \item Uniform over original region, exponentially concentrating on region of interest (until termination volume).
            \item Good for training emulators (HERA~\arxiv{2108.07282}).
        \end{itemize}
        \column{0.3\textwidth}
        \begin{block}{Applications}
            \begin{itemize}
                \item training emulators.
                \item gridding simulations
                \item beta flows
                \item ``dead measure'' 
            \end{itemize}
        \end{block}
    \end{columns}
\end{frame}



\begin{frame}
    \frametitle{Similarities}

    \begin{columns}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth,page=15]{figures/himmelblau}% 
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/tmnre.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why it's hard to do SBI with nested sampling}
    \begin{itemize}
        \item At each iteration $i$, nested sampling requires you to be able to generate a new live point from the prior, subject to a hard likelihood constraint
            \[ \theta\sim\pi : \mathcal{L}(\theta)>\mathcal{L}_i \]
        \item This is hard if you don't have a likelihood!
        \item In addition, nested sampling does not do well if the likelihood is non-deterministic
        \item Previous attempts:
            \begin{itemize}
                \item DNest paper \arxiv{1606.03757}(Section 10: Nested sampling for ABC)
                \item ANRE~\arxiv{2308.08597} using non-box priors driven by current ratio estimate with slice sampling re-population.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Sequential NRE with nested sampling}
    \begin{tikzpicture}[
            node distance=1cm,
            >=stealth, auto,
            every state/.style={
                rectangle, rounded corners, minimum width=2em,
                text width=6.8cm, align=center
            }
        ]


        \node[state, fill=C3!20, minimum height=3cm] (q14) {
            \textbf{NS}\\
            Run nested sampling on $\log r_{i}(\theta, D_\mathrm{obs})$ \\
            to generate $\theta$ dead samples between prior and posterior targeted at $D_\mathrm{obs}$.

        };
        \node[state, fill=C0!20, minimum height=1cm] (q121) [above=of q14] {
            \textbf{Terminate} if $\mathcal{D}_\mathrm{KL}$ has converged
        };
        \node[state, fill=C0!20, minimum height=1cm] (q12) [above=of q121] {
            \textbf{Initialise} $\theta\sim\pi$\\
        };
        \node[state, fill=C2!20, minimum height=3cm] (q34) [right=of q14] {
            \textbf{NRE}\\
            Train Neural ratio estimator $\log r_i$
            with weights initialised from previous run
        };
        \node[state, fill=C1!20, minimum height=3cm] (q23) [above=of q34] {
            \textbf{Simulate}\\
            Generate simulations $\theta\to D$ from all discarded points
        };
        \begin{scope}[bend left]%
            \path[thick,-{Latex[width=2mm]}]   (q14.north) edge node {} (q121.south)
            (q121.east) edge node {}($(q23.south west)!0.16!(q23.north west)$) 
            (q23.south) edge node {} (q34.north)
            (q34.west) edge node {} (q14.east)
            (q12.east) edge node {}($(q23.south west)!0.84!(q23.north west)$) ;
        \end{scope}

        \node[align=center] (e) at (barycentric cs:q121=0.5,q12=0.5,q23=1,q34=1,q14=1) {\Large\textbf{NSNRE}};

    \end{tikzpicture}

\end{frame}

\begin{frame}
    \frametitle{\texttt{PolySwyft}}
    \begin{columns}[t]
        \column{0.45\textwidth}
        \begin{block}{\texttt{PolyChord}}
            \tthref{github.com/PolyChord/PolyChordLite}
            \includegraphics[width=\textwidth]{figures/polychord.png}
            \begin{itemize}
                \item Widely used high-performance nested sampling tool (implementing slice sampling \& clustering in MPI Fortran)
            \end{itemize}
        \end{block}
        \column{0.45\textwidth}
        \begin{block}{\texttt{Swyft}}
            \tthref{github.com/undark-lab/swyft}
            \includegraphics[width=\textwidth]{figures/swyft_logo_wide.png}
            \begin{itemize}
                \item Widely used TMNRE tool in cosmology/astrophysics.
            \end{itemize}
        \end{block}
        However, NSNRE is general, and not specific to these choices.
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Convergence diagnostics}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Example for a $n=5$ dimensional parameter space, with $d=100$ data points, (\texttt{lsbi} gaussian mixture model).
            \item This is the regime for cosmological scale problems.
            \item Determining convergence is a work-in-progress.
            \item Currently track:
                \begin{itemize}
                    \item The change in KL divergence between rounds (\C[0]{blue}), and check when this goes to zero.
                    \item The total KL divergence between prior and posterior estimate (\C[1]{orange}), and check when this levels off (ground truth in \C[3]{red}).
                    \item Also shown is the KL divergence between the estimate and the ground truth (\C[2]{green}).
                \end{itemize}
        \end{itemize}
        
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/GMM_posterior_estimates.pdf}%
        \includegraphics<2|handout:0>[width=\textwidth]{figures/GMM_KL_div_per_round.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{What does NSNRE give you that TMNRE doesn't?}
    \begin{itemize}
        \item Use of dead points which scan from prior to logr peak avoids risk of 'trimming' important regions of the space
        \item Likelihood-driven contours (slightly) allow more parameters $n>2$ to be considered in comparison with box priors
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Considerations of ratio estimation}
    \student{zixiao_hu}{Zixiao Hu}{MPhil}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Neural REs can in practice only estimate in a band of $\log r$ before the activation function saturates (typically $-5 < \log r < 5$).
            \item Consider a posterior $\mathcal{P}$ well approximated by a Gaussian profile in an $n$-dimensional parameter space~\arxiv{2312.00294}
            \item If $\mathcal{D}_\mathrm{KL}\gg1$ between prior and posterior:
                \begin{gather*}
                    \log r = \frac{n}{2} + \mathcal{D}_\mathrm{KL} + \chi^2_{n} \\
            %\[\mathcal{P}(\log r) = \frac{1}{\Gamma(\tfrac{n}{2})} e^{\log r-\frac{n}{2}-\mathcal{D}_\mathrm{KL}} \left(\tfrac{n}{2}+\mathcal{D}_\mathrm{KL}-\log r\right)^{\frac{n}{2}-1}\]
            %i.e. $\log r$ has an offset \& rescaled chi-squared distribution:
            %\[n + 2\mathcal{D}_\mathrm{KL} - 2\log r \sim \chi^2_{n} \]
                    \left\langle \log r \right\rangle_\mathcal{P} = \mathcal{D}_\mathrm{KL}, \qquad \sigma(\log r)_\mathcal{P} = \sqrt{\frac{n}{2}}
                \end{gather*}
            \item Truncation ({\bf T}MNRE) reduces $\mathcal{D}_\mathrm{KL}$, focusing the distribution into the $[-5,5]$ band.
            \item Marginalisation (T{\bf M}NRE) reduces $n$ \& $\sigma$.
        \end{itemize}

        \column{0.5\textwidth}
        \vspace{10pt}
        \includegraphics{figures/anatomy}

    \end{columns}



\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\tthref{github.com/handley-lab}}
    \tikz[overlay,remember picture]
    \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
        \includegraphics[width=0.09\textheight]{figures/students/adam_ormondroyd.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/david_yallup.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/dily_ong.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/felicity_ibrahim.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/george_carter.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/harry_bevins.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/ian_roque.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/kilian_scheutwinkel.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/metha_prathaban.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/namu_kroupa.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sinah_legner.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/thomas_gessey-jones.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/tze_goh.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/wei-ning_deng.jpg}%
    };

    \begin{itemize}
        \item \texttt{PolySwyft} can perform NRE on $n=5$ parameter spaces and $d=100$ data spaces.
        \item This makes it relevant for cosmological applications.
        \item Investigating this raises (?existential) questions regarding NRE.
        \item Is the $-5 <\log r < 5$ saturation of NREs a fundamental problem, or an engineering one?
        \item Is there a ``nested'' approach to crossing this range in larger parameter spaces $n\gg 5$?
        \item Examples produced using \texttt{lsbi} package: \tthref{github.com/handley-lab/lsbi}\\
    \end{itemize}

\end{frame}

\appendix
\begin{frame}
    \frametitle{Cosmological forecasting}
    \framesubtitle{Have you ever done a Fisher forecast, and then felt Bayesian guilt?}
    \vspace{-20pt}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item Cosmologists are interested in forecasting what a Bayesian analysis of future data might produce.
            \item Useful for:
                \begin{itemize}
                    \item white papers/grants,
                    \item optimising existing instruments/strategies,
                    \item picking theory/observation to explore next.
                \end{itemize}
            \item To do this properly:
                \begin{enumerate}
                    \item start from current knowledge $\pi(\theta)$, derived from current data
                    \item Pick potential dataset $D$ that might be collected from $P(D)\: (=\mathcal{Z})$
                    \item Derive posterior $P(\theta|D)$
                    \item Summarise science (e.g. constraint on $\theta$, ability to perform model comparison)
                \end{enumerate}
        \end{itemize}

        \column{0.5\textwidth}
        \begin{itemize}
            \item This procedure should be marginalised over:
                \begin{enumerate}
                    \item All possible parameters $\theta$ (consistent with prior knowledge)
                    \item All possible data $D$
                \end{enumerate}
            \item i.e. marginalised over the joint $P(\theta,D)=P(D|\theta)P(\theta)$.
            \item Historically this has proven very challenging.
            \item Most analyses assume a fiducial cosmology $\theta_*$, and/or a Gaussian likelihood/posterior (c.f. Fisher forecasting).
            \item This runs the risk of biasing forecasts by baking in a given theory/data realisation.
        \end{itemize}

    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Fully Bayesian Forecasting~\arxiv{2309.06942}}
    \student{thomas_gessey-jones}{Thomas Gessey-Jones}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Simulation based inference gives us the language to marginalise over parameters $\theta$ and possible future data $D$.
            \item Evidence networks give us the ability to do this at scale for forecasting~\arxiv{2305.11241}.
            \item Demonstrated in 21cm global experiments, marginalising over:
                \begin{itemize}
                    \item theoretical uncertainty
                    \item foreground uncertainty
                    \item systematic uncertainty
                \end{itemize}
            \item Able to say ``at 67mK radiometer noise'', have a 50\% chance of 5$\sigma$ Bayes factor detection.
            \item Can use to optimise instrument design
            \item Re-usable package: \texttt{prescience}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fbf.pdf}
    \end{columns}
\end{frame}


\end{document}
```
# branch: remotes/origin/queensland_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usetikzlibrary{arrows,arrows.meta,automata,positioning}
\usepackage{slashed}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{ Nested sampling: {\large powering next-generation inference and machine learning tools}}
\subtitle{for astrophysics, cosmology, particle physics and beyond}
\date{16\textsuperscript{th} August 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}


\begin{frame}
    \frametitle{LBI: Likelihood-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        The standard approach if you are fortunate enough to have a likelihood function $\only<1-2>{P(D|\theta)}\only<3->{\C[2]{\mathcal{L}(D|\theta)}}$: 
        \[
            \only<1-2>{
                P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}
            }
            \only<2>{
                \qquad
                \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}}\times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}
            }
            \only<3>{
                \C[0]{\mathcal{P}(\theta|D)} = \frac{\C[2]{\mathcal{L}(D|\theta)}\C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}(D)}}
                \qquad
                \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}}\times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}
            }
            \only<4>{
                P(\theta|D) P(D) = P(\theta,D) = P(D|\theta)P(\theta), \qquad
            }
            \only<5>{
                \C[0]{\mathcal{P}}\times\C[3]{\mathcal{Z}} = \C[4]{\mathcal{J}} = \C[2]{\mathcal{L}}\times\C[1]{\pi}, \qquad \C[4]{\text{Joint}} = \C[4]{\mathcal{J}} = P(\theta,D)
            }
        \]
        \vspace{-10pt}
        \begin{enumerate}
            \item Define \C[1]{prior $\pi(\theta)$} 
                \begin{itemize}
                    \item spend some time being philosophical
                \end{itemize}
            \item Sample \C[0]{posterior $\mathcal{P}(\theta|D)$} 
                \begin{itemize}
                    \item use out-of-the-box MCMC tools such as\\ \texttt{emcee} or \texttt{MultiNest}
                    \item make some triangle plots
                \end{itemize}
            \item Optionally compute \C[3]{evidence $\mathcal{Z}(D)$}
                \begin{itemize}
                    \item e.g. nested sampling or parallel tempering
                    \item do some model comparison (i.e. science)
                    \item talk about tensions
                \end{itemize}
        \end{enumerate}
        \column{0.5\textwidth}
        \hfill%
        \begin{overpic}[width=0.6\textwidth]{figures/des_parameters.pdf}
            \put(-40,90) {DES Y5 SN Ia}
            \put(-40,80) {\arxiv{2401.02929}}
        \end{overpic}
        \includegraphics[width=0.5\textwidth]{figures/des_model_comparison.pdf}%
        \includegraphics[width=0.5\textwidth]{figures/des_suspiciousness.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The three pillars of Bayesian inference}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Parameter estimation}
            What do the data tell us about the parameters of a model?

            \textit{e.g. the size or age of a $\Lambda$CDM universe}
            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Model comparison}
            How much does the data support a particular model?

            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}} \vspace{-7pt}\]
            \[ \frac{\C[3]{\mathcal{Z}_{M}} \C[5]{\Pi_{M}}}{\C[7]{\sum_m Z_m \Pi_m}} \]
            \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Tension quantification}
            Do different datasets make consistent predictions from the same model? 
            \textit{e.g. CMB vs Type IA supernovae data}
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Model comparison $\C[3]{\mathcal{Z}=P(D|M)}$}
    \begin{itemize}
        \item Bayesian model comparison allows mathematical derivation of key philosophical principles.
    \end{itemize}
    \begin{columns}[t]
        \column{0.47\textwidth}
        Viewed from data-space $D$:
        \begin{block}{Popper's falsificationism}
            \begin{itemize}
                \item Prefer models that make bold predictions.
                \item if proven true, model more likely correct.
            \end{itemize}
        \end{block}
        \includegraphics<1>[width=\textwidth, page=1]{figures/popper.pdf}%
        \includegraphics<2>[width=\textwidth, page=2]{figures/popper.pdf}%
        \includegraphics<3>[width=\textwidth, page=3]{figures/popper.pdf}%
        \begin{itemize}
            \item Falsificationism comes from normalisation
        \end{itemize}
        \column{0.47\textwidth}
        Viewed from parameter-space $\theta$:
        \begin{block}{Occam's razor}
            \begin{itemize}
                \item Models should be as simple as possible
                \item \ldots but no simpler
            \end{itemize}
        \end{block}
        \begin{itemize}
            \item Occam's razor equation:
                \[\C[3]{\log\mathcal{Z}} = \av[{\C[0]{\mathcal{P}}}]{\C[2]{\log\mathcal{L}}} - \mathcal{D}_\text{KL}\]
            \item ``Occam penalty'': KL divergence between \C[1]{prior~$\pi$} and \C[0]{posterior~$\mathcal{P}$}.
                \[ \mathcal{D}_\text{KL}\sim \log\frac{\text{\C[1]{Prior volume} }}{\text{\C[0]{Posterior volume}}}\]
        \end{itemize}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Where is Nested Sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item For many purposes, in your Neural Net you should group Nested Sampling with (MCMC) techniques such as:
                \begin{itemize}
                    \item Metropolis-Hastings (PyMC, MontePython)
                    \item Hamiltonian Monte Carlo (Stan, blackjax)
                    \item Ensemble sampling (emcee, zeus). 
                    \item Variational Inference (Pyro)
                    \item Sequential Monte Carlo 
                    \item Thermodynamic integration
                    \item Genetic algorithms
                \end{itemize}
            \item You may have heard of it branded form:
                \begin{itemize}
                    \item MultiNest
                    \item PolyChord
                    \item dynesty
                    \item ultranest
                \end{itemize}
            \end{itemize}
        \column{0.5\textwidth}
        \begin{columns}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/emcee}
        \includegraphics[width=\textwidth]{figures/metropolis-hastings}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/zeus}
        \end{columns}
        \includegraphics[width=\textwidth]{figures/hmc_explained}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Integration in Physics}
    \begin{itemize}
        \item Integration is a fundamental concept in physics, statistics and data science:
    \end{itemize}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Partition functions}
            \vspace{-11pt}
            \[ Z(\beta) = \int e^{-\beta H(q,p)} dq dp \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Path integrals}
            \[ \Psi = \int e^{i S} \mathcal{D}x \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Bayesian marginals}
            \vspace{-11pt}
            \[ \mathcal{Z}(D) = \int \mathcal{L}(D|\theta) \pi(\theta) d\theta \]
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Need numerical tools if analytic solution unavailable.
            \item High-dimensional numerical integration is hard.
            \item Riemannian strategy estimates volumes geometrically:
                \[ \int f(x) d^nx \approx \sum_i f(x_i) \Delta V_i \sim \mathcal{O}(e^n) \]
            \item Curse of dimensionality $\Rightarrow$ exponential scaling.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.48\textwidth}
        \begin{block}{\textbf{MCMC}}
            \only<16>{
                \begin{itemize}
                    \item Single ``walker''
                    \item Explores posterior
                    \item Fast, if proposal matrix is tuned
                    \item Parameter estimation, suspiciousness calculation
                    \item Channel capacity optimised for generating posterior samples
                \end{itemize}
            }
        \end{block}
            \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
            \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
            \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
            \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
            \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
            \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.48\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \only<16>{
                \begin{itemize}
                    \item Ensemble of ``live points''
                    \item Scans from prior to peak of likelihood
                    \item Slower, no tuning required
                    \item Parameter estimation, model comparison, tension quantification
                    \item Channel capacity optimised for computing partition function
                \end{itemize}
            }
        \end{block}
            \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
            \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
            \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
            \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
            \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
            \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
            \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
            \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
            \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} 
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: live points}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n$ random samples over the space.
            \item Delete outermost sample, and replace with a new random one at higher integrand value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n}\only<5->{\pm \frac{1}{n}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \int f(x) dV \approx \sum_i f(x_i) \Delta V_i, \quad V_i = V_0 e^{-\only<5->{(}i\only<5->{\pm\sqrt{i})}/n} \]
%            \item Nested sampling: completely different way to scan.
%            \item Ensemble sampling compresses entire space$\to$peak(s).
%            \item Sequentially update a set $S$ of $n$ samples:
%                \begin{itemize}
%                    \item[$S_0$:]  Generate $n$ samples uniformly over the space (from a measure $\pi$). 
%
%                    \item[$S_{i+1}$:] Delete the lowest likelihood sample in $S_{i}$, and replace it with a new uniform sample with higher likelihood.
%                \end{itemize}
%            \item Requires one to be able to sample uniformly within a region, subject to a {\em hard constraint}:
%                \[\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_*. \}\]
%            \item This procedure optimises (multimodally), and can calculate the \C[3]{evidence}/integral of function \& \C[0]{posterior}/sample weights.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4-         >[width=\textwidth,page=4]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: dead points}
    \includegraphics[width=\textwidth]{figures/dead_measure}
    \begin{columns}
        \column{0.69\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Dead points have a unique scale-invariant distribution $\propto\: \tfrac{dV}{V}$.
            \item Uniform over original region, exponentially concentrating on region of interest (until termination volume).
            \item Good for training emulators (HERA~\arxiv{2108.07282}).
        \end{itemize}
        \column{0.3\textwidth}
        \begin{block}{Applications}
        \begin{itemize}
            \item training emulators.
            \item gridding simulations
            \item beta flows
            \item ``dead measure'' 
        \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    %\begin{columns}
    %    \begin{column}{0.33}
    %        \includegraphics[width=\textwidth]{figures/multinest}
    %    \end{column} 
    %\end{columns}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \texttt{nautilus}~\arxiv{2306.16923} 
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.55\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267} \texttt{jaxnest}~\arxiv{2012.15286}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Types of nested sampler}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
\item Constructs bounding region and draws many invalid points until $\mathcal{L}(\theta)>\mathcal{L}_*$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{5pt}
    \begin{itemize}
        \item Nested samplers usually come with:
            \begin{itemize}
                \item \emph{resolution} parameter $n_\mathrm{live}$ (which improve results as $\sim\mathcal{O}(n_\mathrm{live}^{-1/2})$.
                    \item set of \emph{reliability} parameters~\arxiv{2101.04525}, which don't improve results if set arbitrarily high, but introduce systematic errors if set too low.
                    \item e.g. \texttt{Multinest} efficiency \texttt{eff} or \texttt{PolyChord} chain length $n_\mathrm{repeats}$.
            \end{itemize}
    \end{itemize}
\end{frame}

%\begin{frame}
%    \frametitle{Applications: The three pillars of Bayesian inference}
%    \begin{columns}[t]
%        \column{0.33\textwidth}
%        \begin{block}{Parameter estimation}
%            What do the data tell us about the parameters of a model?
%
%            \textit{e.g. the size or age of a $\Lambda$CDM universe}
%            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
%            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
%            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
%        \end{block}
%        \column{0.3\textwidth}
%        \begin{block}{Model comparison}
%            How much does the data support a particular model?
%
%            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
%            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}} \vspace{-7pt}\]
%            \[ \frac{\C[3]{\mathcal{Z}_{M}} \C[5]{\Pi_{M}}}{\C[7]{\sum_m Z_m \Pi_m}} \]
%            \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}\]
%        \end{block}
%        \column{0.33\textwidth}
%        \begin{block}{Tension quantification}
%            Do different datasets make consistent predictions from the same model? 
%            \textit{e.g. CMB vs Type IA supernovae data}
%            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
%            \[
%                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
%                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
%                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
%                \end{aligned}
%            \]
%        \end{block}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Cosmology}
    \student{adam_ormondroyd}{Adam Ormondroyd}{PhD}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Battle-tested in Bayesian cosmology on
                \begin{itemize}
                    \item Parameter estimation: multimodal alternative to MCMC samplers.
                    \item Model comparison: using integration to compute the Bayesian evidence
                    \item Tension quantification: using deep tail sampling and suspiciousness computations.
                \end{itemize}
            \item Plays a critical role in major cosmology pipelines: Planck, DES, KiDS, BAO, SNe.
            \item The default $\Lambda$CDM cosmology is well-tuned to have Gaussian-like posteriors for CMB data. 
            \item Less true for alternative cosmologies/models and orthogonal datasets, so nested sampling crucial.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/pps_both}
        \includegraphics[width=0.49\textwidth]{figures/reach_fit-cropped.pdf}
        %\includegraphics[width=0.49\textwidth]{figures/curvature_3}
        \includegraphics[width=\textwidth]{figures/omegak_H0_2.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Astrophysics}
\student{metha_prathaban}{Metha Prathaban}{PhD}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item In exoplanets~\arxiv{1806.00518}
                \begin{itemize}
                    \item Parameter estimation: determining properties of planets.
                    \item Model comparison: how many planets? Stellar modelling~\arxiv{2007.07278}.
                    \item exoplanet problems regularly have posterior phase transitions \arxiv{2102.03387}
                \end{itemize}
            \item In gravitational waves
                \begin{itemize}
                    \item Parameter estimation: Binary merger properties
                    \item Model comparison: Modified theories of gravity, selecting phenomenological parameterisations~\arxiv{1803.10210}
                    \item Likelihood reweighting: fast slow properties
                \end{itemize}
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/rv_full.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Particle physics}
    \student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{columns}
            \column{0.67\textwidth}
            \begin{itemize}
                \item Nested sampling for cross section computation/event generation
            \end{itemize}
            \column{0.3\textwidth}
            \[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2.\]
        \end{columns}
        \begin{itemize}
            \item Nested sampling can explore the phase space $\Omega$ and compute integral blind with comparable efficiency to HAAG/RAMBO~\arxiv{2205.02030}.
            \item Bayesian sparse reconstruction~\arxiv{1809.04598} applied to bump hunting allows evidence-based detection of signals in phenomenological backgrounds~\arxiv{2211.10391}.
            \item Fine tuning quantification
            \item Fast estimation of small $p$-values \arxiv{2106.02056}(PRL), just make switch:\\ $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.

        \end{itemize}
        \column{0.17\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1-pdfjam-crop.pdf}
        \column{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/psi_predict-crop.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Lattice field theory}
\student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Consider standard field theory Lagrangian:
                \[ Z(\beta) = \int D\phi e^{-\beta S(\phi)}, \quad S(\phi) = \int dx^\mu \mathcal{L}(\phi) \]
            \item Discretize onto spacetime grid.
            \item Compute partition function
            \item NS unique traits:
                \begin{itemize}
                    \item Get full partition function for free
                    \item allows for critical tuning
                    \item avoids critical slowing down
                \end{itemize}
            \item Applications in lattice gravity, QCD, condensed matter physics
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/potential_shape}
        \includegraphics[width=0.49\textwidth]{figures/2d_phase}
        \includegraphics[width=\textwidth]{figures/lattice_field_theory.pdf}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Machine learning}
\student{kamran_javid}{Kamran Javid}{PDRA}
    \begin{columns}
        \column{0.62\textwidth}
        \begin{itemize}
            \item Machine learning requires:
                \begin{itemize}
                    \item Training to find weights
                    \item Choice of architecture/topology/hyperparameters
                \end{itemize}
            \item Bayesian NNs treat training as a model fitting problem
            \item Compute posterior of weights (parameter estimation), rather than optimisation (gradient descent)
            \item Use evidence to determine best architecture (model comparison), correlates with out-of-sample performance! 
            \item Solving the full ``shallow learning'' problem without compromise \arxiv{2004.12211}\arxiv{2211.10391}. 
                \begin{itemize}
                    \item Promising work ongoing to extend this to transfer learning and deep nets.
                \end{itemize}
            \item More generally, dead points are optimally spaced  for training traditional ML approaches e.g. \arxiv{2309.05697}
        \end{itemize}
        \column{0.38\textwidth}
        \includegraphics[width=\textwidth]{figures/nn_posterior-cropped.pdf}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{Applications of nested sampling}
%    \framesubtitle{Statistics: fast estimation of small $p$-values~\arxiv{2106.02056}(PRL)}
%    \begin{columns}
%        \column{0.57\textwidth}
%        \begin{itemize}
%            \item Nested sampling for frequentist computation!?
%            \item $p$-value: $P(\lambda>\lambda^*|H_0)$ -- probability that test statistic $\lambda$ is at least as great as observed $\lambda^*$.
%            \item Computation of a tail probability from sampling distribution of $\lambda$ under $H_0$.
%            \item For gold-standard $5\sigma$, this is very expensive to simulate directly ($\sim10^9$ by definition).
%            \item Need insight/approximation to make efficient.
%            \item Nested sampling is tailor-made for this, just make switch: $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
%            \item The only real conceptual shift is switching the integrator from parameter- to data-space.
%        \end{itemize}
%        \column{0.43\textwidth}
%        \includegraphics[width=\textwidth]{figures/pvalue.pdf}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{And beyond\ldots}
    \only<1-4>{
        \student{watkinson-headshot}{Catherine Watkinson}{Senior Data Scientist}
    }
    \only<5->{
        \student{mcaloone-headshot}{Thomas Mcaloone}{PhD $\to$ Data Scientist}
    }
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
        \includegraphics[width=0.08\textwidth]{figures/headshots/catherine-watkinson-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/thomas-macaloone-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/parul-janagal-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/angus-peters-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/tamas-stenzel-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/david-yallup-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/rebecca-handley-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/adam-ormondroyd-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/mike-hobson-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/anthony-lasenby-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/mike-handley-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/will-handley-polychord.jpg}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        \includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/tdoa-cropped-1-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/tdoa-cropped-2-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/tdoa-cropped-3-crop.pdf}%
        \includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology {\small \arxiv{2210.07409}(NatAstro)}}
    \student{ian_roque}{Ian Roque}{PhD}
    \vspace{10pt}
    \begin{columns}
        \column{0.65\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
            \item Lead data analysis team (REACH first light in January)
            \item Nested sampling woven in from the ground up (calibrator, beam modelling, signal fitting, likelihood selection).
            \item All treated as parameterised model comparison problems.
        \includegraphics[height=0.3\textwidth]{figures/EDGES_antenna}
        \includegraphics[height=0.3\textwidth]{figures/SARAS}
        \includegraphics[height=0.3\textwidth]{figures/REACH_2.jpg}
        \end{itemize}
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{GAMBIT: combining particle physics \& cosmological data}
%    \begin{columns}
%        \column{0.52\textwidth}
%        \begin{itemize}
%            \item Multinational team of particle physicists, cosmologists and statisticians.
%            \item Combine cosmological data, particle colliders, direct detection, \& neutrino detectors in a statistically principled manner~\arxiv{2205.13549}.
%            \item Lead Cosmo/Dark Matter working group~\arxiv{2009.03286}.
%            \item Nested sampling used for global fitting, and fine-tuning quantification~\arxiv{2101.00428}
%        \end{itemize}
%        \begin{center}
%            \includegraphics[width=0.5\textwidth]{figures/gambit_logo.png}
%        \end{center}
%        \column{0.48\textwidth}
%        \vspace{-40pt}
%        \includegraphics[width=\textwidth]{figures/ALP_2.pdf}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{GAMBIT}
    \framesubtitle{Interdisciplinary case studies}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item GAMBIT is an interdisciplinary community and software framework.
            \item Like \texttt{CosmoMC}/\texttt{Cobaya}/\texttt{Bilby}, an organiser of data, likelihoods \& theory, including:
                \begin{itemize}
                    \item Collider data (e.g. LHC)
                    \item Direct detections (e.g. XENON1T)
                    \item Cosmology (\texttt{MontePython})
                    \item Astrophysics (e.g. Bullet Cluster, Supernovae)
                    \item Pulsar timing
                    \item \ldots \& much more
                \end{itemize}
            \item \texttt{GravBit} and \texttt{LowEnergyBit} arising from GAMBIT@KICC workshop
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[height=0.423\textwidth]{figures/students/gambit.png}
        \includegraphics[height=0.423\textwidth]{figures/gambit_kicc.jpg}
        \includegraphics[width=\textwidth]{figures/gambit_meetingbanner.jpg}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT: sub-GeV Dark matter constraints}
    \framesubtitle{Interdisciplinary case studies}
    \student{gambit}{Felix Kahlhoefer et al}{GAMBIT cosmo/DM working group}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{itemize}
            \item Physical model of sub-GeV thermal dark matter with a dark photon mediator~$A$:
        \end{itemize}
        \vspace{-10pt}
        \begin{align*}
            \small
            \mathcal{L}_\text{int} =& -\frac{1}{2} m_{A'}^2 A'^\mu A'_\mu - \frac{1}{4} A'^{\mu\nu}A'_{\mu\nu} -\kappa e A'^\mu \sum_{f} q_f \overline{f} \gamma_\mu f \,,
            \normalsize
        \end{align*}
        \vspace{-15pt}
        \begin{itemize}
            \item Constrain using cosmological, astrophysical, accelerator \& direct detection data.
            \item Bayesian Model comparison of Fermion~$\psi$ vs scalar~$\Phi$ models (scalar preferred).
        \end{itemize}
        \vspace{-10pt}
        \begin{align*}
            \small
            \mathcal{L}_\psi  =& \bar{\psi}(i \slashed{\partial} - m_\text{DM}) \psi + g_\text{DM} A'^\mu \bar{\psi} \gamma_\mu \psi \,,\\
            \mathcal{L}_\Phi  =& |\partial_\mu \Phi|^2 - m_\text{DM}^2 |\Phi|^2 - g_\text{DM}^2 A'_\mu A'^\mu |\Phi|^2 \\ &+ i g_\text{DM} A'^\mu \left[\Phi^\ast (\partial_\mu \Phi) - (\partial_\mu \Phi^\ast) \Phi\right]\,,
            \normalsize
        \end{align*}
        \column{0.44\textwidth}
        \vspace{10pt}
        \includegraphics[width=\textwidth]{figures/Bayes_SubGeVDM_fermion_RDprior_allDM_asym_observables.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{SBI: Simulation-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item What do you do if you don't know \C[2]{$\mathcal{L}(D|\theta)$}?
            \item If you have a simulator/forward model $\theta \rightarrow D$
                defines an \C[2]{\emph{implicit} likelihood~$\mathcal{L}$}.
            \item Simulator generates samples from $\C[2]{\mathcal{L}(\cdot|\theta)}$.
            \item With a prior $\C[1]{\pi}(\theta)$ can generate samples from \C[4]{joint distribution}~$\C[4]{\mathcal{J}(\theta,D)}=\C[2]{\mathcal{L}(D|\theta)}\C[1]{\pi(\theta)}$\\\hfill \emph{the ``probability of everything''}.
            \item Task of SBI is take joint~$\C[4]{\mathcal{J}}$ samples and learn \C[0]{posterior $\mathcal{P}(\theta|D)$} and \C[3]{evidence $\mathcal{Z}(D)$} \\\hfill and possibly \C[2]{likelihood $\mathcal{L}(D|\theta)$}.
            \item Present state of the art achieves this using \emph{machine learning} (neural networks).
                \begin{itemize}
                    \item My group's research tries to removes machine learning \tthref{github.com/handley-lab/lsbi}.

                \end{itemize}
            %\item Present SotA: NPE, NLE, NJE, NRE
            %\item SBI \& forward modelling force us to think about data space~$D$ \& parameter space~$\theta$.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[page=1, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<2>[page=2, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<3>[page=3, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<4>[page=4, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<5>[page=5, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<6>[page=6, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<7>[page=7, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<8>[page=8, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<9>[page=9, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<10>[page=10, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<11>[page=11, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<12>[page=12, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<13>[page=13, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<14>[page=14, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<15>[page=15, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<16>[page=16, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<17>[page=17, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<18>[page=18, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<19>[page=19, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<20>[page=20, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<21>[page=21, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why SBI?}
    \begin{columns}
        \column{0.6\textwidth}
        SBI is useful because:
        \begin{enumerate}
            \item If you don't have a likelihood, you can still do inference
                \begin{itemize}
                    \item This is the usual case beyond CMB cosmology
                \end{itemize}
            \item Faster than LBI
                \begin{itemize}
                    \item emulation -- also applies to LBI in principle
                \end{itemize}
            \item No need to pragmatically encode fiducial cosmologies
                \begin{itemize}
                    \item Covariance computation implicitly encoded in simulations
                    \item Highly relevant for disentangling tensions \& systematics
                \end{itemize}
            \item Equips AI/ML with Bayesian interpretability
            \item Lower barrier to entry than LBI
                \begin{itemize}
                    \item Much easier to forward model a systematic
                    \item Emerging set of plug-and-play packages
                    \item For this reason alone, it will come to dominate scientific inference
                \end{itemize}
        \end{enumerate}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/sbi_screenshot}
        \href{https://github.com/sbi-dev}{github.com/sbi-dev}
        \includegraphics[width=\textwidth]{figures/swyft_screenshot}
        \href{https://github.com/undark-lab/swyft}{github.com/undark-lab/swyft}
        \includegraphics[width=\textwidth]{figures/selfi_screenshot}
        \href{https://github.com/florent-leclercq/pyselfi}{github.com/florent-leclercq/pyselfi}
        \includegraphics[width=\textwidth]{figures/delfi_screenshot}
        \href{https://github.com/justinalsing/pydelfi}{github.com/justinalsing/pydelfi}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{SBI in cosmology}
    \begin{columns}
        \column{0.4\textwidth}
        \begin{itemize}
            \item 2024 has been the year it has started to be applied to real data.
            \item Mostly for weak lensing
            \item However: SBI requires mock data generation code
            \item Most data analysis codes were built before the generative paradigm.
            \item It's still a lot of work to upgrade cosmological likelihoods  to be able to do this (e.g.\ \texttt{plik} \& \texttt{camspec}).
        \end{itemize}
        \column{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/sbi_papers/clusters.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/des.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/gw.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/center.pdf}
        \column{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/sbi_papers/kidsdes.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/kids.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/population.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/simbig.pdf}
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\tthref{github.com/handley-lab}}
    \tikz[overlay,remember picture]
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
        \includegraphics[width=0.09\textheight]{figures/students/adam_ormondroyd.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/charlotte_priestley.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/david_yallup.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/dily_ong.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/george_carter.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/harry_bevins.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/ian_roque.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/kilian_scheutwinkel.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/metha_prathaban.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/namu_kroupa.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sinah_legner.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sam_leeney.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/wei-ning_deng.jpg}%
    };
    \vspace{-0.1\textheight}
    \begin{columns}
        \column{0.65\textwidth}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Performing Bayesian inference and model comparison.
            \end{itemize}
        \item It is applied widely across cosmology \& particle physics.
        \item It's unique traits as the only numerical Lebesgue integrator mean with compute it will continue to grow in importance.
        \item SBI represents the future of inference beyond LBI.
    \end{itemize}
        \column{0.35\textwidth}
    \includegraphics[height=0.6\textwidth]{figures/pps_both}%
    \includegraphics[height=0.6\textwidth]{figures/REACH_2}%
    \end{columns}
    \includegraphics[height=0.2\textwidth]{figures/integration}%
    \includegraphics[height=0.2\textwidth]{figures/curvature_3}%
    \includegraphics[height=0.2\textwidth]{figures/ALP_3}%
\end{frame}



\appendix

\end{document}
```
# branch: remotes/origin/ras_sbi_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{fit}

% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Simulation Based Inference}
\subtitle{theory, sampling \& model comparison}
\date{12\textsuperscript{th} January 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{How all SBI talks finish}
    \begin{itemize}
        \item There is a standard exchange that tends to happen after giving an SBI talk:

            \begin{description}
                \item[audience] Surely you're only as good as your simulations ---\\What if your forward model is missing physics $X$?
                \item[speaker] The exact same thing affects likelihood-based analysis ---\\
                    All SBI does is make these assumptions explicit.
            \end{description}
        \item I will try to unpack why I think both sides have a point.
    \end{itemize}
    \vspace{10pt}
\hfill \textit{Frontiers of simulation based inference}~\arxiv{1911.01429}
    \includegraphics[width=\textwidth]{figures/sbi_range.pdf}
\end{frame}

\begin{frame}
    \frametitle{LBI: Likelihood-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        The standard approach if you are fortunate enough to have a likelihood function $\only<1-2>{P}\only<3->{\mathcal{L}}(D|\theta)$: 
        \[
            \only<1-2>{
                P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}
        }
            \only<2>{
            \qquad
            \text{Posterior} = \frac{\text{Likelihood}\times\text{Prior}}{\text{Evidence}}
        }
            \only<3>{
                \mathcal{P}(\theta|D) = \frac{\mathcal{L}(D|\theta)\pi(\theta)}{\mathcal{Z}(D)}
            \qquad
            \text{Posterior} = \frac{\text{Likelihood}\times\text{Prior}}{\text{Evidence}}
        }
            \only<4>{
                \mathcal{P}\times\mathcal{Z} = \mathcal{L}\times\pi
        }
            \only<5>{
                \mathcal{P}\times\mathcal{Z} = \mathcal{J} = \mathcal{L}\times\pi, \qquad \text{Joint} = \mathcal{J} = P(D,\theta)
        }
        \]
        \vspace{-10pt}
        \begin{enumerate}
            \item Define prior $\pi(\theta)$ 
                \begin{itemize}
                    \item spend some time being philosophical
                \end{itemize}
            \item Sample posterior $\mathcal{P}(\theta|D)$ 
                \begin{itemize}
                    \item use out-of-the-box MCMC tools such as\\ \texttt{emcee} or \texttt{MultiNest}
                    \item make some triangle plots
                \end{itemize}
            \item Optionally compute evidence $\mathcal{Z}(D)$
                \begin{itemize}
                    \item e.g. nested sampling or parallel tempering
                    \item do some model comparison (i.e. science)
                    \item talk about tensions e.g. \arxiv{2401.02929}
                \end{itemize}
        \end{enumerate}
        \column{0.5\textwidth}
        \hfill\includegraphics[width=0.6\textwidth]{figures/des_parameters.pdf}
        \includegraphics[width=0.5\textwidth]{figures/des_model_comparison.pdf}%
        \includegraphics[width=0.5\textwidth]{figures/des_suspiciousness.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{SBI: Simulation-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Only have access to a forward model $\theta \rightarrow D$.
            \item $(\theta,D)$ plane gives a more expansive theoretical view of inference.
            \item Forward model defines \emph{implicit} likelihood~$\mathcal{L}$:
            \item Simulator generates samples from $\mathcal{L}(D|\theta)$.
            \item With a prior $\pi(\theta)$ can generate samples from joint distribution~$\mathcal{J}(\theta,D)=\mathcal{L}(D|\theta)\pi(\theta)$\\\hfill \emph{the ``probability of everything''}.
            \item Task of SBI is then to go from joint~$\mathcal{J}$ to posterior $\mathcal{P}(\theta|D)$ and evidence $\mathcal{Z}(D)$ -- and possibly likelihood $\mathcal{L}(D|\theta)$.
            \item SBI \& forward modelling force us to think about data space~$D$ \& parameter space~$\theta$.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[page=1, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<2>[page=2, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<3>[page=3, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<4>[page=4, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<5>[page=5, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
    \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Simulation-based inference \& model comparison}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{itemize}
            \item Extend: models $A$ and $B$.
            \item Each with own separate parameters $\theta_A$ and $\theta_B$ (can be same).
            \item The evidence $\mathcal{Z}(D|M)$ compares models
            \item Occams razor: more~predictive $\equiv$~more~probable \\(due to normalisation).
        \end{itemize}
        
        \column{0.7\textwidth}
        \includegraphics<1>[page=1, width=\textwidth]{figures/sbi_model_comparison.pdf}%
        \includegraphics<2>[page=2, width=\textwidth]{figures/sbi_model_comparison.pdf}%
        \includegraphics<3>[page=3, width=\textwidth]{figures/sbi_model_comparison.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Evidence networks~\arxiv{2305.11241}}
    \begin{columns}
        \column{0.5\textwidth}
    \begin{itemize}
        \item Procedure proposed by Jeffrey \& Wandelt:
            \begin{enumerate}
                \item Generate labelled data from model $A$ and model $B$.
                \item Train a probabilistic classifier to distinguish between the two.
                \item Use neural ratio trick to extract Bayes Factor $B = P(D|A)/P(D|B)$.
            \end{enumerate}
        \item NRE for data
        \item Fully marginalises out parameters
        \item Only works in the data space
        \item Model comparison without nested sampling!
        \item Can be extremely effective
    \end{itemize}
        
        \column{0.5\textwidth}
        \includegraphics[width=0.5\textwidth]{figures/sbi-0.pdf}%
        \includegraphics[width=0.5\textwidth]{figures/sbi-1.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{A word of caution on data-space modelling}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{itemize}
            \item In practice the situation is more like this $\Rightarrow$
                \begin{itemize}
                    \item \emph{``No models are true,} \\\emph{(but some are useful)''}
                \end{itemize}
            \item Curse of dimensionality means real data may not lie in either/any evidence distribution $\mathcal{Z}(D)$.
            \item e.g. if you are training an ML method, it will have never seen simulated data like the real data.
        \end{itemize}
        
        \column{0.7\textwidth}
        %\includegraphics<1>[page=4, width=\textwidth]{figures/sbi_model_comparison.pdf}%
        \includegraphics[page=5, width=\textwidth]{figures/sbi_model_comparison.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{A word of caution on data-space modelling}
            \begin{columns}
                \column{0.5\textwidth}
    \begin{itemize}
        \item This concern affects any amortised method 
            \begin{itemize}
                \item means trainining method on simulations\ldots
                \item \ldots and then pass in the real data
                \item They are amortised (over the data) because they can be re-used for any new data.
            \end{itemize}
        \item Observed data is only thing we surely know.
        \item As scientists we should be suspicious of a method that leaves $D_\mathrm{obs}$ until the end.
        \item This can be amelioriated by fitting $\theta$.
            \begin{itemize}
                \item Fitting concentrates parameters \& simulations around the posterior/real data
                \item See this in truncated approaches \& ABC
            \end{itemize}
    \end{itemize}
                \column{0.5\textwidth}
                \includegraphics[page=6, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
            \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Why do amortised methods often work so well?}
        Whilst these concerns sound worrying, many succesful amortised methods exist. Why?
    \begin{enumerate}
        \item Some methods are only validated on data generated from the same simulator as the one used for inference.
        \item Some methods are only validated on simple Gaussian examples, since it's possible to compute the ground truth in these cases.
            \begin{itemize}
                \item Recommendation: also test on Gaussian mixture models, for which full analytics are also known
            \end{itemize}
        \item Real data may not actually be that challenging!
    \end{enumerate}
\end{frame}
\begin{frame}
    \frametitle{\texttt{lsbi}: Linear Simulation Based Inference}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item If the final point holds, then in many cases we may not need expressive ML/AI methods
            \item Often it is the data-intensive ``plug-and-play'' power of ML packages that is most useful.
            \item If your ML is just learning a simple decision boundary, why not just use a linear model?
            \item \texttt{lsbi} is a python package that implements plug-and-play the fiddly linear mathematics.
            \item Also pedagogically useful for persuading people that SBI $\ne$ ML.
            \item Beta-testers wanted:
            \item \texttt{lsbi}: \href{https://github.com/handley-lab/lsbi}{github.com/handley-lab/lsbi}\\ (PyPI \& conda)
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[width=\textwidth]{figures/gaussian.png}%
        \includegraphics<2>[width=\textwidth]{figures/needles.png}%
        \includegraphics<3>[width=\textwidth]{figures/ring.png}%
    \end{columns}
\end{frame}



\begin{frame}
    \frametitle{How this SBI talk finishes}
    \begin{itemize}
        \item There is a standard exchange that tends to happen after giving an SBI talk:

            \begin{description}
                \item[audience] Surely you're only as good as your simulations ---\\What if your forward model is missing physics $X$?
                \item[speaker] The exact same thing affects likelihood-based analysis ---\\
                    All SBI does is make these assumptions explicit.
            \end{description}
        \item The audience is implicitly making a query about the danger of working in data space~$D$, whilst the speaker's comment only applies to parameter space $\theta$.
        \item Discussion point: We should therefore focus on SBI approaches which have tunable parameter spaces (i.e. interpretable posteriors).
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\href{https://www.github.com/handley-lab}{github.com/handley-lab}}
    \tikz[overlay,remember picture]
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
            \includegraphics[width=0.09\textheight]{figures/students/adam_ormondroyd.jpg}%
            \includegraphics[width=0.09\textheight]{figures/students/david_yallup.jpg}%
            \includegraphics[width=0.09\textheight]{figures/students/dily_ong.jpg}%
            \includegraphics[width=0.09\textheight]{figures/students/felicity_ibrahim.jpg}%
            \includegraphics[width=0.09\textheight]{figures/students/george_carter.jpg}%
            \includegraphics[width=0.09\textheight]{figures/students/harry_bevins.jpg}%
            \includegraphics[width=0.09\textheight]{figures/students/ian_roque.jpg}%
            \includegraphics[width=0.09\textheight]{figures/students/kilian_scheutwinkel.jpg}%
            \includegraphics[width=0.09\textheight]{figures/students/metha_prathaban.jpg}%
            \includegraphics[width=0.09\textheight]{figures/students/namu_kroupa.jpg}%
            \includegraphics[width=0.09\textheight]{figures/students/sinah_legner.jpg}%
            \includegraphics[width=0.09\textheight]{figures/students/thomas_gessey-jones.jpg}%
            \includegraphics[width=0.09\textheight]{figures/students/tze_goh.jpg}%
            \includegraphics[width=0.09\textheight]{figures/students/wei-ning_deng.jpg}%
    };
    \begin{itemize}
        \item These musings emerged from conversations with:
            \begin{itemize}
                \item David Yallup
                \item Mike Hobson
                \item Ben Wandelt
                \item Justin Alsing
                \item Niall Jeffrey
            \end{itemize}
        \item As scientists, we should be cautious of amortised approaches
        \item \texttt{lsbi} preview: a package-driven attempt to free SBI from ML
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Cosmological forecasting}
    \framesubtitle{Have you ever done a Fisher forecast, and then felt Bayesian guilt?}
    \vspace{-20pt}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item Cosmologists are interested in forecasting what a Bayesian analysis of future data might produce.
            \item Useful for:
                \begin{itemize}
                    \item white papers/grants,
                    \item optimising existing instruments/strategies,
                    \item picking theory/observation to explore next.
                \end{itemize}
            \item To do this properly:
                \begin{enumerate}
                    \item start from current knowledge $\pi(\theta)$, derived from current data
                    \item Pick potential dataset $D\sim\mathcal{Z}(D)$ that might be collected from $P(D)\: (=\mathcal{Z})$
\item Derive posterior $\mathcal{P}(\theta|D)$
                    \item Summarise science (e.g. constraint on $\theta$, ability to perform model comparison)
                \end{enumerate}
        \end{itemize}

        \column{0.5\textwidth}
        \begin{itemize}
            \item This procedure should be marginalised over:
                \begin{enumerate}
                    \item All possible parameters $\theta$ (consistent with prior knowledge)
                    \item All possible data $D$
                \end{enumerate}
            \item i.e. marginalised over the joint $P(\theta,D)=P(D|\theta)P(\theta)$.
            \item Historically this has proven very challenging.
            \item Most analyses assume a fiducial cosmology $\theta_*$, and/or a Gaussian likelihood/posterior (c.f. Fisher forecasting).
            \item This runs the risk of biasing forecasts by baking in a given theory/data realisation.
        \end{itemize}
        
    \end{columns}

\end{frame}

\appendix
\begin{frame}
    \frametitle{Fully Bayesian Forecasting~\arxiv{2309.06942}}
    \student{thomas_gessey-jones}{Thomas Gessey-Jones}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Simulation based inference gives us the language to marginalise over parameters $\theta$ and possible future data $D$.
            \item Evidence networks give us the ability to do this at scale for forecasting.
            \item Demonstrated in 21cm global experiments, marginalising over:
                \begin{itemize}
                    \item theoretical uncertainty
                    \item foreground uncertainty
                    \item systematic uncertainty
                \end{itemize}
            \item Able to say ``at 67mK radiometer noise'', have a 50\% chance of 5$\sigma$ Bayes factor detection.
            \item Can use to optimise instrument design
            \item Re-usable package: \texttt{prescience}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/fbf.pdf}
    \end{columns}
\end{frame}

\end{document}
```
# branch: remotes/origin/sydney_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usetikzlibrary{arrows,arrows.meta,automata,positioning}
\usepackage{slashed}

% Commands
% --------
% - \arxiv{arxiv number}
% - \arxiv{<number>}            arxiv.org/abs/<number>
% - \oldarxiv{<arxiv number>}   arxiv.org/<number>
% - \doi{<doi>}                 doi.org/<doi>
% - \xkcd{<number>}             xkcd.com/<number>
% - \email{<email>}             <<email>>
% - \tthref{<website>}          <website>
% - \av[dist]{<quantity>}       <quantity>_{dist}
% - \student{<name>}{<detail>}{<photo>}

% Talk details
% ------------
\title{ Nested sampling: {\large powering next-generation inference and machine learning tools}}
\subtitle{for astrophysics, cosmology, particle physics and beyond}
\date{30\textsuperscript{th} August 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{LBI: Likelihood-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        The standard approach if you are fortunate enough to have a likelihood function $\only<1-2>{P(D|\theta)}\only<3->{\C[2]{\mathcal{L}(D|\theta)}}$: 
        \[
            \only<1-2>{
                P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}
            }
            \only<2>{
                \qquad
                \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}}\times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}
            }
            \only<3>{
                \C[0]{\mathcal{P}(\theta|D)} = \frac{\C[2]{\mathcal{L}(D|\theta)}\C[1]{\pi(\theta)}}{\C[3]{\mathcal{Z}(D)}}
                \qquad
                \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}}\times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}
            }
            \only<4>{
                P(\theta|D) P(D) = P(\theta,D) = P(D|\theta)P(\theta), \qquad
            }
            \only<5>{
                \C[0]{\mathcal{P}}\times\C[3]{\mathcal{Z}} = \C[4]{\mathcal{J}} = \C[2]{\mathcal{L}}\times\C[1]{\pi}, \qquad \C[4]{\text{Joint}} = \C[4]{\mathcal{J}} = P(\theta,D)
            }
        \]
        \vspace{-10pt}
        \begin{enumerate}
            \item Define \C[1]{prior $\pi(\theta)$} 
                \begin{itemize}
                    \item spend some time being philosophical
                \end{itemize}
            \item Sample \C[0]{posterior $\mathcal{P}(\theta|D)$} 
                \begin{itemize}
                    \item use out-of-the-box MCMC tools such as\\ \texttt{emcee} or \texttt{MultiNest}
                    \item make some triangle plots
                \end{itemize}
            \item Optionally compute \C[3]{evidence $\mathcal{Z}(D)$}
                \begin{itemize}
                    \item e.g. nested sampling or parallel tempering
                    \item do some model comparison (i.e. science)
                    \item talk about tensions
                \end{itemize}
        \end{enumerate}
        \column{0.5\textwidth}
        \hfill%
        \begin{overpic}[width=0.6\textwidth]{figures/des_parameters.pdf}
            \put(-40,90) {DES Y5 SN Ia}
            \put(-40,80) {\arxiv{2401.02929}}
        \end{overpic}
        \includegraphics[width=0.5\textwidth]{figures/des_model_comparison.pdf}%
        \includegraphics[width=0.5\textwidth]{figures/des_suspiciousness.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The three pillars of Bayesian inference}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \begin{block}{Parameter estimation}
            What do the data tell us about the parameters of a model?\\
            \textit{e.g. the size or age of a $\Lambda$CDM universe}
            \[ \hspace{-4pt}\C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}} \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}\] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Model comparison}
            How much does the data support a particular model?\\
            \textit{e.g. $\Lambda$CDM vs a dynamic dark energy cosmology}
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} \C[5]{P(M)}}{\C[7]{P(D)}} \vspace{-7pt}\]
            \[ \frac{\C[3]{\mathcal{Z}_{M}} \C[5]{\Pi_{M}}}{\C[7]{\sum_m Z_m \Pi_m}} \]
            \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\C[5]{\text{Prior}}}{\C[7]{\text{Normalisation}}}\]
        \end{block}
        \column{0.33\textwidth}
        \begin{block}{Tension quantification}
            Do different datasets make consistent predictions from the same model? 
            \textit{e.g. CMB vs Type IA supernovae data}
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}\] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Model comparison $\C[3]{\mathcal{Z}=P(D|M)}$}
    \begin{itemize}
        \item Bayesian model comparison allows mathematical derivation of key philosophical principles.
    \end{itemize}
    \begin{columns}[t]
        \column{0.47\textwidth}
        Viewed from data-space $D$:
        \begin{block}{Popper's falsificationism}
            \begin{itemize}
                \item Prefer models that make bold predictions.
                \item if proven true, model more likely correct.
            \end{itemize}
        \end{block}
        \includegraphics<1|handout:0>[width=\textwidth, page=1]{figures/popper}%
        \includegraphics<2|handout:0>[width=\textwidth, page=2]{figures/popper}%
        \includegraphics<3>[width=\textwidth, page=3]{figures/popper}%
        \begin{itemize}
            \item Falsificationism comes from normalisation
        \end{itemize}
        \column{0.47\textwidth}
        Viewed from parameter-space $\theta$:
        \begin{block}{Occam's razor}
            \begin{itemize}
                \item Models should be as simple as possible
                \item \ldots but no simpler
            \end{itemize}
        \end{block}
        \begin{itemize}
            \item Occam's razor equation:
                \[\C[3]{\log\mathcal{Z}} = \av[{\C[0]{\mathcal{P}}}]{\C[2]{\log\mathcal{L}}} - \mathcal{D}_\text{KL}\]
            \item ``Occam penalty'': KL divergence between \C[1]{prior~$\pi$} and \C[0]{posterior~$\mathcal{P}$}.
                \[ \mathcal{D}_\text{KL}\sim \log\frac{\text{\C[1]{Prior volume} }}{\text{\C[0]{Posterior volume}}}\]
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{What is Nested Sampling?}
    \begin{itemize}
        \item Nested sampling is a radical, multi-purpose numerical tool.
        \item Given a (scalar) function $f$ with a vector of parameters $\theta$, it can be used for:
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \begin{block}{Optimisation}
            \[\theta_\text{max} = \max_\theta{f(\theta)}\]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Exploration}
            \vspace{-10pt}
            \[\text{draw/sample}\quad \theta\sim f\]
            \vspace{-15pt}
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Integration}
            \[\int f(\theta) dV \]
        \end{block}
    \end{columns}
    \begin{columns}[t]
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=13]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=15]{figures/himmelblau}}
        \column{0.33\textwidth}
        \centerline{\includegraphics[width=0.8\textwidth,page=14]{figures/himmelblau}}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Where is Nested Sampling?}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item For many purposes, in your Neural Net you should group Nested Sampling with (MCMC) techniques such as:
                \begin{itemize}
                    \item Metropolis-Hastings (PyMC, MontePython)
                    \item Hamiltonian Monte Carlo (Stan, blackjax)
                    \item Ensemble sampling (emcee, zeus). 
                    \item Variational Inference (Pyro)
                    \item Sequential Monte Carlo 
                    \item Thermodynamic integration
                    \item Genetic algorithms
                \end{itemize}
            \item You may have heard of it branded form:
                \begin{itemize}
                    \item MultiNest
                    \item PolyChord
                    \item dynesty
                    \item ultranest
                \end{itemize}
            \end{itemize}
        \column{0.5\textwidth}
        \begin{columns}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/emcee}
        \includegraphics[width=\textwidth]{figures/metropolis-hastings}
            \column{0.5\textwidth}
        \includegraphics[width=\textwidth]{figures/zeus}
        \end{columns}
        \includegraphics[width=\textwidth]{figures/hmc_explained}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Integration in Physics}
    \begin{itemize}
        \item Integration is a fundamental concept in physics, statistics and data science:
    \end{itemize}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{block}{Partition functions}
            \vspace{-11pt}
            \[ Z(\beta) = \int e^{-\beta H(q,p)} dq dp \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Path integrals}
            \[ \Psi = \int e^{i S} \mathcal{D}x \]
        \end{block}
        \column{0.3\textwidth}
        \begin{block}{Bayesian marginals}
            \vspace{-11pt}
            \[ \mathcal{Z}(D) = \int \mathcal{L}(D|\theta) \pi(\theta) d\theta \]
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Need numerical tools if analytic solution unavailable.
            \item High-dimensional numerical integration is hard.
            \item Riemannian strategy estimates volumes geometrically:
                \[ \int f(x) d^nx \approx \sum_i f(x_i) \Delta V_i \sim \mathcal{O}(e^n) \]
            \item Curse of dimensionality $\Rightarrow$ exponential scaling.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/integration.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \begin{columns}
        \column{0.48\textwidth}
        \begin{block}{\textbf{MCMC}}
            \only<16>{
                \begin{itemize}
                    \item Single ``walker''
                    \item Explores posterior
                    \item Fast, if proposal matrix is tuned
                    \item Parameter estimation, suspiciousness calculation
                    \item Channel capacity optimised for generating posterior samples
                \end{itemize}
            }
        \end{block}
            \includegraphics<1>[width=\textwidth,page=16]{figures/himmelblau}%
            \includegraphics<2>[width=\textwidth,page=17]{figures/himmelblau}%
            \includegraphics<3>[width=\textwidth,page=18]{figures/himmelblau}%
            \includegraphics<4>[width=\textwidth,page=19]{figures/himmelblau}%
            \includegraphics<5>[width=\textwidth,page=20]{figures/himmelblau}%
            \includegraphics<6-15>[width=\textwidth,page=21]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=19]{figures/himmelblau}}
        \column{0.48\textwidth}
        \begin{block}<7->{\textbf{Nested sampling}}
            \only<16>{
                \begin{itemize}
                    \item Ensemble of ``live points''
                    \item Scans from prior to peak of likelihood
                    \item Slower, no tuning required
                    \item Parameter estimation, model comparison, tension quantification
                    \item Channel capacity optimised for computing partition function
                \end{itemize}
            }
        \end{block}
            \includegraphics<7|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
            \includegraphics<8|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
            \includegraphics<9|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
            \includegraphics<10          >[width=\textwidth,page=4]{figures/himmelblau}%
            \includegraphics<11|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
            \includegraphics<12|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
            \includegraphics<13|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
            \includegraphics<14|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
            \includegraphics<15|handout:0>[width=\textwidth,page=15]{figures/himmelblau}%
        \centerline{\includegraphics<16>[width=0.5\textwidth,page=4]{figures/himmelblau}} 
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: live points}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n$ random samples over the space.
            \item Delete outermost sample, and replace with a new random one at higher integrand value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n}\only<5->{\pm \frac{1}{n}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \int f(x) dV \approx \sum_i f(x_i) \Delta V_i, \quad V_i = V_0 e^{-\only<5->{(}i\only<5->{\pm\sqrt{i})}/n} \]
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4-         >[width=\textwidth,page=4]{figures/himmelblau}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The nested sampling meta-algorithm: dead points}
    \includegraphics[width=\textwidth]{figures/dead_measure}
    \begin{columns}
        \column{0.69\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Dead points have a unique scale-invariant distribution $\propto\: \tfrac{dV}{V}$.
            \item Uniform over original region, exponentially concentrating on region of interest (until termination volume).
            \item Good for training emulators (HERA~\arxiv{2108.07282}).
        \end{itemize}
        \column{0.3\textwidth}
        \begin{block}{Applications}
        \begin{itemize}
            \item training emulators.
            \item gridding simulations
            \item beta flows
            \item ``dead measure'' 
        \end{itemize}
        \end{block}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Implementations of Nested Sampling \arxiv{2205.15570}(NatReview)}
    \begin{columns}[t]
        \column{0.3\textwidth}
        \texttt{MultiNest}~\arxiv{0809.3437}
        \includegraphics[width=\textwidth]{figures/multinest}
        \texttt{UltraNest}~\arxiv{2101.09604}
        \includegraphics[width=\textwidth]{figures/radfriends}
        \texttt{nautilus}~\arxiv{2306.16923} 
        \column{0.4\textwidth}
        \texttt{PolyChord}~\arxiv{1506.00171}
        \includegraphics[width=\textwidth]{figures/polychord}
        \vfill
        \texttt{NeuralNest}~\arxiv{1903.10860}
        \begin{columns}
            \column{0.55\textwidth}
            \includegraphics[width=\textwidth]{figures/rosenbrock_flow.png}
            \includegraphics[width=\textwidth]{figures/himmelblau_flow.png}
            \column{0.45\textwidth}
            \includegraphics[width=\textwidth]{figures/chains.png}
        \end{columns}
        \texttt{nessai}~\arxiv{2102.11056} \texttt{nora}~\arxiv{2305.19267} \texttt{jaxnest}~\arxiv{2012.15286}
        \vfill
        \column{0.3\textwidth}
        \texttt{DNest}~\arxiv{1606.03757}
        \includegraphics[width=\textwidth]{figures/dnest}
        \texttt{ProxNest}~\arxiv{2106.03646}
        \includegraphics[width=\textwidth]{figures/proxnest_diagram}
        \texttt{dynesty}~\arxiv{1904.02180} 
        \vfill
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Types of nested sampler}
    \begin{itemize}
        \item Broadly, most nested samplers can be split into how they create new live points.
        \item i.e. how they sample from the hard likelihood constraint $\{\theta\sim \pi : \mathcal{L}(\theta)>\mathcal{L}_* \}$.
    \end{itemize}
    \vspace{-10pt}
    \begin{columns}[t]
        \column{0.48\textwidth}
        \begin{block}{Rejection samplers}
            \begin{itemize}
                \item e.g. \texttt{MultiNest}, \texttt{UltraNest}.
\item Constructs bounding region and draws many invalid points until $\mathcal{L}(\theta)>\mathcal{L}_*$.
                \item Efficient in low dimensions, exponentially inefficient $\sim\mathcal{O}(e^{d/d_0})$ in high  $d>d_0\sim10$.
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{block}{Chain-based samplers}
            \begin{itemize}
                \item e.g. \texttt{PolyChord}, \texttt{ProxNest}.
                \item Run Markov chain starting at a live point, generating many valid (correlated) points.
                \item Linear $\sim\mathcal{O}(d)$ penalty in decorrelating new live point from the original seed point.
            \end{itemize}
        \end{block}
    \end{columns}
    \vspace{5pt}
    \begin{itemize}
        \item Nested samplers usually come with:
            \begin{itemize}
                \item \emph{resolution} parameter $n_\mathrm{live}$ (which improve results as $\sim\mathcal{O}(n_\mathrm{live}^{-1/2})$.
                    \item set of \emph{reliability} parameters~\arxiv{2101.04525}, which don't improve results if set arbitrarily high, but introduce systematic errors if set too low.
                    \item e.g. \texttt{Multinest} efficiency \texttt{eff} or \texttt{PolyChord} chain length $n_\mathrm{repeats}$.
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Cosmology}
    \student{adam_ormondroyd}{Adam Ormondroyd}{PhD}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Battle-tested in Bayesian cosmology on
                \begin{itemize}
                    \item Parameter estimation: multimodal alternative to MCMC samplers.
                    \item Model comparison: using integration to compute the Bayesian evidence
                    \item Tension quantification: using deep tail sampling and suspiciousness computations.
                \end{itemize}
            \item Plays a critical role in major cosmology pipelines: Planck, DES, KiDS, BAO, SNe.
            \item The default $\Lambda$CDM cosmology is well-tuned to have Gaussian-like posteriors for CMB data. 
            \item Less true for alternative cosmologies/models and orthogonal datasets, so nested sampling crucial.
        \end{itemize}
        \column{0.45\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/pps_both}
        \includegraphics[width=0.49\textwidth]{figures/reach_fit-cropped.pdf}
        \includegraphics[width=\textwidth]{figures/omegak_H0_2.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Astrophysics}
\student{metha_prathaban}{Metha Prathaban}{PhD}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item In exoplanets~\arxiv{1806.00518}
                \begin{itemize}
                    \item Parameter estimation: determining properties of planets.
                    \item Model comparison: how many planets? Stellar modelling~\arxiv{2007.07278}.
                    \item exoplanet problems regularly have posterior phase transitions \arxiv{2102.03387}
                \end{itemize}
            \item In gravitational waves
                \begin{itemize}
                    \item Parameter estimation: Binary merger properties
                    \item Model comparison: Modified theories of gravity, selecting phenomenological parameterisations~\arxiv{1803.10210}
                    \item Likelihood reweighting: fast slow properties
                \end{itemize}
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/rv_full.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Particle physics}
    \student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{columns}
            \column{0.67\textwidth}
            \begin{itemize}
                \item Nested sampling for cross section computation/event generation
            \end{itemize}
            \column{0.3\textwidth}
            \[\sigma = \int_\Omega d\Phi |\mathcal{M}|^2.\]
        \end{columns}
        \begin{itemize}
            \item Nested sampling can explore the phase space $\Omega$ and compute integral blind with comparable efficiency to HAAG/RAMBO~\arxiv{2205.02030}.
            \item Bayesian sparse reconstruction~\arxiv{1809.04598} applied to bump hunting allows evidence-based detection of signals in phenomenological backgrounds~\arxiv{2211.10391}.
            \item Fine tuning quantification
            \item Fast estimation of small $p$-values \arxiv{2106.02056}(PRL), just make switch:\\ $X\leftrightarrow p$, $\mathcal{L}\leftrightarrow\lambda$, $\theta \leftrightarrow x$.
        \end{itemize}
        \column{0.17\textwidth}
        \includegraphics[width=\textwidth]{figures/phase_space_1-pdfjam-crop.pdf}
        \column{0.27\textwidth}
        \includegraphics[width=\textwidth]{figures/psi_predict-crop.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Lattice field theory}
\student{david_yallup}{David Yallup}{PDRA}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Consider standard field theory Lagrangian:
                \[ Z(\beta) = \int D\phi e^{-\beta S(\phi)}, \quad S(\phi) = \int dx^\mu \mathcal{L}(\phi) \]
            \item Discretize onto spacetime grid.
            \item Compute partition function
            \item NS unique traits:
                \begin{itemize}
                    \item Get full partition function for free
                    \item allows for critical tuning
                    \item avoids critical slowing down
                \end{itemize}
            \item Applications in lattice gravity, QCD, condensed matter physics
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=0.49\textwidth]{figures/potential_shape}
        \includegraphics[width=0.49\textwidth]{figures/2d_phase}
        \includegraphics[width=\textwidth]{figures/lattice_field_theory.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{Machine learning}
\student{kamran_javid}{Kamran Javid}{PDRA}
    \begin{columns}
        \column{0.62\textwidth}
        \begin{itemize}
            \item Machine learning requires:
                \begin{itemize}
                    \item Training to find weights
                    \item Choice of architecture/topology/hyperparameters
                \end{itemize}
            \item Bayesian NNs treat training as a model fitting problem
            \item Compute posterior of weights (parameter estimation), rather than optimisation (gradient descent)
            \item Use evidence to determine best architecture (model comparison), correlates with out-of-sample performance! 
            \item Solving the full ``shallow learning'' problem without compromise \arxiv{2004.12211}\arxiv{2211.10391}. 
                \begin{itemize}
                    \item Promising work ongoing to extend this to transfer learning and deep nets.
                \end{itemize}
            \item More generally, dead points are optimally spaced  for training traditional ML approaches e.g. \arxiv{2309.05697}
        \end{itemize}
        \column{0.38\textwidth}
        \includegraphics[width=\textwidth]{figures/nn_posterior-cropped.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Applications of nested sampling}
    \framesubtitle{And beyond\ldots}
    \only<1-4>{
        \student{catherine-watkinson-polychord}{Catherine Watkinson}{Senior Data Scientist}
    }
    \only<5->{
        \student{thomas-macaloone-polychord}{Thomas Mcaloone}{PhD $\to$ Data Scientist}
    }
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
        \includegraphics[width=0.08\textwidth]{people/catherine-watkinson-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{people/thomas-macaloone-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{people/parul-janagal-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{people/angus-peters-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{people/tamas-stenzel-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{people/david-yallup-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{people/rebecca-handley-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{people/adam-ormondroyd-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{people/mike-hobson-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{people/anthony-lasenby-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{people/mike-handley-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{people/will-handley-polychord.jpg}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        \includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        \includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        \includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        \includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        \includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology {\small \arxiv{2210.07409}(NatAstro)}}
    \student{ian_roque}{Ian Roque}{PhD}
    \vspace{10pt}
    \begin{columns}
        \column{0.65\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
            \item Lead data analysis team (REACH first light in January)
            \item Nested sampling woven in from the ground up (calibrator, beam modelling, signal fitting, likelihood selection).
            \item All treated as parameterised model comparison problems.
        \includegraphics[height=0.3\textwidth]{figures/EDGES_antenna}
        \includegraphics[height=0.3\textwidth]{figures/SARAS}
        \includegraphics[height=0.3\textwidth]{figures/REACH_2.jpg}
        \end{itemize}
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT}
    \framesubtitle{Interdisciplinary case studies}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item GAMBIT is an interdisciplinary community and software framework.
            \item Like \texttt{CosmoMC}/\texttt{Cobaya}/\texttt{Bilby}, an organiser of data, likelihoods \& theory, including:
                \begin{itemize}
                    \item Collider data (e.g. LHC)
                    \item Direct detections (e.g. XENON1T)
                    \item Cosmology (\texttt{MontePython})
                    \item Astrophysics (e.g. Bullet Cluster, Supernovae)
                    \item Pulsar timing
                    \item \ldots \& much more
                \end{itemize}
            \item \texttt{GravBit} and \texttt{LowEnergyBit} arising from GAMBIT@KICC workshop
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[height=0.423\textwidth]{people/gambit.png}
        \includegraphics[height=0.423\textwidth]{figures/gambit_kicc.jpg}
        \includegraphics[width=\textwidth]{figures/gambit_meetingbanner.jpg}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT: sub-GeV Dark matter constraints}
    \framesubtitle{Interdisciplinary case studies}
    \student{gambit}{Felix Kahlhoefer et al}{GAMBIT cosmo/DM working group}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{itemize}
            \item Physical model of sub-GeV thermal dark matter with a dark photon mediator~$A$:
        \end{itemize}
        \vspace{-10pt}
        \begin{align*}
            \small
            \mathcal{L}_\text{int} =& -\frac{1}{2} m_{A'}^2 A'^\mu A'_\mu - \frac{1}{4} A'^{\mu\nu}A'_{\mu\nu} -\kappa e A'^\mu \sum_{f} q_f \overline{f} \gamma_\mu f \,,
            \normalsize
        \end{align*}
        \vspace{-15pt}
        \begin{itemize}
            \item Constrain using cosmological, astrophysical, accelerator \& direct detection data.
            \item Bayesian Model comparison of Fermion~$\psi$ vs scalar~$\Phi$ models (scalar preferred).
        \end{itemize}
        \vspace{-10pt}
        \begin{align*}
            \small
            \mathcal{L}_\psi  =& \bar{\psi}(i \slashed{\partial} - m_\text{DM}) \psi + g_\text{DM} A'^\mu \bar{\psi} \gamma_\mu \psi \,,\\
            \mathcal{L}_\Phi  =& |\partial_\mu \Phi|^2 - m_\text{DM}^2 |\Phi|^2 - g_\text{DM}^2 A'_\mu A'^\mu |\Phi|^2 \\ &+ i g_\text{DM} A'^\mu \left[\Phi^\ast (\partial_\mu \Phi) - (\partial_\mu \Phi^\ast) \Phi\right]\,,
            \normalsize
        \end{align*}
        \column{0.44\textwidth}
        \vspace{10pt}
        \includegraphics[width=\textwidth]{figures/Bayes_SubGeVDM_fermion_RDprior_allDM_asym_observables.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{SBI: Simulation-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item What do you do if you don't know \C[2]{$\mathcal{L}(D|\theta)$}?
            \item If you have a simulator/forward model $\theta \rightarrow D$
                defines an \C[2]{\emph{implicit} likelihood~$\mathcal{L}$}.
            \item Simulator generates samples from $\C[2]{\mathcal{L}(\cdot|\theta)}$.
            \item With a prior $\C[1]{\pi}(\theta)$ can generate samples from \C[4]{joint distribution}~$\C[4]{\mathcal{J}(\theta,D)}=\C[2]{\mathcal{L}(D|\theta)}\C[1]{\pi(\theta)}$\\\hfill \emph{the ``probability of everything''}.
            \item Task of SBI is take joint~$\C[4]{\mathcal{J}}$ samples and learn \C[0]{posterior $\mathcal{P}(\theta|D)$} and \C[3]{evidence $\mathcal{Z}(D)$} \\\hfill and possibly \C[2]{likelihood $\mathcal{L}(D|\theta)$}.
            \item Present state of the art achieves this using \emph{machine learning} (neural networks).
                \begin{itemize}
                    \item My group's research tries to removes machine learning \tthref{github.com/handley-lab/lsbi}.
                \end{itemize}
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1>[page=1, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<2>[page=2, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<3>[page=3, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<4>[page=4, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<5>[page=5, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<6>[page=6, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<7>[page=7, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<8>[page=8, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<9>[page=9, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<10>[page=10, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<11>[page=11, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<12>[page=12, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<13>[page=13, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<14>[page=14, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<15>[page=15, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<16>[page=16, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<17>[page=17, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<18>[page=18, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<19>[page=19, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<20>[page=20, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
        \includegraphics<21>[page=21, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Why SBI?}
    \begin{columns}
        \column{0.6\textwidth}
        SBI is useful because:
        \begin{enumerate}
            \item If you don't have a likelihood, you can still do inference
                \begin{itemize}
                    \item This is the usual case beyond CMB cosmology
                \end{itemize}
            \item Faster than LBI
                \begin{itemize}
                    \item emulation -- also applies to LBI in principle
                \end{itemize}
            \item No need to pragmatically encode fiducial cosmologies
                \begin{itemize}
                    \item Covariance computation implicitly encoded in simulations
                    \item Highly relevant for disentangling tensions \& systematics
                \end{itemize}
            \item Equips AI/ML with Bayesian interpretability
            \item Lower barrier to entry than LBI
                \begin{itemize}
                    \item Much easier to forward model a systematic
                    \item Emerging set of plug-and-play packages
                    \item For this reason alone, it will come to dominate scientific inference
                \end{itemize}
        \end{enumerate}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/sbi_screenshot}
        \href{https://github.com/sbi-dev}{github.com/sbi-dev}
        \includegraphics[width=\textwidth]{figures/swyft_screenshot}
        \href{https://github.com/undark-lab/swyft}{github.com/undark-lab/swyft}
        \includegraphics[width=\textwidth]{figures/selfi_screenshot}
        \href{https://github.com/florent-leclercq/pyselfi}{github.com/florent-leclercq/pyselfi}
        \includegraphics[width=\textwidth]{figures/delfi_screenshot}
        \href{https://github.com/justinalsing/pydelfi}{github.com/justinalsing/pydelfi}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{SBI in cosmology}
    \begin{columns}
        \column{0.4\textwidth}
        \begin{itemize}
            \item 2024 has been the year it has started to be applied to real data.
            \item Mostly for weak lensing
            \item However: SBI requires mock data generation code
            \item Most data analysis codes were built before the generative paradigm.
            \item It's still a lot of work to upgrade cosmological likelihoods  to be able to do this (e.g.\ \texttt{plik} \& \texttt{camspec}).
        \end{itemize}
        \column{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/sbi_papers/clusters.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/des.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/gw.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/center.pdf}
        \column{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/sbi_papers/kidsdes.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/kids.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/population.pdf}
        \vspace{10pt}\\
        \includegraphics[width=\textwidth]{figures/sbi_papers/simbig.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\tthref{github.com/handley-lab}}
    \tikz[overlay,remember picture]
        \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
        \includegraphics[width=0.10\textheight]{people/adam_ormondroyd.jpg}%
        \includegraphics[width=0.10\textheight]{people/charlotte_priestley.jpg}%
        \includegraphics[width=0.10\textheight]{people/david_yallup.jpg}%
        \includegraphics[width=0.10\textheight]{people/dily_ong.jpg}%
        \includegraphics[width=0.10\textheight]{people/george_carter.jpg}%
        \includegraphics[width=0.10\textheight]{people/harry_bevins.jpg}%
        \includegraphics[width=0.10\textheight]{people/kilian_scheutwinkel.jpg}%
        \includegraphics[width=0.10\textheight]{people/metha_prathaban.jpg}%
        \includegraphics[width=0.10\textheight]{people/namu_kroupa.jpg}%
        \includegraphics[width=0.10\textheight]{people/sinah_legner.jpg}%
        \includegraphics[width=0.10\textheight]{people/sam_leeney.jpg}%
        \includegraphics[width=0.10\textheight]{people/wei-ning_deng.jpg}%
        \includegraphics[width=0.10\textheight]{people/will_handley.jpg}%
    };
    \vspace{-0.1\textheight}
    \begin{columns}
        \column{0.65\textwidth}
    \begin{itemize}
        \item Nested sampling is a multi-purpose numerical tool for:
            \begin{itemize}
                \item Numerical integration $\int f(x) dV$,
                \item Exploring/scanning/optimising \textit{a priori} unknown functions,
                \item Performing Bayesian inference and model comparison.
            \end{itemize}
        \item It is applied widely across cosmology \& particle physics.
        \item It's unique traits as the only numerical Lebesgue integrator mean with compute it will continue to grow in importance.
        \item SBI represents the future of inference beyond LBI.
    \end{itemize}
        \column{0.35\textwidth}
    \includegraphics[height=0.6\textwidth]{figures/pps_both}%
    \includegraphics[height=0.6\textwidth]{figures/REACH_2}%
    \end{columns}
    \includegraphics[height=0.2\textwidth]{figures/integration}%
    \includegraphics[height=0.2\textwidth]{figures/curvature_3}%
    \includegraphics[height=0.2\textwidth]{figures/ALP_3}%
\end{frame}

\appendix

\end{document}
```
# branch: remotes/origin/ucl_2024
```tex
\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usepackage{slashed}
\usepackage{tikz}
\usetikzlibrary{arrows,arrows.meta,automata,positioning}
\usepackage{bm}
\usepackage[percent]{overpic}


% Commands
% --------
% - \arxiv{arxiv number}
% - \cols{width}{lh column}{rh column}
% -  \begin{fig(left|right)}[fractional width (e.g 0.6) ]{name of image}
%        content of other column
%    \end{fig(left|right)}

% Talk details
% ------------
\title{Next-generation astrophysical inference\\across the interdisciplinary frontier}
\date{24\textsuperscript{th} May 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{The future of astronomy is interdisciplinary}
    \begin{itemize}
        \item Across astronomy, combining data and disciplines will be the key to the next breakthroughs.
    \end{itemize}
    \vspace{-15pt}
    \begin{columns}[t]
        \column{0.72\textwidth}
        \begin{columns}
            \column{0.4\textwidth}
            \begin{block}{CMB+BAO}
                \begin{overpic}[width=\textwidth]{figures/desi_w_constraints.pdf} 
                    \put(70,15) {\tiny \arxiv{2404.03002}}
                \end{overpic}
            \end{block}
            \column{0.5\textwidth}
            \begin{block}{HEP+Astro}
                \begin{overpic}[width=\textwidth]{figures/cosmoalp.pdf}
                    \put(15,12) {\tiny \arxiv{2205.13549}}
                \end{overpic}
            \end{block}
        \end{columns}
        \begin{itemize}
            \item We have spent the last 5 years hair-splitting ``parkable'' tensions.
            \item The next 25 years of data confront the real tensions in our understanding of the Universe.
        \end{itemize}
        \column{0.25\textwidth}
        \begin{block}{GW170817}
            \begin{overpic}[width=\textwidth]{figures/mma_2.pdf}
                \put(50,103) {\tiny \arxiv{1710.05833}}
            \end{overpic}
        \end{block}
    \end{columns}
    \begin{itemize}
        \item I aim to show how my research programme is preparing us for this interdisciplinary challenge.
    \end{itemize}


\end{frame}

%\begin{frame}
%    \frametitle{\textit{Planck}: Inflation \& primordial power spectrum}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \vspace{-5pt}
%        \begin{itemize}
%            \item Began theoretical PhD in 2012 investigating initial conditions for inflation.
%            \item Joined Planck inflation team, working on Bayesian model fitting alongside theory.
%            \item Found I enjoyed the observation \& inference as much as the theory.
%            \item \texttt{FlexKnots} were used to reconstruct the primordial power spectrum, inflationary potential \& reionisation history (now used by Fialkov group)~\arxiv{1908.00906}.
%            \item \texttt{PolyChord} developed for model comparison, particularly axion monodromy.
%                \begin{itemize}
%                    \item Now used widely within cosmology (DES, DESI, CMB) and beyond (exoplanets, GW, galaxies, 21cm, \ldots ) [\textcolor{C0}{\texttt{\href{https://ui.adsabs.harvard.edu/abs/2015MNRAS.453.4384H/citations}{ADS}}}]
%                \end{itemize}
%        \end{itemize}
%
%        \column{0.5\textwidth}
%        \begin{overpic}[height=0.4\textwidth]{figures/kinetic_dominance.pdf}%
%                \put(68,-2) {\tiny \arxiv{1809.07185}}
%        \end{overpic}
%        \begin{overpic}[height=0.4\textwidth]{figures/axion_monodromy.pdf}
%                \put(70,0) {\tiny \arxiv{1502.02114}}
%        \end{overpic}
%
%
%        \begin{overpic}[height=0.4\textwidth]{figures/flexknots.pdf}%
%        \end{overpic}
%        \begin{overpic}[height=0.4\textwidth]{figures/pps_planck.pdf}
%                \put(70,0) {\tiny \arxiv{1908.00906}}
%        \end{overpic}
%
%
%    \end{columns}
%\end{frame}

%\begin{frame}
%    \frametitle{Analytic innovation: from \texttt{MultiNest} to \texttt{PolyChord}}
%    \begin{columns}
%        \column{0.66\textwidth}
%        \begin{itemize}
%            \item \texttt{MultiNest}~\arxiv{0809.3437} was the leading Bayesian numerical model comparison tool in 2013.
%                \begin{itemize}
%                    \item A general purpose \& performant implementation of John Skilling's nested sampling meta-algorithm.
%                    \item Remains the leader of the pack in $n\sim\mathcal{O}(10)$ parameter fits.
%                    \item Careful testing in \textit{Planck} showed that it couldn't handle the many fast-slow nuisance parameters needed for systematics .
%                \end{itemize}
%            \item I analytically developed and numerically implemented \texttt{PolyChord}, which has polynomial scaling efficiency $f_{\texttt{PC}}\sim \frac{1}{n}$ with model parameters (c.f.\ exponential $f_{\texttt{MN}}\sim e^{-n/n_0}$).
%            \item Exemplifies theoretical innovation \& numerical implementation driven by astrophysical challenges.
%        \end{itemize}
%        \texttt{PolyChord} inspired a new generation of nested samplers (\texttt{dynesty}, \texttt{UltraNest}, \texttt{nessai}\ldots), but remains the state of the art in high-dimensional model comparison.
%        \column{0.33\textwidth}
%        \vspace{-10pt}
%        %\includegraphics[width=\textwidth,page=14]{figures/himmelblau.pdf}%
%        \begin{block}{\texttt{MultiNest}~\arxiv{0809.3437}}
%            \includegraphics[width=\textwidth]{figures/multinest.pdf}
%        \end{block}%
%        \vspace{-5pt}
%        \begin{block}{\texttt{PolyChord}~\arxiv{1506.00171}}
%            \includegraphics[width=\textwidth]{figures/polychord.png}
%        \end{block}
%
%
%
%    \end{columns}
%\end{frame}

%\begin{frame}
%    \frametitle{Aside: theoretical work}
%    \begin{columns}
%        \column{0.5\textwidth}
%
%        This talk will focus on my interdisciplinary work, but I have theoretical interests in:
%        \begin{itemize}
%            \item Quantum fields in curved spacetime \\\hfill {\small (Mary Letey, Zak Shumaylov, Fruzsina Agocs)}
%            \item Poincar\'{e} Gauge Theory \\\hfill {\small (Sinah Legner, Will Barker)}
%            \item Future conformal boundary/CPT universes \\\hfill {\small (Metha Prathaban, Wei-Ning Deng)}
%            \item Curved finite inflation \hfill {\small (Lukas Hergt)}
%        \end{itemize}
%        \includegraphics[width=0.125\textwidth]{figures/students/mary_letey.jpg}%
%        \includegraphics[width=0.125\textwidth]{figures/students/zak_shumaylov.jpg}%
%        \includegraphics[width=0.125\textwidth]{figures/students/fruzsina_agocs.jpg}%
%        \includegraphics[width=0.125\textwidth]{figures/students/sinah_legner.jpg}%
%        \includegraphics[width=0.125\textwidth]{figures/students/will_barker.jpg}%
%        \includegraphics[width=0.125\textwidth]{figures/students/metha_prathaban.jpg}%
%        \includegraphics[width=0.125\textwidth]{figures/students/wei-ning_deng.jpg}%
%        \includegraphics[width=0.125\textwidth]{figures/students/lukas_hergt.jpg}%
%
%        \column{0.5\textwidth}
%        \begin{columns}
%            \column{0.5\textwidth}
%            \begin{overpic}[width=\textwidth]{figures/qic.pdf}
%                \put(65,-5) {\tiny \arxiv{1607.04148}}
%            \end{overpic}
%            \vspace{-5pt}
%
%            \begin{overpic}[width=\textwidth]{figures/pgt.pdf}
%                \put(65,-2) {\tiny \arxiv{2003.02690}}
%            \end{overpic}
%            \column{0.5\textwidth}
%            \begin{overpic}[width=\textwidth]{figures/fcb.pdf}
%                \put(65,0) {\tiny \arxiv{2111.14588}}
%            \end{overpic}
%
%            \begin{overpic}[width=\textwidth]{figures/pps_analytic.pdf}
%                \put(65,20) {\tiny \arxiv{1907.08524}}
%            \end{overpic}
%        \end{columns}
%        \vspace{5pt}
%        \begin{overpic}[width=\textwidth]{figures/hergt.pdf}
%            \put(45,0) {\tiny \arxiv{2211.17248}}
%        \end{overpic}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{Interdisciplinary work to date}
    \begin{columns}
        \column{0.35\textwidth}
        \begin{itemize}
            \item CMB cosmology \& inflation
            \item Cosmological tension quantification
            \item \textbf{21cm cosmology}
            \item Radio Instrumentation
            \item \textbf{Gravitational waves}
            \item Exoplanets
            \item \textbf{Particle physics}
            \item Theory of machine learning
            \item Gravitational Gauge theories
            \item Atomistic chemistry
            \item \textbf{Industrial applications}
            \item \ldots
        \end{itemize}
        \column{0.65\textwidth}
        20 minutes is not enough time to cover a decade's publishing.

        \vspace{10pt}

        \includegraphics[width=\textwidth]{figures/publishing.png}

        \hfill\textcolor{C0}{\texttt{\href{https://arxiv.org/a/handley_w_1.html}{arxiv.org/a/handley\_w\_1.html}}} 

        Will showcase a targeted subset.
    \end{columns}
\end{frame}

%\begin{frame}
%    \frametitle{21cm cosmology}
%    \student{harry_bevins}{Harry Bevins}{PhD$\to$KICC fellow}
%
%    \begin{columns}
%        \column{0.6\textwidth}
%        Transferring interdisciplinary ideas into 21cm cosmology with Anastasia Fialkov \& Eloy de Lera Acedo.
%        \begin{itemize}
%            \item \texttt{maxsmooth} \arxiv{2007.14970}
%                \begin{itemize}
%                    \item quadratic programming choice arose from quantitative finance consultancy work
%                \end{itemize}
%            \item \texttt{FlexKnots}
%                \begin{itemize}
%                    \item importing ideas from inflationary reconstruction into reionisation \arxiv{2310.05608}(Heimersheim) \& ionospheric reconstruction \arxiv{2311.14537}(Shen).
%                \end{itemize}
%            \item \texttt{margarine} \arxiv{2205.12841} \arxiv{2207.11457}
%                \begin{itemize}
%                    \item combination of ideas from interdisciplinary fields (emulators, nested sampling, marginal density estimation)
%                \end{itemize}
%        \end{itemize}
%        These techniques are now widely used beyond the Cambridge 21cm community.
%
%        \column{0.4\textwidth}
%        \begin{overpic}[height=0.45\textwidth]{figures/maxsmooth_1.pdf}
%            \put(0,0) {\tiny \arxiv{2007.14970}}
%        \end{overpic}%
%        \includegraphics[height=0.45\textwidth]{figures/maxsmooth_2.pdf}
%        \vspace{5pt}
%
%        \begin{overpic}[width=\textwidth]{figures/margarine_data.pdf}
%            \put(0,0) {\tiny \arxiv{2301.03298}}
%        \end{overpic}
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{REACH: Global 21cm cosmology {\small \arxiv{2210.07409}}}
    \vspace{10pt}
    \begin{columns}
        \column{0.65\textwidth}
        \vspace{-10pt}
        \begin{itemize}
            \item Imaging the universal dark ages using CMB backlight.
            \item $21\text{cm}$ hyperfine line emission from neutral hydrogen.
            \item Global experiments measure monopole across frequency.
            \item Challenge: science hidden in foregrounds $\sim 10^4\times$signal.
            \item Lead data analysis team (REACH first light in January)
            \item Nested sampling woven in from the ground up (calibrator, beam modelling, signal fitting, likelihood selection).
            \item All treated as parameterised model comparison problems.
                \includegraphics[height=0.3\textwidth]{figures/EDGES_antenna.JPG}
                \includegraphics[height=0.3\textwidth]{figures/SARAS.jpg}
                \includegraphics[height=0.3\textwidth]{figures/REACH_2.jpg}
        \end{itemize}
        \column{0.35\textwidth}
        \includegraphics[width=\textwidth]{figures/21cm_1.png}
        \includegraphics[width=\textwidth]{figures/21cm.png}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Gravitational waves}
    \student{metha_prathaban}{Metha Prathaban}{PhD}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Nested sampling has been used in GW since the beginning [\textcolor{C0}{\href{https://arxiv.org/abs/1602.03840}{GW150914}}]
            \item Work with Alvin Chua \& Chris Moore on transdimensional sampling for EMRI~\arxiv{1803.10210}
            \item Recent work with Metha Prathaban showing new chain-based approaches for improving precision \arxiv{2404.16428}

                \includegraphics[width=\textwidth]{figures/gw_chua.pdf}

                \vspace{8pt}
                \includegraphics[width=\textwidth]{figures/gw_prathaban.pdf}
        \end{itemize}
        \column{0.5\textwidth}
        \vspace{4pt}

        \includegraphics[width=0.49\textwidth]{figures/ligo_m1_m2.pdf}%
        \includegraphics[width=0.49\textwidth]{figures/ligo_lambert-skymap.pdf}
        \vspace{5pt}
        \begin{overpic}[height=0.27\textwidth]{figures/product_space.pdf}%
            \put(12,10) {\tiny \arxiv{1803.10210}}
        \end{overpic}%
        \begin{overpic}[height=0.27\textwidth]{figures/gw_chains.pdf}
            \put(65,-4) {\tiny \arxiv{2404.16428}}
        \end{overpic}
        \begin{itemize}
            \item Discussed use of \texttt{margarine}~\arxiv{2207.11457} as alternative to GW hierarchical modelling at inaugural data science discussion group
        \end{itemize}
    \end{columns}

\end{frame}

%\begin{frame}
%    \frametitle{Exoplanets}
%    \student{namu_kroupa}{Namu Kroupa}{PhD}
%
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item Exoplanet science requires solution of subtle inference problems
%                \begin{itemize}
%                    \item Inference from RV data~\arxiv{1806.00518}
%                    \item Survey challenges~\arxiv{2007.07278}
%                    \item Stellar activity~\arxiv{2102.03387} 
%                \end{itemize}
%            \item Gaussian processes+Nested Sampling for transit astronomy~\arxiv{2311.04153}
%            \item Potential for further collaboration with Madhu's group who are seeking beyond \texttt{MultiNest} solutions as their problems scale in dimensionality
%            \item Ongoing cross-disciplinary theoretical chemistry work may be useful in Paul Rimmer's group.
%        \end{itemize}
%
%        \column{0.5\textwidth}
%        \includegraphics[width=\textwidth]{figures/exoplanet_kernel.pdf}%
%        \vspace{10pt}
%        \includegraphics[width=\textwidth]{figures/harps_headline.pdf}%
%        \vspace{10pt}
%        \includegraphics[width=\textwidth]{figures/trappist_headline.pdf}%
%        \vspace{10pt}
%        \includegraphics[width=\textwidth]{figures/rv_headline.pdf}%
%    \end{columns}
%\end{frame}

\begin{frame}
    \frametitle{GAMBIT}
    \framesubtitle{Interdisciplinary case studies}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item GAMBIT is an interdisciplinary community and software framework.
            \item Like \texttt{CosmoMC}/\texttt{Cobaya}/\texttt{Bilby}, an organiser of data, likelihoods \& theory, including:
                \begin{itemize}
                    \item Collider data (e.g. LHC)
                    \item Direct detections (e.g. XENON1T)
                    \item Cosmology (\texttt{MontePython})
                    \item Astrophysics (e.g. Bullet Cluster, Supernovae)
                    \item Pulsar timing
                    \item \ldots \& much more
                \end{itemize}
            \item \texttt{GravBit} and \texttt{LowEnergyBit} arising from GAMBIT@KICC workshop
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[height=0.423\textwidth]{figures/students/gambit.png}
        \includegraphics[height=0.423\textwidth]{figures/gambit_kicc.jpg}
        \includegraphics[width=\textwidth]{figures/gambit_meetingbanner.jpg}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{GAMBIT: sub-GeV Dark matter constraints}
    \framesubtitle{Interdisciplinary case studies}
    \student{gambit}{Felix Kahlhoefer et al}{GAMBIT cosmo/DM working group}
    \begin{columns}
        \column{0.56\textwidth}
        \begin{itemize}
            \item Physical model of sub-GeV thermal dark matter with a dark photon mediator~$A$:
        \end{itemize}
        \vspace{-10pt}
        \begin{align*}
            \small
            \mathcal{L}_\text{int} =& -\frac{1}{2} m_{A'}^2 A'^\mu A'_\mu - \frac{1}{4} A'^{\mu\nu}A'_{\mu\nu} -\kappa e A'^\mu \sum_{f} q_f \overline{f} \gamma_\mu f \,,
            \normalsize
        \end{align*}
        \vspace{-15pt}
        \begin{itemize}
            \item Constrain using cosmological, astrophysical, accelerator \& direct detection data.
            \item Bayesian Model comparison of Fermion~$\psi$ vs scalar~$\Phi$ models (scalar preferred).
        \end{itemize}
        \vspace{-10pt}
        \begin{align*}
            \small
            \mathcal{L}_\psi  =& \bar{\psi}(i \slashed{\partial} - m_\text{DM}) \psi + g_\text{DM} A'^\mu \bar{\psi} \gamma_\mu \psi \,,\\
            \mathcal{L}_\Phi  =& |\partial_\mu \Phi|^2 - m_\text{DM}^2 |\Phi|^2 - g_\text{DM}^2 A'_\mu A'^\mu |\Phi|^2 \\ &+ i g_\text{DM} A'^\mu \left[\Phi^\ast (\partial_\mu \Phi) - (\partial_\mu \Phi^\ast) \Phi\right]\,,
            \normalsize
        \end{align*}
        \column{0.44\textwidth}
        \vspace{10pt}
        \includegraphics[width=\textwidth]{figures/Bayes_SubGeVDM_fermion_RDprior_allDM_asym_observables.pdf}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{PolyChord Ltd: interdisciplinary R\&D spinout}
    \framesubtitle{Interdisciplinary case studies}
    %\only<1-4>{
        \student{watkinson-headshot}{Catherine Watkinson}{Senior Data Scientist}
    %}
    %\only<5->{
    %    \student{mcaloone-headshot}{Thomas Mcaloone}{PhD $\to$ Data Scientist}
    %}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item Techniques have been spun-out (PolyChord Ltd) to:
            \item Protein folding
                \begin{itemize}
                    \item Navigating free energy surface.
                    \item Computing misfolds.
                    \item Thermal motion.
                \end{itemize}
            \item Nuclear fusion reactor optimisation
                \begin{itemize}
                    \item multi-objective.
                    \item uncertainty propagation.
                \end{itemize}
            \item Telecoms \& DSTL research (MIDAS)
                \begin{itemize}
                    \item Optimising placement of transmitters/sensors.
                    \item Maximum information data acquisition strategies.
                \end{itemize}
        \end{itemize}
        \includegraphics[width=0.08\textwidth]{figures/headshots/catherine-watkinson-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/thomas-macaloone-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/angus-peters-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/tamas-stenzel-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/david-yallup-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/rebecca-handley-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/adam-ormondroyd-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/mike-hobson-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/anthony-lasenby-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/mike-handley-polychord.jpg}%
        \includegraphics[width=0.08\textwidth]{figures/headshots/will-handley-polychord.jpg}%
        \column{0.4\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/protein_1.png}%
        \includegraphics<2          >[width=\textwidth]{figures/protein_2.png}%
        \includegraphics<3|handout:0>[width=\textwidth]{figures/protein_3.png}%
        %\includegraphics<4|handout:0>[width=\textwidth]{figures/lcoe.png}%
        %%\includegraphics<5|handout:0>[width=\textwidth]{figures/tdoa-cropped-1-crop.pdf}%
        %%\includegraphics<6|handout:0>[width=\textwidth]{figures/tdoa-cropped-2-crop.pdf}%
        %%\includegraphics<7|handout:0>[width=\textwidth]{figures/tdoa-cropped-3-crop.pdf}%
        %\includegraphics<5|handout:0>[width=\textwidth]{figures/DKL_contour-cropped-crop.pdf}%
        %\includegraphics<6|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-3-crop.pdf}%
        %\includegraphics<7|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-4-crop.pdf}%
        %\includegraphics<8|handout:0>[width=\textwidth]{figures/mean_DKL_optimise-5-crop.pdf}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{DSTL: Bayesian OODA loops}
    \framesubtitle{Interdisciplinary case studies}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Work through Isaac Newton Institute with Defence Science \& Technology Laboratory.
            \item Quantification of ``OODA'' loop concept from litigation, business, law enforcement, management and military strategy
        \end{itemize}


        \begin{columns}
            \column{0.5\textwidth}
            \includegraphics[width=\textwidth]{figures/midas.png}%
            \column{0.5\textwidth}
            \begin{itemize}
                \item Two-way research interaction between government and academia.
                \item techniques now being used in REACH antenna design~\arxiv{2309.06942}
            \end{itemize}
        \end{columns}


        \column{0.5\textwidth}
        \begin{minipage}[]{0.6\textwidth}
            \begin{tikzpicture}[
                    node distance=5mm,
                    >=stealth, auto,
                    every state/.style={
                        rectangle, rounded corners, minimum height=6em, minimum width=2em,
                        text width=3cm, align=center
                    }
                ]


                \node[state, fill=C3!20] (q14) {{\textbf{Act}}
                    \\ 
                    \small
                    Execute strategy $\bm{s}$\\
                    Replace prior $\pi$ with posterior $\mathcal{P}$
                };
                \node[state, fill=C0!20] (q12) [above=of q14] { {\textbf{Observe}} 
                    \\
                    \small
                    Collect data $\bm{D}$ about system $\bm{\theta}$ using strategy $\bm{s}$.
                };
                \node[state, fill=C1!20] (q23) [right=of q12] { {\textbf{Orient}} 
                    \\
                    \small
                    Update knowledge from prior~$\pi(\bm{\theta})$ to posterior~$\mathcal{P}(\bm{\theta}|\bm{D})$.
                };
                \node[state, fill=C2!20] (q34) [below=of q23] { {\textbf{Decide}} 
                    \\
                    \small
                    choose strategy $\bm{s}$ using information $\hat{\mathcal{D}}_\mathrm{KL}(\bm{s}|\mathcal{P})$.
                };
                \begin{scope}[bend left]%
                    \path[thick,-{Latex[width=2mm]}]   (q14.north) edge node {} (q12.south)
                    (q12.east) edge node {} (q23.west)
                    (q23.south) edge node {} (q34.north)
                    (q34.west) edge node {} (q14.east);
                \end{scope}

                \node[align=center] (e) at (barycentric cs:q12=1,q23=1,q34=1,q14=1) {\Large\textbf{MIDAS}};

            \end{tikzpicture}
        \end{minipage}%
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Beginning the golden age of astronomy data}
    \begin{columns}
        \column{0.47\textwidth}
        \begin{itemize}
            \item Over our research lifetimes we will see next-generation data rates across the electromagnetic spectrum \& beyond:
                \begin{description}
                    \item[Radio] SKA \textit{et al}
                    \item[Micro] SO/CMB-S4/LiteBIRD
                    \item[IR] JWST, Roman
                    \item[Optical] Euclid, DESI, Rubin, EELT
                    \item[X-ray] Athena
                    \item[Gamma-ray] e-ASTROGAM
                    \item[Gravitational] LIGO/LVK$^+$/LISA
                    \item[Particle] CTA, IceCube, KM3NeT
                \end{description}
        \end{itemize}
        \column{0.55\textwidth}

        \includegraphics[height=0.145\textwidth]{figures/telescopes/jwst.png}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/roman.jpg}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/euclid.jpeg}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/athena.jpg}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/lisa.jpg}%
        \includegraphics[height=0.145\textwidth]{figures/telescopes/e-ASTROGAM.pdf}%
        \vspace{-1pt}

        \includegraphics[height=0.15183\textwidth]{figures/telescopes/desi.jpg}%
        \includegraphics[height=0.15183\textwidth]{figures/telescopes/eelt.jpg}%
        \includegraphics[height=0.15183\textwidth]{figures/telescopes/ska.jpg}%
        \includegraphics[height=0.15183\textwidth]{figures/telescopes/SO.jpg}%
        \vspace{-1pt}

        \includegraphics[height=0.18428\textwidth]{figures/telescopes/ligo.jpg}%
        \includegraphics[height=0.18428\textwidth]{figures/telescopes/km3n.jpg}%
        \includegraphics[height=0.18428\textwidth]{figures/telescopes/icecube.jpg}%
        \includegraphics[height=0.18428\textwidth]{figures/telescopes/CTA.jpg}%

        \begin{itemize}
            \item This ever-increasing statistical weight will mean true accuracy demands rigorous attention to systematics.
            \item This applies to all of cosmology, astrophysics, particle physics and beyond.
        \end{itemize}

    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Tensions in cosmology}
    \begin{columns}
        \column{0.38\textwidth}
        \begin{block}{Hubble}
            \begin{overpic}[width=\textwidth]{figures/hubble_tension.pdf}
                \put(72,37) {\tiny \arxiv{1907.05922}}
            \end{overpic}
        \end{block}
        \column{0.17\textwidth}
        \begin{block}{Weak lensing}
            \begin{overpic}[width=\textwidth]{figures/kids_tension.pdf}
                \put(13,18) {\tiny \arxiv{2007.15632}}
            \end{overpic}%
        \end{block}
        \column{0.36\textwidth}
        \begin{block}{\hfill other $w_0$/$\Omega_K$/$\nu$?}
            \begin{overpic}[width=0.48\textwidth]{figures/desi_tension.pdf}
                \put(0,90) {\tiny \arxiv{2404.03002}}
            \end{overpic}%
            \begin{overpic}[width=0.48\textwidth]{figures/curvature_3.pdf}
                \put(54,20) {\tiny \arxiv{1908.09139}}
            \end{overpic}%
        \end{block}
    \end{columns}

    \begin{columns}
        \column{0.75\textwidth}
        \begin{itemize}
            \item Tensions have been appearing in cosmology over the last five years.
            \item Though their significance may be debatable, they have revealed a gap in the armour of modern data analysis.
            \item Likelihood-based methods have to assume a fiducial cosmology. 
            \item This will cast greater doubt on the robustness of our conclusions from stage 4 surveys and beyond.
        \end{itemize}
        \column{0.25\textwidth}
        \begin{block}{\tiny DESI BAO paper: \arxiv{2404.03000}}
            \includegraphics[width=\textwidth]{figures/desi_fiducial.pdf}
        \end{block}
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{The real tensions in the room}


    \begin{columns}
        \column{0.34\textwidth}
        \begin{block}{Dark tension}
            \includegraphics[width=\textwidth]{figures/pie_chart.jpg}
        \end{block}
        \column{0.26\textwidth}
        \begin{block}{Initial conditions}
            \includegraphics[width=\textwidth]{figures/multiverse.jpg}
        \end{block}
        \column{0.32\textwidth}
        \begin{block}{Quantum gravity}
            \includegraphics[width=\textwidth]{figures/quantum_gravity.jpeg}
        \end{block}
    \end{columns}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item These are existential questions that don't go away if $\Lambda$CDM is phenomenologically correct\ldots
            \item \ldots and if it is, we will need an interdisciplinary approach that goes far beyond cosmology (using astronomy, collider \& experimental physics).
        \end{itemize}
        \column{0.4\textwidth}
        \begin{itemize}
            \item The GW redshift frontier, time domain astronomy \& 21cm SKA will be the key tools for challenging our astronomical understanding.
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{The future: simulation-based inference}
    \student{kilian_scheutwinkel}{Kilian Scheutwinkel}{PhD}
    \vspace{5pt}
    \begin{itemize}
        \item Traditional Likelihood-based inference (LBI) requires knowledge of the likelihood $P(D|\theta)$
            \begin{itemize}
                \item For the CMB, it is possible to compute Probability(Sky$|\Lambda$CDM) [with caveats]
                \item For almost everybody else (different models, more evolved systems), the likelihood is approximate
            \end{itemize}
        \item Simulation-based inference learns the likelihood from physical simulations $\theta\to D$.
    \end{itemize}
    \begin{columns}
        \column{0.65\textwidth}
        \begin{itemize}
            \item Can extract nonlinear information from data.
            \item Do not need to assume a fiducial model, since data covariance is implicit in simulations.
            \item Users do not need to know advanced statistics
                \begin{itemize}
                    \item For this reason alone, it will come to dominate the next generation $\Rightarrow$ imperative to build a principled understanding!
                \end{itemize}
            %\item The current state of the art is achieved with \textit{Neural}~estimation
            \item My interest has been in understanding how essential machine learning is by building analytics \tthref{github.com/handley-lab/lsbi}
            \item Have also made progress on the first simulation-based nested sampler \texttt{PolySwyft} (presenting at EuCAIFCon)
        \end{itemize}
        \column{0.35\textwidth}
        \begin{overpic}[width=\textwidth]{figures/des_sbi.pdf}
            \put(65,80) {\tiny \arxiv{2403.02314}}
            \put(65,73) {\tiny (Jeffrey et DES)}
        \end{overpic}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{ERC grant: COSMOTENSION \hspace{95pt}\small \tthref{willhandley.co.uk/ERC.pdf}}
    \framesubtitle{Resolving cosmological tensions with diverse data, novel theories and Bayesian machine learning}

    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item ERC starting grant $\Rightarrow$ UKRI Frontier, commencing October 2024.
            \item Funds 3 PDRAs and 4 PhDs over 5 years.
            \item Research programme centered around combining novel theories of gravity, Boltzmann solvers~\arxiv{1906.01421}, reconstruction~\arxiv{1908.00906}, nested sampling \& simulation-based inference (n\'{e}e LFI).
            \item Aims to disentangle cosmological tensions $H_0$, $\sigma_8$, $\Omega_K$ with next-generation data analysis techniques.
        \end{itemize}
        \column{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/denseoutput.pdf}
        \includegraphics[width=\textwidth]{figures/pps.pdf}
        \includegraphics[width=\textwidth]{figures/gantt.pdf}
    \end{columns}

\end{frame}

\begin{frame}
    \frametitle{Conclusions}
    \framesubtitle{\tthref{github.com/handley-lab}}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{itemize}
            \item The astronomy challenges of our generation are interdisciplinary.
            \item Interdisciplinary approaches are two-way streets for innovation.
            \item Simulation-based inference represents the frontier of how we will all be doing data analysis.
            \item Our research programme at the interface between theory, observation and inference is well-placed to explore the interdisciplinary frontier.
        \end{itemize}

        \column{0.4\textwidth}
        \includegraphics[height=0.32\textwidth]{logos/ukri}\hfill
        \includegraphics[height=0.32\textwidth]{logos/ukerc}\hfill
        \includegraphics[height=0.32\textwidth]{logos/royalsociety}

        \centerline{
            \includegraphics[width=0.8\textwidth]{logos/dirac.png}%
        }
    \end{columns}
    \vspace{10pt}


    \tikz[overlay,remember picture]
    \node[anchor=north east] (A) at ($(current page.north east)+(0,0)$) {
        \includegraphics[width=0.09\textheight]{figures/students/adam_ormondroyd.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/david_yallup.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/dily_ong.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/felicity_ibrahim.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/george_carter.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/harry_bevins.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/ian_roque.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/kilian_scheutwinkel.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/metha_prathaban.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/namu_kroupa.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/sinah_legner.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/thomas_gessey-jones.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/tze_goh.jpg}%
        \includegraphics[width=0.09\textheight]{figures/students/wei-ning_deng.jpg}%
    };
\end{frame}


%\begin{frame}
%    \frametitle{unimpeded: PLA for the next generation}
%    \student{dily_ong}{Dily Ong}{PhD}
%    \begin{columns}
%        \column{0.5\textwidth}
%        \begin{itemize}
%            \item DiRAC 2020 RAC allocation of 30MCPUh
%            \item Main goal: Planck Legacy Archive equivalent
%            \item Parameter estimation $\to$ Model comparison
%            \item MCMC $\to$ Nested sampling
%            \item Planck $\to$ $\{\text{Planck}, \text{DESY1}, \text{BAO}, \ldots \}$
%            \item Pairwise combinations
%            \item Suite of tools for processing these 
%                \begin{itemize}
%                    \item \texttt{anesthetic} $2.0$
%                    \item \texttt{unimpeded} $1.0$
%                    \item \texttt{zenodo} archive
%                    \item \texttt{margarine}
%                \end{itemize}
%            \item MCMC chains also available.
%            \item Library of bijectors emulators for fast re-use
%        \end{itemize}
%        \column{0.5\textwidth}
%        \includegraphics[width=\textwidth]{logos/dirac.png}
%        \includegraphics[width=0.5\textwidth,page=21]{figures/himmelblau.pdf}%
%        \includegraphics[width=0.5\textwidth,page=15]{figures/himmelblau.pdf}
%    \end{columns}
%\end{frame}




%\begin{frame}
%    \frametitle{Other frontiers}
%    Copilot
%
%    ChatGPT
%
%    LLMs/translation between disciplines
%    
%\end{frame}

\end{document}
```

Please draft a latex talk using a relevant collection of the above beamer slides

I will need material on:
- Bayesian inference:
  - Bayes theorem
  - Parameter space and data space
  - the three pillars of Bayesian inference
    - Parameter estimation
    - model comparison
    - tension quantification
- Markov chain monte carlo
- Nested sampling

- simulation based inference
  - Neural ratio estimation
  - Neural density estimation
  - the scalability of both
- nuisance-marginalised likelihoods
  - margarine
  - normalising flows

Please also insert two blank slides for recent work on:
- jax-based nested samplers
- accerelated nested sampling with beta flows

The slides should be in the same format, with the same text where relevant. Please mark the git branch (denoted in the markdown title) as a latex comment, as I will be using the git branches to retrieve the relevant figures. For example if it is appropriate to use a slide from the section with `# branch: remotes/origin/ras_sbi_2024`, then the output should be:

```
% remotes/origin/ras_sbi_2024
\begin{frame}
    \frametitle{Simulation-based inference \& model comparison}
    \begin{columns}
        \column{0.3\textwidth}
        \begin{itemize}
            \item Extend: models $A$ and $B$.
            \item Each with own separate parameters $\theta_A$ and $\theta_B$ (can be same).
            \item The evidence $\mathcal{Z}(D|M)$ compares models
            \item Occams razor: more~predictive $\equiv$~more~probable \\(due to normalisation).
        \end{itemize}
        
        \column{0.7\textwidth}
        \includegraphics<1>[page=1, width=\textwidth]{figures/sbi_model_comparison.pdf}%
        \includegraphics<2>[page=2, width=\textwidth]{figures/sbi_model_comparison.pdf}%
        \includegraphics<3>[page=3, width=\textwidth]{figures/sbi_model_comparison.pdf}%
    \end{columns}
\end{frame}
```

I'll need 45 minutes worth of slides, so 10-20 frames should do.

Response:

```tex

\documentclass[aspectratio=169]{beamer}
\usepackage{will_handley_beamer}
\usepackage{title_page}
\usepackage{slashed}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{fit}
\usepackage[percent]{overpic}

% Commands
% --------
% - \arxiv{arxiv number}
% - \arxiv{<number>}            arxiv.org/abs/<number>
% - \oldarxiv{<arxiv number>}   arxiv.org/<number>
% - \doi{<doi>}                 doi.org/<doi>
% - \xkcd{<number>}             xkcd.com/<number>
% - \email{<email>}             <<email>>
% - \tthref{<website>}          <website>
% - \av[dist]{<quantity>}       <quantity>_{dist}

% Talk details
% ------------
\title{Nested sampling \& simulation-based inference}
\subtitle{Next-generation statistical inference tools}
\date{13\textsuperscript{th} November 2024}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

% remotes/origin/ras_sbi_2024
\begin{frame}
    \frametitle{Bayesian inference}
    \begin{itemize}
        \item A ``generative'' model $M$, with tunable parameters $\theta$, describing (compressed) data $D$.
            \begin{itemize}
                \item e.g. $M=\Lambda$CDM, $\theta=\{\Omega_b,\Omega_c, \tau, H_0, A_s, n_s\}$, $D=\{C_\ell\}$.
            \end{itemize}
        \item Described by simulation process $\theta\to D$, or likelihood $P(D|\theta,M)$.
        \item Frequentists \& Bayesians agree on the likelihood.
        \item Bayesians treat parameter space $\theta$ the same as data space $D$.
        \item Quantifying uncertainty with probability using Bayes theorem:
        \[
            \C[0]{P}(\theta|D,M) = \frac{\C[2]{P}(D|\theta,M)\C[1]{P}(\theta|M)}{\C[3]{P}(D|M)}, \quad \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}}\times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}.
        \]
    \end{itemize}
\end{frame}

% remotes/origin/ras_sbi_2024
\begin{frame}
    \frametitle{The three pillars of Bayesian inference}
    \begin{description}
        \item[Parameter estimation] Given a model, which range of parameters best describe the data?
            \[ \C[0]{P(\theta|D,M)} = \frac{\C[2]{P(D|\theta,M)} \C[1]{P(\theta|M)}}{\C[3]{P(D|M)}}, \] 
            \[ \C[0]{\mathcal{P}} = \frac{\C[2]{\mathcal{L}} \times\C[1]{\pi}}{\C[3]{\mathcal{Z}}}, \] 
            \[ \C[0]{\text{Posterior}} = \frac{\C[2]{\text{Likelihood}} \times\C[1]{\text{Prior}}}{\C[3]{\text{Evidence}}}. \]
        \item[Model comparison] Which models do the data prefer?
            \[ \C[4]{P(M|D)} = \frac{\C[3]{P(D|M)} P(M)}{P(D)}, \] \[ \frac{\C[3]{\mathcal{Z}_\mathcal{M}} \Pi_\mathcal{M}}{\sum_m Z_m \Pi_m}, \] \[ \C[4]{\text{Posterior}} = \frac{\C[3]{\text{Evidence}} \times\text{Prior}}{\text{Normalisation}}.\]
        \item[Tension quantification] Are different (sub)sets of data consistent with one another?
            \[ \mathcal{R} = \frac{\C[3]{\mathcal{Z}}_{AB}}{\C[3]{\mathcal{Z}}_A\C[3]{\mathcal{Z}}_\mathcal{B}}, \] 
            \[
                \begin{aligned} \log\mathcal{S} = \av[{\C[0]{\mathcal{P}}_{AB}}]{\C[2]{\log\mathcal{L}}_{AB}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{A}}]{\C[2]{\log\mathcal{L}}_{A}}&\\
                    -\av[{\C[0]{\mathcal{P}}_{B}}]{\C[2]{\log\mathcal{L}}_{B}}&
                \end{aligned}
            \]
    \end{description}
\end{frame}

% remotes/origin/tools_2021
\begin{frame}
    \frametitle{Metropolis Hastings (baseline sampling algorithm)} 
    \begin{itemize}
        \item Turn the $N$-dimensional problem into a one-dimensional one.
        \item Pick start point $\theta_0$.
        \item At step $i$:
            \begin{enumerate}
                \item Propose a new point $\theta_{i+1}$ a small step away from $\theta_{i}$
                \item If uphill $\mathcal{P}(\theta_{i+1}) > \mathcal{P}(\theta_i)$, make step\ldots
                \item \ldots otherwise make step with probability $\alpha = \mathcal{P}(\theta_{i+1}) / \mathcal{P}(\theta_i)$. 
            \end{enumerate}
        \item Requires a proposal distribution $\mathcal{Q}(\theta_{i+1}|\theta_i)$
        \item In general case where $\mathcal{Q}$ is not symmetric, need acceptance ratio:
            \begin{equation*}
                \alpha = \frac{\mathcal{P}(\theta_{i+1})\mathcal{Q}(\theta_{i}|\theta_{i+1})}{\mathcal{P}(\theta_{i})\mathcal{Q}(\theta_{i+1}|\theta_{i})}
            \end{equation*}
    \end{itemize}
\end{frame}

% remotes/origin/tools_2021
\begin{frame}
    \frametitle{Nested sampling (high dimensional integration)}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Start with $n$ random samples over the space.
            \item Delete outermost sample, and replace with a new random one at higher integrand value.
            \item The ``live points'' steadily contract around the peak(s) of the function.
            \item We can use this evolution to estimate volume \emph{probabilistically}.
            \item At each iteration, the contours contract by $\sim\frac{1}{n}\only<9->{\pm \frac{1}{n}}$ of their volume.
            \item This is an exponential contraction, so
                \[  \sum_i f(x_i) \Delta V_i, \qquad V_i = V_0 e^{-\only<9->{(}i\only<9->{\pm\sqrt{i})}/n} \]
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1|handout:0>[width=\textwidth,page=1]{figures/himmelblau}%
        \includegraphics<2|handout:0>[width=\textwidth,page=2]{figures/himmelblau}%
        \includegraphics<3|handout:0>[width=\textwidth,page=3]{figures/himmelblau}%
        \includegraphics<4          >[width=\textwidth,page=4]{figures/himmelblau}%
        \includegraphics<5|handout:0>[width=\textwidth,page=5]{figures/himmelblau}%
        \includegraphics<6|handout:0>[width=\textwidth,page=6]{figures/himmelblau}%
        \includegraphics<7|handout:0>[width=\textwidth,page=7]{figures/himmelblau}%
        \includegraphics<8-|handout:0>[width=\textwidth,page=8]{figures/himmelblau}%
    \end{columns}
\end{frame}

% remotes/origin/tools_2021
\begin{frame}
    \frametitle{The nested sampling meta-algorithm}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item At the end, one is left with a set of discarded ``dead'' points.
            \item Nested sampling estimates the \textbf{density of states} and calculates partition functions
                \[Z(\beta) = \sum_i f(x_i)^\beta \Delta V_i\]
            \item The evolving ensemble of live points allows:
                \begin{itemize}
                    \item implementations to self-tune
                    \item exploration of multimodal functions
                    \item global and local optimisation
                \end{itemize}
            \item For this kind of numerical, generic, high-dimensional integration, it is the only game in town.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics[width=\textwidth,page=14]{figures/himmelblau}%
    \end{columns}
\end{frame}


% remotes/origin/ras_sbi_2024
\begin{frame}
    \frametitle{Simulation-based inference}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item Only have access to a forward model $\theta \rightarrow D$.
            \item $(\theta,D)$ plane gives a more expansive theoretical view of inference.
            \item Forward model defines \emph{implicit} likelihood~$\mathcal{L}$:
            \item Simulator generates samples from $\mathcal{L}(D|\theta)$.
            \item With a prior $\pi(\theta)$ can generate samples from joint distribution~$\mathcal{J}(\theta,D)=\mathcal{L}(D|\theta)\pi(\theta)$\\\hfill \emph{the ``probability of everything''}.
            \item Task of SBI is then to go from joint~$\mathcal{J}$ to posterior $\mathcal{P}(\theta|D)$ and evidence $\mathcal{Z}(D)$ -- and possibly likelihood $\mathcal{L}(D|\theta)$.
            \item SBI \& forward modelling force us to think about data space~$D$ \& parameter space~$\theta$.
        \end{itemize}
        \column{0.5\textwidth}
        \includegraphics<1->[page=21, width=\textwidth]{figures/sbi_parameter_estimation.pdf}%
    \end{columns}
\end{frame}


% remotes/origin/ras_sbi_2024
\begin{frame}
    \frametitle{Neural Ratio Estimation}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{itemize}
            \item SBI flavours:
                {\small
                    \begin{description}
                        \item[NPE] Neural posterior estimation
                        \item[NLE] Neural likelihood estimation
                        \item[NJE] Neural joint estimation
                        \item[NRE] Neural ratio estimation
                    \end{description}
                }
            \item NRE recap:
                \begin{enumerate}
                    \item Generate joint samples $(\theta,D)\sim\mathcal{J}$
                        \begin{itemize}
                            \item \textit{straightforward if you have a simulator:\\ $\theta\sim\pi(\cdot)$, $D\sim\mathcal{L}(\cdot|\theta)$}
                        \end{itemize}
                    \item Generate separated samples $\theta\sim\pi$, $D\sim\mathcal{Z}$
                        \begin{itemize}
                            \item \textit{aside: can shortcut by scrambling the $(\theta,D)$ pairings}
                        \end{itemize}
                    \item Train probabilistic classifier $p$ to distinguish whether $(\theta,D)$ came from $\mathcal{J}$ or $\pi\times\mathcal{Z}$.
                    \item $\frac{p}{1-p} = r = \frac{P(\theta,D)}{P(\theta)P(D)} 
                        =
                        \frac{\mathcal{J}}{\pi\times\mathcal{Z}} = \frac{\mathcal{L}}{\mathcal{Z}} = \frac{\mathcal{P}}{\pi}$.
                    \item Use ratio $r$ for parameter estimation $\mathcal{P} = r\times\pi$
                \end{enumerate}
        \end{itemize}
        \column{0.5\textwidth}
        \only<1|handout:0>{
            \begin{tikzpicture}[node distance=1cm, every neuron/.style={circle, draw, minimum size=1cm},]
                \node[every neuron/.try] (j2)  {};
                \node[every neuron/.try, above left = 0cm and 0.5cm of j2] (theta) { $\theta$};
                \node[every neuron/.try, below left = 0cm and 0.5cm of j2] (D) { $D$};
                \node[every neuron/.try, above = 0.5cm of j2] (j1) {};
                \node[every neuron/.try, below = 0.5cm of j2] (j3) {};
                \node[every neuron/.try, above right = 0cm and 0.5cm of j2] (h1) {};
                \node[every neuron/.try, below right = 0cm and 0.5cm of j2] (h2) {};
                \node[every neuron/.try, right = 1.3cm of j2] (p) { $p$};
                \node[every neuron/.try, right = 0.5cm of p] (logr) { $r$};
                \draw[-] (theta) -- (j1);
                \draw[-] (D) -- (j1);
                \draw[-] (theta) -- (j2);
                \draw[-] (D) -- (j2);
                \draw[-] (theta) -- (j3);
                \draw[-] (D) -- (j3);
                \draw[-] (j1) -- (h1);
                \draw[-] (j1) -- (h2);
                \draw[-] (j2) -- (h1);
                \draw[-] (j2) -- (h2);
                \draw[-] (j3) -- (h1);
                \draw[-] (j3) -- (h2);
                \draw[-] (h1) -- (p);
                \draw[-] (h2) -- (p);
                \draw[-] (p) -- (logr);
                \node[below =0.5cm of logr] {\includegraphics[page=22, width=0.5\textwidth]{figures/sbi_parameter_estimation.pdf}};
            \end{tikzpicture}
        }
    \end{columns}
\end{frame}

% remotes/origin/ras_sbi_2024
\begin{frame}
    \frametitle{TMNRE: Truncated Marginal Neural Ratio Estimation}
    \framesubtitle{\texttt{swyft}: \tthref{github.com/undark-lab/swyft}}
    \begin{columns}
        \column{0.55\textwidth}
        \begin{itemize}
            \item Two tricks for practical NRE:
        \end{itemize}
        \begin{block}{Marginalisation}
            \begin{itemize}
                \item Only consider one or two parameters at a time.
                \item Fine if your goal is to produce triangle plots.
                \item Problematic if information is contained jointly in more than two parameters.
            \end{itemize}
        \end{block}
        \begin{block}{Truncation}
            \begin{itemize}
                \item focus parameters $\theta$ on a subset of the prior which reproduces observed data $D_\text{obs}$
                \item region is somewhat arbitrary (usually a box)
                \item not amortised, sounds a bit like ABC
            \end{itemize}
        \end{block}
        \column{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/tmnre}
    \end{columns}
\end{frame}

% remotes/origin/lcdm_2023
\begin{frame}
    \frametitle{Nuisance marginalised likelihoods: Theory \arxiv{2207.11457}}
    \begin{columns}[t]
        \column{0.5\textwidth}
        \begin{itemize}
            \item Bayes theorem
                \begin{align}
                    \C[2]{\mathcal{L}}(\theta,\alpha) 
                    \times 
                    \C[1]{\pi}(\theta,\alpha) &= 
                    \C[0]{\mathcal{P}}(\theta,\alpha)
                    \times
                    \C[3]{\mathcal{Z}}\\
                    \C[2]{\text{Likelihood}}
                    \times
                    \C[1]{\text{Prior}}
                    &=
                    \C[0]{\text{Posterior}}
                    \times
                    \C[3]{\text{Evidence}}
                    \nonumber
                \end{align}
                \small{$\alpha$: nuisance parameters, $\theta$: cosmo parameters.}
            \item Marginal Bayes theorem
                \begin{equation}
                    \C[2]{\mathcal{L}}(\theta) 
                    \times 
                    \C[1]{\pi}(\theta) = 
                    \C[0]{\mathcal{P}}(\theta)
                    \times
                    \C[3]{\mathcal{Z}}
                \end{equation}
            \item Non-trivially gives \textbf{nuisance-free likelihood}
                \begin{equation}
                    \boxed{
                        \C[2]{\mathcal{L}}(\theta) 
                        = 
                        \frac{
                            \C[0]{\mathcal{P}}(\theta)
                            \C[3]{\mathcal{Z}}
                        }{
                            \C[1]{\pi}(\theta)
                        }
                    }
                    =
                    \frac{
                        \int \C[2]{\mathcal{L}}(\theta,\alpha) \C[1]{\pi}(\theta,\alpha) d{\alpha}
                    }
                    {
                        \int \C[1]{\pi}(\theta,\alpha) d{\alpha}
                    }
                \end{equation}
        \end{itemize}
        \column{0.5\textwidth}
        \textbf{Key properties}
        \begin{itemize}
            \item Given datasets $A$ and $B$, each with own nuisance parameters $\alpha_A$ and $\alpha_B$:
            \item If you use $\mathcal{L}_A(\theta)$, you get the same (marginal) posterior and evidence if you had run with nuisance parameters $\alpha_A$ (ditto $B$).
            \item If you run inference on $\mathcal{L}_A(\theta)\times\mathcal{L}_B(\theta)$, you get the same (marginal) posterior and evidence if you had run with all nuisance parameters $\alpha_A$, $\alpha_B$ on.
            \item[] \textit{(weak marginal consistency requirements on joint $\pi(\theta,\alpha_A,\alpha_B)$ and marginal priors)}
        \end{itemize}
    \end{columns}
\end{frame}

% remotes/origin/lcdm_2023
\begin{frame}
    \frametitle{Nuisance marginalised likelihoods: Practice~{\small\arxiv{2205.12841}}}
    \begin{columns}
        \column{0.6\textwidth}
        \begin{columns}
            \column{0.3\textwidth}
            \[
                \boxed{
                    \C[2]{\mathcal{L}}(\theta) 
                    = 
                    \frac{
                        \C[0]{\mathcal{P}}(\theta)
                        \C[3]{\mathcal{Z}}
                    }{
                        \C[1]{\pi}(\theta)
                    }
                }
            \]
            \column{0.7\textwidth}
            \begin{itemize}
                \item To compute the nuisance marginalised likelihood, need:
                    \begin{enumerate}
                        \item Bayesian evidence $\C[3]{\mathcal{Z}}$
                        \item Marginal prior and posterior \textbf{densities}
                    \end{enumerate}
            \end{itemize}
        \end{columns}
        \begin{enumerate}
            \item Use nested sampling to compute evidence $\C[3]{\mathcal{Z}}$ and marginal samples $\{\theta,\alpha\}_\mathcal{P}$ and $\{\theta,\alpha\}_\pi$.
            \item Use normalising flows to compute density estimators $\C[0]{\mathcal{P}(\theta)}$, $\C[1]{\pi(\theta)}$ from marginal samples.
        \end{enumerate}
        \begin{itemize}
            \item Emulators usually much faster than original likelihoods
            \item \texttt{margarine}: PyPI, \href{https://github.com/htjb/margarine}{github.com/htjb/margarine}
        \end{itemize}

        \column{0.4\textwidth}
        \begin{tikzpicture}[
                rednode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
                bluenode/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
                greennode/.style={rectangle, draw=green!60, very thick, minimum size=5mm},
                node distance=0.5cm,
                remember picture, overlay
            ]
            \node<1->[bluenode, xshift=0.5\textwidth, yshift=-0.25\textwidth](likelihood) at (current page.north)  {$ \mathcal{L}(\theta,\alpha)$};
            \node<1->[bluenode, right = of likelihood.east](prior) {$ \pi(\theta,\alpha)$};

            \coordinate<1-> (likelihoodprior) at ($(likelihood.south)!0.5!(prior.south)$);

            \node<2->[rednode, below = of likelihoodprior](nestedsampling) {Nested Sampling};
            \draw<2->[->](likelihood.south) -- (likelihood|-nestedsampling.north);
            \draw<2->[->](prior.south) -- (prior|-nestedsampling.north);

            \node<3->[bluenode, below = of nestedsampling](posterior) {$ \{\theta,\alpha\}_\mathcal{P}$};
            \draw<3->[->](nestedsampling.south-|posterior) -- (posterior.north);
            \node<4->[bluenode, left = of posterior.west](evidence) {$ \mathcal{Z}$};
            \draw<4->[->](nestedsampling.south-|likelihood) -- (evidence.north);
            \node<5->[bluenode, right = of posterior.east](priorSamples) {$ \{\theta,\alpha\}_\pi$};
            \draw<5->[->](nestedsampling.south-|prior) -- (priorSamples.north);

            \coordinate<5-> (posteriorprior) at ($(posterior.south)!0.5!(priorSamples.south)$);

            \node<6->[rednode, below = of posteriorprior](margarine)  {Density Estimation};

            \draw<6->[->](posterior.south) -- (margarine.north-|posterior.east);
            \draw<6->[->](priorSamples.south) -- (margarine.north-|priorSamples.west);

            \node<7->[bluenode, below = of posterior|-margarine.south](marginalPosterior) {$ \mathcal{P}(\theta)$};


            \draw<7->[->](margarine.south-|marginalPosterior.east) -- (marginalPosterior.north);


            \node<8->[bluenode, below = of marginalPosterior.south-|margarine.south-|priorSamples](marginalPrior) {$ \pi(\theta)$};
            \draw<8->[->](margarine.south-|priorSamples.west) -- (marginalPrior.north);


            \node<9->[bluenode, below = of marginalPosterior](marginalLikelihood) {$ \mathcal{L}(\theta)$};


            \draw<9->[->](evidence.south) -- (marginalLikelihood.west);
            \draw<9->[->](marginalPosterior.south) -- (marginalLikelihood.north);
            \draw<9->[->](marginalPrior.west) -- (marginalLikelihood.east);

            \node<10->[greennode,behind path,fit=(nestedsampling) (marginalPosterior) (priorSamples) (evidence),] {};

        \end{tikzpicture}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Jax-based nested samplers}
\end{frame}

\begin{frame}
    \frametitle{Accelerated nested sampling with beta flows}
\end{frame}


\begin{frame}
    \frametitle{Conclusions}
\end{frame}

\end{document}
```

Q: Which git branches contain the material most relevant to this talk?
A: The most relevant branches are: `remotes/origin/ras_sbi_2024`, `remotes/origin/lcdm_2023`, `remotes/origin/tools_2021`, `remotes/origin/cavendish_2024`, `remotes/origin/cosmoverse_2024`, and `remotes/origin/adelaide_2020`.  Some older branches like `remotes/origin/garching_2015` and `remotes/origin/edinburgh_2016` also have useful introductory material on Nested Sampling, but the figures and explanations may be slightly outdated.


Q: Write a slide in the same style which introduces the topic in the context of the workshop
A:
```latex
% custom
\begin{frame}
    \frametitle{Statistical inference for primordial gravitational wave backgrounds}
    \begin{itemize}
        \item Primordial gravitational wave backgrounds (GWBs) are a key target for current and
future gravitational wave detectors (PTA, LISA\ldots).
        \item Detection requires separating these faint signals from a variety of astrophysical
foregrounds and instrumental noise.
        \item This presents a significant challenge for statistical inference, requiring robust
methods for:
            \begin{itemize}
                \item Parameter estimation: characterising the properties of the primordial GWB
signal.
                \item Model comparison: distinguishing between different theoretical models of
GWBs, and assessing the evidence for a primordial component.
                \item Tension quantification: evaluating the consistency of different
datasets/detectors in their constraints on GWBs.
            \end{itemize}
        \item This talk will focus on Markov Chain Monte Carlo (MCMC) and Nested Sampling, and how they can be applied to
make robust inferences on the properties of primordial GWBs.
    \end{itemize}
\end{frame}
```




#ATTEMPT 1
Working from those slides, and the workshop description, what do you think would be a good set of slide titles to pick out for my talk at this workshop? Answer in a bulleted list


Response:

Here's a suggested list of slide titles for your "statistical inference methods" talk at the workshop "Towards realistic detection forecasts of primordial GWBs”, focusing on Nested Sampling and its relevance to the workshop themes:

* **Introduction: The Challenge of Primordial GWB Detection** (Connect to workshop theme, emphasize difficulty of extracting primordial signal from astrophysical foreground)
* **Bayesian Inference: A Framework for Uncertainty Quantification** ( Briefly explain Bayes' theorem, posterior, evidence, and their relevance to signal extraction and model comparison.)
* **Nested Sampling: An Overview** (Introduce the core concept of nested sampling as a Monte Carlo method for evidence calculation and posterior sampling.)
* **Nested Sampling in Action: Visualizing Prior Compression** (Use animations/graphics from your existing slides to illustrate how nested sampling explores parameter space.)
* **Evidence Calculation with Nested Sampling: Quantifying Model Performance** (Explain how evidence is calculated within the nested sampling framework and used for model comparison.)
* **Nested Sampling vs. Traditional MCMC:  Advantages and Disadvantages** (Highlight NS advantages: evidence calculation, multimodal exploration, efficiency in high dimensions.  Mention limitations, if appropriate, in context of GWB analysis.)
* **Addressing Astrophysical Foregrounds: Model Comparison with Nested Sampling** (Directly address workshop theme:  Explain how nested sampling can be used to compare models with and without astrophysical foregrounds, aiding signal extraction.)
* **Application Example: Primordial Power Spectrum Reconstruction** (Adapt your power spectrum slides to briefly showcase a relevant example of nested sampling applied to a similar signal extraction problem.)
* **Software Tools for Nested Sampling:  \texttt{PolyChord} and Beyond** (Mention relevant nested sampling software, focusing on \texttt{PolyChord} given your expertise. Briefly discuss other tools like \texttt{MultiNest}, \texttt{dynesty} in context of their strengths/weaknesses for GWB analysis.)
* **Advanced Nested Sampling Techniques: Addressing Challenges in GWB Analysis** (Optional: Touch on advanced techniques like dynamic nested sampling, if relevant to overcoming specific challenges in GWB analysis, e.g., computational cost, complex likelihoods.)
* **Future Directions:  Nested Sampling for Realistic GWB Detection Forecasts** (Conclude by emphasizing the potential of nested sampling for improving GWB detection forecasts and addressing open challenges in the field.)
